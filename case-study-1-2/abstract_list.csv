author,shortname,id,url,abstract
Elfar Adalsteinsson,Adalsteinsson_Elfar,arXiv:1608.03907,https://arxiv.org/abs/1608.03907,"Abstract:  We present a robust method to correct for motion and deformations for
in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI
requires robust alignment across time in the presence of substantial and
unpredictable motion. We make a Markov assumption on the nature of deformations
to take advantage of the temporal structure in the image data. Forward message
passing in the corresponding hidden Markov model (HMM) yields an estimation
algorithm that only has to account for relatively small motion between
consecutive frames. We demonstrate the utility of the temporal model by showing
that its use improves the accuracy of the segmentation propagation through
temporal registration. Our results suggest that the proposed model captures
accurately the temporal dynamics of deformations in in-utero MRI time series."
Elfar Adalsteinsson,Adalsteinsson_Elfar,arXiv:0907.2083,https://arxiv.org/abs/0907.2083,"Abstract:  A linear inverse problem is proposed that requires the determination of
multiple unknown signal vectors. Each unknown vector passes through a different
system matrix and the results are added to yield a single observation vector.
Given the matrices and lone observation, the objective is to find a
simultaneously sparse set of unknown vectors that solves the system. We will
refer to this as the multiple-system single-output (MSSO) simultaneous sparsity
problem. This manuscript contrasts the MSSO problem with other simultaneous
sparsity problems and conducts a thorough initial exploration of algorithms
with which to solve it. Seven algorithms are formulated that approximately
solve this NP-Hard problem. Three greedy techniques are developed (matching
pursuit, orthogonal matching pursuit, and least squares matching pursuit) along
with four methods based on a convex relaxation (iteratively reweighted least
squares, two forms of iterative shrinkage, and formulation as a second-order
cone program). The algorithms are evaluated across three experiments: the first
and second involve sparsity profile recovery in noiseless and noisy scenarios,
respectively, while the third deals with magnetic resonance imaging
radio-frequency excitation pulse design."
Akintunde Akinwande,Akinwande_Akintunde,arXiv:1812.07380,https://arxiv.org/abs/1812.07380,"Abstract:  We present a Machine Learning-based method for tomographic reconstruction of
dense layered objects, with range of projection angles limited to $\pm
$10$^\circ$. Whereas previous approaches to phase tomography generally require
two steps, first to retrieve phase projections from intensity projections and
then perform tomographic reconstruction on the retrieved phase projections, in
our work a physics-informed pre-processor followed by a Deep Neural Network
(DNN) conduct the three-dimensional reconstruction directly from the intensity
projections. We demonstrate this single-step method experimentally in the
visible optical domain on a scaled up integrated circuit phantom. We show that
even under conditions of highly attenuated photon fluxes a DNN trained only on
synthetic data can be used to successfully reconstruct physical samples
disjoint from the synthetic training set. Thus, the need of producing a large
number of physical examples for training is ameliorated. The method is
generally applicable to tomography with electromagnetic or other types of
radiation at all bands."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1810.01963,https://arxiv.org/abs/1810.01963,"Abstract:  Efficiently scheduling data processing jobs on distributed compute clusters
requires complex algorithms. Current systems, however, use simple generalized
heuristics and ignore workload structure, since developing and tuning a bespoke
heuristic for each workload is infeasible. In this paper, we show that modern
machine learning techniques can generate highly-efficient policies
automatically.
Decima uses reinforcement learning (RL) and neural networks to learn
workload-specific scheduling algorithms without any human instruction beyond
specifying a high-level objective such as minimizing average job completion
time. Off-the-shelf RL techniques, however, cannot handle the complexity and
scale of the scheduling problem. To build Decima, we had to develop new
representations for jobs' dependency graphs, design scalable RL models, and
invent new RL training methods for continuous job arrivals.
Our prototype integration with Spark on a 25-node cluster shows that Decima
outperforms several heuristics, including hand-tuned ones, by at least 21%.
Further experiments with an industrial production workload trace demonstrate
that Decima delivers up to a 17% reduction in average job completion time and
scales to large clusters."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1809.05088,https://arxiv.org/abs/1809.05088,"Abstract:  With the growing usage of Bitcoin and other cryptocurrencies, many
scalability challenges have emerged. A promising scaling solution, exemplified
by the Lightning Network, uses a network of bidirectional payment channels that
allows fast transactions between two parties. However, routing payments on
these networks efficiently is non-trivial, since payments require finding paths
with sufficient funds, and channels can become unidirectional over time
blocking further transactions through them. Today's payment channel networks
exacerbate these problems by attempting to deliver all payments atomically. In
this paper, we present the Spider network, a new packet-switched architecture
for payment channel networks. Spider splits payments into transaction units and
transmits them over time across different paths. Spider uses congestion
control, payment scheduling, and imbalance-aware routing to optimize delivery
of payments. Our results show that Spider improves the volume and number of
successful payments on the network by 10-45% and 5-40% respectively compared to
state-of-the-art approaches."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)
content delivery to serve Internet video, showing that it can reduce costs to
content providers. Yet, such methods have not become widespread except for a
few niche instances. An important challenge is incentivization: what tangible
benefits does P2P content delivery offer users who bring resources to the
table? In this paper, we ask whether monetary incentives can help attract peers
in P2P content delivery systems. We commissioned a professional survey of
people around theUnited States to answer several relevant questions. We found
that 51% of the 876 respondents--substantially larger than our
expectations--answered ""yes"" to whether they would participate for suitable
financial incentives. Encouraged by the results of the survey, we propose
Gringotts, a system to structure incentives and securely incorporate P2P
delivery into content delivery systems. Gringotts provides a novel Proof of
Delivery mechanism that allows content providers to verify correct delivery of
their files, and shows how to use cryptocurrency to pay peers while guarding
against liars and Sybil attacks."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1807.02264,https://arxiv.org/abs/1807.02264,"Abstract:  We consider reinforcement learning in input-driven environments, where an
exogenous, stochastic input process affects the dynamics of the system. Input
processes arise in many applications, including queuing systems, robotics
control with disturbances, and object tracking. Since the state dynamics and
rewards depend on the input process, the state alone provides limited
information for the expected future returns. Therefore, policy gradient methods
with standard state-dependent baselines suffer high variance during training.
We derive a bias-free, input-dependent baseline to reduce this variance, and
analytically show its benefits over state-dependent baselines. We then propose
a meta-learning approach to overcome the complexity of learning a baseline that
depends on a long sequence of inputs. Our experimental results show that across
environments from queuing systems, computer networks, and MuJoCo robotic
locomotion, input-dependent baselines consistently improve training stability
and result in better eventual policies."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1803.09615,https://arxiv.org/abs/1803.09615,"Abstract:  Homa is a new transport protocol for datacenter networks. It provides
exceptionally low latency, especially for workloads with a high volume of very
short messages, and it also supports large messages and high network
utilization. Homa uses in-network priority queues to ensure low latency for
short messages; priority allocation is managed dynamically by each receiver and
integrated with a receiver-driven flow control mechanism. Homa also uses
controlled overcommitment of receiver downlinks to ensure efficient bandwidth
utilization at high load. Our implementation of Homa delivers 99th percentile
round-trip times less than 15{\mu}s for short messages on a 10 Gbps network
running at 80% load. These latencies are almost 100x lower than the best
published measurements of an implementation. In simulations, Homa's latency is
roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for
almost all message sizes and workloads. Homa can also sustain higher network
loads than pFabric, pHost, or PIAS."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"Abstract:  This paper develops a technique to detect whether the cross traffic competing
with a flow is elastic or not, and shows how to use the elasticity detector to
improve congestion control. If the cross traffic is elastic, i.e., made up of
flows like Cubic or NewReno that increase their rate when they perceive
available bandwidth, then one should use a scheme that competes well with such
traffic. Such a scheme will not be able to control delays because the cross
traffic will not cooperate to maintain low delays. If, however, cross traffic
is inelastic, then one can use a suitable delay-controlled algorithm. Our
elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates
elasticity by computing the frequency response (FFT) of the cross traffic
estimate; we have measured its accuracy to be over 90%. We present the design
and evaluation of Nimbus, a congestion control protocol that uses the
elasticity detector to switch between delay-control and TCP-competitive modes.
Our results on emulated and real-world paths show that Nimbus achieves
throughput comparable to or better than Cubic always, but with delays that are
much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to
Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which
also switches between a delay-controlling and a TCP-competitive mode, Nimbus is
more robust at correctly detecting the nature of cross traffic, and unlike
Copa, it is usable by a variety of delay-based and TCP-competitive methods."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.04948,https://arxiv.org/abs/1802.04948,"Abstract:  Neural networks have been shown to be an effective tool for learning
algorithms over graph-structured data. However, graph representation
techniques---that convert graphs to real-valued vectors for use with neural
networks---are still in their infancy. Recent works have proposed several
approaches (e.g., graph convolutional networks), but these methods have
difficulty scaling and generalizing to graphs with different sizes and shapes.
We present Graph2Seq, a new technique that represents vertices of graphs as
infinite time-series. By not limiting the representation to a fixed dimension,
Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq
is also reversible, allowing full recovery of the graph structure from the
sequences. By analyzing a formal computational model for graph representation,
we show that an unbounded sequence is necessary for scalability. Our
experimental results with Graph2Seq show strong generalization and new
state-of-the-art performance on a variety of graph combinatorial optimization
problems."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"Abstract:  Mapping road networks is currently both expensive and labor-intensive.
High-resolution aerial imagery provides a promising avenue to automatically
infer a road network. Prior work uses convolutional neural networks (CNNs) to
detect which pixels belong to a road (segmentation), and then uses complex
post-processing heuristics to infer graph connectivity. We show that these
segmentation methods have high error rates because noisy CNN outputs are
difficult to correct. We propose RoadTracer, a new method to automatically
construct accurate road network maps from aerial images. RoadTracer uses an
iterative search process guided by a CNN-based decision function to derive the
road network graph directly from the output of the CNN. We compare our approach
with a segmentation method on fifteen cities, and find that at a 5% error rate,
RoadTracer correctly captures 45% more junctions across these cities."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1801.04519,https://arxiv.org/abs/1801.04519,"Abstract:  We define the Fitzpatrick function of a $\sigma$-monotone operator in a way
similar to the original definition given by Fitzpatrick. We show that some
well-known properties of Fitzpatrick function remain valid for the larger class
of premonotone operators. Also, we find some conditions under which the
Fitzpatrick function of a $\sigma$-monotone operator is proper, and give some
results in Hilbert spaces."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1702.02588,https://arxiv.org/abs/1702.02588,"Abstract:  As its price per bit drops, SSD is increasingly becoming the default storage
medium for cloud application databases. However, it has not become the
preferred storage medium for key-value caches, even though SSD offers more than
10x lower price per bit and sufficient performance compared to DRAM. This is
because key-value caches need to frequently insert, update and evict small
objects. This causes excessive writes and erasures on flash storage, since
flash only supports writes and erasures of large chunks of data. These
excessive writes and erasures significantly shorten the lifetime of flash,
rendering it impractical to use for key-value caches. We present Flashield, a
hybrid key-value cache that uses DRAM as a ""filter"" to minimize writes to SSD.
Flashield performs light-weight machine learning profiling to predict which
objects are likely to be read frequently before getting updated; these objects,
which are prime candidates to be stored on SSD, are written to SSD in large
chunks sequentially. In order to efficiently utilize the cache's available
memory, we design a novel in-memory index for the variable-sized objects stored
on flash that requires only 4 bytes per object in DRAM. We describe Flashield's
design and implementation and, we evaluate it on a real-world cache trace.
Compared to state-of-the-art systems that suffer a write amplification of 2.5x
or more, Flashield maintains a median write amplification of 0.5x without any
loss of hit rate or throughput."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"Abstract:  Switches today provide a small set of scheduling algorithms. While we can
tweak scheduling parameters, we cannot modify algorithmic logic, or add a
completely new algorithm, after the switch has been designed. This paper
presents a design for a programmable packet scheduler, which allows scheduling
algorithms---potentially algorithms that are unknown today---to be programmed
into a switch without requiring hardware redesign.
Our design builds on the observation that scheduling algorithms make two
decisions: in what order to schedule packets and when to schedule them.
Further, in many scheduling algorithms these decisions can be made when packets
are enqueued. We leverage this observation to build a programmable scheduler
using a single abstraction: the push-in first-out queue (PIFO), a priority
queue that maintains the scheduling order and time for such algorithms.
We show that a programmable scheduler using PIFOs lets us program a wide
variety of scheduling algorithms. We present a detailed hardware design for
this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area
overhead on a 16-nm standard-cell library. Our design lets us program many
sophisticated algorithms, such as a 5-level hierarchical scheduler with
programmable scheduling algorithms at each level."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"Abstract:  Many algorithms for congestion control, scheduling, network measurement,
active queue management, security, and load balancing require custom processing
of packets as they traverse the data plane of a network switch. To run at line
rate, these data-plane algorithms must be in hardware. With today's switch
hardware, algorithms cannot be changed, nor new algorithms installed, after a
switch has been built.
This paper shows how to program data-plane algorithms in a high-level
language and compile those programs into low-level microcode that can run on
emerging programmable line-rate switching chipsets. The key challenge is that
these algorithms create and modify algorithmic state. The key idea to achieve
line-rate programmability for stateful algorithms is the notion of a packet
transaction : a sequential code block that is atomic and isolated from other
such code blocks. We have developed this idea in Domino, a C-like imperative
language to express data-plane algorithms. We show with many examples that
Domino provides a convenient and natural way to express sophisticated
data-plane algorithms, and show that these algorithms can be run at line rate
with modest estimated die-area overhead."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1512.01271,https://arxiv.org/abs/1512.01271,"Abstract:  Hybrid switching - in which a high bandwidth circuit switch (optical or
wireless) is used in conjunction with a low bandwidth packet switch - is a
promising alternative to interconnect servers in today's large scale
data-centers. Circuit switches offer a very high link rate, but incur a
non-trivial reconfiguration delay which makes their scheduling challenging. In
this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling
algorithm that trades-off configuration costs with the benefits of
reconfiguration that match the traffic demands. The algorithm has strong
connections to submodular optimization, has performance at least half that of
the optimal schedule and strictly outperforms state of the art in a variety of
traffic demand settings. These ideas naturally generalize: we see that indirect
routing leads to exponential connectivity; this is another phenomenon of the
power of multi hop routing, distinct from the well-known load balancing
effects."
Mohammad Alizadeh,Alizadeh_Mohammad,arXiv:1405.7143,https://arxiv.org/abs/1405.7143,"Abstract:  This paper presents a practical approach to rapidly introduce new dataplane
functionality into networks: End-hosts embed tiny programs into packets to
actively query and manipulate a network's internal state. We show how this
""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility
into network behavior, enabling them to work with the network to achieve a
common goal. Our design leverages what each component does best: (a) switches
forward and execute tiny packet programs (at most 5 instructions) at line rate,
and (b) end-hosts perform arbitrary computation on network state, which are
easy to evolve. Using a hardware prototype on a NetFPGA, we show our design is
feasible, at a reasonable cost. By implementing three different research
proposals, we show that TPPs are also useful. And finally, we present an
architecture in which they can be made secure."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1902.02816,https://arxiv.org/abs/1902.02816,"Abstract:  Modern microprocessors are equipped with Single Instruction Multiple Data
(SIMD) or vector instructions which expose data level parallelism at a fine
granularity. Programmers exploit this parallelism by using low-level vector
intrinsics in their code. However, once programs are written using vector
intrinsics of a specific instruction set, the code becomes non-portable. Modern
compilers are unable to analyze and retarget the code to newer vector
instruction sets. Hence, programmers have to manually rewrite the same code
using vector intrinsics of a newer generation to exploit higher data widths and
capabilities of new instruction sets. This process is tedious, error-prone and
requires maintaining multiple code bases. We propose Revec, a compiler
optimization pass which revectorizes already vectorized code, by retargeting it
to use vector instructions of newer generations. The transformation is
transparent, happening at the compiler intermediate representation level, and
enables performance portability of hand-vectorized code.
Revec can achieve performance improvements in real-world performance critical
kernels. In particular, Revec achieves geometric mean speedups of 1.160$\times$
and 1.430$\times$ on fast integer unpacking kernels, and speedups of
1.145$\times$ and 1.195$\times$ on hand-vectorized x265 media codec kernels
when retargeting their SSE-series implementations to use AVX2 and AVX-512
vector instructions respectively. We also extensively test Revec's impact on
216 intrinsic-rich implementations of image processing and stencil kernels
relative to hand-retargeting."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"Abstract:  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1807.01624,https://arxiv.org/abs/1807.01624,"Abstract:  Modern out-of-order processors have increased capacity to exploit instruction
level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide
superscalar pipelines and vector execution units, as well as deep buffers for
in-flight memory requests. These resources, however, often exhibit poor
utilization rates on workloads with large working sets, e.g., in-memory
databases, key-value stores, and graph analytics, as compilers and hardware
struggle to expose ILP and MLP from the instruction stream automatically.
In this paper, we introduce the IMLP (Instruction and Memory Level
Parallelism) task programming model. IMLP tasks execute as coroutines that
yield execution at annotated long-latency operations, e.g., memory accesses,
divisions, or unpredictable branches. IMLP tasks are interleaved on a single
thread, and integrate well with thread parallelism and vectorization. Our DSL
embedded in C++, Cimple, allows exploration of task scheduling and
transformations, such as buffering, vectorization, pipelining, and prefetching.
We demonstrate state-of-the-art performance on core algorithms used in
in-memory databases that operate on arrays, hash tables, trees, and skip lists.
Cimple applications reach 2.5x throughput gains over hardware multithreading on
a multi-core, and 6.4x single thread speedup."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1805.00923,https://arxiv.org/abs/1805.00923,"Abstract:  The performance bottlenecks of graph applications depend not only on the
algorithm and the underlying hardware, but also on the size and structure of
the input graph. Programmers must try different combinations of a large set of
techniques to develop the best implementation for a specific algorithm and type
of graph. Existing graph frameworks lack flexibility, supporting only a limited
set of optimizations.
This paper introduces GraphIt, a new DSL for graph computations that
generates fast implementations for algorithms with different performance
characteristics running on graphs with different sizes and structures. GraphIt
separates what is computed (algorithm) from how it is computed (schedule).
Programmers specify the algorithm using an algorithm language, and performance
optimizations are specified using a scheduling language. The algorithm language
simplifies expressing the algorithms. We formulate graph optimizations,
including edge traversal direction, data layout, parallelization, cache, NUMA,
and kernel fusion optimizations, as tradeoffs among locality, parallelism, and
work-efficiency. The scheduling language enables programmers to easily search
through this complicated tradeoff space by composing together optimizations. We
also built an autotuner to automatically find high-performance schedules. The
compiler uses a new scheduling representation, the graph iteration space, to
model, compose, and ensure the validity of the large number of optimizations.
GraphIt outperforms the next fastest of six state-of-the-art shared-memory
frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24
out of 32 experiments by up to 4.8$\times$, and is never more than 43% slower
than the fastest framework on the other experiments. GraphIt also reduces the
lines of code by up to an order of magnitude compared to the next fastest
framework."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.10694,https://arxiv.org/abs/1804.10694,"Abstract:  This paper introduces Tiramisu, a polyhedral framework designed to generate
high performance code for multiple platforms including multicores, GPUs, and
distributed machines. Tiramisu introduces a scheduling language with novel
extensions to explicitly manage the complexities that arise when targeting
these systems. The framework is designed for the areas of image processing,
stencils, linear algebra and deep learning. Tiramisu has two main features: it
relies on a flexible representation based on the polyhedral model and it has a
rich scheduling language allowing fine-grained control of optimizations.
Tiramisu uses a four-level intermediate representation that allows full
separation between the algorithms, loop transformations, data layouts, and
communication. This separation simplifies targeting multiple hardware
architectures with the same algorithm. We evaluate Tiramisu by writing a set of
image processing, deep learning, and linear algebra benchmarks and compare them
with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu
matches or outperforms existing compilers and libraries on different hardware
architectures, including multicore CPUs, GPUs, and distributed machines."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.10112,https://arxiv.org/abs/1804.10112,"Abstract:  This paper shows how to build a sparse tensor algebra compiler that is
agnostic to tensor formats (data layouts). We develop an interface that
describes formats in terms of their capabilities and properties, and show how
to build a modular code generator where new formats can be added as plugins. We
then describe six implementations of the interface that compose to form the
dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants
thereof. With these implementations at hand, our code generator can generate
code to compute any tensor algebra expression on any combination of the
aforementioned formats.
To demonstrate our technique, we have implemented it in the taco tensor
algebra compiler. Our modular code generator design makes it simple to add
support for new tensor formats, and the performance of the generated code is
competitive with hand-optimized implementations. Furthermore, by extending taco
to support a wider range of formats specialized for different application and
data characteristics, we can improve end-user application performance. For
example, if input data is provided in the COO format, our technique allows
computing a single matrix-vector multiplication directly with the data in COO,
which is up to 3.6$\times$ faster than by first converting the data to CSR."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1804.08733,https://arxiv.org/abs/1804.08733,"Abstract:  Modern microprocessors are equipped with single instruction multiple data
(SIMD) or vector instruction sets which allow compilers to exploit superword
level parallelism (SLP), a type of fine-grained parallelism. Current SLP
auto-vectorization techniques use heuristics to discover vectorization
opportunities in high-level language code. These heuristics are fragile, local
and typically only present one vectorization strategy that is either accepted
or rejected by a cost model. We present goSLP, a novel SLP auto-vectorization
framework which solves the statement packing problem in a pairwise optimal
manner. Using an integer linear programming (ILP) solver, goSLP searches the
entire space of statement packing opportunities for a whole function at a time,
while limiting total compilation time to a few minutes. Furthermore, goSLP
optimally solves the vector permutation selection problem using dynamic
programming. We implemented goSLP in the LLVM compiler infrastructure,
achieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp
and 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1803.00419,https://arxiv.org/abs/1803.00419,"Abstract:  High-performance DSL developers work hard to take advantage of modern
hardware. The DSL compilers have to build their own complex middle-ends before
they can target a common back-end such as LLVM, which only handles single
instruction streams with SIMD instructions. We introduce Tiramisu, a common
middle-end that can generate efficient code for modern processors and
accelerators such as multicores, GPUs, FPGAs and distributed clusters. Tiramisu
introduces a novel three-level IR that separates the algorithm, how that
algorithm is executed, and where intermediate data are stored. This separation
simplifies optimization and makes targeting multiple hardware architectures
from the same algorithm easier. As a result, DSL compilers can be made
considerably less complex with no loss of performance while immediately
targeting multiple hardware or hardware combinations such as distributed nodes
with both CPUs and GPUs. We evaluated Tiramisu by creating a new middle-end for
the Halide and Julia compilers. We show that Tiramisu extends Halide and Julia
with many new capabilities including the ability to: express new algorithms
(such as recurrent filters and non-rectangular iteration spaces), perform new
complex loop nest transformations (such as wavefront parallelization, loop
shifting and loop fusion) and generate efficient code for more architectures
(such as combinations of distributed clusters, multicores, GPUs and FPGAs).
Finally, we demonstrate that Tiramisu can generate very efficient code that
matches the highly optimized Intel MKL gemm (generalized matrix multiplication)
implementation, we also show speedups reaching 4X in Halide and 16X in Julia
due to optimizations enabled by Tiramisu."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1802.10574,https://arxiv.org/abs/1802.10574,"Abstract:  This paper shows how to optimize sparse tensor algebraic expressions by
introducing temporary tensors, called workspaces, into the resulting loop
nests. We develop a new intermediate language for tensor operations called
concrete index notation that extends tensor index notation. Concrete index
notation expresses when and where sub-computations occur and what tensor they
are stored into. We then describe the workspace optimization in this language,
and how to compile it to sparse code by building on prior work in the
literature.
We demonstrate the importance of the optimization on several important sparse
tensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse
tensor addition (SpAdd), and the matricized tensor times Khatri-Rao product
(MTTKRP) used to factorize tensors. Our results show improvements over prior
work on tensor algebra compilation and brings the performance of these kernels
on par with state-of-the-art hand-optimized implementations. For example, SpMM
was not supported by prior tensor algebra compilers, the performance of MTTKRP
on the nell-2 data set improves by 35%, and MTTKRP can for the first time have
sparse results."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1709.06416,https://arxiv.org/abs/1709.06416,"Abstract:  Data analytics applications combine multiple functions from different
libraries and frameworks. Even when each function is optimized in isolation,
the performance of the combined application can be an order of magnitude below
hardware limits due to extensive data movement across these functions. To
address this problem, we propose Weld, a new interface between data-intensive
libraries that can optimize across disjoint libraries and functions. Weld
exposes a lazily-evaluated API where diverse functions can submit their
computations in a simple but general intermediate representation that captures
their data-parallel structure. It then optimizes data movement across these
functions and emits efficient code for diverse hardware. Weld can be integrated
into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without
changing their user-facing APIs. We demonstrate that Weld can speed up
applications using these frameworks by up to 29x."
Saman Amarasinghe,Amarasinghe_Saman,arXiv:1608.01362,https://arxiv.org/abs/1608.01362,"Abstract:  Modern hardware systems are heavily underutilized when running large-scale
graph applications. While many in-memory graph frameworks have made substantial
progress in optimizing these applications, we show that it is still possible to
achieve up to 4 $\times$ speedups over the fastest frameworks by greatly
improving cache utilization. Previous systems have applied out-of-core
processing techniques from the memory/disk boundary to the cache/DRAM boundary.
However, we find that blindly applying such techniques is ineffective because
of the much smaller performance gap between DRAM and cache. We present two
techniques that take advantage of the cache with minimal or no instruction
overhead. The first, frequency based clustering, groups together frequently
accessed vertices to improve the utilization of each cache line with no runtime
overhead. The second, CSR segmenting, partitions the graph to restrict all
random accesses to the cache, makes all DRAM access sequential, and merges
partition results using a very low overhead cache-aware merge. Both techniques
can be easily implemented on top of optimized graph frameworks. Our techniques
combined give speedups of up to 4 $\times$ for PageRank, Label Propagation and
Collaborative Filtering, and 2 $\times$ for Betweenness Centrality over the
best published results"
Dimitri Antoniadis,Antoniadis_Dimitri,arXiv:1807.00272,https://arxiv.org/abs/1807.00272,"Abstract:  We present a compact model for Tunnel Field Effect Transistors (TFET), that
captures sev- eral non-idealities such as the Trap Assisted Tunneling (TAT)
originating from interface traps (Dit), along with Verilog-A implementation. We
show that the TAT, together with band edge non-abruptness known as the Urbach
tail, sets the lower limit of the sub-threshold swing and the minimum
achievable current at a given temperature. Presence of charged trap states also
contributes to reduced gate efficiency. We show that we can decouple the
contribution of each of these processes and extract the intrinsic sub-threshold
swing from a given experimental data. We derive closed form expressions of
channel potential, electric field and effective tunnel energy window to
accurately capture the essential device physics of TFETs. We test the model
against recently published exper- imental data, and simulate simple TFET
circuits using the Verilog-A model. The compact model provides a framework for
TFET technology projections with improved device metrics such as better
electrostatic design, reduced TAT, material with better transport properties
etc."
Dimitri Antoniadis,Antoniadis_Dimitri,arXiv:1603.06654,https://arxiv.org/abs/1603.06654,"Abstract:  We provide a detailed study of the interface Trap Assisted Tunneling (TAT)
mechanism in tunnel field effect transistors to show how it contributes a major
leakage current path before the Band To Band Tunneling (BTBT) is initiated.
With a modified Shockley-Read-Hall formalism, we show that at room temperature,
the phonon assisted TAT current always dominates and obscures the steep turn ON
of the BTBT current for common densities of traps. Our results are applicable
to top gate, double gate and gate all around structures where the traps are
positioned between the source-channel tunneling region. Since the TAT has
strong dependence on electric field, any effort to increase the BTBT current by
enhancing local electric field also increases the leakage current. Unless the
BTBT current can be increased separately, calculations show that the trap
density Dit has to be decreased by 40-100 times compared with the state of the
art in order for the steep turn ON (for III-V materials) to be clearly
observable at room temperature. We find that the combination of the intrinsic
sharpness of the band edges (Urbach tail) and the surface trap density
determines the subthreshold swing."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1808.00826,https://arxiv.org/abs/1808.00826,"Abstract:  Prior research has proposed technical solutions to use peer-to-peer (P2P)
content delivery to serve Internet video, showing that it can reduce costs to
content providers. Yet, such methods have not become widespread except for a
few niche instances. An important challenge is incentivization: what tangible
benefits does P2P content delivery offer users who bring resources to the
table? In this paper, we ask whether monetary incentives can help attract peers
in P2P content delivery systems. We commissioned a professional survey of
people around theUnited States to answer several relevant questions. We found
that 51% of the 876 respondents--substantially larger than our
expectations--answered ""yes"" to whether they would participate for suitable
financial incentives. Encouraged by the results of the survey, we propose
Gringotts, a system to structure incentives and securely incorporate P2P
delivery into content delivery systems. Gringotts provides a novel Proof of
Delivery mechanism that allows content providers to verify correct delivery of
their files, and shows how to use cryptocurrency to pay peers while guarding
against liars and Sybil attacks."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1802.08730,https://arxiv.org/abs/1802.08730,"Abstract:  This paper develops a technique to detect whether the cross traffic competing
with a flow is elastic or not, and shows how to use the elasticity detector to
improve congestion control. If the cross traffic is elastic, i.e., made up of
flows like Cubic or NewReno that increase their rate when they perceive
available bandwidth, then one should use a scheme that competes well with such
traffic. Such a scheme will not be able to control delays because the cross
traffic will not cooperate to maintain low delays. If, however, cross traffic
is inelastic, then one can use a suitable delay-controlled algorithm. Our
elasticity detector uses an asymmetric sinusoidal pulse pattern and estimates
elasticity by computing the frequency response (FFT) of the cross traffic
estimate; we have measured its accuracy to be over 90%. We present the design
and evaluation of Nimbus, a congestion control protocol that uses the
elasticity detector to switch between delay-control and TCP-competitive modes.
Our results on emulated and real-world paths show that Nimbus achieves
throughput comparable to or better than Cubic always, but with delays that are
much lower when cross traffic is inelastic. Unlike BBR, Nimbus is fair to
Cubic, and has significantly lower delay by 40-50 ms. Compared to Copa, which
also switches between a delay-controlling and a TCP-competitive mode, Nimbus is
more robust at correctly detecting the nature of cross traffic, and unlike
Copa, it is usable by a variety of delay-based and TCP-competitive methods."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1802.03680,https://arxiv.org/abs/1802.03680,"Abstract:  Mapping road networks is currently both expensive and labor-intensive.
High-resolution aerial imagery provides a promising avenue to automatically
infer a road network. Prior work uses convolutional neural networks (CNNs) to
detect which pixels belong to a road (segmentation), and then uses complex
post-processing heuristics to infer graph connectivity. We show that these
segmentation methods have high error rates because noisy CNN outputs are
difficult to correct. We propose RoadTracer, a new method to automatically
construct accurate road network maps from aerial images. RoadTracer uses an
iterative search process guided by a CNN-based decision function to derive the
road network graph directly from the output of the CNN. We compare our approach
with a segmentation method on fifteen cities, and find that at a 5% error rate,
RoadTracer correctly captures 45% more junctions across these cities."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1602.06045,https://arxiv.org/abs/1602.06045,"Abstract:  Switches today provide a small set of scheduling algorithms. While we can
tweak scheduling parameters, we cannot modify algorithmic logic, or add a
completely new algorithm, after the switch has been designed. This paper
presents a design for a programmable packet scheduler, which allows scheduling
algorithms---potentially algorithms that are unknown today---to be programmed
into a switch without requiring hardware redesign.
Our design builds on the observation that scheduling algorithms make two
decisions: in what order to schedule packets and when to schedule them.
Further, in many scheduling algorithms these decisions can be made when packets
are enqueued. We leverage this observation to build a programmable scheduler
using a single abstraction: the push-in first-out queue (PIFO), a priority
queue that maintains the scheduling order and time for such algorithms.
We show that a programmable scheduler using PIFOs lets us program a wide
variety of scheduling algorithms. We present a detailed hardware design for
this scheduler for a 64-port 10 Gbit/s shared-memory switch with <4% chip area
overhead on a 16-nm standard-cell library. Our design lets us program many
sophisticated algorithms, such as a 5-level hierarchical scheduler with
programmable scheduling algorithms at each level."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1512.05023,https://arxiv.org/abs/1512.05023,"Abstract:  Many algorithms for congestion control, scheduling, network measurement,
active queue management, security, and load balancing require custom processing
of packets as they traverse the data plane of a network switch. To run at line
rate, these data-plane algorithms must be in hardware. With today's switch
hardware, algorithms cannot be changed, nor new algorithms installed, after a
switch has been built.
This paper shows how to program data-plane algorithms in a high-level
language and compile those programs into low-level microcode that can run on
emerging programmable line-rate switching chipsets. The key challenge is that
these algorithms create and modify algorithmic state. The key idea to achieve
line-rate programmability for stateful algorithms is the notion of a packet
transaction : a sequential code block that is atomic and isolated from other
such code blocks. We have developed this idea in Domino, a C-like imperative
language to express data-plane algorithms. We show with many examples that
Domino provides a convenient and natural way to express sophisticated
data-plane algorithms, and show that these algorithms can be run at line rate
with modest estimated die-area overhead."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:1206.0418,https://arxiv.org/abs/1206.0418,"Abstract:  This paper presents an analysis of spinal codes, a class of rateless codes
proposed recently. We prove that spinal codes achieve Shannon capacity for the
binary symmetric channel (BSC) and the additive white Gaussian noise (AWGN)
channel with an efficient polynomial-time encoder and decoder. They are the
first rateless codes with proofs of these properties for BSC and AWGN. The key
idea in the spinal code is the sequential application of a hash function over
the message bits. The sequential structure of the code turns out to be crucial
for efficient decoding. Moreover, counter to the wisdom of having an expander
structure in good codes, we show that the spinal code, despite its sequential
structure, achieves capacity. The pseudo-randomness provided by a hash function
suffices for this purpose. Our proof introduces a variant of Gallager's result
characterizing the error exponent of random codes for any memoryless channel.
We present a novel application of these error-exponent results within the
framework of an efficient sequential code. The application of a hash function
over the message bits provides a methodical and effective way to de-randomize
Shannon's random codebook construction."
Hari Balakrishnan,Balakrishnan_Hari,arXiv:cs/0104012,https://arxiv.org/abs/cs/0104012,"Abstract:  This paper describes the implementation and evaluation of an operating system
module, the Congestion Manager (CM), which provides integrated network flow
management and exports a convenient programming interface that allows
applications to be notified of, and adapt to, changing network conditions. We
describe the API by which applications interface with the CM, and the
architectural considerations that factored into the design. To evaluate the
architecture and API, we describe our implementations of TCP; a streaming
layered audio/video application; and an interactive audio application using the
CM, and show that they achieve adaptive behavior without incurring much
end-system overhead. All flows including TCP benefit from the sharing of
congestion information, and applications are able to incorporate new
functionality such as congestion control and adaptive behavior."
Regina Barzilay,Barzilay_Regina,arXiv:1812.01070,https://arxiv.org/abs/1812.01070,"Abstract:  We view molecular optimization as a graph-to-graph translation problem. The
goal is to learn to map from one molecular graph to another with better
properties based on an available corpus of paired molecules. Since molecules
can be optimized in different ways, there are multiple viable translations for
each input graph. A key challenge is therefore to model diverse translation
outputs. Our primary contributions include a junction tree encoder-decoder for
learning diverse graph translations along with a novel adversarial training
method for aligning distributions of molecules. Diverse output distributions in
our model are explicitly realized by low-dimensional latent vectors that
modulate the translation process. We evaluate our model on multiple molecular
optimization tasks and show that our model outperforms previous
state-of-the-art baselines."
Regina Barzilay,Barzilay_Regina,arXiv:1810.13083,https://arxiv.org/abs/1810.13083,"Abstract:  Most modern Information Extraction (IE) systems are implemented as sequential
taggers and focus on modelling local dependencies. Non-local and non-sequential
context is, however, a valuable source of information to improve predictions.
In this paper, we introduce GraphIE, a framework that operates over a graph
representing both local and non-local dependencies between textual units (i.e.
words or sentences). The algorithm propagates information between connected
nodes through graph convolutions and exploits the richer representation to
improve word level predictions. The framework is evaluated on three different
tasks, namely social media, textual and visual information extraction. Results
show that GraphIE outperforms a competitive baseline (BiLSTM+CRF) in all tasks
by a significant margin."
Regina Barzilay,Barzilay_Regina,arXiv:1809.02256,https://arxiv.org/abs/1809.02256,"Abstract:  We propose a mixture-of-experts approach for unsupervised domain adaptation
from multiple sources. The key idea is to explicitly capture the relationship
between a target example and different source domains. This relationship,
expressed by a point-to-set metric, determines how to combine predictors
trained on various domains. The metric is learned in an unsupervised fashion
using meta-training. Experimental results on sentiment analysis and
part-of-speech tagging demonstrate that our approach consistently outperforms
multiple baselines and can robustly handle negative transfer."
Regina Barzilay,Barzilay_Regina,arXiv:1808.09367,https://arxiv.org/abs/1808.09367,"Abstract:  Attention-based models are successful when trained on large amounts of data.
In this paper, we demonstrate that even in the low-resource scenario, attention
can be learned effectively. To this end, we start with discrete human-annotated
rationales and map them into continuous attention. Our central hypothesis is
that this mapping is general across domains, and thus can be transferred from
resource-rich domains to low-resource ones. Our model jointly learns a
domain-invariant representation and induces the desired mapping between
rationales and attention. Our empirical results validate this hypothesis and
show that our approach delivers significant gains over state-of-the-art
baselines, yielding over 15% average error reduction on benchmark datasets."
Regina Barzilay,Barzilay_Regina,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Regina Barzilay,Barzilay_Regina,arXiv:1802.04364,https://arxiv.org/abs/1802.04364,"Abstract:  We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin."
Regina Barzilay,Barzilay_Regina,arXiv:1709.04555,https://arxiv.org/abs/1709.04555,"Abstract:  The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts."
Regina Barzilay,Barzilay_Regina,arXiv:1708.00133,https://arxiv.org/abs/1708.00133,"Abstract:  In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. Specifically, by learning to ground the
meaning of text to the dynamics of the environment such as transitions and
rewards, an autonomous agent can effectively bootstrap policy learning on a new
domain given its description. We employ a model-based RL approach consisting of
a differentiable planning module, a model-free component and a factorized state
representation to effectively use entity descriptions. Our model outperforms
prior work on both transfer and multi-task scenarios in a variety of different
environments. For instance, we achieve up to 14% and 11.5% absolute improvement
over previously existing models in terms of average and initial rewards,
respectively."
Regina Barzilay,Barzilay_Regina,arXiv:1707.03938,https://arxiv.org/abs/1707.03938,"Abstract:  The interpretation of spatial references is highly contextual, requiring
joint inference over both language and the environment. We consider the task of
spatial reasoning in a simulated environment, where an agent can act and
receive rewards. The proposed model learns a representation of the world
steered by instruction text. This design allows for precise alignment of local
neighborhoods with corresponding verbalizations, while also handling global
references in the instructions. We train our model with reinforcement learning
using a variant of generalized value iteration. The model outperforms
state-of-the-art approaches on several metrics, yielding a 45% reduction in
goal localization error."
Regina Barzilay,Barzilay_Regina,arXiv:1705.09655,https://arxiv.org/abs/1705.09655,"Abstract:  This paper focuses on style transfer on the basis of non-parallel text. This
is an instance of a broad family of problems including machine translation,
decipherment, and sentiment modification. The key challenge is to separate the
content from other aspects such as style. We assume a shared latent content
distribution across different text corpora, and propose a method that leverages
refined alignment of latent representations to perform style transfer. The
transferred sentences from one style should match example sentences from the
other style as a population. We demonstrate the effectiveness of this
cross-alignment method on three tasks: sentiment modification, decipherment of
word substitution ciphers, and recovery of word order."
Regina Barzilay,Barzilay_Regina,arXiv:1705.09037,https://arxiv.org/abs/1705.09037,"Abstract:  The design of neural architectures for structured objects is typically guided
by experimental insights rather than a formal process. In this work, we appeal
to kernels over combinatorial structures, such as sequences and graphs, to
derive appropriate neural operations. We introduce a class of deep recurrent
neural operations and formally characterize their associated kernel spaces. Our
recurrent modules compare the input to virtual reference objects (cf. filters
in CNN) via the kernels. Similar to traditional neural operations, these
reference objects are parameterized and directly optimized in end-to-end
training. We empirically evaluate the proposed class of neural architectures on
standard applications such as language modeling and molecular graph regression,
achieving state-of-the-art results across these applications."
Regina Barzilay,Barzilay_Regina,arXiv:1702.07015,https://arxiv.org/abs/1702.07015,"Abstract:  This paper focuses on unsupervised modeling of morphological families,
collectively comprising a forest over the language vocabulary. This formulation
enables us to capture edgewise properties reflecting single-step morphological
derivations, along with global distributional properties of the entire forest.
These global properties constrain the size of the affix set and encourage
formation of tight morphological families. The resulting objective is solved
using Integer Linear Programming (ILP) paired with contrastive estimation. We
train the model by alternating between optimizing the local log-linear model
and the global ILP objective. We evaluate our system on three tasks: root
detection, clustering of morphological families and segmentation. Our
experiments demonstrate that our model yields consistent gains in all three
tasks compared with the best published results."
Regina Barzilay,Barzilay_Regina,arXiv:1701.00188,https://arxiv.org/abs/1701.00188,"Abstract:  We introduce a neural method for transfer learning between two (source and
target) classification tasks or aspects over the same domain. Rather than
training on target labels, we use a few keywords pertaining to source and
target aspects indicating sentence relevance instead of document class labels.
Documents are encoded by learning to embed and softly select relevant sentences
in an aspect-dependent manner. A shared classifier is trained on the source
encoded documents and labels, and applied to target encoded documents. We
ensure transfer through aspect-adversarial training so that encoded documents
are, as sets, aspect-invariant. Experimental results demonstrate that our
approach outperforms different baselines and model variants on two datasets,
yielding an improvement of 27% on a pathology dataset and 5% on a review
dataset."
Regina Barzilay,Barzilay_Regina,arXiv:1608.03000,https://arxiv.org/abs/1608.03000,"Abstract:  This paper explores the task of translating natural language queries into
regular expressions which embody their meaning. In contrast to prior work, the
proposed neural model does not utilize domain-specific crafting, learning to
translate directly from a parallel corpus. To fully explore the potential of
neural models, we propose a methodology for collecting a large corpus of
regular expression, natural language pairs. Our resulting model achieves a
performance gain of 19.6% over previous state-of-the-art models."
Regina Barzilay,Barzilay_Regina,arXiv:1607.02902,https://arxiv.org/abs/1607.02902,"Abstract:  We present a novel technique for automatic program correction in MOOCs,
capable of fixing both syntactic and semantic errors without manual, problem
specific correction strategies. Given an incorrect student program, it
generates candidate programs from a distribution of likely corrections, and
checks each candidate for correctness against a test suite.
The key observation is that in MOOCs many programs share similar code
fragments, and the seq2seq neural network model, used in the natural-language
processing task of machine translation, can be modified and trained to recover
these fragments.
Experiment shows our scheme can correct 29% of all incorrect submissions and
out-performs state of the art approach which requires manual, problem specific
correction strategies."
Regina Barzilay,Barzilay_Regina,arXiv:1606.04155,https://arxiv.org/abs/1606.04155,"Abstract:  Prediction without justification has limited applicability. As a remedy, we
learn to extract pieces of input text as justifications -- rationales -- that
are tailored to be short and coherent, yet sufficient for making the same
prediction. Our approach combines two modular components, generator and
encoder, which are trained to operate well together. The generator specifies a
distribution over text fragments as candidate rationales and these are passed
through the encoder for prediction. Rationales are never given during training.
Instead, the model is regularized by desiderata for rationales. We evaluate the
approach on multi-aspect sentiment analysis against manually annotated test
cases. Our approach outperforms attention-based baseline by a significant
margin. We also successfully illustrate the method on the question retrieval
task."
Regina Barzilay,Barzilay_Regina,arXiv:1603.07954,https://arxiv.org/abs/1603.07954,"Abstract:  Most successful information extraction systems operate with access to a large
collection of documents. In this work, we explore the task of acquiring and
incorporating external evidence to improve extraction accuracy in domains where
the amount of training data is scarce. This process entails issuing search
queries, extraction from new sources and reconciliation of extracted values,
which are repeated until sufficient evidence is collected. We approach the
problem using a reinforcement learning framework where our model learns to
select optimal actions based on contextual information. We employ a deep
Q-network, trained to optimize a reward function that reflects extraction
accuracy while penalizing extra effort. Our experiments on two databases -- of
shooting incidents, and food adulteration cases -- demonstrate that our system
significantly outperforms traditional extractors and a competitive
meta-classifier baseline."
Regina Barzilay,Barzilay_Regina,arXiv:1512.05726,https://arxiv.org/abs/1512.05726,"Abstract:  Question answering forums are rapidly growing in size with no effective
automated ability to refer to and reuse answers already available for previous
posted questions. In this paper, we develop a methodology for finding
semantically related questions. The task is difficult since 1) key pieces of
information are often buried in extraneous details in the question body and 2)
available annotations on similar questions are scarce and fragmented. We design
a recurrent and convolutional model (gated convolution) to effectively map
questions to their semantic representations. The models are pre-trained within
an encoder-decoder framework (from body to title) on the basis of the entire
raw corpus, and fine-tuned discriminatively from limited annotations. Our
evaluation demonstrates that our model yields substantial gains over a standard
IR baseline and various neural network architectures (including CNNs, LSTMs and
GRUs)."
Regina Barzilay,Barzilay_Regina,arXiv:1508.04112,https://arxiv.org/abs/1508.04112,"Abstract:  The success of deep learning often derives from well-chosen operational
building blocks. In this work, we revise the temporal convolution operation in
CNNs to better adapt it to text processing. Instead of concatenating word
representations, we appeal to tensor algebra and use low-rank n-gram tensors to
directly exploit interactions between words already at the convolution stage.
Moreover, we extend the n-gram convolution to non-consecutive words to
recognize patterns with intervening words. Through a combination of low-rank
tensors, and pattern weighting, we can efficiently evaluate the resulting
convolution operation via dynamic programming. We test the resulting
architecture on standard sentiment classification and news categorization
tasks. Our model achieves state-of-the-art performance both in terms of
accuracy and training speed. For instance, we obtain 51.2% accuracy on the
fine-grained sentiment classification task."
Regina Barzilay,Barzilay_Regina,arXiv:1506.08941,https://arxiv.org/abs/1506.08941,"Abstract:  In this paper, we consider the task of learning control policies for
text-based games. In these games, all interactions in the virtual world are
through text and the underlying state is not observed. The resulting language
barrier makes such environments challenging for automatic game players. We
employ a deep reinforcement learning framework to jointly learn state
representations and action policies using game rewards as feedback. This
framework enables us to map text descriptions into vector representations that
capture the semantics of the game states. We evaluate our approach on two game
worlds, comparing against baselines using bag-of-words and bag-of-bigrams for
state representations. Our algorithm outperforms the baselines on both worlds
demonstrating the importance of learning expressive representations."
Regina Barzilay,Barzilay_Regina,arXiv:1503.02335,https://arxiv.org/abs/1503.02335,"Abstract:  Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish."
Regina Barzilay,Barzilay_Regina,arXiv:1401.6422,https://arxiv.org/abs/1401.6422,"Abstract:  We present a model for aggregation of product review snippets by joint aspect
identification and sentiment analysis. Our model simultaneously identifies an
underlying set of ratable aspects presented in the reviews of a product (e.g.,
sushi and miso for a Japanese restaurant) and determines the corresponding
sentiment of each aspect. This approach directly enables discovery of
highly-rated or inconsistent aspects of a product. Our generative model admits
an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model
structure and inference. We test our model on two tasks, joint aspect
identification and sentiment analysis on a set of Yelp reviews and aspect
identification alone on a set of medical summaries. We evaluate the performance
of the model on aspect identification, sentiment analysis, and per-word
labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction
on aspect identification and up to 20% relative error reduction on sentiment
analysis."
Regina Barzilay,Barzilay_Regina,arXiv:1401.5695,https://arxiv.org/abs/1401.5695,"Abstract:  We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases."
Regina Barzilay,Barzilay_Regina,arXiv:1401.5390,https://arxiv.org/abs/1401.5390,"Abstract:  Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization."
Regina Barzilay,Barzilay_Regina,arXiv:1401.3488,https://arxiv.org/abs/1401.3488,"Abstract:  We present a novel Bayesian topic model for learning discourse-level document
structure. Our model leverages insights from discourse theory to constrain
latent topic assignments in a way that reflects the underlying organization of
document topics. We propose a global model in which both topic selection and
ordering are biased to be similar across a collection of related documents. We
show that this space of orderings can be effectively represented using a
distribution over permutations called the Generalized Mallows Model. We apply
our method to three complementary discourse-level tasks: cross-document
alignment, document segmentation, and information ordering. Our experiments
show that incorporating our permutation-based model in these applications
yields substantial improvements in performance over previously proposed
methods."
Regina Barzilay,Barzilay_Regina,arXiv:1401.3457,https://arxiv.org/abs/1401.3457,"Abstract:  This paper presents a new method for inferring the semantic properties of
documents by leveraging free-text keyphrase annotations. Such annotations are
becoming increasingly abundant due to the recent dramatic growth in
semi-structured, user-generated online content. One especially relevant domain
is product reviews, which are often annotated by their authors with pros/cons
keyphrases such as a real bargain or good value. These annotations are
representative of the underlying semantic properties; however, unlike expert
annotations, they are noisy: lay authors may use different labels to denote the
same property, and some labels may be missing. To learn using such noisy
annotations, we find a hidden paraphrase structure which clusters the
keyphrases. The paraphrase structure is linked with a latent topic model of the
review texts, enabling the system to predict the properties of unannotated
documents and to effectively aggregate the semantic properties of multiple
reviews. Our approach is implemented as a hierarchical Bayesian model with
joint inference. We find that joint inference increases the robustness of the
keyphrase clustering and encourages the latent topics to correlate with
semantically meaningful properties. Multiple evaluations demonstrate that our
model substantially outperforms alternative approaches for summarizing single
and multiple documents into a set of semantically salient keyphrases."
Regina Barzilay,Barzilay_Regina,arXiv:cs/0405039,https://arxiv.org/abs/cs/0405039,"Abstract:  We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods."
Regina Barzilay,Barzilay_Regina,arXiv:cs/0304006,https://arxiv.org/abs/cs/0304006,"Abstract:  We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems."
Regina Barzilay,Barzilay_Regina,arXiv:cs/0205065,https://arxiv.org/abs/cs/0205065,"Abstract:  An important component of any generation system is the mapping dictionary, a
lexicon of elementary semantic expressions and corresponding natural language
realizations. Typically, labor-intensive knowledge-based methods are used to
construct the dictionary. We instead propose to acquire it automatically via a
novel multiple-pass algorithm employing multiple-sequence alignment, a
technique commonly used in bioinformatics. Crucially, our method leverages
latent information contained in multi-parallel corpora -- datasets that supply
several verbalizations of the corresponding semantics rather than just one.
We used our techniques to generate natural language versions of
computer-generated mathematical proofs, with good results on both a
per-component and overall-output basis. For example, in evaluations involving a
dozen human judges, our system produced output whose readability and
faithfulness to the semantic input rivaled that of a traditional generation
system."
Karl Berggren,Berggren_Karl,arXiv:1901.09702,https://arxiv.org/abs/1901.09702,"Abstract:  Interaction-free measurement (IFM) has been proposed as a means of
high-resolution, low-damage imaging of radiation-sensitive samples, such as
biomolecules and proteins. The basic setup for IFM is a Mach-Zehnder
interferometer, and recent progress in nanofabricated electron diffraction
gratings has made it possible to incorporate a Mach-Zehnder interferometer in a
transmission-electron microscope (TEM). Therefore, the limits of performance of
IFM with such an interferometer and a shot-noise limited electron source (such
as that in a TEM) are of interest. In this work, we compared the error
probability and sample damage for ideal IFM and classical imaging schemes,
through theoretical analysis and numerical simulation. We considered a sample
that is either completely transparent or completely opaque at each pixel. In
our analysis, we also evaluated the impact of an additional detector for
scattered electrons. The additional detector resulted in reduction of error by
up to an order of magnitude, for both IFM and classical schemes. We also
investigated a sample re-illumination scheme based on updating priors after
each round of illumination and found that this scheme further reduced error by
a factor of two. Implementation of these methods is likely achievable with
existing instrumentation and would result in improved resolution in low-dose
electron microscopy."
Karl Berggren,Berggren_Karl,arXiv:1901.03988,https://arxiv.org/abs/1901.03988,"Abstract:  Local, bulk response functions, e.g permittivity, and the macroscopic Maxwell
equations completely specify the classical electromagnetic problem, which
features only wavelength $\lambda$ and geometric scales. The above neglect of
intrinsic electronic length scales $L_{\text{e}}$ leads to an eventual
breakdown in the nanoscopic limit. Here, we present a general theoretical and
experimental framework for treating nanoscale electromagnetic phenomena. The
framework features surface-response functions---known as the Feibelman
$d$-parameters---which reintroduce the missing electronic length scales. As a
part of our framework, we establish an experimental procedure to measure these
complex, dispersive surface response functions, enabled by quasi-normal-mode
perturbation theory and observations of pronounced nonclassical
effects---spectral shifts in excess of 30% and the breakdown of Kreibig-like
broadening---in a quintessential multiscale architecture: film-coupled
nanoresonators, with feature-sizes comparable to both $L_{\text{e}}$ and
$\lambda$."
Karl Berggren,Berggren_Karl,arXiv:1812.05559,https://arxiv.org/abs/1812.05559,"Abstract:  We present the use of a commercially available fixed-angle multi-wavelength
ellipsometer for quickly measuring the thickness of NbN thin films for the
fabrication and performance improvement of superconducting nanowire single
photon detectors. The process can determine the optical constants of absorbing
thin films, removing the need for inaccurate approximations. The tool can be
used to observe oxidation growth and allows thickness measurements to be
integrated into the characterization of various fabrication processes."
Karl Berggren,Berggren_Karl,arXiv:1811.05192,https://arxiv.org/abs/1811.05192,"Abstract:  The method of negative-tone-PMMA electron-beam lithography is investigated to
improve the performance of nanowire-based superconducting detectors. Using this
approach, the superconducting nanowire single-photon detectors (SNSPDs) have
been fabricated from thick 5-nm NbN film sputtered at the room temperature. To
investigate the impact of this process, SNSPDs were prepared by positive-tone
and negative-tone-PMMA lithography, and their electrical and photodetection
characteristics at 4.2 K were compared. The SNSPDs made by negative-tone-PMMA
lithography show higher critical-current density and higher photon count rate
at various wavelengths. Our results suggest a higher negative-tone-PMMA
technology may be preferable to the standard positive-tone-PMMA lithography for
this application."
Karl Berggren,Berggren_Karl,arXiv:1811.03991,https://arxiv.org/abs/1811.03991,"Abstract:  Conventional readout of a superconducting nanowire single-photon detector
(SNSPD) sets an upper bound on the output voltage to be the product of the bias
current and the load impedance, $I_\mathrm{B}\times Z_\mathrm{load}$, where
$Z_\mathrm{load}$ is limited to 50 $\Omega$ in standard r.f. electronics. Here,
we break this limit by interfacing the 50 $\Omega$ load and the SNSPD using an
integrated superconducting transmission line taper. The taper is a transformer
that effectively loads the SNSPD with high impedance without latching. It
increases the amplitude of the detector output while preserving the fast rising
edge. Using a taper with a starting width of 500 nm, we experimentally observed
a 3.6$\times$ higher pulse amplitude, 3.7$\times$ faster slew rate, and 25.1 ps
smaller timing jitter. The results match our numerical simulation, which
incorporates both the hotspot dynamics in the SNSPD and the distributed nature
in the transmission line taper. The taper studied here may become a useful tool
to interface high-impedance superconducting nanowire devices to conventional
low-impedance circuits."
Karl Berggren,Berggren_Karl,arXiv:1810.09542,https://arxiv.org/abs/1810.09542,"Abstract:  The basis for superconducting electronics can broadly be divided between two
technologies: the Josephson junction and the superconducting nanowire. While
the Josephson junction (JJ) remains the dominant technology due to its high
speed and low power dissipation, recently proposed nanowire devices offer
improvements such as gain, high fanout, and compatibility with CMOS circuits.
Despite these benefits, nanowire-based electronics have largely been limited to
binary operations, with devices switching between the superconducting state and
a high-impedance resistive state dominated by uncontrolled hotspot dynamics.
Unlike the JJ, they cannot increment an output through successive switching,
and their operation speeds are limited by their slow thermal reset times. Thus,
there is a need for an intermediate device with the interfacing capabilities of
a nanowire but a faster, moderated response allowing for modulation of the
output. Here, we present a nanowire device based on controlled fluxon
transport. We show that the device is capable of responding proportionally to
the strength of its input, unlike other nanowire technologies. The device can
be operated to produce a multilevel output with distinguishable states, which
can be tuned by circuit parameters. Agreement between experimental results and
electrothermal circuit simulations demonstrates that the device is classical
and may be readily engineered for applications including use as a multilevel
memory."
Karl Berggren,Berggren_Karl,arXiv:1808.03363,https://arxiv.org/abs/1808.03363,"Abstract:  Semi-transparent mirrors are standard elements in light optics for splitting
light beams or creating two versions of the same image. Such mirrors do not
exist in electron optics, although they could be beneficial in existing
techniques such as electron interferometry and holography and enable novel
electron imaging and spectroscopy techniques. We propose a design for an
electron beam splitter using the concept of quantum interaction-free
measurement (IFM). The design combines an electron resonator with a weak phase
grating. Fast switching gates allow electrons to enter and exit the resonator.
While in the resonator, the phase grating transfers intensity from the direct
beam into one of the weakly diffracted beams at each pass. To make the beam
splitter an efficient two-port splitter, the intensity in all other diffracted
beams is blocked by an aperture. The IFM principle minimizes the loss of total
intensity by this aperture. We use a scattering matrix method to analyze the
performance of the beam splitter, including the effects of inelastic scattering
in the phase grating. This design can be generalized to beam splitters for not
only electrons, but also photons, neutrons, atoms, and other quantum mechanical
systems."
Karl Berggren,Berggren_Karl,arXiv:1805.05601,https://arxiv.org/abs/1805.05601,"Abstract:  To analyze the switching dynamics and output performance of a superconducting
nanowire single photon detector (SNSPD), the nanowire is usually modelled as an
inductor in series with a time-varying resistor induced by absorption of a
photon. Our recent experimental results show that, due to the effect of kinetic
inductance, for a SNSPD made of a nanowire of sufficient length, its geometry
length can be comparable to or even longer than the effective wavelength of
frequencies contained in the output pulse. In other words, a superconducting
nanowire can behave as a distributed transmission line so that the readout
pulse depends on the photon detection location and the transmission line
properties of the nanowire. Here, we develop a distributed model for a
superconducting nanowire and apply it to simulate the output performance of a
long nanowire designed into a coplanar waveguide. We compare this coplanar
waveguide geometry to a conventional meander nanowire geometry. The simulation
results agree well with our experimental observations. With this distributed
model, we discussed the importance of microwave design of a nanowire and how
impedance matching can affect the output pulse shape. We also discuss how the
distributed model affects the growth and decay of the photon-triggered
resistive hotspot."
Karl Berggren,Berggren_Karl,arXiv:1805.00130,https://arxiv.org/abs/1805.00130,"Abstract:  We analyze the origin of the intrinsic timing jitter in superconducting
nanowire single photon detectors (SNSPDs) in terms of fluctuations in the
latency of the detector response, which is determined by the microscopic
physics of the photon detection process. We demonstrate that fluctuations in
the physical parameters which determine the latency give rise to the intrinsic
timing jitter. We develop a general description of latency by introducing the
explicit time dependence of the internal detection efficiency. By considering
the dynamic Fano fluctuations together with static spatial inhomogeneities, we
study the details of the connection between latency and timing jitter. We
develop both a simple phenomenological model and a more general microscopic
model of detector latency and timing jitter based on the solution of the
generalized time-dependent Ginzburg-Landau equations for the 1D hotbelt
geometry. While the analytical model is sufficient for qualitative
interpretation of recent data, the general approach establishes the framework
for a quantitative analysis of detector latency and the fundamental limits of
intrinsic timing jitter. These theoretical advances can be used to interpret
the results of recent experiments measuring the dependence of detection latency
and timing jitter on photon energy to the few-picosecond level."
Karl Berggren,Berggren_Karl,arXiv:1803.11306,https://arxiv.org/abs/1803.11306,"Abstract:  Report of the first workshop to identify approaches and techniques in the
domain of quantum sensing that can be utilized by future High Energy Physics
applications to further the scientific goals of High Energy Physics."
Karl Berggren,Berggren_Karl,arXiv:1711.10546,https://arxiv.org/abs/1711.10546,"Abstract:  Coincidence detection of single photons is crucial in numerous quantum
technologies and usually requires multiple time-resolved single-photon
detectors. However, the electronic readout becomes a major challenge when the
measurement basis scales to large numbers of spatial modes. Here, we address
this problem by introducing a two-terminal coincidence detector that enables
scalable readout of an array of detector segments based on superconducting
nanowire microstrip transmission line. Exploiting timing logic, we demonstrate
a 16-element detector that resolves all 136 possible single-photon and
two-photon coincidence events. We further explore the pulse shapes of the
detector output and resolve up to four-photon coincidence events in a 4-element
device, giving the detector photon-number-resolving capability. This new
detector architecture and operating scheme will be particularly useful for
multi-photon coincidence detection in large-scale photonic integrated circuits."
Karl Berggren,Berggren_Karl,arXiv:1711.08290,https://arxiv.org/abs/1711.08290,"Abstract:  A superconducting loop stores persistent current without any ohmic loss,
making it an ideal platform for energy efficient memories. Conventional
superconducting memories use an architecture based on Josephson junctions (JJs)
and have demonstrated access times less than 10 ps and power dissipation as low
as $10^{-19}$ J. However, their scalability has been slow to develop due to the
challenges in reducing the dimensions of JJs and minimizing the area of the
superconducting loops. In addition to the memory itself, complex readout
circuits require additional JJs and inductors for coupling signals, increasing
the overall area. Here, we have demonstrated a superconducting memory based
solely on lithographic nanowires. The small dimensions of the nanowire ensure
that the device can be fabricated in a dense area in multiple layers, while the
high kinetic inductance makes the loop essentially independent of geometric
inductance, allowing it to be scaled down without sacrificing performance. The
memory is operated by a group of nanowire cryotrons patterned alongside the
storage loop, enabling us to reduce the entire memory cell to 3 {\mu}m $\times
$ 7 {\mu}m in our proof-of-concept device. In this work we present the
operation principles of a superconducting nanowire memory (nMem) and
characterize its bit error rate, speed, and power dissipation."
Karl Berggren,Berggren_Karl,arXiv:1711.01305,https://arxiv.org/abs/1711.01305,"Abstract:  We present the performance of a superconducting nanowire that can be operated
in two detection modes: i) as a kinetic inductance detector (KID) or ii) as a
single-photon detector (SPD). Two superconducting nanowires developed for use
as single-photon detectors (SNSPDs) are embedded as the inductive (L) component
in resonant inductor/capacitor (LC) circuits coupled to a microwave
transmission line. The capacitors are low loss commercial chip capacitors and
limit the internal quality factor of the resonators to approximately $Q_i =
170$. The resonator quality factor, $Q_r \simeq 23$, is dominated by the
coupling to the feedline and limits the detection bandwidth to on the order of
1MHz. When operated in KID mode, the detectors are AC biased with tones at
their resonant frequencies of 45.85 and 91.81MHz. In the low-bias, standard KID
mode, a single photon produces a hot spot that does not turn an entire section
of the line normal but only increases the kinetic inductance. In the high-bias,
critical KID mode, a photon event turns a section of the line normal and the
resonance is destroyed until the normal region is dissipated. When operated as
an SPD in Geiger mode, the resonators are DC biased through cryogenic bias tees
and each photon produces a sharp voltage step followed by a ringdown signal at
the resonant frequency of the detector which is converted to a standard pulse
with an envelop detector. We show that AC biasing in the critical KID mode is
inferior to the sensitivity achieved in DC-biased SPD mode due to the small
fraction of time spent near the critical current with an AC bias."
Karl Berggren,Berggren_Karl,arXiv:1710.05358,https://arxiv.org/abs/1710.05358,"Abstract:  Recent advances in the fabrication of nanostructures and nanoscale features
in metasurfaces offer a new prospect for generating visible, light emission
from low energy electrons. In this paper, we present the experimental
observation of visible light emission from low-energy free electrons
interacting with nanoscale periodic surfaces through the Smith-Purcell (SP)
effect. SP radiation is emitted when electrons pass in close proximity over a
periodic structure, inducing collective charge motion or dipole excitations
near the surface, thereby giving rise to electromagnetic radiation. We
demonstrate a controlled emission of SP light from nanoscale gold gratings with
periodicity as small as 50 nm, enabling the observation of visible SP radiation
by low energy electrons (1.5 to 6 keV), an order of magnitude lower than
previously reported. We study the emission wavelength and intensity dependence
on the grating pitch and electron energy, showing agreement between experiment
and theory. Further reduction of structure periodicity should enable the
production of SP-based devices that operate with even slower electrons that
allow an even smaller footprint and facilitate the investigation of quantum
effects for light generation in nanoscale devices. A tunable light source
integrated in an electron microscope would enable the development of novel
electron-optical correlated spectroscopic techniques, with additional
applications ranging from biological imaging to solid-state lighting."
Karl Berggren,Berggren_Karl,arXiv:1709.06598,https://arxiv.org/abs/1709.06598,"Abstract:  Many superconducting technologies such as rapid single flux quantum computing
(RSFQ) and superconducting quantum interference devices (SQUIDs) rely on the
modulation of nonlinear dynamics in Josephson junctions for functionality. More
recently, however, superconducting devices have been developed based on the
switching and thermal heating of nanowires for use in fields such as single
photon detection and digital logic. In this paper, we use resistive shunting to
control the nonlinear heating of a superconducting nanowire and compare the
resulting dynamics to those observed in Josephson junctions. We show that
interaction of the hotspot growth with the external shunt produces high
frequency relaxation oscillations with similar behavior as observed in
Josephson junctions due to their rapid time constants and ability to be
modulated by a weak periodic signal. In particular, we use a microwave drive to
pull and mix the oscillation frequency, resulting in phase locked features that
resemble the AC Josephson effect. New nanowire devices based on these
conclusions have promising applications in fields such as parametric
amplification and frequency multiplexing."
Karl Berggren,Berggren_Karl,arXiv:1703.08034,https://arxiv.org/abs/1703.08034,"Abstract:  The lack of energy dissipation and abrupt electrical phase transition of
superconductors favorite them for nanoscale technologies, including radiation
detectors, and quantum technologies. Moreover, understanding the nanoscale
behavior of superconductivity is significant for revealing the onset of
collective-electron behavior in nature. Nevertheless, the limited number of
accessible superconductors restricts availability of the superconducting
properties, encumbering the realization of their potential. Superconducting
nanowire single photon detectors (SNSPDs) sense single-IR photons faster and
more efficient with respect to competing technologies. However, these
advantageous properties are material-dependent causing an undesirable
speed-efficiency payoff. Usually, SNSPDs based on granular materials are
faster, while those based on amorphous materials are more efficient. Here we
optimized ultrathin films of granular NbN on SiO2 and of amorphous W5Si3. We
showed that hybrid superconducting nanowire single photon detectors (SNSPDs)
made of 2-nm-thick W5Si3 films over 2-nm-thick NbN films exhibit advantageous
coexistence of timing (< 5-ns reset time and 52-ps timing jitter) and
efficiency (> 96% quantum efficiency) performance. We propose that the
governing mechanism of this hybridization is the presence of a dual
superconducting behavior: native superconductivity of each of the films and
superconductivity that is induced from the neighboring film via the proximity
effect. In addition to improvement in SNSPDs performance, our results suggest
that such hybridization can expand the range of available superconducting
properties, impacting nano-superconducting technologies. Lastly, this
hybridization may be used to tune the amorphous character of superconducting
films and to illuminate the elusive onset of collective-electron behavior near
the superconducting-to-insulating transition."
Karl Berggren,Berggren_Karl,arXiv:1610.09349,https://arxiv.org/abs/1610.09349,"Abstract:  Integration with conventional electronics offers a straightforward and
economical approach to upgrading existing superconducting technologies, such as
scaling up superconducting detectors into large arrays and combining single
flux quantum (SFQ) digital circuits with semiconductor logic and memories.
However, direct output signals from superconducting devices (e.g., Josephson
junctions) are usually not compatible with the input requirements of
conventional devices (e.g., transistors). Here, we demonstrate the use of a
single three-terminal superconducting-nanowire device, called the nanocryotron
(nTron), as a digital comparator to combine SFQ circuits with mature
semiconductor circuits such as complementary metal oxide semiconductor (CMOS)
circuits. Since SFQ circuits can digitize output signals from general
superconducting devices and CMOS circuits can interface existing
CMOS-compatible electronics, our results demonstrate the feasibility of a
general architecture that uses an nTron as an interface to realize a
super-hybrid system consisting of superconducting detectors, superconducting
quantum electronics, CMOS logic and memories, and other conventional
electronics."
Karl Berggren,Berggren_Karl,arXiv:1608.08616,https://arxiv.org/abs/1608.08616,"Abstract:  We report a self-aligned, monolithic electron interferometer, consisting of
two 45 nm thick silicon layers separated by 20 $\mu$m. This interferometer was
fabricated from a single crystal silicon cantilever on a transmission electron
microscope grid by gallium focused ion-beam milling. Using this interferometer,
we demonstrate beam path-separation, and obtain interference fringes in a
Mach-Zehnder geometry, in an unmodified 200 kV transmission electron
microscope. The fringes have a period of 0.32 nm, which corresponds to the
$\left[\bar{1}\bar{1}1\right]$ lattice planes of silicon, and a maximum
contrast of 15 %. This design can potentially be scaled to millimeter-scale,
and used in electron holography. It can also be applied to perform fundamental
physics experiments, such as interaction-free measurement with electrons."
Karl Berggren,Berggren_Karl,arXiv:1607.06713,https://arxiv.org/abs/1607.06713,"Abstract:  Detection jitter quantifies variance introduced by the detector in the
determination of photon arrival time. It is a crucial performance parameter for
systems using superconducting nanowire single photon detectors (SNSPDs). In
this work, we have demonstrated that the detection timing jitter is limited in
part by the spatial variation of photon detection events along the length of
the wire. This distribution causes the generated electrical pulses to arrive at
the readout at varied times. We define this jitter source as geometric jitter
since it is related to the length and area of the SNSPD. To characterize the
geometric jitter, we have constructed a novel differential cryogenic readout
with less than 7 ps of electronic jitter that can amplify the pulses generated
from the two ends of an SNSPD. By differencing the measured arrival times of
the two electrical pulses, we were able to partially cancel out the difference
of the propagation times and thus reduce the uncertainty of the photon arrival
time. Our experimental data indicates that the variation of the differential
propagation time was a few ps for a 3 {\mu}m x 3 {\mu}m device while it
increased up to 50 ps for a 20 {\mu}m x 20 {\mu}m device. In a 20 {\mu}m x 20
{\mu}m large SNSPD, we achieved a 20% reduction in the overall detection timing
jitter for detecting telecom-wavelength photons by using the differential
cryogenic readout. The geometric jitter hypothesis was further confirmed by
studying jitter in devices that consisted of long wires with 1-{\mu}m-long
narrowed regions used for sensing photons."
Karl Berggren,Berggren_Karl,arXiv:1606.01395,https://arxiv.org/abs/1606.01395,"Abstract:  We describe a superconducting three-terminal device that uses a simple
geometric effect known as current crowding to sense the flow of current and
actuate a readout signal. The device consists of a ""Y""-shaped current combiner,
with two currents (sense and bias) entering through the top arms of the ""Y"",
intersecting, and then exiting through the bottom leg of the ""Y""'. This
geometry--mixing two inputs at a sharp intersection point--takes its
inspiration from Y-shaped combiners in fluid flow systems, where variations in
the input pressures can produce at turbulence and mixing at the intersection.
When current is added to or removed from one of the arms (the sense arm), the
superconducting critical current in the other arm (the bias arm) is modulated.
The current in the sense arm can thus be determined by measuring the critical
current of the bias arm. The dependence of the bias critical current on the
sense current is possible because current crowding causes the sense current to
interact locally with the bias arm. Measurement of the critical current in the
bias arm does not break the superconducting state of the sense arm or of the
bottom leg, and thus the signal to be sensed is fully restored after the
measurement process. This device thus has potential for broad applicability
across superconducting technologies and materials."
Karl Berggren,Berggren_Karl,arXiv:1605.08693,https://arxiv.org/abs/1605.08693,"Abstract:  Detecting spatial and temporal information of individual photons by using
single-photon-detector (SPD) arrays is critical to applications in
spectroscopy, communication, biological imaging, astronomical observation, and
quantum-information processing. Among the current SPDs1,detectors based on
superconducting nanowires have outstanding performance2, but are limited in
their ability to be integrated into large scale arrays due to the engineering
difficulty of high-bandwidth cryogenic electronic readout3-8. Here, we address
this problem by demonstrating a scalable single-photon imager using a single
continuous photon-sensitive superconducting nanowire microwave-plasmon
transmission line. By appropriately designing the nanowire's local
electromagnetic environment so that the nanowire guides microwave plasmons, the
propagating voltages signals generated by a photon-detection event were slowed
down to ~ 2% of the speed of light. As a result, the time difference between
arrivals of the signals at the two ends of the nanowire naturally encoded the
position and time of absorption of the photon. Thus, with only two readout
lines, we demonstrated that a 19.7-mm-long nanowire meandered across an area of
286 {\mu}m * 193 {\mu}m was capable of resolving ~590 effective pixels while
simultaneously recording the arrival times of photons with a temporal
resolution of 50 ps. The nanowire imager presents a scalable approach to
realizing high-resolution photon imaging in time and space."
Karl Berggren,Berggren_Karl,arXiv:1602.06895,https://arxiv.org/abs/1602.06895,"Abstract:  We study the microwave impedance of extremely high aspect ratio (length/width
~ 5,000) superconducting niobium nitride nanowires. The nanowires are
fabricated in a compact meander geometry that is in series with the center
conductor of a 50 ohm coplanar waveguide transmission line. The transmission
coefficient of the sample is measured up to 20 GHz. At high frequency, a peak
in the transmission coefficient is seen. Numerical simulations show that this
is a half-wave resonance along the length of the nanowire, where the nanowire
acts as a high impedance, slow wave transmission line. This resonance sets the
upper frequency limit for these nanowires as inductive elements. Fitting
simulations to the measured resonance enables a precise determination of the
nanowire's complex sheet impedance at the resonance frequency. The real part is
a measure of dissipation, while the imaginary part is dominated by kinetic
inductance. We characterize the dependence of the sheet resistance and sheet
inductance on both temperature and current and compare the results to recent
theoretical predictions for disordered superconductors. These results can aid
in the understanding of high frequency devices based on superconducting
nanowires. They may also lead to the development of novel superconducting
devices such as ultra-compact resonators and slow-wave structures."
Karl Berggren,Berggren_Karl,arXiv:1511.05786,https://arxiv.org/abs/1511.05786,"Abstract:  This paper describes the construction of a cryostat and an optical system
with a free-space coupling efficiency of 56.5% +/- 3.4% to a superconducting
nanowire single-photon detector (SNSPD) for infrared quantum communication and
spectrum analysis. A 1K pot decreases the base temperature to T = 1.7 K from
the 2.9 K reached by the cold head cooled by a pulse-tube cryocooler. The
minimum spot size coupled to the detector chip was 6.6 +/- 0.11 {\mu}m starting
from a fiber source at wavelength, {\lambda} = 1.55 {\mu}m. We demonstrated
efficient photon counting on a detector with an 8 x 7.3 {\mu}m^2 area. We
measured a dark count rate of 95 +/- 3.35 kcps and a system detection
efficiency of 1.64% +/- 0.13%. We explain the key steps that are required to
further improve the coupling efficiency."
Karl Berggren,Berggren_Karl,arXiv:1510.05946,https://arxiv.org/abs/1510.05946,"Abstract:  One of the astounding consequences of quantum mechanics is that it allows the
detection of a target using an incident probe, with only a low probability of
interaction of the probe and the target. This 'quantum weirdness' could be
applied in the field of electron microscopy to generate images of
beam-sensitive specimens with substantially reduced damage to the specimen. A
reduction of beam-induced damage to specimens is especially of great importance
if it can enable imaging of biological specimens with atomic resolution.
Following a recent suggestion that interaction-free measurements are possible
with electrons, we now analyze the difficulties of actually building an atomic
resolution interaction-free electron microscope, or ""quantum electron
microscope"". A quantum electron microscope would require a number of unique
components not found in conventional transmission electron microscopes. These
components include a coherent electron-beam splitter or two-state-coupler, and
a resonator structure to allow each electron to interrogate the specimen
multiple times, thus supporting high success probabilities for interaction-free
detection of the specimen. Different system designs are presented here, which
are based on four different choices of two-state-couplers: a thin crystal, a
grating mirror, a standing light wave and an electro-dynamical pseudopotential.
Challenges for the detailed electron optical design are identified as future
directions for development. While it is concluded that it should be possible to
build an atomic resolution quantum electron microscope, we have also identified
a number of hurdles to the development of such a microscope and further
theoretical investigations that will be required to enable a complete
interpretation of the images produced by such a microscope."
Karl Berggren,Berggren_Karl,arXiv:1508.01877,https://arxiv.org/abs/1508.01877,"Abstract:  Methods for patterning biomolecules on a substrate at the single molecule
level have been studied as a route to sensors with single-molecular sensitivity
or as a way to probe biological phenomena at the single-molecule level.
However, the arrangement and orientation of single biomolecules on substrates
has been less investigated. Here, we examined the arrangement and orientation
of two rod-like coiled-coil proteins, cortexillin and tropomyosin, around
patterned gold nanostructures. The high aspect ratio of the coiled coils made
it possible to study their orientations and to pursue a strategy of protein
orientation via two-point attachment. The proteins were anchored to the
surfaces using thiol groups, and the number of cysteine residues in tropomyosin
was varied to test how this variation affected the structure and arrangement of
the surface-attached proteins. Molecular dynamics studies were used to
interpret the observed positional distributions. Based on initial studies of
protein attachment to gold post structures, two 31-nm-long tropomyosin
molecules were aligned between the two sidewalls of a trench with a width of 68
nm. Because the approach presented in this study uses one of twenty natural
amino acids, this method provides a convenient way to pattern biomolecules on
substrates using standard chemistry."
Karl Berggren,Berggren_Karl,arXiv:1503.07135,https://arxiv.org/abs/1503.07135,"Abstract:  We present an optical setup that can be used to characterize the thicknesses
of thin NbN films to screen samples for fabrication and to better model the
performance of the resulting superconducting nanowire single photon detectors.
The infrared transmissometer reported here is easy to use, gives results within
minutes and is non-destructive. Thus, the thickness measurement can be easily
integrated into the workflow of deposition and characterization. Comparison to
a similar visible-wavelength transmissometer is provided."
Karl Berggren,Berggren_Karl,arXiv:1408.1124,https://arxiv.org/abs/1408.1124,"Abstract:  Superconducting nanowire avalanche single-photon detectors (SNAPs) with n
parallel nanowires are advantageous over single-nanowire detectors because
their output signal amplitude scales linearly with n. However, the SNAP
architecture has not been viably demonstrated for n > 4. To increase n for
larger signal amplification, we designed a multi-stage, successive-avalanche
architecture which used nanowires, connected via choke inductors in a
binary-tree layout. We demonstrated an avalanche detector with n = 8 parallel
nanowires and achieved eight-fold signal amplification, with a timing jitter of
54 ps."
Karl Berggren,Berggren_Karl,arXiv:1407.5945,https://arxiv.org/abs/1407.5945,"Abstract:  Thin superconducting films form a unique platform for geometrically-confined,
strongly-interacting electrons. They allow an inherent competition between
disorder and superconductivity, which in turn enables the intriguing
superconducting-to-insulator transition and believed to facilitate the
comprehension of high-Tc superconductivity. Furthermore, understanding thin
film superconductivity is technologically essential e.g. for photo-detectors,
and quantum-computers. Consequently, the absence of an established universal
relationships between critical temperature ($T_c$), film thickness ($d$) and
sheet resistance ($R_s$) hinders both our understanding of the onset of the
superconductivity and the development of miniaturised superconducting devices.
We report that in thin films, superconductivity scales as $d^.$$T_c(R_s)$. We
demonstrated this scaling by analysing the data published over the past 46
years for different materials (and facilitated this database for further
analysis). Moreover, we experimentally confirmed the discovered scaling for NbN
films, quantified it with a power law, explored its possible origin and
demonstrated its usefulness for superconducting film-based devices."
Karl Berggren,Berggren_Karl,arXiv:1405.4244,https://arxiv.org/abs/1405.4244,"Abstract:  Photonic integrated circuits (PICs) have emerged as a scalable platform for
complex quantum technologies using photonic and atomic systems. A central goal
has been to integrate photon-resolving detectors to reduce optical losses,
latency, and wiring complexity associated with off-chip detectors.
Superconducting nanowire single-photon detectors (SNSPDs) are particularly
attractive because of high detection efficiency, sub-50-ps timing jitter,
nanosecond-scale reset time, and sensitivity from the visible to the
mid-infrared spectrum. However, while single SNSPDs have been incorporated into
individual waveguides, the system efficiency of multiple SNSPDs in one photonic
circuit has been limited below 0.2% due to low device yield. Here we introduce
a micrometer-scale flip-chip process that enables scalable integration of
SNSPDs on a range of PICs. Ten low-jitter detectors were integrated on one PIC
with 100% device yield. With an average system efficiency beyond 10% for
multiple SNSPDs on one PIC, we demonstrate high-fidelity on-chip photon
correlation measurements of non-classical light."
Karl Berggren,Berggren_Karl,arXiv:1403.6423,https://arxiv.org/abs/1403.6423,"Abstract:  In existing superconducting electronic systems, Josephson junctions play a
central role in processing and transmitting small-amplitude electrical signals.
However, Josephson-junction-based devices have a number of limitations
including: (1) sensitivity to magnetic fields, (2) limited gain, (3) inability
to drive large impedances, and (4) difficulty in controlling the junction
critical current (which depends sensitively on sub-Angstrom-scale thickness
variation of the tunneling barrier). Here we present a nanowire-based
superconducting electronic device, which we call the nanocryotron (nTron), that
does not rely on Josephson junctions and can be patterned from a single thin
film of superconducting material with conventional electron-beam lithography.
The nTron is a 3-terminal, T-shaped planar device with a gain of ~20 that is
capable of driving impedances of more than 100 k{\Omega}, and operates in
typical ambient magnetic fields at temperatures of 4.2K. The device uses a
localized, Joule-heated hotspot formed in the gate to modulate current flow in
a perpendicular superconducting channel. We have characterized the nTron,
matched it to a theoretical framework, and applied it both as a digital logic
element in a half-adder circuit, and as a digital amplifier for superconducting
nanowire single-photon detectors pulses. The nTron has immediate applications
in classical and quantum communications, photon sensing and astronomy, and its
performance characteristics make it compatible with existing superconducting
technologies. Furthermore, because the hotspot effect occurs in all known
superconductors, we expect the design to be extensible to other materials,
providing a path to digital logic, switching, and amplification in
high-temperature superconductors."
Karl Berggren,Berggren_Karl,arXiv:1202.2835,https://arxiv.org/abs/1202.2835,"Abstract:  The optimal orientations are determined for polarized substrate side
illumination of three superconducting nanowire single-photon detector (SNSPD)
designs: (1) periodic niobium-nitride (NbN) stripes standing in air with
dimensions according to conventional SNSPDs, (2) same NbN patterns below
~quarter-wavelength hydrogensilsesquioxane-filled nano-cavity, (3) analogous
NbN patterns in HSQ nano-cavity closed by a thin gold reflector. Numerical
computation results have shown that the optical response and near-field
distribution vary significantly with polar-angle, fi, and these variations are
analogous across all azimuthal-angles, gamma, but are fundamentally different
in various device designs. Larger absorptance is available due to p-polarized
illumination of NbN patterns in P-structure configuration, while s-polarized
illumination results in higher absorptance in S-structure arrangement. As a
result of p-polarized illumination a global maximum appears on absorptance of
bare NbN pattern at polar angle corresponding to NbN-related ATIR; integration
with HSQ nano-cavity results in a global absorptance maximum at polar angle
corresponding to TIR at sapphire-air interface; while the highest absorptance
is observable at perpendicular incidence on P-structures aligned below gold
reflector covered HSQ nano-cavity. S-polarized light illumination results in a
global absorptance maximum at TIR on bare NbN patterns; the highest absorptance
is available below HSQ nano-cavity at polar angle corresponding to ATIR
phenomenon; while the benefit of gold reflector is large and polar angle
independent absorptance."
Karl Berggren,Berggren_Karl,arXiv:1109.4881,https://arxiv.org/abs/1109.4881,"Abstract:  In this paper we calculate the critical currents in thin superconducting
strips with sharp right-angle turns, 180-degree turnarounds, and more
complicated geometries, where all the line widths are much smaller than the
Pearl length $\Lambda = 2 \lambda^2/d$. We define the critical current as the
current that reduces the Gibbs free-energy barrier to zero. We show that
current crowding, which occurs whenever the current rounds a sharp turn, tends
to reduce the critical current, but we also show that when the radius of
curvature is less than the coherence length this effect is partially
compensated by a radius-of-curvature effect. We propose several patterns with
rounded corners to avoid critical-current reduction due to current crowding.
These results are relevant to superconducting nanowire single-photon detectors,
where they suggest a means of improving the bias conditions and reducing dark
counts. These results also have relevance to normal-metal nanocircuits, as
these patterns can reduce the electrical resistance, electromigration, and hot
spots caused by nonuniform heating."
Karl Berggren,Berggren_Karl,arXiv:1106.3591,https://arxiv.org/abs/1106.3591,"Abstract:  A novel finite-element method for calculating the illumination-dependence of
absorption in three-dimensional nanostructures is presented based on the RF
module of the COMSOL software package. This method is capable of numerically
determining the optical response and near-field distribution of sub-wavelength
periodic structures as a function of illumination orientations specified by
polar angle, fi, and azimuthal angle, gamma. The method was applied to
determine the illumination-angle-dependent absorptance in cavity-based
superconducting-nanowire single-photon detector (SNSPD) designs.
Niobium-nitride stripes based on dimensions of conventional SNSPDs and
integrated with ~ quarter-wavelength hydrogensilsesquioxane-filled nano-optical
cavities and covered by a thin gold film acting as a reflector were illuminated
from below by p-polarized light in this study. The numerical results were
compared to results from complementary transfer-matrix-method calculations on
composite layers made of analogous film-stacks. This comparison helped to
uncover the optical phenomena contributing to the appearance of extrema in the
optical response. This paper presents an approach to optimizing the absorptance
of different sensing and detecting devices via simultaneous numerical
optimization of the polar and azimuthal illumination angles."
Karl Berggren,Berggren_Karl,arXiv:1012.3964,https://arxiv.org/abs/1012.3964,"Abstract:  We developed an electro thermal model of NbN superconducting nanowire
avalanche photodetectors (SNAPs) on sapphire substrates. SNAPs are single
photon detectors consisting of the parallel connection of N superconducting
nanowires. We extrapolated the physical constants of the model from
experimental data and we simulated the time evolution of the device resistance,
temperature and current by solving two coupled electrical and thermal
differential equations describing the nanowires. The predictions of the model
were in good quantitative agreement with the experimental results."
Karl Berggren,Berggren_Karl,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and
trap single 88Sr ions at cryogenic temperatures. The superconducting transition
is verified and characterized by measuring the resistance and critical current
using a 4-wire measurement on the trap structure, and observing change in the
rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz
at 6 K and shows no significant change across the superconducting transition,
suggesting that anomalous heating is primarily caused by noise sources on the
surface. This demonstration of superconducting ion traps opens up possibilities
for integrating trapped ions and molecular ions with superconducting devices."
Karl Berggren,Berggren_Karl,arXiv:0812.4670,https://arxiv.org/abs/0812.4670,"Abstract:  Transitions in an artificial atom, driven non-adiabatically through an
energy-level avoided crossing, can be controlled by carefully engineering the
driving protocol. We have driven a superconducting persistent-current qubit
with a large-amplitude, radio-frequency field. By applying a bi-harmonic
waveform generated by a digital source, we demonstrate a mapping between the
amplitude and phase of the harmonics produced at the source and those received
by the device. This allows us to image the actual waveform at the device. This
information is used to engineer a desired time dependence, as confirmed by
detailed comparison with simulation."
Karl Berggren,Berggren_Karl,arXiv:0812.0290,https://arxiv.org/abs/0812.0290,"Abstract:  We investigate the role of electrothermal feedback in the operation of
superconducting nanowire single-photon detectors (SNSPDs). It is found that the
desired mode of operation for SNSPDs is only achieved if this feedback is
unstable, which happens naturally through the slow electrical response
associated with their relatively large kinetic inductance. If this response is
sped up in an effort to increase the device count rate, the electrothermal
feedback becomes stable and results in an effect known as latching, where the
device is locked in a resistive state and can no longer detect photons. We
present a set of experiments which elucidate this effect, and a simple model
which quantitatively explains the results."
Karl Berggren,Berggren_Karl,arXiv:0806.3194,https://arxiv.org/abs/0806.3194,"Abstract:  We measured the optical absorptance of superconducting nanowire single photon
detectors. We found that 200-nm-pitch, 50%-fill-factor devices had an average
absorptance of 21% for normally-incident front-illumination of
1.55-um-wavelength light polarized parallel to the nanowires, and only 10% for
perpendicularly-polarized light. We also measured devices with lower
fill-factors and narrower wires that were five times more sensitive to
parallel-polarized photons than perpendicular-polarized photons. We developed a
numerical model that predicts the absorptance of our structures. We also used
our measurements, coupled with measurements of device detection efficiencies,
to determine the probability of photon detection after an absorption event. We
found that, remarkably, absorbed parallel-polarized photons were more likely to
result in detection events than perpendicular-polarized photons, and we present
a hypothesis that qualitatively explains this result. Finally, we also
determined the enhancement of device detection efficiency and absorptance due
to the inclusion of an integrated optical cavity over a range of wavelengths
(700-1700 nm) on a number of devices, and found good agreement with our
numerical model."
Karl Berggren,Berggren_Karl,arXiv:0805.2397,https://arxiv.org/abs/0805.2397,"Abstract:  A photon-number-resolving detector based on a four-element superconducting
nanowire single photon detector is demonstrated to have sub-30-ps resolution in
measuring the arrival time of individual photons. This detector can be used to
characterize the photon statistics of non-pulsed light sources and to mitigate
dead-time effects in high-speed photon counting applications. Furthermore, a
25% system detection efficiency at 1550 nm was demonstrated, making the
detector useful for both low-flux source characterization and high-speed
photon-counting and quantum communication applications. The design, fabrication
and testing of this detector are described, and a comparison between the
measured and theoretical performance is presented."
Karl Berggren,Berggren_Karl,arXiv:0805.1552,https://arxiv.org/abs/0805.1552,"Abstract:  The energy-level structure of a quantum system plays a fundamental role in
determining its behavior and manifests itself in a discrete absorption and
emission spectrum. Conventionally, spectra are probed via frequency
spectroscopy whereby the frequency \nu of a harmonic driving field is varied to
fulfill the conditions \Delta E = h \nu, where the driving field is resonant
with the level separation \Delta E (h is Planck's constant). Although this
technique has been successfully employed in a variety of physical systems,
including natural and artificial atoms and molecules, its application is not
universally straightforward, and becomes extremely challenging for frequencies
in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative
approach, whereby a harmonic driving field sweeps the atom through its
energy-level avoided crossings at a fixed frequency, surmounting many of the
limitations of the conventional approach. Spectroscopic information is obtained
from the amplitude dependence of the system response. The resulting
``spectroscopy diamonds'' contain interference patterns and population
inversion that serve as a fingerprint of the atom's spectrum. By analyzing
these features, we determine the energy spectrum of a manifold of states with
energies from 0.01 to 120 GHz \times h in a superconducting artificial atom,
using a driving frequency near 0.1 GHz. This approach provides a means to
manipulate and characterize systems over a broad bandwidth, using only a single
driving frequency that may be orders of magnitude smaller than the energy
scales being probed."
Karl Berggren,Berggren_Karl,arXiv:0711.5021,https://arxiv.org/abs/0711.5021,"Abstract:  Novel optical phenomena, including electromagnetically induced transparency,
slow light, superluminal light propagation, have recently been demonstrated in
diverse physical implementations. These phenomena are challenging to realize in
practical systems because they require quantum coherence as well as careful
preparation and control of prescribed quantum states. Here we present a unified
approach to engineering optical materials that exhibit these phenomena by using
mixtures of active and passive optical materials at frequencies near their
resonances. Our approach does not depend on quantum coherence and can realize
large and small (much less than 1) indices of refraction and negative
permittivity ($\epsilon<0$), normal and anomalous dispersion, all while
maintaining transparency."
Karl Berggren,Berggren_Karl,arXiv:physics/0611260,https://arxiv.org/abs/physics/0611260,"Abstract:  We investigate the source of large variations in the observed detection
effiiencies of superconducting nanowire single-photon detectors between many
nominally identical devices. Through both electrical and optical measurements,
we infer that these variations arise from ""constrictions:"" highly localized
regions of the nanowires where the effective cross-sectional area for
superconducting current is reduced. These constrictions limit the DC bias
current density to well below its critical value over the remainder of the
wire, and thus prevent the detection efficiency from reaching the high values
that occur in these devices only when they are biased near the critical current
density."
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0609561,https://arxiv.org/abs/cond-mat/0609561,"Abstract:  A nonlinear resonant circuit comprising a SQUID magnetometer and a parallel
capacitor is studied as a readout scheme for a persistent-current (PC) qubit.
The flux state of the qubit is detected as a change in the Josephson inductance
of the SQUID magnetometer, which in turn mediates a shift in the resonance
frequency of the readout circuit. The nonlinearity and resulting hysteresis in
the resonant behavior are characterized as a function of the power of both the
input drive and the associated resonance peak response. Numerical simulations
based on a phenomenological circuit model are presented which display the
features of the observed nonlinearity."
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0512691,https://arxiv.org/abs/cond-mat/0512691,"Abstract:  We demonstrate Mach-Zehnder-type interferometry in a superconducting flux
qubit. The qubit is a tunable artificial atom, whose ground and excited states
exhibit an avoided crossing. Strongly driving the qubit with harmonic
excitation sweeps it through the avoided crossing two times per period. As the
induced Landau-Zener transitions act as coherent beamsplitters, the accumulated
phase between transitions, which varies with microwave amplitude, results in
quantum interference fringes for n=1...20 photon transitions. The
generalization of optical Mach-Zehnder interferometry, performed in qubit phase
space, provides an alternative means to manipulate and characterize the qubit
in the strongly-driven regime."
Karl Berggren,Berggren_Karl,arXiv:physics/0510238,https://arxiv.org/abs/physics/0510238,"Abstract:  We investigate the recovery of superconducting NbN-nanowire photon counters
after detection of an optical pulse at a wavelength of 1550 nm, and present a
model that quantitatively accounts for our observations. The reset time is
found to be limited by the large kinetic inductance of these nanowires, which
forces a tradeoff between counting rate and either detection efficiency or
active area. Devices of usable size and high detection efficiency are found to
have reset times orders of magnitude longer than their intrinsic photoresponse
time."
Karl Berggren,Berggren_Karl,arXiv:physics/0509228,https://arxiv.org/abs/physics/0509228,"Abstract:  Quantum optical techniques may yield immersion fluids with high indices of
refraction without absorption. We describe one such technique in which a probe
field experiences a large index of refraction with amplification rather than
absorption, and examine its practicality for an immersion lithography
application. Enhanced index can be observed in a three-level system with a
tunable, near-resonant, coherent probe and incoherent pump field that inverts
population of the probe transition. This observation contradicts the common
belief that large indices of refraction are impossible without absorption,
however it is well in accord with existing electromagnetic theory and practice.
Calculations show that a refractive index >> 2 is possible with practical
experimental parameters. A scheme with an incoherent mixture of pumped and
unpumped atoms is also examined, and is seen to have a lower refractive index
(~2) accompanied by neither gain nor loss."
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0501283,https://arxiv.org/abs/cond-mat/0501283,"Abstract:  We have implemented a resonant circuit that uses a SQUID as a flux-sensitive
Josephson inductor for qubit readout. In contrast to the conventional switching
current measurement that generates undesired quasi-particles when the SQUID
switches to the voltage state, our approach keeps the readout SQUID biased
along the supercurrent branch during the measurement. By incorporating the
SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux
states of a niobium persistent-current (PC) qubit by observing a shift in the
resonant frequency of both the magnitude and the phase spectra. The readout
circuit was also characterized in the nonlinear regime to investigate its
potential use as a nonlinear amplifier."
Karl Berggren,Berggren_Karl,arXiv:cond-mat/0311289,https://arxiv.org/abs/cond-mat/0311289,"Abstract:  We measured the intrawell energy relaxation time \tau_{d} between macroscopic
quantum levels in the double well potential of a Nb persistent-current qubit.
Interwell population transitions were generated by irradiating the qubit with
microwaves. Zero population in the initial well was then observed due to a
multi-level decay process in which the initial population relaxed to the lower
energy levels during transitions. The qubit's decoherence time, determined from
\tau_{d}, is longer than 20 microseconds, holding the promise of building a
quantum computer with Nb-based superconducting qubits."
Karl Berggren,Berggren_Karl,arXiv:quant-ph/0310157,https://arxiv.org/abs/quant-ph/0310157,"Abstract:  A numerical method for solving Schrodinger's equation based upon a
Baker-Campbell-Hausdorff (BCH) expansion of the time evolution operator is
presented herein. The technique manifestly preserves wavefunction norm, and it
can be applied to problems in any number of spatial dimensions. We also
identify a particular dimensionless ratio of potential to kinetic energies as a
key coupling constant. This coupling establishes characteristic length and time
scales for a large class of low energy quantum states, and it guides the choice
of step sizes in numerical work. Using the BCH method in conjunction with an
imaginary time rotation, we compute low energy eigenstates for several quantum
systems coupled to non-trivial background potentials. The approach is
subsequently applied to the study of 1D propagating wave packets and 2D bound
state time development. Failures of classical expectations uncovered by
simulations of these simple systems help develop quantum intuition.
Finally, we investigate the response of a Superconducting Quantum
Interference Device (SQUID) to a time dependent potential. We discuss how to
engineer the potential's energy and time scales so that the SQUID acts as a
quantum NOT gate. The notional simulation we present for this gate provides
useful insight into the design of one candidate building block for a quantum
computer."
Karl Berggren,Berggren_Karl,arXiv:nlin/0111010,https://arxiv.org/abs/nlin/0111010,"Abstract:  The probability current statistics of two-dimensional open chaotic ballistic
billiards is studied both analytically and numerically. Assuming that the real
and imaginary parts of the scattering wave function are both random Gaussian
fields, we find a universal distribution function for the probability current.
In by-passing we recover previous analytic forms for wave function statistics.
The expressions bridge the entire region from GOE to GUE type statistics. Our
analytic expressions are verified numerically by explicit quantum-mechanical
calculations of transport through a Bunimovich billiard."
Karl Berggren,Berggren_Karl,arXiv:nlin/0012019,https://arxiv.org/abs/nlin/0012019,"Abstract:  According to Berry a wave-chaotic state may be viewed as a superposition of
monochromatic plane waves with random phases and amplitudes. Here we consider
the distribution of nodal points associated with this state. Using the property
that both the real and imaginary parts of the wave function are random Gaussian
fields we analyze the correlation function and densities of the nodal points.
Using two approaches (the Poisson and Bernoulli) we derive the distribution of
nearest neighbor separations. Furthermore the distribution functions for nodal
points with specific chirality are found. Comparison is made with results from
from numerical calculations for the Berry wave function."
Karl Berggren,Berggren_Karl,arXiv:chao-dyn/9910011,https://arxiv.org/abs/chao-dyn/9910011,"Abstract:  Streamlines and distributions of nodal points are used as signatures of chaos
in coherent electron transport through three types of billiards, Sinai,
Bunimovich and rectangular. Numerical averaged distribution functions of
nearest distances between nodal points are presented. We find the same form for
the Sinai and Bunimovich billiards and suggest that there is a universal form
that can be used as a signature of quantum chaos for electron transport in open
billiards. The universal distribution function is found to be insensitive to
the way avaraging is performed (over positions of leads, over an energy
interval with a few conductance fluctuations, or both). The integrable
rectangular billiard, on the other hand, displays nonuniversal distribution
with a central peak related to partial order of nodal points for the case of
symmetric attachment of leads. However cases with nonsymmetric leads tend to
the universal form.
Also it is shown how nodal points in rectangular billiard can lead to
""channeling of quantum flows"" while disorder in nodal points in the Sinai
billiard gives rise to unstable irregular behavior of the flow."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1804.04577,https://arxiv.org/abs/1804.04577,"Abstract:  In this paper we discuss policy iteration methods for approximate solution of
a finite-state discounted Markov decision problem, with a focus on
feature-based aggregation methods and their connection with deep reinforcement
learning schemes. We introduce features of the states of the original problem,
and we formulate a smaller ""aggregate"" Markov decision problem, whose states
relate to the features. We discuss properties and possible implementations of
this type of aggregation, including a new approach to approximate policy
iteration. In this approach the policy improvement operation combines
feature-based aggregation with feature construction using deep neural networks
or other calculations. We argue that the cost function of a policy may be
approximated much more accurately by the nonlinear function of the features
provided by aggregation, than by the linear function of the features provided
by neural network-based reinforcement learning, thereby potentially leading to
more effective policy improvement."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1712.06659,https://arxiv.org/abs/1712.06659,"Abstract:  We consider discrete-time infinite horizon deterministic optimal control
problems with nonnegative cost per stage, and a destination that is cost-free
and absorbing. The classical linear-quadratic regulator problem is a special
case. Our assumptions are very general, and allow the possibility that the
optimal policy may not be stabilizing the system, e.g., may not reach the
destination either asymptotically or in a finite number of steps. We introduce
a new unifying notion of stable feedback policy, based on perturbation of the
cost per stage, which in addition to implying convergence of the generated
states to the destination, quantifies the speed of convergence. We consider the
properties of two distinct cost functions: $\jstar$, the overall optimal, and
$\hat J$, the restricted optimal over just the stable policies. Different
classes of stable policies (with different speeds of convergence) may yield
different values of $\hat J$. We show that for any class of stable policies,
$\hat J$ is a solution of Bellman's equation, and we characterize the smallest
and the largest solutions: they are $\jstar$, and $J^+$, the restricted optimal
cost function over the class of (finitely) terminating policies. We also
characterize the regions of convergence of various modified versions of value
and policy iteration algorithms, as substitutes for the standard algorithms,
which may not work in general."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1711.10129,https://arxiv.org/abs/1711.10129,"Abstract:  We consider stochastic shortest path problems with infinite state and control
spaces, a nonnegative cost per stage, and a termination state. We extend the
notion of a proper policy, a policy that terminates within a finite expected
number of steps, from the context of finite state space to the context of
infinite state space. We consider the optimal cost function $\jstar$, and the
optimal cost function $\hat J$ over just the proper policies. We show that
$\jstar$ and $\hat J$ are the smallest and largest solutions of Bellman's
equation, respectively, within a suitable class of Lyapounov-like functions. If
the cost per stage is bounded, these functions are those that are bounded over
the effective domain of $\hat J$. The standard value iteration algorithm may be
attracted to either $\jstar$ or $\hat J$, depending on the initial condition.
In the favorable case where $\jstar=\hat J$, strong analytical and algorithmic
results are obtained."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1610.05427,https://arxiv.org/abs/1610.05427,"Abstract:  We consider large linear and nonlinear fixed point problems, and solution
with proximal algorithms. We show that there is a close connection between two
seemingly different types of methods from distinct fields: 1) Proximal
iterations for linear systems of equations, which are prominent in numerical
analysis and convex optimization, and 2) Temporal difference (TD) type methods,
such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in
simulation-based approximate dynamic programming/reinforcement learning
(DP/RL), and its recent prominent successes in large-scale game contexts, among
others.
One benefit of this connection is a new and simple way to accelerate the
standard proximal algorithm by extrapolation towards the TD iteration, which
generically has a faster convergence rate. Another benefit is the potential
integration into the proximal algorithmic context of several new ideas that
have emerged in the DP/RL context. We discuss some of the possibilities, and in
particular, algorithms that project each proximal iterate onto the subspace
spanned by a small number of basis functions, using low-dimensional
calculations and simulation. A third benefit is that insights and analysis from
proximal algorithms can be brought to bear on the enhancement of TD methods.
The linear fixed point methodology can be extended to nonlinear fixed point
problems involving a contraction, thus providing guaranteed and potentially
substantial acceleration of the proximal and forward backward splitting
algorithms at no extra cost. Moreover, the connection of proximal and TD
methods can be extended to nonlinear (nondifferentiable) fixed point problems
through new proximal-like algorithms that involve successive linearization,
similar to policy iteration in DP."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1609.03115,https://arxiv.org/abs/1609.03115,"Abstract:  We consider challenging dynamic programming models where the associated
Bellman equation, and the value and policy iteration algorithms commonly
exhibit complex and even pathological behavior. Our analysis is based on the
new notion of regular policies. These are policies that are well-behaved with
respect to value and policy iteration, and are patterned after proper policies,
which are central in the theory of stochastic shortest path problems. We show
that the optimal cost function over regular policies may have favorable value
and policy iteration properties, which the optimal cost function over all
policies need not have. We accordingly develop a unifying methodology to
address long standing analytical and algorithmic issues in broad classes of
undiscounted models, including stochastic and minimax shortest path problems,
as well as positive cost, negative cost, risk-sensitive, and multiplicative
cost problems."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1608.01670,https://arxiv.org/abs/1608.01670,"Abstract:  In this paper we consider shortest path problems in a directed graph where
the transitions between nodes are subject to uncertainty. We use a minimax
formulation, where the objective is to guarantee that a special destination
state is reached with a minimum cost path under the worst possible instance of
the uncertainty. Problems of this type arise, among others, in planning and
pursuit-evasion contexts, and in model predictive control. Our analysis makes
use of the recently developed theory of abstract semicontractive dynamic
programming models. We investigate questions of existence and uniqueness of
solution of the optimality equation, existence of optimal paths, and the
validity of various algorithms patterned after the classical methods of value
and policy iteration, as well as a Dijkstra-like algorithm for problems with
nonnegative arc lengths."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1608.01393,https://arxiv.org/abs/1608.01393,"Abstract:  In this paper we consider a broad class of infinite horizon discrete-time
optimal control models that involve a nonnegative cost function and an affine
mapping in their dynamic programming equation. They include as special cases
classical models such as stochastic undiscounted nonnegative cost problems,
stochastic multiplicative cost problems, and risk-sensitive problems with
exponential cost. We focus on the case where the state space is finite and the
control space has some compactness properties. We assume that the affine
mapping has a semicontractive character, whereby for some policies it is a
contraction, while for others it is not. In one line of analysis, we impose
assumptions that guarantee that the latter policies cannot be optimal. Under
these assumptions, we prove strong results that resemble those for discounted
Markovian decision problems, such as the uniqueness of solution of Bellman's
equation, and the validity of forms of value and policy iteration. In the
absence of these assumptions, the results are weaker and unusual in character:
the optimal cost function need not be a solution of Bellman's equation, and an
optimal policy may not be found by value or policy iteration. Instead the
optimal cost function over just the contractive policies solves Bellman's
equation, and can be computed by a variety of algorithms."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1509.09257,https://arxiv.org/abs/1509.09257,"Abstract:  We consider minimization of the sum of a large number of convex functions,
and we propose an incremental aggregated version of the proximal algorithm,
which bears similarity to the incremental aggregated gradient and subgradient
methods that have received a lot of recent attention. Under cost function
differentiability and strong convexity assumptions, we show linear convergence
for a sufficiently small constant stepsize. This result also applies to
distributed asynchronous variants of the method, involving bounded
interprocessor communication delays.
We then consider dual versions of incremental proximal algorithms, which are
incremental augmented Lagrangian methods for separable equality-constrained
optimization problems. Contrary to the standard augmented Lagrangian method,
these methods admit decomposition in the minimization of the augmented
Lagrangian, and update the multipliers far more frequently. Our incremental
aggregated augmented Lagrangian methods bear similarity to several known
decomposition algorithms, including the alternating direction method of
multipliers (ADMM) and more recent variations. We compare these methods in
terms of their properties, and highlight their potential advantages and
limitations.
We also address the solution of separable inequality-constrained optimization
problems through the use of nonquadratic augmented Lagrangiias such as the
exponential, and we dually consider a corresponding incremental aggregated
version of the proximal algorithm that uses nonquadratic regularization, such
as an entropy function. We finally propose a closely related linearly
convergent method for minimization of large differentiable sums subject to an
orthant constraint, which may be viewed as an incremental aggregated version of
the mirror descent method."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01030,https://arxiv.org/abs/1507.01030,"Abstract:  We survey incremental methods for minimizing a sum $\sum_{i=1}^mf_i(x)$
consisting of a large number of convex component functions $f_i$. Our methods
consist of iterations applied to single components, and have proved very
effective in practice. We introduce a unified algorithmic framework for a
variety of such methods, some involving gradient and subgradient iterations,
which are known, and some involving combinations of subgradient and proximal
methods, which are new and offer greater flexibility in exploiting the special
structure of $f_i$. We provide an analysis of the convergence and rate of
convergence properties of these methods, including the advantages offered by
randomization in the selection of components. We also survey applications in
inference/machine learning, signal processing, and large-scale and distributed
optimization."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01029,https://arxiv.org/abs/1507.01029,"Abstract:  In this paper we discuss $\l$-policy iteration, a method for exact and
approximate dynamic programming. It is intermediate between the classical value
iteration (VI) and policy iteration (PI) methods, and it is closely related to
optimistic (also known as modified) PI, whereby each policy evaluation is done
approximately, using a finite number of VI. We review the theory of the method
and associated questions of bias and exploration arising in simulation-based
cost function approximation. We then discuss various implementations, which
offer advantages over well-established PI methods that use LSPE($\l$),
LSTD($\l$), or TD($\l$) for policy evaluation with cost function approximation.
One of these implementations is based on a new simulation scheme, called
geometric sampling, which uses multiple short trajectories rather than a single
infinitely long trajectory."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.01026,https://arxiv.org/abs/1507.01026,"Abstract:  In this paper, we consider discrete-time infinite horizon problems of optimal
control to a terminal set of states. These are the problems that are often
taken as the starting point for adaptive dynamic programming. Under very
general assumptions, we establish the uniqueness of solution of Bellman's
equation, and we provide convergence results for value and policy iteration."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1507.00702,https://arxiv.org/abs/1507.00702,"Abstract:  We consider Newton methods for common types of single commodity and
multi-commodity network flow problems. Despite the potentially very large
dimension of the problem, they can be implemented using the conjugate gradient
method and low-dimensional network operations, as shown nearly thirty years
ago. We revisit these methods, compare them to more recent proposals, and
describe how they can be implemented in a distributed computing system. We also
discuss generalizations, including the treatment of arc gains, linear side
constraints, and related special structures."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1308.3814,https://arxiv.org/abs/1308.3814,"Abstract:  We consider stochastic control models with Borel spaces and universally
measurable policies. For such models the standard policy iteration is known to
have difficult measurability issues and cannot be carried out in general. We
present a mixed value and policy iteration method that circumvents this
difficulty. The method allows the use of stationary policies in computing the
optimal cost function, in a manner that resembles policy iteration. It can also
be used to address similar difficulties of policy iteration in the context of
upper and lower semicontinuous models. We analyze the convergence of the method
in infinite horizon total cost problems, for the discounted case where the
one-stage costs are bounded, and for the undiscounted case where the one-stage
costs are nonpositive or nonnegative.
For undiscounted total cost problems with nonnegative one-stage costs, we
also give a new convergence theorem for value iteration, which shows that value
iteration converges whenever it is initialized with a function that is above
the optimal cost function and yet bounded by a multiple of the optimal cost
function. This condition resembles Whittle's bridging condition and is partly
motivated by it. The theorem is also partly motivated by a result of Maitra and
Sudderth, which showed that value iteration, when initialized with the constant
function zero, could require a transfinite number of iterations to converge. We
use the new convergence theorem for value iteration to establish the
convergence of our mixed value and policy iteration method for the nonnegative
cost case."
Dimitri Bertsekas,Bertsekas_Dimitri,arXiv:1207.4154,https://arxiv.org/abs/1207.4154,"Abstract:  In this paper, we propose a new lower approximation scheme for POMDP with
discounted and average cost criterion. The approximating functions are
determined by their values at a finite number of belief points, and can be
computed efficiently using value iteration algorithms for finite-state MDP.
While for discounted problems several lower approximation schemes have been
proposed earlier, ours seems the first of its kind for average cost problems.
We focus primarily on the average cost case, and we show that the corresponding
approximation can be computed efficiently using multi-chain algorithms for
finite-state MDP. We give a preliminary analysis showing that regardless of the
existence of the optimal average cost J in the POMDP, the approximation
obtained is a lower bound of the liminf optimal average cost function, and can
also be used to calculate an upper bound on the limsup optimal average cost
function, as well as bounds on the cost of executing the stationary policy
associated with the approximation. Weshow the convergence of the cost
approximation, when the optimal average cost is constant and the optimal
differential cost is continuous."
Robert Berwick,Berwick_Robert,arXiv:1811.02611,https://arxiv.org/abs/1811.02611,"Abstract:  While long short-term memory (LSTM) neural net architectures are designed to
capture sequence information, human language is generally composed of
hierarchical structures. This raises the question as to whether LSTMs can learn
hierarchical structures. We explore this question with a well-formed bracket
prediction task using two types of brackets modeled by an LSTM. Demonstrating
that such a system is learnable by an LSTM is the first step in demonstrating
that the entire class of CFLs is also learnable. We observe that the model
requires exponential memory in terms of the number of characters and embedded
depth, where a sub-linear memory should suffice. Still, the model does more
than memorize the training input. It learns how to distinguish between relevant
and irrelevant information. On the other hand, we also observe that the model
does not generalize well. We conclude that LSTMs do not learn the relevant
underlying context-free rules, suggesting the good overall performance is
attained rather by an efficient way of evaluating nuisance variables. LSTMs are
a way to quickly reach good results for many natural language tasks, but to
understand and generate natural language one has to investigate other concepts
that can make more direct use of natural language's structural nature."
Robert Berwick,Berwick_Robert,arXiv:1803.09832,https://arxiv.org/abs/1803.09832,"Abstract:  We consider two different data sets of syntactic parameters and we discuss
how to detect relations between parameters through a heat kernel method
developed by Belkin-Niyogi, which produces low dimensional representations of
the data, based on Laplace eigenfunctions, that preserve neighborhood
information. We analyze the different connectivity and clustering structures
that arise in the two datasets, and the regions of maximal variance in the
two-parameter space of the Belkin-Niyogi construction, which identify
preferable choices of independent variables. We compute clustering coefficients
and their variance."
Robert Berwick,Berwick_Robert,arXiv:1712.01719,https://arxiv.org/abs/1712.01719,"Abstract:  Using Phylogenetic Algebraic Geometry, we analyze computationally the
phylogenetic tree of subfamilies of the Indo-European language family, using
data of syntactic structures. The two main sources of syntactic data are the
SSWL database and Longobardi's recent data of syntactic parameters. We compute
phylogenetic invariants and likelihood functions for two sets of Germanic
languages, a set of Romance languages, a set of Slavic languages and a set of
early Indo-European languages, and we compare the results with what is known
through historical linguistics."
Robert Berwick,Berwick_Robert,arXiv:cmp-lg/9503012,https://arxiv.org/abs/cmp-lg/9503012,"Abstract:  In Phys. Rev. Letters (73:2, 5 Dec. 94), Mantegna et al. conclude on the
basis of Zipf rank frequency data that noncoding DNA sequence regions are more
like natural languages than coding regions. We argue on the contrary that an
empirical fit to Zipf's ``law'' cannot be used as a criterion for similarity to
natural languages. Although DNA is a presumably an ``organized system of
signs'' in Mandelbrot's (1961) sense, an observation of statistical features of
the sort presented in the Mantegna et al. paper does not shed light on the
similarity between DNA's ``grammar'' and natural language grammars, just as the
observation of exact Zipf-like behavior cannot distinguish between the
underlying processes of tossing an $M$ sided die or a finite-state branching
process."
Sangeeta Bhatia,Bhatia_Sangeeta,arXiv:1610.00077,https://arxiv.org/abs/1610.00077,"Abstract:  Modellers of large scale genome rearrangement events, in which segments of
DNA are inverted, moved, swapped, or even inserted or deleted, have found a
natural syntax in the language of permutations. Despite this, there has been a
wide range of modelling choices, assumptions and interpretations that make
navigating the literature a significant challenge. Indeed, even authors of
papers that use permutations to model genome rearrangement can struggle to
interpret each others' work, because of subtle differences in basic assumptions
that are often deeply ingrained (and consequently sometimes not even
mentioned). In this paper, we describe the different ways in which permutations
have been used to model genomes and genome rearrangement events, presenting
some features and limitations of each approach, and show how the various models
are related. This paper will help researchers navigate the landscape of genome
rearrangement models, and make it easier for authors to present clear and
consistent models."
Sangeeta Bhatia,Bhatia_Sangeeta,arXiv:1409.7146,https://arxiv.org/abs/1409.7146,"Abstract:  Establishing a distance between genomes is a significant problem in
computational genomics, because its solution can be used to establish
evolutionary relationships including phylogeny.
The ""double cut and join"" (DCJ) model of chromosomal rearrangement proposed
by Yancopoulos et al. has received attention as it can model inversions,
translocations, fusion and fission on a multichromosomal genome that may
contain both linear and circular chromosomes. In this paper, we realize the DCJ
operator as a group action on the space of multichromosomal genomes. We study
this group action, deriving some properties of the group and finding
group-theoretic analogues for the key results in the DCJ theory."
Duane Boning,Boning_Duane,arXiv:1901.04684,https://arxiv.org/abs/1901.04684,"Abstract:  The adversarial training procedure proposed by Madry et al. (2018) is one of
the most effective methods to defend against adversarial examples in deep
neural networks (DNNs). In our paper, we shed some lights on the practicality
and the hardness of adversarial training by showing that the effectiveness
(robustness on test set) of adversarial training has a strong correlation with
the distance between a test point and the manifold of training data embedded by
the network. Test examples that are relatively far away from this manifold are
more likely to be vulnerable to adversarial attacks. Consequentially, an
adversarial training based defense is susceptible to a new class of attacks,
the ""blind-spot attack"", where the input images reside in ""blind-spots"" (low
density regions) of the empirical distribution of training data but is still on
the ground-truth data manifold. For MNIST, we found that these blind-spots can
be easily found by simply scaling and shifting image pixel values. Most
importantly, for large datasets with high dimensional and complex data manifold
(CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training
makes defending on any valid test examples difficult due to the curse of
dimensionality and the scarcity of training data. Additionally, we find that
blind-spots also exist on provable defenses including (Wong & Kolter, 2018) and
(Sinha et al., 2018) because these trainable robustness certificates can only
be practically optimized on a limited set of training data."
Duane Boning,Boning_Duane,arXiv:1804.09699,https://arxiv.org/abs/1804.09699,"Abstract:  Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms Fast-Lin and Fast-Lip that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions Fast-Lin, or by bounding the local
Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network."
Duane Boning,Boning_Duane,arXiv:1703.09876,https://arxiv.org/abs/1703.09876,"Abstract:  In this paper, we propose a novel method to estimate and characterize spatial
variations on dies or wafers. This new technique exploits recent developments
in matrix completion, enabling estimation of spatial variation across wafers or
dies with a small number of randomly picked sampling points while still
achieving fairly high accuracy. This new approach can be easily generalized,
including for estimation of mixed spatial and structure or device type
information."
Guy Bresler,Bresler_Guy,arXiv:1811.10106,https://arxiv.org/abs/1811.10106,"Abstract:  Sparse Principal Component Analysis (SPCA) and Sparse Linear Regression (SLR)
have a wide range of applications and have attracted a tremendous amount of
attention in the last two decades as canonical examples of statistical problems
in high dimension. A variety of algorithms have been proposed for both SPCA and
SLR, but an explicit connection between the two had not been made. We show how
to efficiently transform a black-box solver for SLR into an algorithm for SPCA:
assuming the SLR solver satisfies prediction error guarantees achieved by
existing efficient algorithms such as those based on the Lasso, the SPCA
algorithm derived from it achieves near state of the art guarantees for testing
and for support recovery for the single spiked covariance model as obtained by
the current best polynomialtime algorithms. Our reduction not only highlights
the inherent similarity between the two problems, but also, from a practical
standpoint, allows one to obtain a collection of algorithms for SPCA directly
from known algorithms for SLR. We provide experimental results on simulated
data comparing our proposed framework to other algorithms for SPCA."
Guy Bresler,Bresler_Guy,arXiv:1806.07508,https://arxiv.org/abs/1806.07508,"Abstract:  The prototypical high-dimensional statistics problem entails finding a
structured signal in noise. Many of these problems exhibit an intriguing
phenomenon: the amount of data needed by all known computationally efficient
algorithms far exceeds what is needed for inefficient algorithms that search
over all possible structures. A line of work initiated by Berthet and Rigollet
in 2013 has aimed to explain these statistical-computational gaps by reducing
from conjecturally hard average-case problems in computer science. However, the
delicate nature of average-case reductions has limited the applicability of
this approach. In this work we introduce several new techniques to give a web
of average-case reductions showing strong computational lower bounds based on
the planted clique conjecture using natural problems as intermediates. These
include tight lower bounds for Planted Independent Set, Planted Dense Subgraph,
Sparse Spiked Wigner, Sparse PCA, a subgraph variant of the Stochastic Block
Model and a biased variant of Sparse PCA. We also give algorithms matching our
lower bounds and identify the information-theoretic limits of the models we
consider."
Guy Bresler,Bresler_Guy,arXiv:1805.10262,https://arxiv.org/abs/1805.10262,"Abstract:  Graphical models are a rich language for describing high-dimensional
distributions in terms of their dependence structure. While there are
algorithms with provable guarantees for learning undirected graphical models in
a variety of settings, there has been much less progress in the important
scenario when there are latent variables. Here we study Restricted Boltzmann
Machines (or RBMs), which are a popular model with wide-ranging applications in
dimensionality reduction, collaborative filtering, topic modeling, feature
extraction and deep learning.
The main message of our paper is a strong dichotomy in the feasibility of
learning RBMs, depending on the nature of the interactions between variables:
ferromagnetic models can be learned efficiently, while general models cannot.
In particular, we give a simple greedy algorithm based on influence
maximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn
a description of the distribution on the observed variables as a Markov Random
Field. Our analysis is based on tools from mathematical physics that were
developed to show the concavity of magnetization. Our algorithm extends
straighforwardly to general ferromagnetic Ising models with latent variables.
Conversely, we show that even for a contant number of latent variables with
constant degree, without ferromagneticity the problem is as hard as sparse
parity with noise. This hardness result is based on a sharp and surprising
characterization of the representational power of bounded degree RBMs: the
distribution on their observed variables can simulate any bounded order MRF.
This result is of independent interest since RBMs are the building blocks of
deep belief networks."
Guy Bresler,Bresler_Guy,arXiv:1805.03027,https://arxiv.org/abs/1805.03027,"Abstract:  Most information systems store data by modifying the local state of matter,
in the hope that atomic (or sub-atomic) local interactions would stabilize the
state for a sufficiently long time, thereby allowing later recovery. In this
work we initiate the study of information retention in locally-interacting
systems. The evolution in time of the interacting particles is modeled via the
stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as
the user-controlled input. The output configuration $X_t$ is produced by
running $t$ steps of the Glauber chain. Our main goal is to evaluate the
information capacity $I_n(t)\triangleq\max_{p_{X_0}}I(X_0;X_t)$ when the time
$t$ scales with the size of the system $n$. For the zero-temperature SIM on the
two-dimensional $\sqrt{n}\times\sqrt{n}$ grid and free boundary conditions, it
is easy to show that $I_n(t) = \Theta(n)$ for $t=O(n)$. In addition, we show
that on the order of $\sqrt{n}$ bits can be stored for infinite time in striped
configurations. The $\sqrt{n}$ achievability is optimal when $t\to\infty$ and
$n$ is fixed.
One of the main results of this work is an achievability scheme that stores
more than $\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$)
times. The analysis of the scheme decomposes the system into $\Omega(\sqrt{n})$
independent Z-channels whose crossover probability is found via the (recently
rigorously established) Lifshitz law of phase boundary movement. We also
provide results for the positive but small temperature regime. We show that an
initial configuration drawn according to the Gibbs measure cannot retain more
than a single bit for $t\geq e^{cn^{\frac{1}{4}+\epsilon}}$. On the other hand,
when scaling time with $\beta$, the stripe-based coding scheme (that stores for
infinite time at zero temperature) is shown to retain its bits for time that is
exponential in $\beta$."
Guy Bresler,Bresler_Guy,arXiv:1802.06186,https://arxiv.org/abs/1802.06186,"Abstract:  We study the problem of testing, using only a single sample, between mean
field distributions (like Curie-Weiss, Erdős-Rényi) and structured Gibbs
distributions (like Ising model on sparse graphs and Exponential Random
Graphs). Our goal is to test without knowing the parameter values of the
underlying models: only the \emph{structure} of dependencies is known. We
develop a new approach that applies to both the Ising and Exponential Random
Graph settings based on a general and natural statistical test. The test can
distinguish the hypotheses with high probability above a certain threshold in
the (inverse) temperature parameter, and is optimal in that below the threshold
no test can distinguish the hypotheses.
The thresholds do not correspond to the presence of long-range order in the
models. By aggregating information at a global scale, our test works even at
very high temperatures.
The proofs are based on distributional approximation and sharp concentration
of quadratic forms, when restricted to Hamming spheres. The restriction to
Hamming spheres is necessary, since otherwise any scalar statistic is useless
without explicit knowledge of the temperature parameter. At the same time, this
restriction radically changes the behavior of the functions under
consideration, resulting in a much smaller variance than in the independent
setting; this makes it hard to directly apply standard methods (i.e., Stein's
method) for concentration of weakly dependent variables. Instead, we carry out
an additional tensorization argument using a Markov chain that respects the
symmetry of the Hamming sphere."
Guy Bresler,Bresler_Guy,arXiv:1712.05743,https://arxiv.org/abs/1712.05743,"Abstract:  We develop a new technique, based on Stein's method, for comparing two
stationary distributions of irreducible Markov Chains whose update rules are
`close enough'. We apply this technique to compare Ising models on $d$-regular
expander graphs to the Curie-Weiss model (complete graph) in terms of pairwise
correlations and more generally $k$th order moments. Concretely, we show that
$d$-regular Ramanujan graphs approximate the $k$th order moments of the
Curie-Weiss model to within average error $k/\sqrt{d}$ (averaged over the size
$k$ subsets). The result applies even in the low-temperature regime; we also
derive some simpler approximation results for functionals of Ising models that
hold only at high enough temperatures."
Guy Bresler,Bresler_Guy,arXiv:1711.02198,https://arxiv.org/abs/1711.02198,"Abstract:  We consider an online model for recommendation systems, with each user being
recommended an item at each time-step and providing 'like' or 'dislike'
feedback. A latent variable model specifies the user preferences: both users
and items are clustered into types. All users of a given type have identical
preferences for the items, and similarly, items of a given type are either all
liked or all disliked by a given user. The model captures structure in both the
item and user spaces, and in this paper, we assume that the type preference
matrix is randomly generated. We describe two algorithms inspired by user-user
and item-item collaborative filtering (CF), modified to explicitly make
exploratory recommendations, and prove performance guarantees in terms of their
expected regret. For two regimes of model parameters, with structure only in
item space or only in user space, we prove information-theoretic lower bounds
on regret that match our upper bounds up to logarithmic factors. Our analysis
elucidates system operating regimes in which existing CF algorithms are nearly
optimal."
Guy Bresler,Bresler_Guy,arXiv:1604.06749,https://arxiv.org/abs/1604.06749,"Abstract:  We study the problem of learning a tree Ising model from samples such that
subsequent predictions made using the model are accurate. The prediction task
considered in this paper is that of predicting the values of a subset of
variables given values of some other subset of variables. Virtually all
previous work on graphical model learning has focused on recovering the true
underlying graph. We define a distance (""small set TV"" or ssTV) between
distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$
of a given size, of the total variation between the marginals of $P$ and $Q$ on
$\mathcal{S}$; this distance captures the accuracy of the prediction task of
interest. We derive non-asymptotic bounds on the number of samples needed to
get a distribution (from the same class) with small ssTV relative to the one
generating the samples. One of the main messages of this paper is that far
fewer samples are needed than for recovering the underlying tree, which means
that accurate predictions are possible using the wrong tree."
Guy Bresler,Bresler_Guy,arXiv:1507.05371,https://arxiv.org/abs/1507.05371,"Abstract:  There is much empirical evidence that item-item collaborative filtering works
well in practice. Motivated to understand this, we provide a framework to
design and analyze various recommendation algorithms. The setup amounts to
online binary matrix completion, where at each time a random user requests a
recommendation and the algorithm chooses an entry to reveal in the user's row.
The goal is to minimize regret, or equivalently to maximize the number of +1
entries revealed at any time. We analyze an item-item collaborative filtering
algorithm that can achieve fundamentally better performance compared to
user-user collaborative filtering. The algorithm achieves good ""cold-start""
performance (appropriately defined) by quickly making good recommendations to
new users about whom there is little information."
Guy Bresler,Bresler_Guy,arXiv:1412.1443,https://arxiv.org/abs/1412.1443,"Abstract:  In this paper we investigate the computational complexity of learning the
graph structure underlying a discrete undirected graphical model from i.i.d.
samples. We first observe that the notoriously difficult problem of learning
parities with noise can be captured as a special case of learning graphical
models. This leads to an unconditional computational lower bound of $\Omega
(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree
$d$, for the class of so-called statistical algorithms recently introduced by
Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime
required to exhaustively search over neighborhoods cannot be significantly
improved without restricting the class of models.
Aside from structural assumptions on the graph such as it being a tree,
hypertree, tree-like, etc., many recent papers on structure learning assume
that the model has the correlation decay property. Indeed, focusing on
ferromagnetic Ising models, Bento and Montanari (2009) showed that all known
low-complexity algorithms fail to learn simple graphs when the interaction
strength exceeds a number related to the correlation decay threshold. Our
second set of results gives a class of repelling (antiferromagnetic) models
that have the opposite behavior: very strong interaction allows efficient
learning in time $O(p^2)$. We provide an algorithm whose performance
interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the
repulsion."
Guy Bresler,Bresler_Guy,arXiv:1411.6591,https://arxiv.org/abs/1411.6591,"Abstract:  Despite the prevalence of collaborative filtering in recommendation systems,
there has been little theoretical development on why and how well it works,
especially in the ""online"" setting, where items are recommended to users over
time. We address this theoretical gap by introducing a model for online
recommendation systems, cast item recommendation under the model as a learning
problem, and analyze the performance of a cosine-similarity collaborative
filtering method. In our model, each of $n$ users either likes or dislikes each
of $m$ items. We assume there to be $k$ types of users, and all the users of a
given type share a common string of probabilities determining the chance of
liking each item. At each time step, we recommend an item to each user, where a
key distinction from related bandit literature is that once a user consumes an
item (e.g., watches a movie), then that item cannot be recommended to the same
user again. The goal is to maximize the number of likable items recommended to
users over time. Our main result establishes that after nearly $\log(km)$
initial learning time steps, a simple collaborative filtering algorithm
achieves essentially optimal performance without knowing $k$. The algorithm has
an exploitation step that uses cosine similarity and two types of exploration
steps, one to explore the space of items (standard in the literature) and the
other to explore similarity between users (novel to this work)."
Guy Bresler,Bresler_Guy,arXiv:1411.6156,https://arxiv.org/abs/1411.6156,"Abstract:  We consider the problem of reconstructing the graph underlying an Ising model
from i.i.d. samples. Over the last fifteen years this problem has been of
significant interest in the statistics, machine learning, and statistical
physics communities, and much of the effort has been directed towards finding
algorithms with low computational cost for various restricted classes of
models. Nevertheless, for learning Ising models on general graphs with $p$
nodes of degree at most $d$, it is not known whether or not it is possible to
improve upon the $p^{d}$ computation needed to exhaustively search over all
possible neighborhoods for each node.
In this paper we show that a simple greedy procedure allows to learn the
structure of an Ising model on an arbitrary bounded-degree graph in time on the
order of $p^2$. We make no assumptions on the parameters except what is
necessary for identifiability of the model, and in particular the results hold
at low-temperatures as well as for highly non-uniform models. The proof rests
on a new structural property of Ising models: we show that for any node there
exists at least one neighbor with which it has a high mutual information. This
structural property may be of independent interest."
Guy Bresler,Bresler_Guy,arXiv:1410.7659,https://arxiv.org/abs/1410.7659,"Abstract:  In this paper we consider the problem of learning undirected graphical models
from data generated according to the Glauber dynamics. The Glauber dynamics is
a Markov chain that sequentially updates individual nodes (variables) in a
graphical model and it is frequently used to sample from the stationary
distribution (to which it converges given sufficient time). Additionally, the
Glauber dynamics is a natural dynamical model in a variety of settings. This
work deviates from the standard formulation of graphical model learning in the
literature, where one assumes access to i.i.d. samples from the distribution.
Much of the research on graphical model learning has been directed towards
finding algorithms with low computational cost. As the main result of this
work, we establish that the problem of reconstructing binary pairwise graphical
models is computationally tractable when we observe the Glauber dynamics.
Specifically, we show that a binary pairwise graphical model on $p$ nodes with
maximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function
$f(d)$, using nearly the information-theoretic minimum number of samples."
Guy Bresler,Bresler_Guy,arXiv:1409.3836,https://arxiv.org/abs/1409.3836,"Abstract:  We consider the problem of learning the canonical parameters specifying an
undirected graphical model (Markov random field) from the mean parameters. For
graphical models representing a minimal exponential family, the canonical
parameters are uniquely determined by the mean parameters, so the problem is
feasible in principle. The goal of this paper is to investigate the
computational feasibility of this statistical task. Our main result shows that
parameter estimation is in general intractable: no algorithm can learn the
canonical parameters of a generic pair-wise binary graphical model from the
mean parameters in time bounded by a polynomial in the number of variables
(unless RP = NP). Indeed, such a result has been believed to be true (see the
monograph by Wainwright and Jordan (2008)) but no proof was known.
Our proof gives a polynomial time reduction from approximating the partition
function of the hard-core model, known to be hard, to learning approximate
parameters. Our reduction entails showing that the marginal polytope boundary
has an inherent repulsive property, which validates an optimization procedure
over the polytope that does not use any knowledge of its structure (as required
by the ellipsoid method and others)."
Guy Bresler,Bresler_Guy,arXiv:1303.5678,https://arxiv.org/abs/1303.5678,"Abstract:  We study vector space interference alignment for the MIMO interference
channel with no time or frequency diversity, and no symbol extensions. We prove
both necessary and sufficient conditions for alignment. In particular, we
characterize the feasibility of alignment for the symmetric three-user channel
where all users transmit along d dimensions, all transmitters have M antennas
and all receivers have N antennas, as well as feasibility of alignment for the
fully symmetric (M=N) channel with an arbitrary number of users.
An implication of our results is that the total degrees of freedom available
in a K-user interference channel, using only spatial diversity from the
multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees
of freedom shown to be possible by Cadambe and Jafar with arbitrarily large
time or frequency diversity.
Moving beyond the question of feasibility, we additionally discuss
computation of the number of solutions using Schubert calculus in cases where
there are a finite number of solutions."
Guy Bresler,Bresler_Guy,arXiv:1301.0068,https://arxiv.org/abs/1301.0068,"Abstract:  We present a framework for the design of optimal assembly algorithms for
shotgun sequencing under the criterion of complete reconstruction. We derive a
lower bound on the read length and the coverage depth required for
reconstruction in terms of the repeat statistics of the genome. Building on
earlier works, we design a de Brujin graph based assembly algorithm which can
achieve very close to the lower bound for repeat statistics of a wide range of
sequenced genomes, including the GAGE datasets. The results are based on a set
of necessary and sufficient conditions on the DNA sequence and the reads for
reconstruction. The conditions can be viewed as the shotgun sequencing analogue
of Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by
Hybridization."
Guy Bresler,Bresler_Guy,arXiv:1203.6233,https://arxiv.org/abs/1203.6233,"Abstract:  DNA sequencing is the basic workhorse of modern day biology and medicine.
Shotgun sequencing is the dominant technique used: many randomly located short
fragments called reads are extracted from the DNA sequence, and these reads are
assembled to reconstruct the original sequence. A basic question is: given a
sequencing technology and the statistics of the DNA sequence, what is the
minimum number of reads required for reliable reconstruction? This number
provides a fundamental limit to the performance of {\em any} assembly
algorithm. For a simple statistical model of the DNA sequence and the read
process, we show that the answer admits a critical phenomena in the asymptotic
limit of long DNA sequences: if the read length is below a threshold,
reconstruction is impossible no matter how many reads are observed, and if the
read length is above the threshold, having enough reads to cover the DNA
sequence is sufficient to reconstruct. The threshold is computed in terms of
the Renyi entropy rate of the DNA sequence. We also study the impact of noise
in the read process on the performance."
Guy Bresler,Bresler_Guy,arXiv:1110.5092,https://arxiv.org/abs/1110.5092,"Abstract:  This paper studies vector space interference alignment for the three-user
MIMO interference channel with no time or frequency diversity. The main result
is a characterization of the feasibility of interference alignment in the
symmetric case where all transmitters have M antennas and all receivers have N
antennas. If N >= M and all users desire d transmit dimensions, then alignment
is feasible if and only if (2r+1)d <= max(rN,(r+1)M) for all nonnegative
integers r. The analogous result holds with M and N switched if M >= N.
It turns out that, just as for the 3-user parallel interference channel
\cite{BT09}, the length of alignment paths captures the essence of the problem.
In fact, for each feasible value of M and N the maximum alignment path length
dictates both the converse and achievability arguments.
One of the implications of our feasibility criterion is that simply counting
equations and comparing to the number of variables does not predict
feasibility. Instead, a more careful investigation of the geometry of the
alignment problem is required. The necessary condition obtained by counting
equations is implied by our new feasibility criterion."
Guy Bresler,Bresler_Guy,arXiv:1104.0888,https://arxiv.org/abs/1104.0888,"Abstract:  Determining the feasibility conditions for vector space interference
alignment in the K-user MIMO interference channel with constant channel
coefficients has attracted much recent attention yet remains unsolved. The main
result of this paper is restricted to the symmetric square case where all
transmitters and receivers have N antennas, and each user desires d transmit
dimensions. We prove that alignment is possible if and only if the number of
antennas satisfies N>= d(K+1)/2. We also show a necessary condition for
feasibility of alignment with arbitrary system parameters. An algebraic
geometry approach is central to the results."
Guy Bresler,Bresler_Guy,arXiv:0812.2265,https://arxiv.org/abs/0812.2265,"Abstract:  Exponential random graphs are used extensively in the sociology literature.
This model seeks to incorporate in random graphs the notion of reciprocity,
that is, the larger than expected number of triangles and other small
subgraphs. Sampling from these distributions is crucial for parameter
estimation hypothesis testing, and more generally for understanding basic
features of the network model itself. In practice sampling is typically carried
out using Markov chain Monte Carlo, in particular either the Glauber dynamics
or the Metropolis-Hasting procedure.
In this paper we characterize the high and low temperature regimes of the
exponential random graph model. We establish that in the high temperature
regime the mixing time of the Glauber dynamics is $\Theta(n^2 \log n)$, where
$n$ is the number of vertices in the graph; in contrast, we show that in the
low temperature regime the mixing is exponentially slow for any local Markov
chain. Our results, moreover, give a rigorous basis for criticisms made of such
models. In the high temperature regime, where sampling with MCMC is possible,
we show that any finite collection of edges are asymptotically independent;
thus, the model does not possess the desired reciprocity property, and is not
appreciably different from the Erdős-Rényi random graph."
Guy Bresler,Bresler_Guy,arXiv:0809.3554,https://arxiv.org/abs/0809.3554,"Abstract:  Recently, Etkin, Tse, and Wang found the capacity region of the two-user
Gaussian interference channel to within one bit/s/Hz. A natural goal is to
apply this approach to the Gaussian interference channel with an arbitrary
number of users. We make progress towards this goal by finding the capacity
region of the many-to-one and one-to-many Gaussian interference channels to
within a constant number of bits. The result makes use of a deterministic model
to provide insight into the Gaussian channel. The deterministic model makes
explicit the dimension of signal scale. A central theme emerges: the use of
lattice codes for alignment of interfering signals on the signal scale."
Guy Bresler,Bresler_Guy,arXiv:0807.3222,https://arxiv.org/abs/0807.3222,"Abstract:  This paper explores the two-user Gaussian interference channel through the
lens of a natural deterministic channel model. The main result is that the
deterministic channel uniformly approximates the Gaussian channel, the capacity
regions differing by a universal constant. The problem of finding the capacity
of the Gaussian channel to within a constant error is therefore reduced to that
of finding the capacity of the far simpler deterministic channel. Thus, the
paper provides an alternative derivation of the recent constant gap capacity
characterization of Etkin, Tse, and Wang. Additionally, the deterministic model
gives significant insight towards the Gaussian channel."
Guy Bresler,Bresler_Guy,arXiv:0712.1402,https://arxiv.org/abs/0712.1402,"Abstract:  Markov random fields are used to model high dimensional distributions in a
number of applied areas. Much recent interest has been devoted to the
reconstruction of the dependency structure from independent samples from the
Markov random fields. We analyze a simple algorithm for reconstructing the
underlying graph defining a Markov random field on $n$ nodes and maximum degree
$d$ given observations. We show that under mild non-degeneracy conditions it
reconstructs the generating graph with high probability using $\Theta(d
\epsilon^{-2}\delta^{-4} \log n)$ samples where $\epsilon,\delta$ depend on the
local interactions. For most local interaction $\eps,\delta$ are of order
$\exp(-O(d))$.
Our results are optimal as a function of $n$ up to a multiplicative constant
depending on $d$ and the strength of the local interactions. Our results seem
to be the first results for general models that guarantee that {\em the}
generating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}
\epsilon^{-2}\delta^{-4} \log n)$ running time bound. In cases where the
measure on the graph has correlation decay, the running time is $O(n^2 \log n)$
for all fixed $d$. We also discuss the effect of observing noisy samples and
show that as long as the noise level is low, our algorithm is effective. On the
other hand, we construct an example where large noise implies
non-identifiability even for generic noise and interactions. Finally, we
briefly show that in some simple cases, models with hidden nodes can also be
recovered."
Tamara Broderick,Broderick_Tamara,arXiv:1811.11790,https://arxiv.org/abs/1811.11790,"Abstract:  Until recently, transcriptomics was limited to bulk RNA sequencing, obscuring
the underlying expression patterns of individual cells in favor of a global
average. Thanks to technological advances, we can now profile gene expression
across thousands or millions of individual cells in parallel. This new type of
data has led to the intriguing discovery that individual cell profiles can
reflect the imprint of time or dynamic processes. However, synthesizing this
information to reconstruct dynamic biological phenomena from data that are
noisy, heterogenous, and sparse---and from processes that may unfold
asynchronously---poses a complex computational and statistical challenge. Here,
we develop a full generative model for probabilistically reconstructing trees
of cellular differentiation from single-cell RNA-seq data. Specifically, we
extend the framework of the classical Dirichlet diffusion tree to
simultaneously infer branch topology and latent cell states along continuous
trajectories over the full tree. In tandem, we construct a novel Markov chain
Monte Carlo sampler that interleaves Metropolis-Hastings and message passing to
leverage model structure for efficient inference. Finally, we demonstrate that
these techniques can recover latent trajectories from simulated single-cell
transcriptomes. While this work is motivated by cellular differentiation, we
derive a tractable model that provides flexible densities for any data (coupled
with an appropriate noise model) that arise from continuous evolution along a
latent nonparametric tree."
Tamara Broderick,Broderick_Tamara,arXiv:1810.06587,https://arxiv.org/abs/1810.06587,"Abstract:  A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment. However, like all Bayesian approaches, BNP requires the
specification of a prior. In practice, it is important to quantitatively
establish that the prior is not too informative, particularly when the
particular form of the prior is chosen for mathematical convenience rather than
because of a considered subjective belief.
We derive local sensitivity measures for a truncated variational Bayes (VB)
approximation and approximate nonlinear dependence of a VB optimum on prior
parameters using a local Taylor series approximation. Using a stick-breaking
representation of a Dirichlet process, we consider perturbations both to the
scalar concentration parameter and to the functional form of the stick-
breaking distribution.
Unlike previous work on local Bayesian sensitivity for BNP, we pay special
attention to the ability of our sensitivity measures to extrapolate to
different priors, rather than treating the sensitivity as a measure of
robustness per se. Extrapolation motivates the use of multiplicative
perturbations to the functional form of the prior for VB. Additionally, we
linearly approximate only the computationally intensive part of inference --
the optimization of the global parameters -- and retain the nonlinearity of
easily computed quantities as functions of the global parameters.
We apply our methods to estimate sensitivity of the expected number of
distinct clusters present in the Iris dataset to the BNP prior specification.
We evaluate the accuracy of our approximations by comparing to the much more
expensive process of re-fitting the model."
Tamara Broderick,Broderick_Tamara,arXiv:1810.04249,https://arxiv.org/abs/1810.04249,"Abstract:  Kernel methods offer the flexibility to learn complex relationships in
modern, large data sets while enjoying strong theoretical guarantees on
quality. Unfortunately, these methods typically require cubic running time in
the data set size, a prohibitive cost in the large-data setting. Random feature
maps (RFMs) and the Nystrom method both consider low-rank approximations to the
kernel matrix as a potential solution. But, in order to achieve desirable
theoretical guarantees, the former may require a prohibitively large number of
features J+, and the latter may be prohibitively expensive for high-dimensional
problems. We propose to combine the simplicity and generality of RFMs with a
data-dependent feature selection scheme to achieve desirable theoretical
approximation properties of Nystrom with just O(log J+) features. Our key
insight is to begin with a large set of random features, then reduce them to a
small number of weighted features in a data-dependent, computationally
efficient way, while preserving the statistical guarantees of using the
original large set of features. We demonstrate the efficacy of our method with
theory and experiments--including on a data set with over 50 million
observations. In particular, we show that our method achieves small kernel
matrix approximation error and better test set accuracy with provably fewer
random features than state- of-the-art methods."
Tamara Broderick,Broderick_Tamara,arXiv:1809.09505,https://arxiv.org/abs/1809.09505,"Abstract:  Bayesian inference typically requires the computation of an approximation to
the posterior distribution. An important requirement for an approximate
Bayesian inference algorithm is to output high-accuracy posterior mean and
uncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain
Monte Carlo, remain the gold standard for approximate Bayesian inference
because they have a robust finite-sample theory and reliable convergence
diagnostics. However, alternative methods, which are more scalable or apply to
problems where Markov Chain Monte Carlo cannot be used, lack the same
finite-data approximation theory and tools for evaluating their accuracy. In
this work, we develop a flexible new approach to bounding the error of mean and
uncertainty estimates of scalable inference algorithms. Our strategy is to
control the estimation errors in terms of Wasserstein distance, then bound the
Wasserstein distance via a generalized notion of Fisher distance. Unlike
computing the Wasserstein distance, which requires access to the normalized
posterior distribution, the Fisher distance is tractable to compute because it
requires access only to the gradient of the log posterior density. We
demonstrate the usefulness of our Fisher distance approach by deriving bounds
on the Wasserstein error of the Laplace approximation and Hilbert coresets. We
anticipate that our approach will be applicable to many other approximate
inference methods such as the integrated Laplace approximation, variational
inference, and approximate Bayesian computation"
Tamara Broderick,Broderick_Tamara,arXiv:1806.10234,https://arxiv.org/abs/1806.10234,"Abstract:  Gaussian processes (GPs) offer a flexible class of priors for nonparametric
Bayesian regression, but popular GP posterior inference methods are typically
prohibitively slow or lack desirable finite-data guarantees on quality. We
develop an approach to scalable approximate GP regression with finite-data
guarantees on the accuracy of pointwise posterior mean and variance estimates.
Our main contribution is a novel objective for approximate inference in the
nonparametric setting: the preconditioned Fisher (pF) divergence. We show that
unlike the Kullback--Leibler divergence (used in variational inference), the pF
divergence bounds the 2-Wasserstein distance, which in turn provides tight
bounds the pointwise difference of the mean and variance functions. We
demonstrate that, for sparse GP likelihood approximations, we can minimize the
pF divergence efficiently. Our experiments show that optimizing the pF
divergence has the same computational requirements as variational sparse GPs
while providing comparable empirical performance--in addition to our novel
finite-data quality guarantees."
Tamara Broderick,Broderick_Tamara,arXiv:1806.00550,https://arxiv.org/abs/1806.00550,"Abstract:  The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data. The ubiquitous tools of cross-validation (CV) and the bootstrap are
examples of this technique. These methods are powerful in large part due to
their model agnosticism but can be slow to run on modern, large data sets due
to the need to repeatedly re-fit the model. In this work, we use a linear
approximation to the dependence of the fitting procedure on the weights,
producing results that can be faster than repeated re-fitting by orders of
magnitude. This linear approximation is sometimes known as the ""infinitesimal
jackknife"" in the statistics literature, where it is mostly used to as a
theoretical tool to prove asymptotic results. We provide explicit finite-sample
error bounds for the infinitesimal jackknife in terms of a small number of
simple, verifiable assumptions. Our results apply whether the weights and data
are stochastic, deterministic, or even adversarially chosen, and so can be used
as a tool for proving the accuracy of the infinitesimal jackknife on a wide
variety of problems. As a corollary, we state mild regularity conditions under
which our approximation consistently estimates true leave-k-out
cross-validation for any fixed k. These theoretical results, together with
modern automatic differentiation software, support the application of the
infinitesimal jackknife to a wide variety of practical problems in machine
learning, providing a ""Swiss Army infinitesimal jackknife."" We demonstrate the
accuracy of our methods on a range of simulated and real datasets."
Tamara Broderick,Broderick_Tamara,arXiv:1803.05554,https://arxiv.org/abs/1803.05554,"Abstract:  Learning a Bayesian network (BN) from data can be useful for decision-making
or discovering causal relationships. However, traditional methods often fail in
modern applications, which exhibit a larger number of observed variables than
data points. The resulting uncertainty about the underlying network as well as
the desire to incorporate prior information recommend a Bayesian approach to
learning the BN, but the highly combinatorial structure of BNs poses a striking
challenge for inference. The current state-of-the-art methods such as order
MCMC are faster than previous methods but prevent the use of many natural
structural priors and still have running time exponential in the maximum
indegree of the true directed acyclic graph (DAG) of the BN. We here propose an
alternative posterior approximation based on the observation that, if we
incorporate empirical conditional independence tests, we can focus on a
high-probability DAG associated with each order of the vertices. We show that
our method allows the desired flexibility in prior specification, removes
timing dependence on the maximum indegree and yields provably good posterior
approximations; in addition, we show that it achieves superior accuracy,
scalability, and sampler mixing on several datasets."
Tamara Broderick,Broderick_Tamara,arXiv:1802.01737,https://arxiv.org/abs/1802.01737,"Abstract:  Coherent uncertainty quantification is a key strength of Bayesian methods.
But modern algorithms for approximate Bayesian posterior inference often
sacrifice accurate posterior uncertainty estimation in the pursuit of
scalability. This work shows that previous Bayesian coreset construction
algorithms---which build a small, weighted subset of the data that approximates
the full dataset---are no exception. We demonstrate that these algorithms scale
the coreset log-likelihood suboptimally, resulting in underestimated posterior
uncertainty. To address this shortcoming, we develop greedy iterative geodesic
ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales
the coreset log-likelihood optimally. GIGA provides geometric decay in
posterior approximation error as a function of coreset size, and maintains the
fast running time of its predecessors. The paper concludes with validation of
GIGA on both synthetic and real datasets, demonstrating that it reduces
posterior approximation error by orders of magnitude compared with previous
coreset constructions."
Tamara Broderick,Broderick_Tamara,arXiv:1712.01435,https://arxiv.org/abs/1712.01435,"Abstract:  Clustering procedures typically estimate which data points are clustered
together, a quantity of primary importance in many analyses. Often used as a
preliminary step for dimensionality reduction or to facilitate interpretation,
finding robust and stable clusters is often crucial for appropriate for
downstream analysis. In the present work, we consider Bayesian nonparametric
(BNP) models, a particularly popular set of Bayesian models for clustering due
to their flexibility. Because of its complexity, the Bayesian posterior often
cannot be computed exactly, and approximations must be employed. Mean-field
variational Bayes forms a posterior approximation by solving an optimization
problem and is widely used due to its speed. An exact BNP posterior might vary
dramatically when presented with different data. As such, stability and
robustness of the clustering should be assessed.
A popular mean to assess stability is to apply the bootstrap by resampling
the data, and rerun the clustering for each simulated data set. The time cost
is thus often very expensive, especially for the sort of exploratory analysis
where clustering is typically used. We propose to use a fast and automatic
approximation to the full bootstrap called the ""linear bootstrap"", which can be
seen by local data perturbation. In this work, we demonstrate how to apply this
idea to a data analysis pipeline, consisting of an MFVB approximation to a BNP
clustering posterior of time course gene expression data. We show that using
auto-differentiation tools, the necessary calculations can be done
automatically, and that the linear bootstrap is a fast but approximate
alternative to the bootstrap."
Tamara Broderick,Broderick_Tamara,arXiv:1710.05053,https://arxiv.org/abs/1710.05053,"Abstract:  The automation of posterior inference in Bayesian data analysis has enabled
experts and nonexperts alike to use more sophisticated models, engage in faster
exploratory modeling and analysis, and ensure experimental reproducibility.
However, standard automated posterior inference algorithms are not tractable at
the scale of massive modern datasets, and modifications to make them so are
typically model-specific, require expert tuning, and can break theoretical
guarantees on inferential quality. Building on the Bayesian coresets framework,
this work instead takes advantage of data redundancy to shrink the dataset
itself as a preprocessing step, providing fully-automated, scalable Bayesian
inference with theoretical guarantees. We begin with an intuitive reformulation
of Bayesian coreset construction as sparse vector sum approximation, and
demonstrate that its automation and performance-based shortcomings arise from
the use of the supremum norm. To address these shortcomings we develop Hilbert
coresets, i.e., Bayesian coresets constructed under a norm induced by an
inner-product on the log-likelihood function space. We propose two Hilbert
coreset construction algorithms---one based on importance sampling, and one
based on the Frank-Wolfe algorithm---along with theoretical guarantees on
approximation quality as a function of coreset size. Since the exact
computation of the proposed inner-products is model-specific, we automate the
construction with a random finite-dimensional projection of the log-likelihood
functions. The resulting automated coreset construction algorithm is simple to
implement, and experiments on a variety of models with real and synthetic
datasets show that it provides high-quality posterior approximations and a
significant reduction in the computational cost of inference."
Tamara Broderick,Broderick_Tamara,arXiv:1709.09216,https://arxiv.org/abs/1709.09216,"Abstract:  Generalized linear models (GLMs) -- such as logistic regression, Poisson
regression, and robust regression -- provide interpretable models for diverse
data types. Probabilistic approaches, particularly Bayesian ones, allow
coherent estimates of uncertainty, incorporation of prior information, and
sharing of power across experiments via hierarchical models. In practice,
however, the approximate Bayesian methods necessary for inference have either
failed to scale to large data sets or failed to provide theoretical guarantees
on the quality of inference. We propose a new approach based on constructing
polynomial approximate sufficient statistics for GLMs (PASS-GLM). We
demonstrate that our method admits a simple algorithm as well as trivial
streaming and distributed extensions that do not compound error across
computations. We provide theoretical guarantees on the quality of point (MAP)
estimates, the approximate posterior, and posterior mean and uncertainty
estimates. We validate our approach empirically in the case of logistic
regression using a quadratic approximation and show competitive performance
with stochastic gradient descent, MCMC, and the Laplace approximation in terms
of speed and multiple measures of accuracy -- including on an advertising data
set with 40 million data points and 20,000 covariates."
Tamara Broderick,Broderick_Tamara,arXiv:1709.02536,https://arxiv.org/abs/1709.02536,"Abstract:  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC."
Tamara Broderick,Broderick_Tamara,arXiv:1612.05519,https://arxiv.org/abs/1612.05519,"Abstract:  Many popular network models rely on the assumption of (vertex)
exchangeability, in which the distribution of the graph is invariant to
relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that
these graphs are dense or empty with probability one, whereas many real-world
graphs are sparse. We present an alternative notion of exchangeability for
random graphs, which we call edge exchangeability, in which the distribution of
a graph sequence is invariant to the order of the edges. We demonstrate that
edge-exchangeable models, unlike models that are traditionally vertex
exchangeable, can exhibit sparsity. To do so, we outline a general framework
for graph generative models; by contrast to the pioneering work of Caron and
Fox (2015), models within our framework are stationary across steps of the
graph sequence. In particular, our model grows the graph by instantiating more
latent atoms of a single random measure as the dataset size increases, rather
than adding new atoms to the measure."
Tamara Broderick,Broderick_Tamara,arXiv:1611.07469,https://arxiv.org/abs/1611.07469,"Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a
prior and a likelihood. One hopes that the posterior is robust to reasonable
variation in the choice of prior, since this choice is made by the modeler and
is often somewhat subjective. A different, equally subjectively plausible
choice of prior may result in a substantially different posterior, and so
different conclusions drawn from the data. Were this to be the case, our
conclusions would not be robust to the choice of prior. To determine whether
our model is robust, we must quantify how sensitive our posterior is to
perturbations of our prior. Despite the importance of the problem and a
considerable body of literature, generic, easy-to-use methods to quantify
Bayesian robustness are still lacking.
Abstract In this paper, we demonstrate that powerful measures of robustness
can be easily calculated from Variational Bayes (VB) approximate posteriors. We
begin with local robustness, which measures the effect of infinitesimal changes
to the prior on a posterior mean of interest. In particular, we show that the
influence function of Gustafson (2012) has a simple, easy-to-calculate closed
form expression for VB approximations. We then demonstrate how local robustness
measures can be inadequate for non-local prior changes, such as replacing one
prior entirely with another. We propose a simple approximate non-local
robustness measure and demonstrate its effectiveness on a simulated data set."
Tamara Broderick,Broderick_Tamara,arXiv:1611.05559,https://arxiv.org/abs/1611.05559,"Abstract:  Variational inference (VI) provides fast approximations of a Bayesian
posterior in part because it formulates posterior approximation as an
optimization problem: to find the closest distribution to the exact posterior
over some family of distributions. For practical reasons, the family of
distributions in VI is usually constrained so that it does not include the
exact posterior, even as a limit point. Thus, no matter how long VI is run, the
resulting approximation will not approach the exact posterior. We propose to
instead consider a more flexible approximating family consisting of all
possible finite mixtures of a parametric base distribution (e.g., Gaussian).
For efficient inference, we borrow ideas from gradient boosting to develop an
algorithm we call boosting variational inference (BVI). BVI iteratively
improves the current approximation by mixing it with a new component from the
base distribution family and thereby yields progressively more accurate
posterior approximations as more computing time is spent. Unlike a number of
common VI variants including mean-field VI, BVI is able to capture
multimodality, general posterior covariance, and nonstandard posterior shapes."
Tamara Broderick,Broderick_Tamara,arXiv:1609.09147,https://arxiv.org/abs/1609.09147,"Abstract:  Trait allocations are a class of combinatorial structures in which data may
belong to multiple groups and may have different levels of belonging in each
group. Often the data are also exchangeable, i.e., their joint distribution is
invariant to reordering. In clustering---a special case of trait
allocation---exchangeability implies the existence of both a de Finetti
representation and an exchangeable partition probability function (EPPF),
distributional representations useful for computational and theoretical
purposes. In this work, we develop the analogous de Finetti representation and
exchangeable trait probability function (ETPF) for trait allocations, along
with a characterization of all trait allocations with an ETPF. Unlike previous
feature allocation characterizations, our proofs fully capture
single-occurrence ""dust"" groups. We further introduce a novel constrained
version of the ETPF that we use to establish an intuitive connection between
the probability functions for clustering, feature allocations, and trait
allocations. As an application of our general theory, we characterize the
distribution of all edge-exchangeable graphs, a class of recently-developed
models that captures realistic sparse graph sequences."
Tamara Broderick,Broderick_Tamara,arXiv:1606.07153,https://arxiv.org/abs/1606.07153,"Abstract:  Bayesian hierarchical models are increasing popular in economics. When using
hierarchical models, it is useful not only to calculate posterior expectations,
but also to measure the robustness of these expectations to reasonable
alternative prior choices. We use variational Bayes and linear response methods
to provide fast, accurate posterior means and robustness measures with an
application to measuring the effectiveness of microcredit in the developing
world."
Tamara Broderick,Broderick_Tamara,arXiv:1605.06423,https://arxiv.org/abs/1605.06423,"Abstract:  The use of Bayesian methods in large-scale data settings is attractive
because of the rich hierarchical models, uncertainty quantification, and prior
specification they provide. Standard Bayesian inference algorithms are
computationally expensive, however, making their direct application to large
datasets difficult or infeasible. Recent work on scaling Bayesian inference has
focused on modifying the underlying algorithms to, for example, use only a
random data subsample at each iteration. We leverage the insight that data is
often redundant to instead obtain a weighted subset of the data (called a
coreset) that is much smaller than the original dataset. We can then use this
small coreset in any number of existing posterior inference algorithms without
modification. In this paper, we develop an efficient coreset construction
algorithm for Bayesian logistic regression models. We provide theoretical
guarantees on the size and approximation quality of the coreset -- both for
fixed, known datasets, and in expectation for a wide class of data generative
models. Crucially, the proposed approach also permits efficient construction of
the coreset in both streaming and parallel settings, with minimal additional
effort. We demonstrate the efficacy of our approach on a number of synthetic
and real-world datasets, and find that, in practice, the size of the coreset is
independent of the original dataset size. Furthermore, constructing the coreset
takes a negligible amount of time compared to that required to run MCMC on it."
Tamara Broderick,Broderick_Tamara,arXiv:1603.06915,https://arxiv.org/abs/1603.06915,"Abstract:  Network data appear in a number of applications, such as online social
networks and biological networks, and there is growing interest in both
developing models for networks as well as studying the properties of such data.
Since individual network datasets continue to grow in size, it is necessary to
develop models that accurately represent the real-life scaling properties of
networks. One behavior of interest is having a power law in the degree
distribution. However, other types of power laws that have been observed
empirically and considered for applications such as clustering and feature
allocation models have not been studied as frequently in models for graph data.
In this paper, we enumerate desirable asymptotic behavior that may be of
interest for modeling graph data, including sparsity and several types of power
laws. We outline a general framework for graph generative models using
completely random measures; by contrast to the pioneering work of Caron and Fox
(2015), we consider instantiating more of the existing atoms of the random
measure as the dataset size increases rather than adding new atoms to the
measure. We see that these two models can be complementary; they respectively
yield interpretations as (1) time passing among existing members of a network
and (2) new individuals joining a network. We detail a particular instance of
this framework and show simulated results that suggest this model exhibits some
desirable asymptotic power-law behavior."
Tamara Broderick,Broderick_Tamara,arXiv:1603.06898,https://arxiv.org/abs/1603.06898,"Abstract:  A known failing of many popular random graph models is that the Aldous-Hoover
Theorem guarantees these graphs are dense with probability one; that is, the
number of edges grows quadratically with the number of nodes. This behavior is
considered unrealistic in observed graphs. We define a notion of edge
exchangeability for random graphs in contrast to the established notion of
infinite exchangeability for random graphs --- which has traditionally relied
on exchangeability of nodes (rather than edges) in a graph. We show that,
unlike node exchangeability, edge exchangeability encompasses models that are
known to provide a projective sequence of random graphs that circumvent the
Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the
number of edges with the number of nodes. We show how edge-exchangeability of
graphs relates naturally to existing notions of exchangeability from clustering
(a.k.a. partitions) and other familiar combinatorial structures."
Tamara Broderick,Broderick_Tamara,arXiv:1603.00861,https://arxiv.org/abs/1603.00861,"Abstract:  Completely random measures (CRMs) and their normalizations are a rich source
of Bayesian nonparametric priors. Examples include the beta, gamma, and
Dirichlet processes. In this paper we detail two major classes of sequential
CRM representations---series representations and superposition
representations---within which we organize both novel and existing sequential
representations that can be used for simulation and posterior inference. These
two classes and their constituent representations subsume existing ones that
have previously been developed in an ad hoc manner for specific processes.
Since a complete infinite-dimensional CRM cannot be used explicitly for
computation, sequential representations are often truncated for tractability.
We provide truncation error analyses for each type of sequential
representation, as well as their normalized versions, thereby generalizing and
improving upon existing truncation error bounds in the literature. We analyze
the computational complexity of the sequential representations, which in
conjunction with our error bounds allows us to directly compare representations
and discuss their relative efficiency. We include numerous applications of our
theoretical results to commonly-used (normalized) CRMs, demonstrating that our
results enable a straightforward representation and analysis of CRMs that has
not previously been available in a Bayesian nonparametric context."
Tamara Broderick,Broderick_Tamara,arXiv:1512.02578,https://arxiv.org/abs/1512.02578,"Abstract:  In Bayesian analysis, the posterior follows from the data and a choice of a
prior and a likelihood. One hopes that the posterior is robust to reasonable
variation in the choice of prior and likelihood, since this choice is made by
the modeler and is necessarily somewhat subjective. Despite the fundamental
importance of the problem and a considerable body of literature, the tools of
robust Bayes are not commonly used in practice. This is in large part due to
the difficulty of calculating robustness measures from MCMC draws. Although
methods for computing robustness measures from MCMC draws exist, they lack
generality and often require additional coding or computation.
In contrast to MCMC, variational Bayes (VB) techniques are readily amenable
to robustness analysis. The derivative of a posterior expectation with respect
to a prior or data perturbation is a measure of local robustness to the prior
or likelihood. Because VB casts posterior inference as an optimization problem,
its methodology is built on the ability to calculate derivatives of posterior
quantities with respect to model parameters, even in very complex models. In
the present work, we develop local prior robustness measures for mean-field
variational Bayes(MFVB), a VB technique which imposes a particular
factorization assumption on the variational posterior approximation. We start
by outlining existing local prior measures of robustness. Next, we use these
results to derive closed-form measures of the sensitivity of mean-field
variational posterior approximation to prior specification. We demonstrate our
method on a meta-analysis of randomized controlled interventions in access to
microcredit in developing countries."
Tamara Broderick,Broderick_Tamara,arXiv:1512.01229,https://arxiv.org/abs/1512.01229,"Abstract:  This article is a translation of Bruno de Finetti's paper ""Funzione
Caratteristica di un fenomeno aleatorio"" which appeared in Atti del Congresso
Internazionale dei Matematici, Bologna 3-10 Settembre 1928, Tomo VI, pp.
179-190, originally published by Nicola Zanichelli Editore S.p.A. The
translation was made as close as possible to the original in form and style,
except for apparent mistakes found in the original document, which were
corrected and are mentioned as footnotes. Most of these were resolved by
comparing against a longer version of this work by de Finetti, published
shortly after this one under the same titlea. The interested reader is highly
encouraged to consult this other version for a more detailed treatment of the
topics covered here. Footnotes regarding the translation are labeled with
letters to distinguish them from de Finetti's original footnotes."
Tamara Broderick,Broderick_Tamara,arXiv:1506.04088,https://arxiv.org/abs/1506.04088,"Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance.
We generalize linear response methods from statistical physics to deliver
accurate uncertainty estimates for model variables---both for individual
variables and coherently across variables. We call our method linear response
variational Bayes (LRVB). When the MFVB posterior approximation is in the
exponential family, LRVB has a simple, analytic form, even for non-conjugate
models. Indeed, we make no assumptions about the form of the true posterior. We
demonstrate the accuracy and scalability of our method on a range of models for
both simulated and real data."
Tamara Broderick,Broderick_Tamara,arXiv:1502.07685,https://arxiv.org/abs/1502.07685,"Abstract:  Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance. We develop a fast, general methodology for exponential
families that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We also show how LRVB can be
used to quickly calculate a measure of the influence of individual data points
on parameter point estimates. We demonstrate the accuracy and scalability of
our method by learning Gaussian mixture models for both simulated and real
data."
Tamara Broderick,Broderick_Tamara,arXiv:1410.6853,https://arxiv.org/abs/1410.6853,"Abstract:  Mean Field Variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is its (sometimes severe) underestimates of
the uncertainty of model variables and lack of information about model variable
covariance. We develop a fast, general methodology for exponential families
that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We demonstrate the accuracy of
our method on simulated data sets."
Tamara Broderick,Broderick_Tamara,arXiv:1410.6843,https://arxiv.org/abs/1410.6843,"Abstract:  We demonstrate how to calculate posteriors for general CRM-based priors and
likelihoods for Bayesian nonparametric models. We further show how to represent
Bayesian nonparametric priors as a sequence of finite draws using a
size-biasing approach---and how to represent full Bayesian nonparametric models
via finite marginals. Motivated by conjugate priors based on exponential family
representations of likelihoods, we introduce a notion of exponential families
for CRMs, which we call exponential CRMs. This construction allows us to
specify automatic Bayesian nonparametric conjugate priors for exponential CRM
likelihoods. We demonstrate that our exponential CRMs allow particularly
straightforward recipes for size-biased and marginal representations of
Bayesian nonparametric models. Along the way, we prove that the gamma process
is a conjugate prior for the Poisson likelihood process and the beta prime
process is a conjugate prior for a process we call the odds Bernoulli process.
We deliver a size-biased representation of the gamma process and a marginal
representation of the gamma process coupled with a Poisson likelihood process."
Tamara Broderick,Broderick_Tamara,arXiv:1410.4792,https://arxiv.org/abs/1410.4792,"Abstract:  Bayesian entity resolution merges together multiple, noisy databases and
returns the minimal collection of unique individuals represented, together with
their true, latent record values. Bayesian methods allow flexible generative
models that share power across databases as well as principled quantification
of uncertainty for queries of the final, resolved database. However, existing
Bayesian methods for entity resolution use Markov monte Carlo method (MCMC)
approximations and are too slow to run on modern databases containing millions
or billions of records. Instead, we propose applying variational approximations
to allow scalable Bayesian inference in these models. We derive a
coordinate-ascent approximation for mean-field variational Bayes, qualitatively
compare our algorithm to existing methods, note unique challenges for inference
that arise from the expected distribution of cluster sizes in entity
resolution, and discuss directions for future work in this domain."
Tamara Broderick,Broderick_Tamara,arXiv:1307.8049,https://arxiv.org/abs/1307.8049,"Abstract:  Research on distributed machine learning algorithms has focused primarily on
one of two extremes - algorithms that obey strict concurrency constraints or
algorithms that obey few or no such constraints. We consider an intermediate
alternative in which algorithms optimistically assume that conflicts are
unlikely and if conflicts do arise a conflict-resolution protocol is invoked.
We view this ""optimistic concurrency control"" paradigm as particularly
appropriate for large-scale machine learning algorithms, particularly in the
unsupervised setting. We demonstrate our approach in three problem areas:
clustering, feature learning and online facility location. We evaluate our
methods via large-scale experiments in a cluster computing environment."
Tamara Broderick,Broderick_Tamara,arXiv:1307.6769,https://arxiv.org/abs/1307.6769,"Abstract:  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,
(A)synchronous computation of a Bayesian posterior. The framework makes
streaming updates to the estimated posterior according to a user-specified
approximation batch primitive. We demonstrate the usefulness of our framework,
with variational Bayes (VB) as the primitive, by fitting the latent Dirichlet
allocation model to two large-scale document collections. We demonstrate the
advantages of our algorithm over stochastic variational inference (SVI) by
comparing the two after a single pass through a known amount of data---a case
where SVI may be applied---and in the streaming setting, where SVI does not
apply."
Tamara Broderick,Broderick_Tamara,arXiv:1301.6647,https://arxiv.org/abs/1301.6647,"Abstract:  The problem of inferring a clustering of a data set has been the subject of
much research in Bayesian analysis, and there currently exists a solid
mathematical foundation for Bayesian approaches to clustering. In particular,
the class of probability distributions over partitions of a data set has been
characterized in a number of ways, including via exchangeable partition
probability functions (EPPFs) and the Kingman paintbox. Here, we develop a
generalization of the clustering problem, called feature allocation, where we
allow each data point to belong to an arbitrary, non-negative integer number of
groups, now called features or topics. We define and study an ""exchangeable
feature probability function"" (EFPF)---analogous to the EPPF in the clustering
setting---for certain types of feature models. Moreover, we introduce a
""feature paintbox"" characterization---analogous to the Kingman paintbox for
clustering---of the class of exchangeable feature models. We provide a further
characterization of the subclass of feature allocations that have EFPF
representations."
Tamara Broderick,Broderick_Tamara,arXiv:1212.2126,https://arxiv.org/abs/1212.2126,"Abstract:  The classical mixture of Gaussians model is related to K-means via
small-variance asymptotics: as the covariances of the Gaussians tend to zero,
the negative log-likelihood of the mixture of Gaussians model approaches the
K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis
& Jordan (2012) used this observation to obtain a novel K-means-like algorithm
from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead
consider applying small-variance asymptotics directly to the posterior in
Bayesian nonparametric models. This framework is independent of any specific
Bayesian inference algorithm, and it has the major advantage that it
generalizes immediately to a range of models beyond the DP mixture. To
illustrate, we apply our framework to the feature learning setting, where the
beta process and Indian buffet process provide an appropriate Bayesian
nonparametric prior. We obtain a novel objective function that goes beyond
clustering to learn (and penalize new) groupings for which we relax the mutual
exclusivity and exhaustivity assumptions of clustering. We demonstrate several
other algorithms, all of which are scalable and simple to implement. Empirical
results demonstrate the benefits of the new framework."
Tamara Broderick,Broderick_Tamara,arXiv:1209.3550,https://arxiv.org/abs/1209.3550,"Abstract:  We develop algorithms for performing semiparametric regression analysis in
real time, with data processed as it is collected and made immediately
available via modern telecommunications technologies. Our definition of
semiparametric regression is quite broad and includes, as special cases,
generalized linear mixed models, generalized additive models, geostatistical
models, wavelet nonparametric regression models and their various combinations.
Fast updating of regression fits is achieved by couching semiparametric
regression into a Bayesian hierarchical model or, equivalently, graphical model
framework and employing online mean field variational ideas. An internet site
attached to this article, realtime-semiparametric-regression.net, illustrates
the methodology for continually arriving stock market, real estate and airline
data. Flexible real-time analyses, based on increasingly ubiquitous streaming
data sources stand to benefit."
Tamara Broderick,Broderick_Tamara,arXiv:1206.5862,https://arxiv.org/abs/1206.5862,"Abstract:  One of the focal points of the modern literature on Bayesian nonparametrics
has been the problem of clustering, or partitioning, where each data point is
modeled as being associated with one and only one of some collection of groups
called clusters or partition blocks. Underlying these Bayesian nonparametric
models are a set of interrelated stochastic processes, most notably the
Dirichlet process and the Chinese restaurant process. In this paper we provide
a formal development of an analogous problem, called feature modeling, for
associating data points with arbitrary nonnegative integer numbers of groups,
now called features or topics. We review the existing combinatorial stochastic
process representations for the clustering problem and develop analogous
representations for the feature modeling problem. These representations include
the beta process and the Indian buffet process as well as new representations
that provide insight into the connections between these processes. We thereby
bring the same level of completeness to the treatment of Bayesian nonparametric
feature modeling that has previously been achieved for Bayesian nonparametric
clustering."
Tamara Broderick,Broderick_Tamara,arXiv:1203.3486,https://arxiv.org/abs/1203.3486,"Abstract:  We introduce a new graphical model for tracking radio-tagged animals and
learning their movement patterns. The model provides a principled way to
combine radio telemetry data with an arbitrary set of userdefined, spatial
features. We describe an efficient stochastic gradient algorithm for fitting
model parameters to data and demonstrate its effectiveness via asymptotic
analysis and synthetic experiments. We also apply our model to real datasets,
and show that it outperforms the most popular radio telemetry software package
used in ecology. We conclude that integration of different data sources under a
single statistical framework, coupled with appropriate parameter and state
estimation procedures, produces both accurate location estimates and an
interpretable statistical model of animal movement."
Tamara Broderick,Broderick_Tamara,arXiv:1112.3654,https://arxiv.org/abs/1112.3654,"Abstract:  As the number of observed Gamma-Ray Bursts (GRBs) continues to grow,
follow-up resources need to be used more efficiently in order to maximize
science output from limited telescope time. As such, it is becoming
increasingly important to rapidly identify bursts of interest as soon as
possible after the event, before the afterglows fade beyond detectability.
Studying the most distant (highest redshift) events, for instance, remains a
primary goal for many in the field. Here we present our Random forest Automated
Triage Estimator for GRB redshifts (RATE GRB-z) for rapid identification of
high-redshift candidates using early-time metrics from the three telescopes
onboard Swift. While the basic RATE methodology is generalizable to a number of
resource allocation problems, here we demonstrate its utility for
telescope-constrained follow-up efforts with the primary goal to identify and
study high-z GRBs. For each new GRB, RATE GRB-z provides a recommendation -
based on the available telescope time - of whether the event warrants
additional follow-up resources. We train RATE GRB-z using a set consisting of
135 Swift bursts with known redshifts, only 18 of which are z > 4.
Cross-validated performance metrics on this training data suggest that ~56% of
high-z bursts can be captured from following up the top 20% of the ranked
candidates, and ~84% of high-z bursts are identified after following up the top
~40% of candidates. We further use the method to rank 200+ Swift bursts with
unknown redshifts according to their likelihood of being high-z."
Tamara Broderick,Broderick_Tamara,arXiv:1111.1802,https://arxiv.org/abs/1111.1802,"Abstract:  We develop a Bayesian nonparametric approach to a general family of latent
class problems in which individuals can belong simultaneously to multiple
classes and where each class can be exhibited multiple times by an individual.
We introduce a combinatorial stochastic process known as the negative binomial
process (NBP) as an infinite-dimensional prior appropriate for such problems.
We show that the NBP is conjugate to the beta process, and we characterize the
posterior distribution under the beta-negative binomial process (BNBP) and
hierarchical models based on the BNBP (the HBNBP). We study the asymptotic
properties of the BNBP and develop a three-parameter extension of the BNBP that
exhibits power-law behavior. We derive MCMC algorithms for posterior inference
under the HBNBP, and we present experiments using these algorithms in the
domains of image segmentation, object recognition, and document analysis."
Tamara Broderick,Broderick_Tamara,arXiv:1106.0539,https://arxiv.org/abs/1106.0539,"Abstract:  The beta-Bernoulli process provides a Bayesian nonparametric prior for models
involving collections of binary-valued features. A draw from the beta process
yields an infinite collection of probabilities in the unit interval, and a draw
from the Bernoulli process turns these into binary-valued features. Recent work
has provided stick-breaking representations for the beta process analogous to
the well-known stick-breaking representation for the Dirichlet process. We
derive one such stick-breaking representation directly from the
characterization of the beta process as a completely random measure. This
approach motivates a three-parameter generalization of the beta process, and we
study the power laws that can be obtained from this generalized beta process.
We present a posterior inference algorithm for the beta-Bernoulli process that
exploits the stick-breaking representation, and we present experimental results
for a discrete factor-analysis model."
Tamara Broderick,Broderick_Tamara,arXiv:0909.2450,https://arxiv.org/abs/0909.2450,"Abstract:  Selection methods that require only a single-switch input, such as a button
click or blink, are potentially useful for individuals with motor impairments,
mobile technology users, and individuals wishing to transmit information
securely. We present a single-switch selection method, ""Nomon,"" that is general
and efficient. Existing single-switch selection methods require selectable
options to be arranged in ways that limit potential applications. By contrast,
traditional operating systems, web browsers, and free-form applications (such
as drawing) place options at arbitrary points on the screen. Nomon, however,
has the flexibility to select any point on a screen. Nomon adapts automatically
to an individual's clicking ability; it allows a person who clicks precisely to
make a selection quickly and allows a person who clicks imprecisely more time
to make a selection without error. Nomon reaps gains in information rate by
allowing the specification of beliefs (priors) about option selection
probabilities and by avoiding tree-based selection schemes in favor of direct
(posterior) inference. We have developed both a Nomon-based writing application
and a drawing application. To evaluate Nomon's performance, we compared the
writing application with a popular existing method for single-switch writing
(row-column scanning). Novice users wrote 35% faster with the Nomon interface
than with the scanning interface. An experienced user (author TB, with > 10
hours practice) wrote at speeds of 9.3 words per minute with Nomon, using 1.2
clicks per character and making no errors in the final text."
Tamara Broderick,Broderick_Tamara,arXiv:0904.4891,https://arxiv.org/abs/0904.4891,"Abstract:  Recognizing the successes of treed Gaussian process (TGP) models as an
interpretable and thrifty model for nonparametric regression, we seek to extend
the model to classification. Both treed models and Gaussian processes (GPs)
have, separately, enjoyed great success in application to classification
problems. An example of the former is Bayesian CART. In the latter, real-valued
GP output may be utilized for classification via latent variables, which
provide classification rules by means of a softmax function. We formulate a
Bayesian model averaging scheme to combine these two models and describe a
Monte Carlo method for sampling from the full posterior distribution with joint
proposals for the tree topology and the GP parameters corresponding to latent
variables at the leaves. We concentrate on efficient sampling of the latent
variables, which is important to obtain good mixing in the expanded parameter
space. The tree structure is particularly helpful for this task and also for
developing an efficient scheme for handling categorical predictors, which
commonly arise in classification problems. Our proposed classification TGP
(CTGP) methodology is illustrated on a collection of synthetic and real data
sets. We assess performance relative to existing methods and thereby show how
CTGP is highly flexible, offers tractable inference, produces rules that are
easy to interpret, and performs well out of sample."
Tamara Broderick,Broderick_Tamara,arXiv:0712.2437,https://arxiv.org/abs/0712.2437,"Abstract:  Recent work has shown that probabilistic models based on pairwise
interactions-in the simplest case, the Ising model-provide surprisingly
accurate descriptions of experiments on real biological networks ranging from
neurons to genes. Finding these models requires us to solve an inverse problem:
given experimentally measured expectation values, what are the parameters of
the underlying Hamiltonian? This problem sits at the intersection of
statistical physics and machine learning, and we suggest that more efficient
solutions are possible by merging ideas from the two fields. We use a
combination of recent coordinate descent algorithms with an adaptation of the
histogram Monte Carlo method, and implement these techniques to take advantage
of the sparseness found in data on real neurons. The resulting algorithm learns
the parameters of an Ising model describing a network of forty neurons within a
few minutes. This opens the possibility of analyzing much larger data sets now
emerging, and thus testing hypotheses about the collective behaviors of these
networks."
Tamara Broderick,Broderick_Tamara,arXiv:astro-ph/0507108,https://arxiv.org/abs/astro-ph/0507108,"Abstract:  We present the results of attempts to detect the ellipticity of dark matter
halos using galaxy-galaxy weak lensing with SDSS data. We use 2,020,256
galaxies brighter than r=19 with photometric redshifts (divided into colour and
luminosity subsamples) as lenses and 31,697,869 source galaxies. We search for
and identify several signal contaminants, which if not removed lead to a
spurious detection. These include systematic shear that leads to a slight
spurious alignment of lens and source ellipticities, intrinsic alignments (due
to contamination of the source sample by physically-associated lens source
pairs), and anisotropic magnification bias. We develop methods that allow us to
remove these contaminants to the signal. We split the analysis into blue
(spiral) and red (elliptical) galaxies. Assuming Gaussian errors as in previous
work and a power-law profile, we find f_h=e_h/e_g=0.1+/-0.06 for red galaxies
and -0.8+/-0.4 for blue galaxies using 20-300 kpc/h, averaged over luminosity.
Inclusion of the more realistic non-Gaussian error distributions and of the NFW
density profile (which predicts much smaller ellipticity of the shear for
scales above the scale radius) yields 0.60+/-0.38 for ellipticals and
-1.4+1.7-2.0 for spirals. While there is no concrete detection of alignment in
either case, there is a suggestion in the data of a positive alignment in the
brightest lens sample of ellipticals. Our results appear to be mildly
inconsistent with a previously reported detection by Hoekstra et al. (2004),
but more data and further tests are needed to clarify whether the discrepancy
is real or a consequence of differences in the lens galaxy samples used and
analysis methods."
Tamara Broderick,Broderick_Tamara,arXiv:astro-ph/0402002,https://arxiv.org/abs/astro-ph/0402002,"Abstract:  We investigate the required redshift accuracy of type Ia supernova and
cluster number-count surveys in order for the redshift uncertainties not to
contribute appreciably to the dark energy parameter error budget. For the SNAP
supernova experiment, we find that, without the assistance of ground-based
measurements, individual supernova redshifts would need to be determined to
about 0.002 or better, which is a challenging but feasible requirement for a
low-resolution spectrograph. However, we find that accurate redshifts for z<0.1
supernovae, obtained with ground-based experiments, are sufficient to immunize
the results against even relatively large redshift errors at high z. For the
future cluster number-count surveys such as the South Pole Telescope, Planck or
DUET, we find that the purely statistical error in photometric redshift is less
important, and that the irreducible, systematic bias in redshift drives the
requirements. The redshift bias will have to be kept below 0.001-0.005 per
redshift bin (which is determined by the filter set), depending on the sky
coverage and details of the definition of the minimal mass of the survey.
Furthermore, we find that X-ray surveys have a more stringent required redshift
accuracy than Sunyaev-Zeldovich (SZ) effect surveys since they use a shorter
lever arm in redshift; conversely, SZ surveys benefit from their high redshift
reach only so long as some redshift information is available for distant (z>1)
clusters."
Rodney Brooks,Brooks_Rodney,arXiv:1710.10291,https://arxiv.org/abs/1710.10291,"Abstract:  The measurement problem and three other vexing experiments in quantum physics
are described. It is shown how Quantum Field Theory, as formulated by Julian
Schwinger, provides simple solutions for all four experiments. It is also shown
how this theory resolves many other problems of Quantum Mechanics and
Relativity, including a new and simple derivation of E = mc2."
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1901.08637,https://arxiv.org/abs/1901.08637,"Abstract:  Photon recycling is required for a solar cell to achieve an open-circuit
voltage ($V_{OC}$) and power conversion efficiency (PCE) approaching the
Shockley-Queisser theoretical limit. In metal halide perovskite solar cells,
the achievable performance gains from photon recycling remain uncertain due to
high variability in perovskite material quality and the non-radiative
recombination rate ($k_{1}$). In this work, we study state-of-the-art
$\textrm{Cs}_{0.05}(\textrm{MA}_{0.17}\textrm{FA}_{0.83})_{0.95}\textrm{Pb}(\textrm{I}_{0.83}\textrm{Br}_{0.17})_{3}$
films and analyze the impact of varying non-radiative recombination rates on
photon recycling and device performance. Importantly, we predict the impact of
photon recycling at the maximum power point (MPP), demonstrating an absolute
PCE increase of up to 2.0% in the radiative limit, primarily due to a 77 mV
increase in $V_{MPP}$. Even with finite non-radiative recombination, benefits
from photon recycling can be achieved when non-radiative lifetimes and external
LED electroluminescence efficiencies measured at open-circuit,
$Q_{e}^{LED}(\textrm{V}_{OC})$, exceed 2 $\mu$s and 10%, respectively. This
analysis clarifies the opportunity to fully exploit photon recycling to push
the real-world performance of perovskite solar cells toward theoretical limits."
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1803.01192,https://arxiv.org/abs/1803.01192,"Abstract:  Halide perovskites are promising semiconductors for inexpensive,
high-performance optoelectronics. Despite a remarkable defect tolerance
compared to conventional semiconductors, perovskite thin films still show
substantial microscale heterogeneity in key properties such as luminescence
efficiency and device performance. This behavior has been attributed to spatial
fluctuations in the population of sub-bandgap electronic states that act as
trap-mediated non-radiative recombination sites. However, the origin of the
variations, trap states and extent of the defect tolerance remains a topic of
debate, and a precise understanding is critical to the rational design of
defect management strategies. By combining scanning X-ray diffraction beamlines
at two different synchrotrons with high-resolution transmission electron
microscopy, we reveal levels of heterogeneity on the ten-micrometer scale
(super-grains) and even ten-nanometer scale (sub-grain domains). We find that
local strain is associated with enhanced defect concentrations, and
correlations between the local structure and time-resolved photoluminescence
reveal that these strain-related defects are the cause of non-radiative
recombination. We reveal a direct connection between defect concentrations and
non-radiative losses, as well as complex heterogeneity across multiple length
scales, shedding new light on the presence and influence of structural defects
in halide perovskites."
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1609.04643,https://arxiv.org/abs/1609.04643,"Abstract:  Unique optical properties of colloidal semiconductor quantum dots (QDs),
arising from quantum mechanical confinement of charge within these structures,
present a versatile testbed for the study of how high electric fields affect
the electronic structure of nanostructured solids. Earlier studies of quasi-DC
electric field modulation of QD properties have been limited by the
electrostatic breakdown processes under the high externally applied electric
fields, which have restricted the range of modulation of QD properties. In
contrast, in the present work we drive CdSe:CdS core:shell QD films with
high-field THz-frequency electromagnetic pulses whose duration is only a few
picoseconds. Surprisingly, in response to the THz excitation we observe QD
luminescence even in the absence of an external charge source. Our experiments
show that QD luminescence is associated with a remarkably high and rapid
modulation of the QD band-gap, which is changing by more than 0.5 eV
(corresponding to 25% of the unperturbed bandgap energy) within the picosecond
timeframe of THz field profile. We show that these colossal energy shifts can
be consistently explained by the quantum confined Stark effect. Our work
demonstrates a route to extreme modulation of material properties without
configurational changes in material sets or geometries. Additionally, we expect
that this platform can be adapted to a novel compact THz detection scheme where
conversion of THz fields (with meV-scale photon energies) to the
visible/near-IR band (with eV-scale photon energies) can be achieved at room
temperature with high bandwidth and sensitivity."
Vladimir Bulovic,Bulovic_Vladimir,arXiv:1509.03687,https://arxiv.org/abs/1509.03687,"Abstract:  Plexcitons are polaritonic modes that result from the strong coupling between
excitons and plasmons. We consider plexcitons emerging from the interaction of
excitons in an organic molecular layer with surface plasmons in a metallic
film. We predict the emergence of Dirac cones in the two-dimensional
bandstructure of plexcitons due to the inherent alignment of the excitonic
transitions in the organic layer. These Dirac cones may open up in energy by
simultaneously interfacing the metal with a magneto-optical layer and
subjecting the whole system to a perpendicular magnetic field. The resulting
energy gap becomes populated with topologically protected one-way modes which
travel at the interface of this plexcitonic system. Our theoretical proposal
suggests that plexcitons are a convenient and simple platform for the
exploration of exotic phases of matter as well as of novel ways to direct
energy flow at the nanoscale."
Michael Carbin,Carbin_Michael,arXiv:1809.05859,https://arxiv.org/abs/1809.05859,"Abstract:  When a computational task tolerates a relaxation of its specification or when
an algorithm tolerates the effects of noise in its execution, hardware,
programming languages, and system software can trade deviations from correct
behavior for lower resource usage. We present, for the first time, a synthesis
of research results on computing systems that only make as many errors as their
users can tolerate, from across the disciplines of computer aided design of
circuits, digital system design, computer architecture, programming languages,
operating systems, and information theory.
Rather than over-provisioning resources at each layer to avoid errors, it can
be more efficient to exploit the masking of errors occurring at one layer which
can prevent them from propagating to a higher layer. We survey tradeoffs for
individual layers of computing systems from the circuit level to the operating
system level and illustrate the potential benefits of end-to-end approaches
using two illustrative examples. To tie together the survey, we present a
consistent formalization of terminology, across the layers, which does not
significantly deviate from the terminology traditionally used by research
communities in their layer of focus."
Michael Carbin,Carbin_Michael,arXiv:1808.07412,https://arxiv.org/abs/1808.07412,"Abstract:  Statically estimating the number of processor clock cycles it takes to
execute a basic block of assembly instructions in steady state (throughput) is
important for compiler backend optimizations such as register allocation,
instruction selection and instruction scheduling. This is complicated specially
in modern x86-64 Complex Instruction Set Computer (CISC) machines with
sophisticated processor microarchitectures. Traditionally, compiler writers
invest time experimenting and referring to processor manuals to analytically
model modern processors with incomplete specifications. This is tedious, error
prone and should be done for each processor generation. We present Ithemal, the
first automatically learnt estimator to statically predict throughput of a set
of basic block instructions using machine learning. Ithemal uses a novel
Directed Acyclic Graph-Recurrent Neural Network (DAG-RNN) based data-driven
approach for throughput estimation. We show that Ithemal is accurate than
state-of-the-art hand written tools used in compiler backends and static
machine code analyzers. In particular, our model has a worst case average error
of 10.53% on actual throughput values when compared to best case average errors
of 19.57% for the LLVM scheduler (llvm-mca) and 22.51% for IACA, Intel's
machine code analyzer when compared on three different microarchitectures,
while predicting throughput values at a faster rate than aforementioned tools.
We also show that Ithemal is portable, learning throughput estimation for Intel
Nehalem, Haswell and Skylake microarchitectures without requiring changes to
its structure."
Michael Carbin,Carbin_Michael,arXiv:1805.06090,https://arxiv.org/abs/1805.06090,"Abstract:  Researchers have recently designed a number of application-specific fault
tolerance mechanisms that enable applications to either be naturally resilient
to errors or include additional detection and correction steps that can bring
the overall execution of an application back into an envelope for which an
acceptable execution is eventually guaranteed. A major challenge to building an
application that leverages these mechanisms, however, is to verify that the
implementation satisfies the basic invariants that these mechanisms
require--given a model of how faults may manifest during the application's
execution.
To this end we present Leto, an SMT based automatic verification system that
enables developers to verify their applications with respect to a first-class
execution model specification. Namely, Leto enables software and platform
developers to programmatically specify the execution semantics of the
underlying hardware system as well as verify assertions about the behavior of
the application's resulting execution. In this paper, we present the Leto
programming language and its corresponding verification system. We also
demonstrate Leto on several applications that leverage application-specific
fault tolerance mechanisms."
Michael Carbin,Carbin_Michael,arXiv:1805.01863,https://arxiv.org/abs/1805.01863,"Abstract:  Researchers have recently proposed several systems that ease the process of
performing Bayesian probabilistic inference. These include systems for
automatic inference algorithm synthesis as well as stronger abstractions for
manual algorithm development. However, existing systems whose performance
relies on the developer manually constructing a part of the inference algorithm
have limited support for reasoning about the correctness of the resulting
algorithm.
In this paper, we present Shuffle, a programming language for manually
developing inference procedures that 1) enforces the basic rules of probability
theory, 2) enforces the statistical dependencies of the algorithm's
corresponding probabilistic model, and 3) generates an optimized
implementation. We have used Shuffle to develop inference algorithms for
several standard probabilistic models. Our results demonstrate that Shuffle
enables a developer to deliver correct and performant implementations of these
algorithms."
Michael Carbin,Carbin_Michael,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"Abstract:  Though many safety-critical software systems use floating point to represent
real-world input and output, programmers usually have idealized versions in
mind that compute with real numbers. Significant deviations from the ideal can
cause errors and jeopardize safety. Some programming systems implement exact
real arithmetic, which resolves this matter but complicates others, such as
decision making. In these systems, it is impossible to compute (total and
deterministic) discrete decisions based on connected spaces such as
$\mathbb{R}$. We present programming-language semantics based on constructive
topology with variants allowing nondeterminism and/or partiality. Either
nondeterminism or partiality suffices to allow computable decision making on
connected spaces such as $\mathbb{R}$. We then introduce pattern matching on
spaces, a language construct for creating programs on spaces, generalizing
pattern matching in functional programming, where patterns need not represent
decidable predicates and also may overlap or be inexhaustive, giving rise to
nondeterminism or partiality, respectively. Nondeterminism and/or partiality
also yield formal logics for constructing approximate decision procedures. We
implemented these constructs in the Marshall language for exact real
arithmetic."
Michael Carbin,Carbin_Michael,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Michael Carbin,Carbin_Michael,arXiv:1803.03635,https://arxiv.org/abs/1803.03635,"Abstract:  Neural network pruning techniques can reduce the parameter counts of trained
networks by over 90%, decreasing storage requirements and improving
computational performance of inference without compromising accuracy. However,
contemporary experience is that the sparse architectures produced by pruning
are difficult to train from the start, which would similarly improve training
performance.
We find that a standard technique for pruning weights naturally uncovers
subnetworks whose initializations made them capable of training effectively.
Based on these results, we articulate the ""lottery ticket hypothesis"": dense,
randomly-initialized feed-forward networks contain subnetworks (""winning
tickets"") that - when trained in isolation - arrive at comparable test accuracy
in a comparable number of iterations. The winning tickets we find have won the
initialization lottery: their connections have initial weights that make
training particularly effective.
We present an algorithm to identify winning tickets and a series of
experiments that support the lottery ticket hypothesis and the importance of
these fortuitous initializations. We consistently find winning tickets that are
less than 10-20% of the size of several fully-connected and convolutional
feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning
tickets we find above that size learn faster than the original network and
exhibit higher test accuracy."
Michael Carbin,Carbin_Michael,arXiv:1202.0359,https://arxiv.org/abs/1202.0359,"Abstract:  We propose a novel approach to improving software security called
Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities
in software from attackers through the use of provably secure and obfuscated
cryptographic devices to harden paths in programs.
By ""harden"" we mean that certain error-checking if-conditionals in a given
program P are replaced by equivalent"" we mean that adversaries cannot use
semi-automatic program analysis techniques to reason about the hardened program
paths and thus cannot discover as-yet-unknown errors along those paths, except
perhaps through black-box dictionary attacks or random testing (which we can
never prevent).
Other than these unpreventable attack methods, we can make program analysis
aimed at error-finding ""provably hard"" for a resource-bounded attacker, in the
same sense that cryptographic schemes are hard to break. Unlike
security-through-obscurity, in Cryptographic Path Hardening we use
provably-secure crypto devices to hide errors and our mathematical arguments of
security are the same as the standard ones used in cryptography.
One application of Cryptographic Path Hardening is that software patches or
filters often reveal enough information to an attacker that they can be used to
construct error-revealing inputs to exploit an unpatched version of the
program. By ""hardening"" the patch we make it difficult for the attacker to
analyze the patched program to construct error-revealing inputs, and thus
prevent him from potentially constructing exploits."
Vincent Chan,Chan_Vincent,arXiv:1307.1174,https://arxiv.org/abs/1307.1174,"Abstract:  Let $E \subseteq R^n$ be a closed set of Hausdorff dimension $\alpha$. For $m
\geq n$, let $\{B_1,\ldots,B_k\}$ be $n \times (m-n)$ matrices. We prove that
if the system of matrices $B_j$ is non-degenerate in a suitable sense, $\alpha$
is sufficiently close to $n$, and if $E$ supports a probability measure obeying
appropriate dimensionality and Fourier decay conditions, then for a range of
$m$ depending on $n$ and $k$, the set $E$ contains a translate of a non-trivial
$k$-point configuration $\{B_1y,\ldots,B_ky\}$. As a consequence, we are able
to establish existence of certain geometric configurations in Salem sets (such
as parallelograms in $ R^n$ and isosceles right triangles in $R^2$). This can
be viewed as a multidimensional analogue of an earlier result of Laba and
Pramanik on 3-term arithmetic progressions in subsets of $R$."
Vincent Chan,Chan_Vincent,arXiv:1107.1000,https://arxiv.org/abs/1107.1000,"Abstract:  We study the spindown of isolated neutron stars from initially rapid rotation
rates, driven by two factors: (i) gravitational wave emission due to r-modes
and (ii) magnetic braking. In the context of isolated neutron stars, we present
the first study including self-consistently the magnetic damping of r-modes in
the spin evolution. We track the spin evolution employing the RNS code, which
accounts for the rotating structure of neutron stars for various equations of
state. We find that, despite the strong damping due to the magnetic field,
r-modes alter the braking rate from pure magnetic braking for B<10^{13}G. For
realistic values of the saturation amplitude, the r-mode can also decrease the
time to reach the threshold central density for quark deconfinement. Within a
phenomenological model, we assess the gravitational waveform that would result
from r-mode driven spindown of a magnetized neutron star. To contrast with the
persistent signal during the spindown phase, we also present a preliminary
estimate of the transient gravitational wave signal from an explosive
quark-hadron phase transition, which can be a signal for the deconfinement of
quarks inside neutron stars."
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:1801.05507,https://arxiv.org/abs/1801.05507,"Abstract:  The growing popularity of cloud-based machine learning raises a natural
question about the privacy guarantees that can be provided in such a setting.
Our work tackles this problem in the context where a client wishes to classify
private images using a convolutional neural network (CNN) trained by a server.
Our goal is to build efficient protocols whereby the client can acquire the
classification result without revealing their input to the server, while
guaranteeing the privacy of the server's neural network.
To this end, we design Gazelle, a scalable and low-latency system for secure
neural network inference, using an intricate combination of homomorphic
encryption and traditional two-party computation techniques (such as garbled
circuits). Gazelle makes three contributions. First, we design the Gazelle
homomorphic encryption library which provides fast algorithms for basic
homomorphic operations such as SIMD (single instruction multiple data)
addition, SIMD multiplication and ciphertext permutation. Second, we implement
the Gazelle homomorphic linear algebra kernels which map neural network layers
to optimized homomorphic matrix-vector multiplication and convolution routines.
Third, we design optimized encryption switching protocols which seamlessly
convert between homomorphic and garbled circuit encodings to enable
implementation of complete neural network inference.
We evaluate our protocols on benchmark neural networks trained on the MNIST
and CIFAR-10 datasets and show that Gazelle outperforms the best existing
systems such as MiniONN (ACM CCS 2017) by 20 times and Chameleon (Crypto Eprint
2017/1164) by 30 times in online runtime. Similarly when compared with fully
homomorphic approaches like CryptoNets (ICML 2016) we demonstrate three orders
of magnitude faster online run-time."
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:0710.4815,https://arxiv.org/abs/0710.4815,"Abstract:  Ultra-wideband (UWB) communication is an emerging wireless technology that
promises high data rates over short distances and precise locationing. The
large available bandwidth and the constraint of a maximum power spectral
density drives a unique set of system challenges. This paper addresses these
challenges using two UWB transceivers and a discrete prototype platform."
Anantha Chandrakasan,Chandrakasan_Anantha,arXiv:0710.4732,https://arxiv.org/abs/0710.4732,"Abstract:  Wireless microsensor networks, which have been the topic of intensive
research in recent years, are now emerging in industrial applications. An
important milestone in this transition has been the release of the IEEE
802.15.4 standard that specifies interoperable wireless physical and medium
access control layers targeted to sensor node radios. In this paper, we
evaluate the potential of an 802.15.4 radio for use in an ultra low power
sensor node operating in a dense network. Starting from measurements carried
out on the off-the-shelf radio, effective radio activation and link adaptation
policies are derived. It is shown that, in a typical sensor network scenario,
the average power per node can be reduced down to 211m mm mW. Next, the energy
consumption breakdown between the different phases of a packet transmission is
presented, indicating which part of the transceiver architecture can most
effectively be optimized in order to further reduce the radio power, enabling
self-powered wireless microsensor networks."
Adam Chlipala,Chlipala_Adam,arXiv:1805.00468,https://arxiv.org/abs/1805.00468,"Abstract:  Though many safety-critical software systems use floating point to represent
real-world input and output, programmers usually have idealized versions in
mind that compute with real numbers. Significant deviations from the ideal can
cause errors and jeopardize safety. Some programming systems implement exact
real arithmetic, which resolves this matter but complicates others, such as
decision making. In these systems, it is impossible to compute (total and
deterministic) discrete decisions based on connected spaces such as
$\mathbb{R}$. We present programming-language semantics based on constructive
topology with variants allowing nondeterminism and/or partiality. Either
nondeterminism or partiality suffices to allow computable decision making on
connected spaces such as $\mathbb{R}$. We then introduce pattern matching on
spaces, a language construct for creating programs on spaces, generalizing
pattern matching in functional programming, where patterns need not represent
decidable predicates and also may overlap or be inexhaustive, giving rise to
nondeterminism or partiality, respectively. Nondeterminism and/or partiality
also yield formal logics for constructing approximate decision procedures. We
implemented these constructs in the Marshall language for exact real
arithmetic."
Adam Chlipala,Chlipala_Adam,arXiv:1803.04870,https://arxiv.org/abs/1803.04870,"Abstract:  It is a neat result from functional programming that libraries of parser
combinators can support rapid construction of decoders for quite a range of
formats. With a little more work, the same combinator program can denote both a
decoder and an encoder. Unfortunately, the real world is full of gnarly
formats, as with the packet formats that make up the standard Internet protocol
stack. Most past parser-combinator approaches cannot handle these formats, and
the few exceptions require redundancy -- one part of the natural grammar needs
to be hand-translated into hints in multiple parts of a parser program. We show
how to recover very natural and nonredundant format specifications, covering
all popular network packet formats and generating both decoders and encoders
automatically. The catch is that we use the Coq proof assistant to derive both
kinds of artifacts using tactics, automatically, in a way that guarantees that
they form inverses of each other. We used our approach to reimplement packet
processing for a full Internet protocol stack, inserting our replacement into
the OCaml-based MirageOS unikernel, resulting in minimal performance
degradation."
Adam Chlipala,Chlipala_Adam,arXiv:1401.7694,https://arxiv.org/abs/1401.7694,"Abstract:  We describe our experience implementing a broad category-theory library in
Coq. Category theory and computational performance are not usually mentioned in
the same breath, but we have needed substantial engineering effort to teach Coq
to cope with large categorical constructions without slowing proof script
processing unacceptably. In this paper, we share the lessons we have learned
about how to represent very abstract mathematical objects and arguments in Coq
and how future proof assistants might be designed to better support such
reasoning. One particular encoding trick to which we draw attention allows
category-theoretic arguments involving duality to be internalized in Coq's
logic with definitional equality. Ours may be the largest Coq development to
date that uses the relatively new Coq version developed by homotopy type
theorists, and we reflect on which new features were especially helpful."
Adam Chlipala,Chlipala_Adam,arXiv:1305.6543,https://arxiv.org/abs/1305.6543,"Abstract:  We describe a method for building composable and extensible verification
procedures within the Coq proof assistant. Unlike traditional methods that rely
on run-time generation and checking of proofs, we use verified-correct
procedures with Coq soundness proofs. Though they are internalized in Coq's
logic, our provers support sound extension by users with hints over new
domains, enabling automated reasoning about user-defined abstract predicates.
We maintain soundness by developing an architecture for modular packaging,
construction, and composition of hint databases, which had previously only been
implemented in Coq at the level of its dynamically typed, proof-generating
tactic language. Our provers also include rich handling of unification
variables, enabling integration with other tactic-based deduction steps within
Coq. We have implemented our techniques in MirrorShard, an open-source
framework for reflective verification. We demonstrate its applicability by
instantiating it to separation logic in order to reason about imperative
program verification."
Adam Chlipala,Chlipala_Adam,arXiv:1301.4779,https://arxiv.org/abs/1301.4779,"Abstract:  We report on the implementation of a certified compiler for a high-level
hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).
Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded
atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.
The target language of the compiler corresponds to a synthesisable subset of
Verilog or VHDL. A key aspect of our approach is that input programs to the
compiler can be defined and proved correct inside Coq. Then, we use extraction
and a Verilog back-end (written in OCaml) to get a certified version of a
hardware design."
Isaac Chuang,Chuang_Isaac,arXiv:1811.02124,https://arxiv.org/abs/1811.02124,"Abstract:  While quantum devices rely on interactions between constituent subsystems and
with their environment to operate, native interactions alone often fail to
deliver targeted performance. Coherent pulsed control provides the ability to
tailor effective interactions, known as Hamiltonian engineering. We propose a
Hamiltonian engineering method that maximizes desired interactions while
mitigating deleterious ones by conducting a pulse sequence search using
constrained optimization. The optimization formulation incorporates pulse
sequence length and cardinality penalties consistent with linear or integer
programming. We apply the general technique to magnetometry with solid state
spin ensembles in which inhomogeneous interactions between sensing spins limit
coherence. Defining figures of merit for broadband Ramsey magnetometry, we
present novel pulse sequences which outperform known techniques for homonuclear
spin decoupling in both spin-1/2 and spin-1 systems. When applied to nitrogen
vacancy (NV) centers in diamond, this scheme partially preserves the Zeeman
interaction while zeroing dipolar coupling between negatively charged
NV$^{\text -}$ centers. Such a scheme is of interest for NV$^\text{-}$
magnetometers which have reached the NV$^\text{-}$-NV$^\text{-}$ coupling
limit. We discuss experimental implementation in NV ensembles, as well as
applicability of the current approach to more general spin bath decoupling and
superconducting qubit control."
Isaac Chuang,Chuang_Isaac,arXiv:1807.09912,https://arxiv.org/abs/1807.09912,"Abstract:  Compared to humans, machine learning models generally require significantly
more training examples and fail to extrapolate from experience to solve
previously unseen challenges. To help close this performance gap, we augment
single-task neural networks with a meta-recognition model which learns a
succinct model code via its autoencoder structure, using just a few informative
examples. The model code is then employed by a meta-generative model to
construct parameters for the task-specific model. We demonstrate that for
previously unseen tasks, without additional training, this Meta-Learning
Autoencoder (MeLA) framework can build models that closely match the true
underlying models, with loss significantly lower than given by fine-tuned
baseline networks, and performance that compares favorably with
state-of-the-art meta-learning algorithms. MeLA also adds the ability to
identify influential training examples and predict which additional data will
be most valuable to acquire to improve model prediction."
Isaac Chuang,Chuang_Isaac,arXiv:1801.07618,https://arxiv.org/abs/1801.07618,"Abstract:  Each time a learner in a self-paced online course is trying to answer an
assessment question, it takes some time to submit the answer, and if multiple
attempts are allowed and the first answer was incorrect, it takes some time to
submit the second attempt, and so on. Here we study the distribution of such
""response times"". We find that the log-normal statistical model for such times,
previously suggested in the literature, holds for online courses qualitatively.
Users who, according to this model, tend to take longer on submits are more
likely to complete the course, have a higher level of engagement and achieve a
higher grade. This finding can be the basis for designing interventions in
online courses, such as MOOCs, which would encourage some users to slow down."
Isaac Chuang,Chuang_Isaac,arXiv:1801.01081,https://arxiv.org/abs/1801.01081,"Abstract:  We present a novel set of reversible modular multipliers applicable to
quantum computing, derived from three classical techniques: 1) traditional
integer division, 2) Montgomery residue arithmetic, and 3) Barrett reduction.
Each multiplier computes an exact result for all binary input values, while
maintaining the asymptotic resource complexity of a single (non-modular)
integer multiplier. We additionally conduct an empirical resource analysis of
our designs in order to determine the total gate count and circuit depth of
each fully constructed circuit, with inputs as large as 2048 bits. Our
comparative analysis considers both circuit implementations which allow for
arbitrary (controlled) rotation gates, as well as those restricted to a typical
fault-tolerant gate set."
Isaac Chuang,Chuang_Isaac,arXiv:1709.05302,https://arxiv.org/abs/1709.05302,"Abstract:  We establish a symmetry-operator framework for designing quantum error
correcting~(QEC) codes based on fundamental properties of the underlying system
dynamics. Based on this framework, we propose three hardware-efficient bosonic
QEC codes that are suitable for $\chi^{(2)}$-interaction based quantum
computation: the $\chi^{(2)}$ parity-check code, the $\chi^{(2)}$ embedded
error-correcting code, and the $\chi^{(2)}$ binomial code, all of which detect
photon-loss or photon-gain errors by means of photon-number parity measurements
and then correct them via $\chi^{(2)}$ Hamiltonian evolutions and linear-optics
transformations. Our symmetry-operator framework provides a systematic
procedure for finding QEC codes that are not stabilizer codes. The $\chi^{(2)}$
binomial code is of special interest because, with $m\le N$ identified from
channel monitoring, it can correct $m$-photon loss errors, $m$-photon gain
errors, and $(m-1)$th-order dephasing errors using logical qudits that are
encoded in $O(N)$ photons. In comparison, other bosonic QEC codes require
$O(N^2)$ photons to correct the same degree of bosonic errors. Such improved
photon-efficiency underscores the additional error-correction power that can be
provided by channel monitoring. We develop quantum Hamming bounds for
photon-loss errors in the code subspaces associated with the $\chi^{(2)}$
parity-check code and the $\chi^{(2)}$ embedded error-correcting code, and we
prove that these codes saturate their respective bounds. Our $\chi^{(2)}$ QEC
codes exhibit hardware efficiency in that they address the principal error
mechanisms and exploit the available physical interactions of the underlying
hardware, thus reducing the physical resources required for implementing their
encoding, decoding, and error-correction operations, and their universal
encoded-basis gate sets."
Isaac Chuang,Chuang_Isaac,arXiv:1707.05391,https://arxiv.org/abs/1707.05391,"Abstract:  The exponential speedups promised by Hamiltonian simulation on a quantum
computer depends crucially on structure in both the Hamiltonian $\hat{H}$, and
the quantum circuit $\hat{U}$ that encodes its description. In the quest to
better approximate time-evolution $e^{-i\hat{H}t}$ with error $\epsilon$, we
motivate a systematic approach to understanding and exploiting structure, in a
setting where Hamiltonians are encoded as measurement operators of unitary
circuits $\hat{U}$ for generalized measurement. This allows us to define a
\emph{uniform spectral amplification} problem on this framework for expanding
the spectrum of encoded Hamiltonian with exponentially small distortion. We
present general solutions to uniform spectral amplification in a hierarchy
where factoring $\hat{U}$ into $n=1,2,3$ unitary oracles represents increasing
structural knowledge of the encoding. Combined with structural knowledge of the
Hamiltonian, specializing these results allow us simulate time-evolution by
$d$-sparse Hamiltonians using $\mathcal{O}\left(t(d \|\hat
H\|_{\text{max}}\|\hat H\|_{1})^{1/2}\log{(t\|\hat{H}\|/\epsilon)}\right)$
queries, where $\|\hat H\|\le \|\hat H\|_1\le d\|\hat H\|_{\text{max}}$. Up to
logarithmic factors, this is a polynomial improvement upon prior art using
$\mathcal{O}\left(td\|\hat
H\|_{\text{max}}+\frac{\log{(1/\epsilon)}}{\log\log{(1/\epsilon)}}\right)$ or
$\mathcal{O}(t^{3/2}(d \|\hat H\|_{\text{max}}\|\hat H\|_{1}\|\hat
H\|/\epsilon)^{1/2})$ queries. In the process, we also prove a matching lower
bound of $\Omega(t(d\|\hat H\|_{\text{max}}\|\hat H\|_{1})^{1/2})$ queries,
present a distortion-free generalization of spectral gap amplification, and an
amplitude amplification algorithm that performs multiplication on unknown state
amplitudes."
Isaac Chuang,Chuang_Isaac,arXiv:1707.00012,https://arxiv.org/abs/1707.00012,"Abstract:  A non-Clifford gate is required for universal quantum computation, and,
typically, this is the most error-prone and resource intensive logical
operation on an error-correcting code. Small, single-qubit rotations are
popular choices for this non-Clifford gate, but certain three-qubit gates, such
as Toffoli or controlled-controlled-Z (CCZ), are equivalent options that are
also more suited for implementing some quantum algorithms, for instance those
with coherent classical subroutines. Here, we calculate error rates and
resource overheads for implementing logical CCZ with pieceable fault-tolerance,
a non-transversal method for implementing logical gates. We provide a
comparison with a non-local magic-state scheme on a concatenated code and a
local magic-state scheme on the surface code. We find the pieceable
fault-tolerance scheme particularly advantaged over magic states on
concatenated codes and in certain regimes over magic states on the surface
code. Our results suggest that pieceable fault-tolerance is a promising
candidate for fault-tolerance in a near-future quantum computer."
Isaac Chuang,Chuang_Isaac,arXiv:1705.01936,https://arxiv.org/abs/1705.01936,"Abstract:  Noisy PN learning is the problem of binary classification when training
examples may be mislabeled (flipped) uniformly with noise rate rho1 for
positive examples and rho0 for negative examples. We propose Rank Pruning (RP)
to solve noisy PN learning and the open problem of estimating the noise rates,
i.e. the fraction of wrong positive and negative labels. Unlike prior
solutions, RP is time-efficient and general, requiring O(T) for any
unrestricted choice of probabilistic classifier with T fitting time. We prove
RP has consistent noise estimation and equivalent expected risk as learning
with uncorrupted labels in ideal conditions, and derive closed-form solutions
when conditions are non-ideal. RP achieves state-of-the-art noise estimation
and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the
amount of noise and performs similarly impressively when a large portion of
training examples are noise drawn from a third distribution. To highlight, RP
with a CNN classifier can predict if an MNIST digit is a ""one""or ""not"" with
only 0.25% error, and 0.46 error across all digits, even when 50% of positive
examples are mislabeled and 50% of observed positive labels are mislabeled
negative examples."
Isaac Chuang,Chuang_Isaac,arXiv:1704.03431,https://arxiv.org/abs/1704.03431,"Abstract:  We prove that universal quantum computation can be realized---using only
linear optics and $\chi^{(2)}$ (three-wave mixing) interactions---in any
$(n+1)$-dimensional qudit basis of the $n$-pump-photon subspace. First, we
exhibit a strictly universal gate set for the qubit basis in the
one-pump-photon subspace. Next, we demonstrate qutrit-basis universality by
proving that $\chi^{(2)}$ Hamiltonians and photon-number operators generate the
full $\mathfrak{u}(3)$ Lie algebra in the two-pump-photon subspace, and showing
how the qutrit controlled-$Z$ gate can be implemented with only linear optics
and $\chi^{(2)}$ interactions. We then use proof by induction to obtain our
general qudit result. Our induction proof relies on coherent photon
injection/subtraction, a technique enabled by $\chi^{(2)}$ interaction between
the encoding modes and ancillary modes. Finally, we show that coherent photon
injection is more than a conceptual tool in that it offers a route to preparing
high-photon-number Fock states from single-photon Fock states."
Isaac Chuang,Chuang_Isaac,arXiv:1610.06546,https://arxiv.org/abs/1610.06546,"Abstract:  Given a Hermitian operator $\hat{H}=\langle G|\hat{U}|G\rangle$ that is the
projection of an oracle $\hat{U}$ by state $|G\rangle$ created with oracle
$\hat{G}$, the problem of Hamiltonian simulation is approximating the time
evolution operator $e^{-i\hat{H}t}$ at time $t$ with error $\epsilon$. We show
that this can be done with query complexity
$\mathcal{O}\big(t+\frac{\log{(1/\epsilon)}}{\log\log{(1/\epsilon)}}\big)$ to
$\hat{G},\hat{U}$ that is optimal, not just in asymptotic limits, but for all
values $t,\epsilon$. Furthermore, only $2$ additional ancilla qubits are
required in total, together with $\mathcal{O}(1)$ additional single and
two-qubit gates per query. Our approach to Hamiltonian simulation subsumes
important prior art considering Hamiltonians which are $d$-sparse or a linear
combination of unitaries, leading to significant improvements in space
complexity, as well as a quadratic speed-up for precision simulations. It also
motivates useful new instances, such as where $\langle G|\hat{U}|G\rangle$ is a
density matrix. A key technical result is `qubitization' which uses
controlled-$\hat{U}$ and controlled-$\hat{G}$ to embed $\hat{H}$ in an
invariant $\text{SU}(2)$ subspace. A large class of operator functions of
$\hat{H}$ can then be computed with optimal query complexity, of which
$e^{-i\hat{H}t}$ is a special case."
Isaac Chuang,Chuang_Isaac,arXiv:1609.03603,https://arxiv.org/abs/1609.03603,"Abstract:  Fixed-point quantum search algorithms succeed at finding one of $M$ target
items among $N$ total items even when the run time of the algorithm is longer
than necessary. While the famous Grover's algorithm can search quadratically
faster than a classical computer, it lacks the fixed-point property --- the
fraction of target items must be known precisely to know when to terminate the
algorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search
algorithm with the fixed-point property. Meanwhile, it is known that an
adiabatic quantum algorithm, operating by continuously varying a Hamiltonian,
can reproduce the quadratic speedup of gate-model Grover search. We ask, can an
adiabatic algorithm also reproduce the fixed-point property? We show that the
answer depends on what interpolation schedule is used, so as in the gate model,
there are both fixed-point and non-fixed-point versions of adiabatic search,
only some of which attain the quadratic quantum speedup. Guided by geometric
intuition on the Bloch sphere, we rigorously justify our claims with an
explicit upper bound on the error in the adiabatic approximation. We also show
that the fixed-point adiabatic search algorithm can be simulated in the gate
model with neither loss of the quadratic Grover speedup nor of the fixed-point
property. Finally, we discuss natural uses of fixed-point algorithms such as
preparation of a relatively prime state and oblivious amplitude amplification."
Isaac Chuang,Chuang_Isaac,arXiv:1606.02685,https://arxiv.org/abs/1606.02685,"Abstract:  The physics of quantum mechanics is the inspiration for, and underlies,
quantum computation. As such, one expects physical intuition to be highly
influential in the understanding and design of many quantum algorithms,
particularly simulation of physical systems. Surprisingly, this has been
challenging, with current Hamiltonian simulation algorithms remaining abstract
and often the result of sophisticated but unintuitive constructions. We contend
that physical intuition can lead to optimal simulation methods by showing that
a focus on simple single-qubit rotations elegantly furnishes an optimal
algorithm for Hamiltonian simulation, a universal problem that encapsulates all
the power of quantum computation. Specifically, we show that the query
complexity of implementing time evolution by a $d$-sparse Hamiltonian $\hat{H}$
for time-interval $t$ with error $\epsilon$ is
$\mathcal{O}(td\|\hat{H}\|_{\text{max}}+\frac{\log{(1/\epsilon)}}{\log{\log{(1/\epsilon)}}})$,
which matches lower bounds in all parameters. This connection is made through
general three-step ""quantum signal processing"" methodology, comprised of (1)
transducing eigenvalues of $\hat{H}$ into a single ancilla qubit, (2)
transforming these eigenvalues through an optimal-length sequence of
single-qubit rotations, and (3) projecting this ancilla with near unity success
probability."
Isaac Chuang,Chuang_Isaac,arXiv:1606.02188,https://arxiv.org/abs/1606.02188,"Abstract:  Classical imaging works by scattering photons from an object to be imaged,
and achieves resolution scaling as $1/\sqrt{t}$, with $t$ the imaging time. By
contrast, the laws of quantum mechanics allow one to utilize quantum coherence
to obtain imaging resolution that can scale as quickly as $1/t$ -- the
so-called ""Heisenberg limit."" However, ambiguities in the obtained signal often
preclude taking full advantage of this quantum enhancement, while imaging
techniques designed to be unambiguous often lose this optimal Heisenberg
scaling. Here, we demonstrate an imaging technique which combines unambiguous
detection of the target with Heisenberg scaling of the resolution. We also
demonstrate a binary search algorithm which can efficiently locate a coherent
target using the technique, resolving a target trapped ion to within 0.3% of
the $1/e^2$ diameter of the excitation beam."
Isaac Chuang,Chuang_Isaac,arXiv:1605.04210,https://arxiv.org/abs/1605.04210,"Abstract:  We report on a method for measuring the branching ratios of dipole
transitions of trapped atomic ions by performing nested sequences of population
inversions. This scheme is broadly applicable and does not use ultrafast pulsed
or narrow linewidth lasers. It is simple to perform and insensitive to
experimental variables such as laser and magnetic field noise as well as ion
heating. To demonstrate its effectiveness, we make the most accurate
measurements thus far of the branching ratios of both 5P1/2 and 5P3/2 states in
88Sr+ with sub-1% uncertainties. We measure 17.175(27) for the branching ratio
of 5P1/2-5S1/2, 15.845(71) for 5P3/2-5S1/2, and 0.05609(21) for 5P3/2-4D5/2,
ten- fold and thirty-fold improvements in precision for 5P1/2 and 5P3/2
branching ratios respectively over the best previous experimental values."
Isaac Chuang,Chuang_Isaac,arXiv:1603.03996,https://arxiv.org/abs/1603.03996,"Abstract:  The creation of composite quantum gates that implement quantum response
functions $\hat{U}(\theta)$ dependent on some parameter of interest $\theta$ is
often more of an art than a science. Through inspired design, a sequence of $L$
primitive gates also depending on $\theta$ can engineer a highly nontrivial
$\hat{U}(\theta)$ that enables myriad precision metrology, spectroscopy, and
control techniques. However, discovering new, useful examples of
$\hat{U}(\theta)$ requires great intuition to perceive the possibilities, and
often brute-force to find optimal implementations. We present a systematic and
efficient methodology for composite gate design of arbitrary length, where
phase-controlled primitive gates all rotating by $\theta$ act on a single spin.
We fully characterize the realizable family of $\hat{U}(\theta)$, provide an
efficient algorithm that decomposes a choice of $\hat{U}(\theta)$ into its
shortest sequence of gates, and show how to efficiently choose an achievable
$\hat{U}(\theta)$ that for fixed $L$, is an optimal approximation to objective
functions on its quadratures. A strong connection is forged with
\emph{classical} discrete-time signal processing, allowing us to swiftly
construct, as examples, compensated gates with optimal bandwidth that implement
arbitrary single spin rotations with sub-wavelength spatial selectivity."
Isaac Chuang,Chuang_Isaac,arXiv:1603.03948,https://arxiv.org/abs/1603.03948,"Abstract:  It is an oft-cited fact that no quantum code can support a set of
fault-tolerant logical gates that is both universal and transversal. This no-go
theorem is generally responsible for the interest in alternative universality
constructions including magic state distillation. Widely overlooked, however,
is the possibility of non-transversal, yet still fault-tolerant, gates that
work directly on small quantum codes. Here we demonstrate precisely the
existence of such gates. In particular, we show how the limits of
non-transversality can be overcome by performing rounds of intermediate
error-correction to create logical gates on stabilizer codes that use no
ancillas other than those required for syndrome measurement. Moreover, the
logical gates we construct, the most prominent examples being Toffoli and
controlled-controlled-Z, often complete universal gate sets on their codes. We
detail such universal constructions for the smallest quantum codes, the 5-qubit
and 7-qubit codes, and then proceed to generalize the approach. One remarkable
result of this generalization is that any nondegenerate stabilizer code with a
complete set of fault-tolerant single-qubit Clifford gates has a universal set
of fault-tolerant gates. Another is the interaction of logical qubits across
different stabilizer codes, which, for instance, implies a broadly applicable
method of code switching."
Isaac Chuang,Chuang_Isaac,arXiv:1508.05699,https://arxiv.org/abs/1508.05699,"Abstract:  We describe a cheating strategy enabled by the features of massive open
online courses (MOOCs) and detectable by virtue of the sophisticated data
systems that MOOCs provide. The strategy, Copying Answers using Multiple
Existences Online (CAMEO), involves a user who gathers solutions to assessment
questions using a ""harvester"" account and then submits correct answers using a
separate ""master"" account. We use ""clickstream"" learner data to detect CAMEO
use among 1.9 million course participants in 115 MOOCs from two universities.
Using conservative thresholds, we estimate CAMEO prevalence at 1,237
certificates, accounting for 1.3% of the certificates in the 69 MOOCs with
CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO
strategy. CAMEO users are more likely to be young, male, and international than
other MOOC certificate earners. We identify preventive strategies that can
decrease CAMEO rates and show evidence of their effectiveness in science
courses."
Isaac Chuang,Chuang_Isaac,arXiv:1507.08852,https://arxiv.org/abs/1507.08852,"Abstract:  Quantum computers are able to outperform classical algorithms. This was long
recognized by the visionary Richard Feynman who pointed out in the 1980s that
quantum mechanical problems were better solved with quantum machines. It was
only in 1994 that Peter Shor came up with an algorithm that is able to
calculate the prime factors of a large number vastly more efficiently than
known possible with a classical computer. This paradigmatic algorithm
stimulated the flourishing research in quantum information processing and the
quest for an actual implementation of a quantum computer. Over the last fifteen
years, using skillful optimizations, several instances of a Shor algorithm have
been implemented on various platforms and clearly proved the feasibility of
quantum factoring. For general scalability, though, a different approach has to
be pursued. Here, we report the realization of a fully scalable Shor algorithm
as proposed by Kitaev. For this, we demonstrate factoring the number fifteen by
effectively employing and controlling seven qubits and four ""cache-qubits"",
together with the implementation of generalized arithmetic operations, known as
modular multipliers. The scalable algorithm has been realized with an ion-trap
quantum computer exhibiting success probabilities in excess of 90%."
Isaac Chuang,Chuang_Isaac,arXiv:1505.03381,https://arxiv.org/abs/1505.03381,"Abstract:  We study the vacuum-induced degradation of high-finesse optical cavities with
mirror coatings composed of SiO$_2$-Ta$_{2}$O$_{5}$ dielectric stacks, and
present methods to protect these coatings and to recover their initial quality
factor. For separate coatings with reflectivities centered at 370 nm and 422
nm, a vacuum-induced continuous increase in optical loss occurs if the
surface-layer coating is made of Ta$_{2}$O$_{5}$, while it does not occur if it
is made of SiO$_2$. The incurred optical loss can be reversed by filling the
vacuum chamber with oxygen at atmospheric pressure, and the recovery rate can
be strongly accelerated by continuous laser illumination at 422 nm. Both the
degradation and the recovery processes depend strongly on temperature. We find
that a 1 nm-thick layer of SiO$_2$ passivating the Ta$_{2}$O$_{5}$ surface
layer is sufficient to reduce the degradation rate by more than a factor of 10,
strongly supporting surface oxygen depletion as the primary degradation
mechanism."
Isaac Chuang,Chuang_Isaac,arXiv:1502.05739,https://arxiv.org/abs/1502.05739,"Abstract:  Scaling-up from prototype systems to dense arrays of ions on chip, or vast
networks of ions connected by photonic channels, will require developing
entirely new technologies that combine miniaturized ion trapping systems with
devices to capture, transmit and detect light, while refining how ions are
confined and controlled. Building a cohesive ion system from such diverse parts
involves many challenges, including navigating materials incompatibilities and
undesired coupling between elements. Here, we review our recent efforts to
create scalable ion systems incorporating unconventional materials such as
graphene and indium tin oxide, integrating devices like optical fibers and
mirrors, and exploring alternative ion loading and trapping techniques."
Isaac Chuang,Chuang_Isaac,arXiv:1409.7993,https://arxiv.org/abs/1409.7993,"Abstract:  Conventional wisdom dictates that to image the position of fluorescent atoms
or molecules, one should stimulate as much emission and collect as many photons
as possible. That is, in this classical case, it has always been assumed that
the coherence time of the system should be made short, and that the statistical
scaling $\sim1/\sqrt{t}$ defines the resolution limit for imaging time $t$.
However, here we show in contrast that given the same resources, a long
coherence time permits a higher resolution image. In this quantum regime, we
give a procedure for determining the position of a single two-level system, and
demonstrate that the standard errors of our position estimates scale at the
Heisenberg limit as $\sim 1/t$, a quadratic, and notably optimal, improvement
over the classical case."
Isaac Chuang,Chuang_Isaac,arXiv:1409.3305,https://arxiv.org/abs/1409.3305,"Abstract:  Grover's quantum search and its generalization, quantum amplitude
amplification, provide quadratic advantage over classical algorithms for a
diverse set of tasks, but are tricky to use without knowing beforehand what
fraction $\lambda$ of the initial state is comprised of the target states. In
contrast, fixed-point search algorithms need only a reliable lower bound on
this fraction, but, as a consequence, lose the very quadratic advantage that
makes Grover's algorithm so appealing. Here we provide the first version of
amplitude amplification that achieves fixed-point behavior without sacrificing
the quantum speedup. Our result incorporates an adjustable bound on the failure
probability, and, for a given number of oracle queries, guarantees that this
bound is satisfied over the broadest possible range of $\lambda$."
Isaac Chuang,Chuang_Isaac,arXiv:1402.7359,https://arxiv.org/abs/1402.7359,"Abstract:  Performing exact inference on Bayesian networks is known to be #P-hard.
Typically approximate inference techniques are used instead to sample from the
distribution on query variables given the values $e$ of evidence variables.
Classically, a single unbiased sample is obtained from a Bayesian network on
$n$ variables with at most $m$ parents per node in time
$\mathcal{O}(nmP(e)^{-1})$, depending critically on $P(e)$, the probability the
evidence might occur in the first place. By implementing a quantum version of
rejection sampling, we obtain a square-root speedup, taking
$\mathcal{O}(n2^mP(e)^{-\frac12})$ time per sample. We exploit the Bayesian
network's graph structure to efficiently construct a quantum state, a q-sample,
representing the intended classical distribution, and also to efficiently apply
amplitude amplification, the source of our speedup. Thus, our speedup is
notable as it is unrelativized -- we count primitive operations and require no
blackbox oracle queries."
Isaac Chuang,Chuang_Isaac,arXiv:1310.3173,https://arxiv.org/abs/1310.3173,"Abstract:  Massive Open Online Courses are an exciting new avenue for instruction and
research, yet they are full of unknowns. In the Spring of 2013, MITx released
its first introductory physics MOOC through the edX platform, generating a
total enrollment of 43,000 students from around the world. We describe the
population of participants in terms of their age, gender, level of education,
and country of origin, highlighting both the diversity of 8.02x enrollees as
well as gender gap and retention. Using three midterm exams and the final as
waypoints, we highlight performance by different demographic subpopulations and
their retention rates. Our work is generally aimed at making a bridge between
available MOOC data and topics associated with the Physics Education Research
community."
Isaac Chuang,Chuang_Isaac,arXiv:1307.2211,https://arxiv.org/abs/1307.2211,"Abstract:  Implementing a single qubit unitary is often hampered by imperfect control.
Systematic amplitude errors $\epsilon$, caused by incorrect duration or
strength of a pulse, are an especially common problem. But a sequence of
imperfect pulses can provide a better implementation of a desired operation, as
compared to a single primitive pulse. We find optimal pulse sequences
consisting of $L$ primitive $\pi$ or $2\pi$ rotations that suppress such errors
to arbitrary order $\mathcal{O}(\epsilon^{n})$ on arbitrary initial states.
Optimality is demonstrated by proving an $L=\mathcal{O}(n)$ lower bound and
saturating it with $L=2n$ solutions. Closed-form solutions for arbitrary
rotation angles are given for $n=1,2,3,4$. Perturbative solutions for any $n$
are proven for small angles, while arbitrary angle solutions are obtained by
analytic continuation up to $n=12$. The derivation proceeds by a novel
algebraic and non-recursive approach, in which finding amplitude error
correcting sequences can be reduced to solving polynomial equations."
Isaac Chuang,Chuang_Isaac,arXiv:1302.2904,https://arxiv.org/abs/1302.2904,"Abstract:  We present a novel hybrid system where an optical cavity is integrated with a
microfabricated planar-electrode ion trap. The trap electrodes produce a
tunable periodic potential allowing the trapping of up to 50 separate ion
chains spaced by 160 $\mu$m along the cavity axis. Each chain can contain up to
20 individually addressable Yb\textsuperscript{+} ions coupled to the cavity
mode. We demonstrate deterministic distribution of ions between the sites of
the electrostatic periodic potential and control of the ion-cavity coupling.
The measured strength of this coupling should allow access to the strong
collective coupling regime with $\lesssim$10 ions. The optical cavity could
serve as a quantum information bus between ions or be used to generate a strong
wavelength-scale periodic optical potential."
Isaac Chuang,Chuang_Isaac,arXiv:1212.1443,https://arxiv.org/abs/1212.1443,"Abstract:  Fluorescence collection sets the efficiency of state detection and the rate
of entanglement generation between remote trapped ion qubits. Despite efforts
to improve light collection using various optical elements, solid angle capture
is limited to ~10% for implementations that are scalable to many ions. We
present an approach based on fluorescence detection through a transparent trap
using an integrated photodetector, combining collection efficiency approaching
50% with scalability. We microfabricate transparent surface traps with indium
tin oxide and verify stable trapping of single ions. The fluorescence from a
cloud of ions is detected using a photodiode sandwiched with a transparent
trap."
Isaac Chuang,Chuang_Isaac,arXiv:1207.5846,https://arxiv.org/abs/1207.5846,"Abstract:  Fermions, as a major class of quantum particles, provide platforms for
quantum information processing beyond the possibilities of spins or bosons
which have been studied more extensively. One particularly interesting model to
study, in view of recent progress in manipulating ultracold fermion gases, is
the fermionic version of measurement-based quantum computation (MBQC), which
implements full quantum computation with only single site measurements on a
proper fermionic many-body resource state. However, it is not known which
fermionic states can be used as the resource states for MBQC and how to find
them. In this paper, we generalize the framework of spin MBQC to fermions. In
particular, we provide a general formalism to construct many-body entangled
fermion resource states for MBQC based on the fermionic projected entangled
pair state representation. We give a specific fermionic state which enables
universal MBQC and demonstrate that the non-locality inherent in fermion
systems can be properly taken care of with suitable measurement schemes. Such a
framework opens up possibilities of finding MBQC resource states which can be
more readily realized in the lab."
Isaac Chuang,Chuang_Isaac,arXiv:1202.6640,https://arxiv.org/abs/1202.6640,"Abstract:  Previous analyses of conditional \phi-phase gates for photonic qubits that
treat cross-phase modulation (XPM) in a causal, multimode, quantum field
setting suggest that a large (~\pi rad) nonlinear phase shift is always
accompanied by fidelity-degrading noise [J. H. Shapiro, Phys. Rev. A 73, 062305
(2006); J. Gea-Banacloche, Phys. Rev. A 81, 043823 (2010)]. Using an atomic
V-system to model an XPM medium, we present a conditional phase gate that, for
sufficiently small nonzero \phi, has high fidelity. The gate is made cascadable
by using using a special measurement, principal mode projection, to exploit the
quantum Zeno effect and preclude the accumulation of fidelity-degrading
departures from the principal-mode Hilbert space when both control and target
photons illuminate the gate."
Isaac Chuang,Chuang_Isaac,arXiv:1109.2995,https://arxiv.org/abs/1109.2995,"Abstract:  We model electric field noise from fluctuating patch potentials on conducting
surfaces by taking into account the finite geometry of the ion trap electrodes
to gain insight into the origin of anomalous heating in ion traps. The scaling
of anomalous heating rates with surface distance, $d$, is obtained for several
generic geometries of relevance to current ion trap designs, ranging from
planar to spheroidal electrodes. The influence of patch size is studied both by
solving Laplace's equation in terms of the appropriate Green's function as well
as through an eigenfunction expansion. Scaling with surface distance is found
to be highly dependent on the choice of geometry and the relative scale between
the spatial extent of the electrode, the ion-electrode distance, and the patch
size. Our model generally supports the $d^{-4}$ dependence currently found by
most experiments and models, but also predicts geometry-driven deviations from
this trend."
Isaac Chuang,Chuang_Isaac,arXiv:1108.0092,https://arxiv.org/abs/1108.0092,"Abstract:  Electrical charging of metal surfaces due to photoelectric generation of
carriers is of concern in trapped ion quantum computation systems, due to the
high sensitivity of the ions' motional quantum states to deformation of the
trapping potential. The charging induced by typical laser frequencies involved
in doppler cooling and quantum control is studied here, with microfabricated
surface electrode traps made of aluminum, copper, and gold, operated at 6 K
with a single Sr$^+$ ion trapped 100 $\mu$m above the trap surface. The lasers
used are at 370, 405, 460, and 674 nm, and the typical photon flux at the trap
is 10$^{14}$ photons/cm$^2$/sec. Charging is detected by monitoring the ion's
micromotion signal, which is related to the number of charges created on the
trap. A wavelength and material dependence of the charging behavior is
observed: lasers at lower wavelengths cause more charging, and aluminum
exhibits more charging than copper or gold. We describe the charging dynamic
based on a rate equation approach."
Isaac Chuang,Chuang_Isaac,arXiv:1103.5256,https://arxiv.org/abs/1103.5256,"Abstract:  An atomic ion is trapped at the tip of a single-mode optical fiber in a
cryogenic (8 K) surface-electrode ion trap. The fiber serves as an integrated
source of laser light, which drives the quadrupole qubit transition of
$^{88}$Sr$^+$. Through \emph{in situ} translation of the nodal point of the
trapping field, the Gaussian beam profile of the fiber output is imaged, and
the fiber-ion displacement, in units of the mode waist at the ion, is optimized
to within $0.13\pm0.10$ of the mode center despite an initial offset of
$3.30\pm0.10$. Fiber-induced charging at $125 \mu$W is observed to be
${\sim}10$ V/m at an ion height of $670 \mu$m, with charging and discharging
time constants of $1.6\pm0.3$ s and $4.7\pm0.6$ s respectively. This work is of
importance to large-scale, ion-based quantum information processing, where
optics integration in surface-electrode designs may be a crucial enabling
technology."
Isaac Chuang,Chuang_Isaac,arXiv:1011.5259,https://arxiv.org/abs/1011.5259,"Abstract:  A novel approach to optics integration in ion traps is demonstrated based on
a surface electrode ion trap that is microfabricated on top of a dielectric
mirror. Additional optical losses due to fabrication are found to be as low as
80 ppm for light at 422 nm. The integrated mirror is used to demonstrate light
collection from, and imaging of, a single 88 Sr+ ion trapped $169\pm4 \mu$m
above the mirror."
Isaac Chuang,Chuang_Isaac,arXiv:1010.6108,https://arxiv.org/abs/1010.6108,"Abstract:  We fabricate superconducting ion traps with niobium and niobium nitride and
trap single 88Sr ions at cryogenic temperatures. The superconducting transition
is verified and characterized by measuring the resistance and critical current
using a 4-wire measurement on the trap structure, and observing change in the
rf reflection. The lowest observed heating rate is 2.1(3) quanta/sec at 800 kHz
at 6 K and shows no significant change across the superconducting transition,
suggesting that anomalous heating is primarily caused by noise sources on the
surface. This demonstration of superconducting ion traps opens up possibilities
for integrating trapped ions and molecular ions with superconducting devices."
Isaac Chuang,Chuang_Isaac,arXiv:1010.2717,https://arxiv.org/abs/1010.2717,"Abstract:  It is well known that the ground state energy of many-particle Hamiltonians
involving only 2-body interactions can be obtained using constrained
optimizations over density matrices which arise from reducing an N-particle
state. While determining which 2-particle density matrices are ""N-
representable"" is a computationally hard problem, all known extreme
N-representable 2-particle reduced density matrices arise from a unique
N-particle pre-image, satisfying a conjecture established in 1972. We present
explicit counterexamples to this conjecture through giving Hamiltonians with
2-body interactions which have degenerate ground states that cannot be
distinguished by any 2-body operator. We relate the existence of such
counterexamples to quantum error correction codes and topologically ordered
spin systems."
Isaac Chuang,Chuang_Isaac,arXiv:1009.0036,https://arxiv.org/abs/1009.0036,"Abstract:  Two-dimensional crystals of trapped ions are a promising system with which to
implement quantum simulations of challenging problems such as spin frustration.
Here, we present a design for a surface-electrode elliptical ion trap which
produces a 2-D ion crystal and is amenable to microfabrication, which would
enable higher simulated coupling rates, as well as interactions based on
magnetic forces generated by on-chip currents. Working in an 11 K cryogenic
environment, we experimentally verify to within 5% a numerical model of the
structure of ion crystals in the trap. We also explore the possibility of
implementing quantum simulation using magnetic forces, and calculate J-coupling
rates on the order of 10^3 / s for an ion crystal height of 10 microns, using a
current of 1 A."
Isaac Chuang,Chuang_Isaac,arXiv:1008.1603,https://arxiv.org/abs/1008.1603,"Abstract:  We present a model as well as experimental results for a surface electrode
radio-frequency Paul trap that has a circular electrode geometry well-suited
for trapping of single ions and two-dimensional planar ion crystals. The trap
design is compatible with microfabrication and offers a simple method by which
the height of the trapped ions above the surface may be changed \emph{in situ}.
We demonstrate trapping of single and few Sr+ ions over an ion height range of
200-1000 microns for several hours under Doppler laser cooling, and use these
to characterize the trap, finding good agreement with our model."
Isaac Chuang,Chuang_Isaac,arXiv:1003.1774,https://arxiv.org/abs/1003.1774,"Abstract:  The tensor product representation of quantum states leads to a promising
variational approach to study quantum phase and quantum phase transitions,
especially topological ordered phases which are impossible to handle with
conventional methods due to their long range entanglement. However, an
important issue arises when we use tensor product states (TPS) as variational
states to find the ground state of a Hamiltonian: can arbitrary variations in
the tensors that represent ground state of a Hamiltonian be induced by local
perturbations to the Hamiltonian? Starting from a tensor product state which is
the exact ground state of a Hamiltonian with $\mathbb{Z}_2$ topological order,
we show that, surprisingly, not all variations of the tensors correspond to the
variation of the ground state caused by local perturbations of the Hamiltonian.
Even in the absence of any symmetry requirement of the perturbed Hamiltonian,
one necessary condition for the variations of the tensors to be physical is
that they respect certain $\mathbb{Z}_2$ symmetry. We support this claim by
calculating explicitly the change in topological entanglement entropy with
different variations in the tensors. This finding will provide important
guidance to numerical variational study of topological phase and phase
transitions. It is also a crucial step in using TPS to study universal
properties of a quantum phase and its topological order."
Isaac Chuang,Chuang_Isaac,arXiv:1002.0085,https://arxiv.org/abs/1002.0085,"Abstract:  Entanglement, as studied in quantum information science, and non-local
quantum correlations, as studied in condensed matter physics, are fundamentally
akin to each other. However, their relationship is often hard to quantify due
to the lack of a general approach to study both on the same footing. In
particular, while entanglement and non-local correlations are properties of
states, both arise from symmetries of global operators that commute with the
system Hamiltonian. Here, we introduce a framework for completely classifying
the local and non-local properties of all such global operators, given the
Hamiltonian and a bi-partitioning of the system. This framework is limited to
descriptions based on stabilizer quantum codes, but may be generalized. We
illustrate the use of this framework to study entanglement and non-local
correlations by analyzing global symmetries in topological order, distribution
of entanglement and entanglement entropy."
Isaac Chuang,Chuang_Isaac,arXiv:0912.4892,https://arxiv.org/abs/0912.4892,"Abstract:  We demonstrate quantum control techniques for a single trapped ion in a
cryogenic, surface-electrode trap. A narrow optical transition of Sr+ along
with the ground and first excited motional states of the harmonic trapping
potential form a two-qubit system. The optical qubit transition is susceptible
to magnetic field fluctuations, which we stabilize with a simple and compact
method using superconducting rings. Decoherence of the motional qubit is
suppressed by the cryogenic environment. AC Stark shift correction is
accomplished by controlling the laser phase in the pulse sequencer, eliminating
the need for an additional laser. Quantum process tomography is implemented on
atomic and motional states using conditional pulse sequences. With these
techniques we demonstrate a Cirac-Zoller Controlled-NOT gate in a single ion
with a mean fidelity of 91(1)%."
Isaac Chuang,Chuang_Isaac,arXiv:0910.4129,https://arxiv.org/abs/0910.4129,"Abstract:  Graphs are closely related to quantum error-correcting codes: every
stabilizer code is locally equivalent to a graph code, and every codeword
stabilized code can be described by a graph and a classical code. For the
construction of good quantum codes of relatively large block length,
concatenated quantum codes and their generalizations play an important role. We
develop a systematic method for constructing concatenated quantum codes based
on ""graph concatenation"", where graphs representing the inner and outer codes
are concatenated via a simple graph operation called ""generalized local
complementation."" Our method applies to both binary and non-binary concatenated
quantum codes as well as their generalizations."
Isaac Chuang,Chuang_Isaac,arXiv:0905.0148,https://arxiv.org/abs/0905.0148,"Abstract:  We report a demonstration and quantitative characterization of
one-dimensional cavity cooling of a single trapped 88Sr+ ion in the resolved
sideband regime. We measure the spectrum of cavity transitions, the rates of
cavity heating and cooling, and the steady-state cooling limit. The cavity
cooling dynamics and cooling limit of 22.5(3) motional quanta, limited by the
moderate coupling between the ion and the cavity, are consistent with a simple
model [Phys. Rev. A 64, 033405] without any free parameters, validating the
rate equation model for cavity cooling."
Isaac Chuang,Chuang_Isaac,arXiv:0812.4067,https://arxiv.org/abs/0812.4067,"Abstract:  Many-body entangled quantum states studied in condensed matter physics can be
primary resources for quantum information, allowing any quantum computation to
be realized using measurements alone, on the state. Such a universal state
would be remarkably valuable, if only it were thermodynamically stable and
experimentally accessible, by virtue of being the unique ground state of a
physically reasonable Hamiltonian made of two-body, nearest neighbor
interactions. We introduce such a state, composed of six-state particles on a
hexagonal lattice, and describe a general method for analyzing its properties
based on its projected entangled pair state representation."
Isaac Chuang,Chuang_Isaac,arXiv:0811.2422,https://arxiv.org/abs/0811.2422,"Abstract:  Dense array of ions in microfabricated traps represent one possible way to
scale up ion trap quantum computing. The ability to address individual ions is
an important component of such a scheme. We demonstrate individual addressing
of trapped ions in a microfabricated surface-electrode trap using a magnetic
field gradient generated on-chip. A frequency splitting of 310(2) kHz for two
ions separated by 5 um is achieved. Selective single qubit operations are
performed on one of two trapped ions with an average of 2.2+/-1.0% crosstalk.
Coherence time as measured by the spin-echo technique is unaffected by the
field gradient."
Isaac Chuang,Chuang_Isaac,arXiv:0809.2824,https://arxiv.org/abs/0809.2824,"Abstract:  Quantum simulations of spin systems could enable the solution of problems
which otherwise require infeasible classical resources. Such a simulation may
be implemented using a well-controlled system of effective spins, such as a
two-dimensional lattice of locally interacting ions. We propose here a layered
planar rf trap design that can be used to create arbitrary two-dimensional
lattices of ions. The design also leads naturally to ease of microfabrication.
As a first experimental demonstration, we confine strontium-88 ions in a
mm-scale lattice trap and verify numerical models of the trap by measuring the
motional frequencies. We also confine 440 nm diameter charged microspheres and
observe ion-ion repulsion between ions in neighboring lattice sites. Our
design, when scaled to smaller ion-ion distances, is appropriate for quantum
simulation schemes, e.g. that of Porras and Cirac (PRL 92 207901 (2004)). We
note, however, that in practical realizations of the trap, an increase in the
secular frequency with decreasing ion spacing may make a coupling rate that is
large relative to the decoherence rate in such a trap difficult to achieve."
Isaac Chuang,Chuang_Isaac,arXiv:0808.3086,https://arxiv.org/abs/0808.3086,"Abstract:  The codeword stabilized (CWS) quantum codes formalism presents a unifying
approach to both additive and nonadditive quantum error-correcting codes
(arXiv:0708.1021 [quant-ph]), but only for binary states. Here we generalize
the CWS framework to the nonbinary case (of both prime and nonprime dimension)
and map the search for nonbinary quantum codes to a corresponding search
problem for classical nonbinary codes with specific error patterns. We show
that while the additivity properties of nonbinary CWS codes are similar to the
binary case, the structural properties of the nonbinary codes differ
substantially from the binary case, even for prime dimensions. In particular,
we identify specific structure patterns of stabilizer groups, based on which
efficient constructions might be possible for codes that encode more dimensions
than any stabilizer codes of the same length and distance; similar methods
cannot be applied in the binary case. Understanding of these structural
properties can help prune the search space and facilitate the identification of
good nonbinary CWS codes."
Isaac Chuang,Chuang_Isaac,arXiv:0804.2665,https://arxiv.org/abs/0804.2665,"Abstract:  Electric field noise from fluctuating patch potentials is a significant
problem for a broad range of precision experiments, including trapped ion
quantum computation and single spin detection. Recent results demonstrated
strong suppression of this noise by cryogenic cooling, suggesting an underlying
thermal process. We present measurements characterizing the temperature and
frequency dependence of the noise from 7 to 100 K, using a single Sr+ ion
trapped 75 um above the surface of a gold plated surface electrode ion trap.
The noise amplitude is observed to have an approximate 1/f spectrum around 1
MHz, and grows rapidly with temperature as T^beta for beta from 2 to 4. The
data are consistent with microfabricated cantilever measurements of non-contact
friction but do not extrapolate to the DC measurements with neutral atoms or
contact potential probes."
Isaac Chuang,Chuang_Isaac,arXiv:0803.3232,https://arxiv.org/abs/0803.3232,"Abstract:  The codeword stabilized (""CWS"") quantum codes formalism presents a unifying
approach to both additive and nonadditive quantum error-correcting codes
(arXiv:0708.1021). This formalism reduces the problem of constructing such
quantum codes to finding a binary classical code correcting an error pattern
induced by a graph state. Finding such a classical code can be very difficult.
Here, we consider an algorithm which maps the search for CWS codes to a problem
of identifying maximum cliques in a graph. While solving this problem is in
general very hard, we prove three structure theorems which reduce the search
space, specifying certain admissible and optimal ((n,K,d)) additive codes. In
particular, we find there does not exist any ((7,3,3)) CWS code though the
linear programming bound does not rule it out. The complexity of the CWS search
algorithm is compared with the contrasting method introduced by Aggarwal and
Calderbank (arXiv:cs/0610159)."
Isaac Chuang,Chuang_Isaac,arXiv:0801.2360,https://arxiv.org/abs/0801.2360,"Abstract:  A long-standing open problem in fault-tolerant quantum computation has been
to find a universal set of transversal gates. As three of us proved in arXiv:
0706.1382, such a set does not exist for binary stabilizer codes. Here we
generalize our work to show that for subsystem stabilizer codes in $d$
dimensional Hilbert space, such a universal set of transversal gates cannot
exist for even one encoded qudit, for any dimension $d$, prime or nonprime.
This result strongly supports the idea that other primitives, such as quantum
teleportation, are necessary for universal fault-tolerant quantum computation,
and may be an important factor for fault tolerance noise thresholds."
Isaac Chuang,Chuang_Isaac,arXiv:0712.2084,https://arxiv.org/abs/0712.2084,"Abstract:  Teleportation is a crucial element in fault-tolerant quantum computation and
a complete understanding of its capacity is very important for the practical
implementation of optimal fault-tolerant architectures. It is known that
stabilizer codes support a natural set of gates that can be more easily
implemented by teleportation than any other gates. These gates belong to the so
called $\mathcal{C}_k$ hierarchy introduced by Gottesman and Chuang (Nature
\textbf{402}, 390). Moreover, a subset of $\mathcal{C}_k$ gates, called
semi-Clifford operations, can be implemented by an even simpler architecture
than the traditional teleportation setup (Phys. Rev. \textbf{A62}, 052316).
However, the precise set of gates in $\mathcal{C}_k$ remains unknown, even for
a fixed number of qubits $n$, which prevents us from knowing exactly what
teleportation is capable of. In this paper we study the structure of
$\mathcal{C}_k$ in terms of semi-Clifford operations, which send by conjugation
at least one maximal abelian subgroup of the $n$-qubit Pauli group into another
one. We show that for $n=1,2$, all the $\mathcal{C}_k$ gates are semi-Clifford,
which is also true for $\{n=3,k=3\}$. However, this is no longer true for
$\{n>2,k>3\}$. To measure the capability of this teleportation primitive, we
introduce a quantity called `teleportation depth', which characterizes how many
teleportation steps are necessary, on average, to implement a given gate. We
calculate upper bounds for teleportation depth by decomposing gates into both
semi-Clifford $\mathcal{C}_k$ gates and those $\mathcal{C}_k$ gates beyond
semi-Clifford operations, and compare their efficiency."
Isaac Chuang,Chuang_Isaac,arXiv:0706.3763,https://arxiv.org/abs/0706.3763,"Abstract:  Dense arrays of trapped ions provide one way of scaling up ion trap quantum
information processing. However, miniaturization of ion traps is currently
limited by sharply increasing motional state decoherence at sub-100 um
ion-electrode distances. We characterize heating rates in cryogenically cooled
surface-electrode traps, with characteristic sizes in 75 um to 150 um range.
Upon cooling to 6 K, the measured rates are suppressed by 7 orders of
magnitude, two orders of magnitude below previously published data of similarly
sized traps operated at room temperature. The observed noise depends strongly
on fabrication process, which suggests further improvements are possible."
Isaac Chuang,Chuang_Isaac,arXiv:0706.3374,https://arxiv.org/abs/0706.3374,"Abstract:  We demonstrate loading by laser ablation of $^{88}$Sr$^+$ ions into a
mm-scale surface-electrode ion trap. The laser used for ablation is a pulsed,
frequency-tripled Nd:YAG with pulse energies of 1-10 mJ and durations of 3-5
ns. An additional laser is not required to photoionize the ablated material.
The efficiency and lifetime of several candidate materials for the laser
ablation target are characterized by measuring the trapped ion fluorescence
signal for a number of consecutive loads. Additionally, laser ablation is used
to load traps with a trap depth (40 meV) below where electron impact ionization
loading is typically successful ($\gtrsim$ 500 meV)."
Isaac Chuang,Chuang_Isaac,arXiv:0706.1382,https://arxiv.org/abs/0706.1382,"Abstract:  Certain quantum codes allow logic operations to be performed on the encoded
data, such that a multitude of errors introduced by faulty gates can be
corrected. An important class of such operations are {\em transversal}, acting
bitwise between corresponding qubits in each code block, thus allowing error
propagation to be carefully limited. If any quantum operation could be
implemented using a set of such gates, the set would be {\em universal}; codes
with such a universal, transversal gate set have been widely desired for
efficient fault-tolerant quantum computation. We study the structure of
GF(4)-additive quantum codes and prove that no universal set of transversal
logic operations exists for these codes. This result strongly supports the idea
that additional primitive operations, based for example on quantum
teleportation, are necessary to achieve universal fault-tolerant computation on
additive codes."
Isaac Chuang,Chuang_Isaac,arXiv:physics/0702025,https://arxiv.org/abs/physics/0702025,"Abstract:  We produce large numbers of low-energy ions by photoionization of
laser-cooled atoms inside a surface-electrode-based Paul trap. The
isotope-selective trap loading rate of $4\times10^{5}$ Yb$^{+}$ ions/s exceeds
that attained by photoionization (electron impact ionization) of an atomic beam
by four (six) orders of magnitude. Traps as shallow as 0.13 eV are easily
loaded with this technique. The ions are confined in the same spatial region as
the laser-cooled atoms, which will allow the experimental investigation of
interactions between cold ions and cold atoms or Bose-Einstein condensates."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0611214,https://arxiv.org/abs/quant-ph/0611214,"Abstract:  The equivalence of stabilizer states under local transformations is of
fundamental interest in understanding properties and uses of entanglement. Two
stabilizer states are equivalent under the usual stochastic local operations
and classical communication criterion if and only if they are equivalent under
local unitary (LU) operations. More surprisingly, under certain conditions, two
LU equivalent stabilizer states are also equivalent under local Clifford (LC)
operations, as was shown by Van den Nest et al. [Phys. Rev. \textbf{A71},
062323]. Here, we broaden the class of stabilizer states for which LU
equivalence implies LC equivalence ($LU\Leftrightarrow LC$) to include all
stabilizer states represented by graphs with neither cycles of length 3 nor 4.
To compare our result with Van den Nest et al.'s, we show that any stabilizer
state of distance $\delta=2$ is beyond their criterion. We then further prove
that $LU\Leftrightarrow LC$ holds for a more general class of stabilizer states
of $\delta=2$. We also explicitly construct graphs representing $\delta>2$
stabilizer states which are beyond their criterion: we identify all 58 graphs
with up to 11 vertices and construct graphs with $2^m-1$ ($m\geq 4$) vertices
using quantum error correcting codes which have non-Clifford transversal gates."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0604070,https://arxiv.org/abs/quant-ph/0604070,"Abstract:  The assumption of maximum parallelism support for the successful realization
of scalable quantum computers has led to homogeneous, ``sea-of-qubits''
architectures. The resulting architectures overcome the primary challenges of
reliability and scalability at the cost of physically unacceptable system area.
We find that by exploiting the natural serialization at both the application
and the physical microarchitecture level of a quantum computer, we can reduce
the area requirement while improving performance. In particular we present a
scalable quantum architecture design that employs specialization of the system
into memory and computational regions, each individually optimized to match
hardware support to the available parallelism. Through careful application and
system analysis, we find that our new architecture can yield up to a factor of
thirteen savings in area due to specialization. In addition, by providing a
memory hierarchy design for quantum computers, we can increase time performance
by a factor of eight. This result brings us closer to the realization of a
quantum processor that can solve meaningful problems."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0603142,https://arxiv.org/abs/quant-ph/0603142,"Abstract:  We demonstrate a method for loading surface electrode ion traps by electron
impact ionization. The method relies on the property of surface electrode
geometries that the trap depth can be increased at the cost of more
micromotion. By introducing a buffer gas, we can counteract the rf heating
assocated with the micromotion and benefit from the larger trap depth. After an
initial loading of the trap, standard compensation techniques can be used to
cancel the stray fields resulting from charged dielectric and allow for the
loading of the trap at ultra-high vacuum."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0601021,https://arxiv.org/abs/quant-ph/0601021,"Abstract:  Quantum simulation uses a well-known quantum system to predict the behavior
of another quantum system. Certain limitations in this technique arise,
however, when applied to specific problems, as we demonstrate with a
theoretical and experimental study of an algorithm to find the low-lying
spectrum of a Hamiltonian. While the number of elementary quantum gates does
scale polynomially with the size of the system, it increases inversely to the
desired error bound $\epsilon$. Making such simulations robust to decoherence
using fault-tolerance constructs requires an additional factor of $1/ \epsilon$
gates. These constraints are illustrated by using a three qubit nuclear
magnetic resonance system to simulate a pairing Hamiltonian, following the
algorithm proposed by Wu, Byrd, and Lidar."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0601001,https://arxiv.org/abs/quant-ph/0601001,"Abstract:  We present an efficient family of quantum circuits for a fundamental
primitive in quantum information theory, the Schur transform. The Schur
transform on n d-dimensional quantum systems is a transform between a standard
computational basis to a labelling related to the representation theory of the
symmetric and unitary groups. If we desire to implement the Schur transform to
an accuracy of epsilon, then our circuit construction uses a number of gates
which is polynomial in n, d and log(1/epsilon). The important insights we use
to perform this construction are the selection of the appropriate subgroup
adapted basis and the Wigner-Eckart theorem. Our efficient circuit construction
renders numerous protocols in quantum information theory computationally
tractable and is an important new efficient quantum circuit family which goes
significantly beyond the standard paradigm of the quantum Fourier transform."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0509051,https://arxiv.org/abs/quant-ph/0509051,"Abstract:  Recent experimental advances have demonstrated technologies capable of
supporting scalable quantum computation. A critical next step is how to put
those technologies together into a scalable, fault-tolerant system that is also
feasible. We propose a Quantum Logic Array (QLA) microarchitecture that forms
the foundation of such a system. The QLA focuses on the communication resources
necessary to efficiently support fault-tolerant computations. We leverage the
extensive groundwork in quantum error correction theory and provide analysis
that shows that our system is both asymptotically and empirically fault
tolerant. Specifically, we use the QLA to implement a hierarchical, array-based
design and a logarithmic expense quantum-teleportation communication protocol.
Our goal is to overcome the primary scalability challenges of reliability,
communication, and quantum resource distribution that plague current proposals
for large-scale quantum computing."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0409170,https://arxiv.org/abs/quant-ph/0409170,"Abstract:  The role of mixed state entanglement in liquid-state nuclear magnetic
resonance (NMR) quantum computation is not yet well-understood. In particular,
despite the success of quantum information processing with NMR, recent work has
shown that quantum states used in most of those experiments were not entangled.
This is because these states, derived by unitary transforms from the thermal
equilibrium state, were too close to the maximally mixed state. We are thus
motivated to determine whether a given NMR state is entanglable - that is, does
there exist a unitary transform that entangles the state? The boundary between
entanglable and nonentanglable thermal states is a function of the spin system
size $N$ and its temperature $T$. We provide new bounds on the location of this
boundary using analytical and numerical methods; our tightest bound scales as
$N \sim T$, giving a lower bound requiring at least $N \sim 22,000$ proton
spins to realize an entanglable thermal state at typical laboratory NMR
magnetic fields. These bounds are tighter than known bounds on the
entanglability of effective pure states."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0408166,https://arxiv.org/abs/quant-ph/0408166,"Abstract:  Nuclear Magnetic Resonance (NMR) has provided a valuable experimental testbed
for quantum information processing (QIP). Here, we briefly review the use of
nuclear spins as qubits, and discuss the current status of NMR-QIP. Advances in
the techniques available for control are described along with the various
implementations of quantum algorithms and quantum simulations that have been
performed using NMR. The recent application of NMR control techniques to other
quantum computing systems are reviewed before concluding with a description of
the efforts currently underway to transition to solid state NMR systems that
hold promise for scalable architectures."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0407082,https://arxiv.org/abs/quant-ph/0407082,"Abstract:  The Schur basis on n d-dimensional quantum systems is a generalization of the
total angular momentum basis that is useful for exploiting symmetry under
permutations or collective unitary rotations. We present efficient (size
poly(n,d,log(1/\epsilon)) for accuracy \epsilon) quantum circuits for the Schur
transform, which is the change of basis between the computational and the Schur
bases. These circuits are based on efficient circuits for the Clebsch-Gordan
transformation. We also present an efficient circuit for a limited version of
the Schur transform in which one needs only to project onto different Schur
subspaces. This second circuit is based on a generalization of phase estimation
to any nonabelian finite group for which there exists a fast quantum Fourier
transform."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0407022,https://arxiv.org/abs/quant-ph/0407022,"Abstract:  Systematic errors in quantum operations can be the dominating source of
imperfection in achieving control over quantum systems. This problem, which has
been well studied in nuclear magnetic resonance, can be addressed by replacing
single operations with composite sequences of pulsed operations, which cause
errors to cancel by symmetry. Remarkably, this can be achieved without
knowledge of the amount of error epsilon. Independent of the initial state of
the system, current techniques allow the error to be reduced to O(epsilon^3).
Here, we extend the composite pulse technique to cancel errors to O(epsilon^n),
for arbitrary n."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0406168,https://arxiv.org/abs/quant-ph/0406168,"Abstract:  We define a multi-partite entanglement measure for stabilizer states, which
can be computed efficiently from a set of generators of the stabilizer group.
Our measure applies to qubits, qudits and continuous variables."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0404064,https://arxiv.org/abs/quant-ph/0404064,"Abstract:  Fifty years of developments in nuclear magnetic resonance (NMR) have resulted
in an unrivaled degree of control of the dynamics of coupled two-level quantum
systems. This coherent control of nuclear spin dynamics has recently been taken
to a new level, motivated by the interest in quantum information processing.
NMR has been the workhorse for the experimental implementation of quantum
protocols, allowing exquisite control of systems up to seven qubits in size.
Here, we survey and summarize a broad variety of pulse control and tomographic
techniques which have been developed for and used in NMR quantum computation.
Many of these will be useful in other quantum systems now being considered for
implementation of quantum information processing tasks."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0302057,https://arxiv.org/abs/quant-ph/0302057,"Abstract:  We report the realization of a nuclear magnetic resonance computer with three
quantum bits that simulates an adiabatic quantum optimization algorithm.
Adiabatic quantum algorithms offer new insight into how quantum resources can
be used to solve hard problems. This experiment uses a particularly well suited
three quantum bit molecule and was made possible by introducing a technique
that encodes general instances of the given optimization problem into an easily
applicable Hamiltonian. Our results indicate an optimal run time of the
adiabatic algorithm that agrees well with the prediction of a simple
decoherence model."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0112176,https://arxiv.org/abs/quant-ph/0112176,"Abstract:  The number of steps any classical computer requires in order to find the
prime factors of an $l$-digit integer $N$ increases exponentially with $l$, at
least using algorithms known at present. Factoring large integers is therefore
conjectured to be intractable classically, an observation underlying the
security of widely used cryptographic codes. Quantum computers, however, could
factor integers in only polynomial time, using Shor's quantum factoring
algorithm. Although important for the study of quantum computers, experimental
demonstration of this algorithm has proved elusive. Here we report an
implementation of the simplest instance of Shor's algorithm: factorization of
${N=15}$ (whose prime factors are 3 and 5). We use seven spin-1/2 nuclei in a
molecule as quantum bits, which can be manipulated with room temperature liquid
state nuclear magnetic resonance techniques. This method of using nuclei to
store quantum information is in principle scalable to many quantum bit systems,
but such scalability is not implied by the present work. The significance of
our work lies in the demonstration of experimental and theoretical techniques
for precise control and modelling of complex quantum computers. In particular,
we present a simple, parameter-free but predictive model of decoherence effects
in our system."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0111031,https://arxiv.org/abs/quant-ph/0111031,"Abstract:  Quantum compiling addresses the problem of approximating an arbitrary quantum
gate with a string of gates drawn from a particular finite set. It has been
shown that this is possible for almost all choices of base sets and furthermore
that the number of gates required for precision epsilon is only polynomial in
log 1/epsilon. Here we prove that using certain sets of base gates quantum
compiling requires a string length that is linear in log 1/epsilon, a result
which matches the lower bound from counting volume up to constant factor."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0105147,https://arxiv.org/abs/quant-ph/0105147,"Abstract:  Current experiments in liquid-state nuclear magnetic resonance quantum
computing are limited by low initial polarization. To address this problem, we
have investigated the use of optical pumping techniques to enhance the
polarization of a 2-qubit NMR quantum computer (13C and 1H in 13CHCl3). To
efficiently use the increased polarization, we have generalized the procedure
for effective pure state preparation. With this new, more flexible scheme, an
effective pure state was prepared with polarization-enhancement of a factor of
10 compared to the thermal state. An implementation of Grover's quantum search
algorithm was demonstrated using this new technique."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0105032,https://arxiv.org/abs/quant-ph/0105032,"Abstract:  We present a quantum digital signature scheme whose security is based on
fundamental principles of quantum physics. It allows a sender (Alice) to sign a
message in such a way that the signature can be validated by a number of
different people, and all will agree either that the message came from Alice or
that it has been tampered with. To accomplish this task, each recipient of the
message must have a copy of Alice's ""public key,"" which is a set of quantum
states whose exact identity is known only to Alice. Quantum public keys are
more difficult to deal with than classical public keys: for instance, only a
limited number of copies can be in circulation, or the scheme becomes insecure.
However, in exchange for this price, we achieve unconditionally secure digital
signatures. Sending an m-bit message uses up O(m) quantum bits for each
recipient of the public key. We briefly discuss how to securely distribute
quantum public keys, and show the signature scheme is absolutely secure using
one method of key distribution. The protocol provides a model for importing the
ideas of classical public key cryptography into the quantum world."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0012108,https://arxiv.org/abs/quant-ph/0012108,"Abstract:  This in an introduction on quantum computing and on the use of NMR to build
quantum computers, geared towards an NMR audience."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0012032,https://arxiv.org/abs/quant-ph/0012032,"Abstract:  Quantum process tomography is a procedure by which the unknown dynamical
evolution of an open quantum system can be fully experimentally characterized.
We demonstrate explicitly how this procedure can be implemented with a nuclear
magnetic resonance quantum computer. This allows us to measure the fidelity of
a controlled-not logic gate and to experimentally investigate the error model
for our computer. Based on the latter analysis, we test an important assumption
underlying nearly all models of quantum error correction, the independence of
errors on different qubits."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0008070,https://arxiv.org/abs/quant-ph/0008070,"Abstract:  Although the conditions for performing arbitrary unitary operations to
simulate the dynamics of a closed quantum system are well understood, the same
is not true of the more general class of quantum operations (also known as
superoperators) corresponding to the dynamics of open quantum systems. We
propose a framework for the generation of Markovian quantum dynamics and study
the resources needed for universality. For the case of a single qubit, we show
that a single nonunitary process is necessary and sufficient to generate all
unital Markovian quantum dynamics, whereas a set of processes parametrized by
one continuous parameter is needed in general. We also obtain preliminary
results for the unital case in higher dimensions."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0008065,https://arxiv.org/abs/quant-ph/0008065,"Abstract:  Although the initial proposal for ion trap quantum computation made use of an
auxiliary internal level to perform logic between ions, this resource is not
necessary in principle. Instead, one may perform such operations directly using
sideband laser pulses, operating with an arbitrary (sufficiently small)
Lamb-Dicke parameter. We explore the potential of this technique, showing how
to perform logical operations between the internal state of an ion and the
collective motional state and giving explicit constructions for a
controlled-not gate between ions."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0007017,https://arxiv.org/abs/quant-ph/0007017,"Abstract:  We report the realization of a nuclear magnetic resonance (NMR) quantum
computer which combines the quantum Fourier transform (QFT) with exponentiated
permutations, demonstrating a quantum algorithm for order-finding. This
algorithm has the same structure as Shor's algorithm and its speed-up over
classical algorithms scales exponentially. The implementation uses a
particularly well-suited five quantum bit molecule and was made possible by a
new state initialization procedure and several quantum control techniques."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0005092,https://arxiv.org/abs/quant-ph/0005092,"Abstract:  The clock synchronization problem is to determine the time difference
$\Delta$ between two spatially separated clocks. When message delivery times
between the two clocks are uncertain, $O(2^{2n})$ classical messages must be
exchanged between the clocks to determine $n$ digits of $\Delta$. On the other
hand, as we show, there exists a quantum algorithm to obtain $n$ digits of
$\Delta$ while communicating only O(n) quantum messages."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/0002039,https://arxiv.org/abs/quant-ph/0002039,"Abstract:  We present a general method to construct fault-tolerant quantum logic gates
with a simple primitive, which is an analog of quantum teleportation. The
technique extends previous results based on traditional quantum teleportation
(Gottesman and Chuang, Nature {\bf 402}, 390, 1999) and leads to
straightforward and systematic construction of many fault-tolerant encoded
operations, including the $\pi/8$ and Toffoli gates. The technique can also be
applied to the construction of remote quantum operations that cannot be
directly performed."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9910075,https://arxiv.org/abs/quant-ph/9910075,"Abstract:  We report the experimental implementation of Grover's quantum search
algorithm on a quantum computer with three quantum bits. The computer consists
of molecules of $^{13}$C-labeled CHFBr$_2$, in which the three weakly coupled
spin-1/2 nuclei behave as the bits and are initialized, manipulated, and read
out using magnetic resonance techniques. This quantum computation is made
possible by the introduction of two techniques which significantly reduce the
complexity of the experiment and by the surprising degree of cancellation of
systematic errors which have previously limited the total possible number of
quantum gates."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9908010,https://arxiv.org/abs/quant-ph/9908010,"Abstract:  We present a method to create a variety of interesting gates by teleporting
quantum bits through special entangled states. This allows, for instance, the
construction of a quantum computer based on just single qubit operations, Bell
measurements, and GHZ states. We also present straightforward constructions of
a wide variety of fault-tolerant quantum gates."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9907063,https://arxiv.org/abs/quant-ph/9907063,"Abstract:  Liquid crystals offer several advantages as solvents for molecules used for
nuclear magnetic resonance quantum computing (NMRQC). The dipolar coupling
between nuclear spins manifest in the NMR spectra of molecules oriented by a
liquid crystal permits a significant increase in clock frequency, while short
spin-lattice relaxation times permit fast recycling of algorithms, and save
time in calibration and signal-enhancement experiments. Furthermore, the use of
liquid crystal solvents offers scalability in the form of an expanded library
of spin-bearing molecules suitable for NMRQC. These ideas are demonstrated with
the successful execution of a 2-qubit Grover search using a molecule
($^{13}$C$^{1}$HCl$_3$) oriented in a liquid crystal and a clock speed eight
times greater than in an isotropic solvent. Perhaps more importantly, five
times as many logic operations can be executed within the coherence time using
the liquid crystal solvent."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9906112,https://arxiv.org/abs/quant-ph/9906112,"Abstract:  Realistic physical implementations of quantum computers can entail tradeoffs
which depart from the ideal model of quantum computation. Although these
tradeoffs have allowed successful demonstration of certain quantum algorithms,
a crucial question is whether they fundamentally limit the computational
capacity of such machines. We study the limitations of a quantum computation
model in which only ensemble averages of measurement observables are
accessible. Furthermore, we stipulate that input qubits may only be prepared in
highly random, ``hot'' mixed states. In general, these limitations are believed
to dramatically detract from the computational power of the system. However, we
construct a class of algorithms for this limited model, which, surprisingly,
are polynomially equivalent to the ideal case. This class includes the well
known Deutsch-Jozsa algorithm."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9905041,https://arxiv.org/abs/quant-ph/9905041,"Abstract:  We report the first use of ""logical labeling"" to perform a quantum
computation with a room-temperature bulk system. This method entails the
selection of a subsystem which behaves as if it were at zero temperature -
except for a decrease in signal strength - conditioned upon the state of the
remaining system. No averaging over differently prepared molecules is required.
In order to test this concept, we execute a quantum search algorithm in a
subspace of two nuclear spins, labeled by a third spin, using solution nuclear
magnetic resonance (NMR), and employing a novel choice of reference frame to
uncouple nuclei."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9904100,https://arxiv.org/abs/quant-ph/9904100,"Abstract:  We present an efficient scheme which couples any designated pair of spins in
heteronuclear spin systems. The scheme is based on the existence of Hadamard
matrices. For a system of $n$ spins with pairwise coupling, the scheme
concatenates $cn$ intervals of system evolution and uses at most $c n^2$ pulses
where $c \approx 1$. Our results demonstrate that, in many systems, selective
recoupling is possible with linear overhead, contrary to common speculation
that exponential effort is always required."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9811068,https://arxiv.org/abs/quant-ph/9811068,"Abstract:  Using nuclear magnetic resonance techniques, we experimentally investigated
the effects of applying a two bit phase error detection code to preserve
quantum information in nuclear spin systems. Input states were stored with and
without coding, and the resulting output states were compared with the
originals and with each other. The theoretically expected result, net reduction
of distortion and conditional error probabilities to second order, was indeed
observed, despite imperfect coding operations which increased the error
probabilities by approximately 5%. Systematic study of the deviations from the
ideal behavior provided quantitative measures of different sources of error,
and good agreement was found with a numerical model. Theoretical questions in
quantum error correction in bulk nuclear spin systems including fidelity
measures, signal strength and syndrome measurements are discussed."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9801037,https://arxiv.org/abs/quant-ph/9801037,"Abstract:  Nuclear magnetic resonance techniques are used to realize a quantum algorithm
experimentally. The algorithm allows a simple NMR quantum computer to determine
global properties of an unknown function requiring fewer function ``calls''
than is possible using a classical computer."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9706053,https://arxiv.org/abs/quant-ph/9706053,"Abstract:  In bulk quantum computation one can manipulate a large number of
indistinguishable quantum computers by parallel unitary operations and measure
expectation values of certain observables with limited sensitivity. The initial
state of each computer in the ensemble is known but not pure. Methods for
obtaining effective pure input states by a series of manipulations have been
described by Gershenfeld and Chuang (logical labeling) and Cory et al. (spatial
averaging) for the case of quantum computation with nuclear magnetic resonance.
We give a different technique called temporal averaging. This method is based
on classical randomization, requires no ancilla qubits and can be implemented
in nuclear magnetic resonance without using gradient fields. We introduce
several temporal averaging algorithms suitable for both high temperature and
low temperature bulk quantum computing and analyze the signal to noise behavior
of each."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9703032,https://arxiv.org/abs/quant-ph/9703032,"Abstract:  We show how to construct quantum gate arrays that can be programmed to
perform different unitary operations on a data register, depending on the input
to some program register. It is shown that a universal quantum gate array - a
gate array which can be programmed to perform any unitary operation - exists
only if one allows the gate array to operate in a probabilistic fashion. The
universal quantum gate array we construct requires an exponentially smaller
number of gates than a classical universal gate array."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9610001,https://arxiv.org/abs/quant-ph/9610001,"Abstract:  We give an explicit prescription for experimentally determining the evolution
operators which completely describe the dynamics of a quantum mechanical black
box -- an arbitrary open quantum system. We show necessary and sufficient
conditions for this to be possible, and illustrate the general theory by
considering specifically one and two quantum bit systems. These procedures may
be useful in the comparative evaluation of experimental quantum measurement,
communication, and computation systems."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9604031,https://arxiv.org/abs/quant-ph/9604031,"Abstract:  Decoherence and loss will limit the practicality of quantum cryptography and
computing unless successful error correction techniques are developed. To this
end, we have discovered a new scheme for perfectly detecting and rejecting the
error caused by loss (amplitude damping to a reservoir at T=0), based on using
a dual-rail representation of a quantum bit. This is possible because (1)
balanced loss does not perform a ``which-path'' measurement in an
interferometer, and (2) balanced quantum nondemolition measurement of the
``total'' photon number can be used to detect loss-induced quantum jumps
without disturbing the quantum coherence essential to the quantum bit. Our
results are immediately applicable to optical quantum computers using single
photonics devices."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9604030,https://arxiv.org/abs/quant-ph/9604030,"Abstract:  The construction of large, coherent quantum systems necessary for quantum
computation remains an entreating but elusive goal, due to the ubiquitous
nature of decoherence. Recent progress in quantum error correction schemes have
given new hope to this field, but thus far, the codes presented in the
literature assume a restricted number of errors and error free encoding,
decoding, and measurement. We investigate a specific scenario without these
assumptions; in particular, we evaluate a scheme to preserve a single quantum
bit against phase damping using a three-qubit encoding based on Shor. By
applying a new formalism which gives simple operators for decoherence and noisy
logic gates, we find the fidelity of the stored qubit as a function of time,
including decoherence which occurs not only during storage but also during
processing. We generalize our results to include any source of error, and
derive an upper limit on the allowable decoherence per timestep. Physically,
our results suggest the feasibility of engineering artificial metastable states
through repeated error correction."
Isaac Chuang,Chuang_Isaac,arXiv:quant-ph/9602018,https://arxiv.org/abs/quant-ph/9602018,"Abstract:  We investigate the impact of loss (amplitude damping) and decoherence (phase
damping) on the performance of a simple quantum computer which solves the
one-bit Deutsch problem. The components of this machine are beamsplitters and
nonlinear optical Kerr cells, but errors primarily originate from the latter.
We develop models to describe the effect of these errors on a quantum optical
Fredkin gate. The results are used to analyze possible error correction
strategies in a complete quantum computer. We find that errors due to loss can
be avoided perfectly by appropriate design techniques, while decoherence can be
partially dealt with using projective error correction."
David Clark,Clark_David,arXiv:1806.10235,https://arxiv.org/abs/1806.10235,"Abstract:  Traditional program analysis analyses a program language, that is, all
programs that can be written in the language. There is a difference, however,
between all possible programs that can be written and the corpus of actual
programs written in a language. We seek to exploit this difference: for a given
program, we apply a bespoke program transformation Indexify to convert
expressions that current SMT solvers do not, in general, handle, such as
constraints on strings, into equisatisfiable expressions that they do handle.
To this end, Indexify replaces operators in hard-to-handle expressions with
homomorphic versions that behave the same on a finite subset of the domain of
the original operator, and return bottom denoting unknown outside of that
subset. By focusing on what literals and expressions are most useful for
analysing a given program, Indexify constructs a small, finite theory that
extends the power of a solver on the expressions a target program builds.
Indexify's bespoke nature necessarily means that its evaluation must be
experimental, resting on a demonstration of its effectiveness in practice. We
have developed Indexif}, a tool for Indexify. We demonstrate its utility and
effectiveness by applying it to two real world benchmarks --- string
expressions in coreutils and floats in fdlibm53. Indexify reduces
time-to-completion on coreutils from Klee's 49.5m on average to 6.0m. It
increases branch coverage on coreutils from 30.10% for Klee and 14.79% for
Zesti to 66.83%. When indexifying floats in fdlibm53, Indexifyl increases
branch coverage from 34.45% to 71.56% over Klee. For a restricted class of
inputs, Indexify permits the symbolic execution of program paths unreachable
with previous techniques: it covers more than twice as many branches in
coreutils as Klee."
David Clark,Clark_David,arXiv:1805.08889,https://arxiv.org/abs/1805.08889,"Abstract:  Neuromorphic architectures achieve low-power operation by using many simple
spiking neurons in lieu of traditional hardware. Here, we develop methods for
precise linear computations in spiking neural networks and use these methods to
map the evolution of a linear dynamical system (LDS) onto an existing
neuromorphic chip: IBM's TrueNorth. We analytically characterize, and
numerically validate, the discrepancy between the spiking LDS state sequence
and that of its non-spiking counterpart. These analytical results shed light on
the multiway tradeoff between time, space, energy, and accuracy in neuromorphic
computation. To demonstrate the utility of our work, we implemented a
neuromorphic Kalman filter (KF) and used it for offline decoding of human vocal
pitch from neural data. The neuromorphic KF could be used for low-power
filtering in domains beyond neuroscience, such as navigation or robotics."
David Clark,Clark_David,arXiv:1711.06355,https://arxiv.org/abs/1711.06355,"Abstract:  We present a study of comet C/2017 K2 (PANSTARRS) using prediscovery archival
data taken from 2013 to 2017. Our measurements show that the comet has been
marginally increasing in activity since at least 2013 May (heliocentric
distance of $r_{\mathrm{H}} = 23.7$ AU pre-perihelion). We estimate the
mass-loss rate during the period 2013--2017 as $\overline{\dot{M}} \approx
\left(2.4 \pm 1.1 \right) \times 10^{2}$ kg s$^{-1}$, which requires a minimum
active surface area of $\sim$10--10$^2$ km$^{2}$ for sublimation of
supervolatiles such as CO and CO$_2$, by assuming a nominal cometary albedo
$p_V = 0.04 \pm 0.02$. The corresponding lower limit to the nucleus radius is a
few kilometers. Our Monte Carlo dust simulations show that dust grains in the
coma are $\gtrsim0.5$ mm in radius, with ejection speeds from $\sim$1--3 m
s$^{-1}$, and have been emitted in a protracted manner since 2013, confirming
estimates by Jewitt et al. (2017). The current heliocentric orbit is
hyperbolic. Our N-body backward dynamical integration of the orbit suggests
that the comet is most likely (with a probability of $\sim$98\%) from the Oort
spike. The calculated median reciprocal of the semimajor axis 1 Myr ago was
$a_{\mathrm{med}}^{-1} = \left( 3.61 \pm 1.71 \right) \times 10^{-5}$ AU$^{-1}$
(in a reference system of the solar-system barycentre)."
David Clark,Clark_David,arXiv:1609.02404,https://arxiv.org/abs/1609.02404,"Abstract:  Malware creators have been getting their way for too long now. String-based
similarity measures can leverage ground truth in a scalable way and can operate
at a level of abstraction that is difficult to combat from the code level. We
introduce ITect, a scalable approach to malware similarity detection based on
information theory. ITect targets file entropy patterns in different ways to
achieve 100% precision with 90% accuracy but it could target 100% recall
instead. It outperforms VirusTotal for precision and accuracy on combined
Kaggle and VirusShare malware."
David Clark,Clark_David,arXiv:1602.03123,https://arxiv.org/abs/1602.03123,"Abstract:  Temporarily Captured Orbiters (TCOs) are Near-Earth Objects (NEOs) which make
a few orbits of Earth before returning to heliocentric orbits. Only one TCO has
been observed to date, 2006 RH120, captured by Earth for one year before
escaping. Detailed modeling predicts capture should occur from the NEO
population predominantly through the Sun-Earth L1 and L2 points, with 1% of
TCOs impacting Earth and approximately 0.1% of meteoroids being TCOs. Although
thousands of meteoroid orbits have been measured, none until now have
conclusively exhibited TCO behaviour, largely due to difficulties in measuring
initial meteoroid speed with sufficient precision. We report on a precise
meteor observation of January 13, 2014 by a new generation of all-sky fireball
digital camera systems operated in the Czech Republic as part of the European
Fireball Network, providing the lowest natural object entry speed observed in
decades long monitoring by networks world-wide. Modeling atmospheric
deceleration and fragmentation yields an initial mass of ~5 kg and diameter of
15 cm, with a maximum Earth-relative velocity just over 11.0 km/s. Spectral
observations prove its natural origin. Back-integration across observational
uncertainties yields a 92 - 98% probability of TCO behaviour, with close lunar
dynamical interaction. The capture duration varies across observational
uncertainties from 48 days to 5+ years. We also report on two low-speed impacts
recorded by US Government sensors, and we examine Prairie Network event PN39078
from 1965 having an extremely low entry speed of 10.9 km/s. In these cases
uncertainties in measurement and origin make TCO designation uncertain."
David Clark,Clark_David,arXiv:1509.07606,https://arxiv.org/abs/1509.07606,"Abstract:  We present studies of C/2015 D1 (SOHO), the first sunskirting comet ever seen
from ground stations over the past half century. The Solar and Heliospheric
Observatory (SOHO) witnessed its peculiar light curve with a huge dip followed
by a flareup around perihelion: the dip was likely caused by sublimation of
olivines, directly evidenced by a coincident temporary disappearance of the
tail. The flareup likely reflects a disintegration event, which we suggest was
triggered by intense thermal stress established within the nucleus interior.
Photometric data reveal an increasingly dusty coma, indicative of volatile
depletion. A catastrophic mass loss rate of $\sim$10$^{5}$ kg s$^{-1}$ around
perihelion was seen. Ground-based Xingming Observatory spotted the
post-perihelion debris cloud. Our morphological simulations of post-perihelion
images find newly released dust grains of size $a \gtrsim 10$ $\mu$m in radius,
however, a temporal increase in $a_{\min}$ was also witnessed, possibly due to
swift dispersions of smaller grains swept away by radiation forces without
replenishment. Together with the fading profile of the light curve, a power law
dust size distribution with index $\gamma = 3.2 \pm 0.1$ is derived. We
detected no active remaining cometary nuclei over $\sim$0.1 km in radius in
post-perihelion images acquired at Lowell Observatory. Applying radial
non-gravitational parameter, $\mathcal{A}_{1} = \left(1.209 \pm 0.118 \right)
\times 10^{-6}$ AU day$^{-2}$, from an isothermal water-ice sublimation model
to the SOHO astrometry significantly reduces residuals and sinusoidal trends in
the orbit determination. The nucleus mass $\sim$10$^{8}$--10$^{9}$ kg, and the
radius $\sim$50--150 m (bulk density $\rho_{\mathrm{d}} = 0.4$ g cm$^{-3}$
assumed) before the disintegration are deduced from the photometric data;
consistent results were determined from the non-gravitational effects."
David Clark,Clark_David,arXiv:1506.03482,https://arxiv.org/abs/1506.03482,"Abstract:  A common and natural intuition among software testers is that test cases need
to differ if a software system is to be tested properly and its quality
ensured. Consequently, much research has gone into formulating distance
measures for how test cases, their inputs and/or their outputs differ. However,
common to these proposals is that they are data type specific and/or calculate
the diversity only between pairs of test inputs, traces or outputs.
We propose a new metric to measure the diversity of sets of tests: the test
set diameter (TSDm). It extends our earlier, pairwise test diversity metrics
based on recent advances in information theory regarding the calculation of the
normalized compression distance (NCD) for multisets. An advantage is that TSDm
can be applied regardless of data type and on any test-related information, not
only the test inputs. A downside is the increased computational time compared
to competing approaches.
Our experiments on four different systems show that the test set diameter can
help select test sets with higher structural and fault coverage than random
selection even when only applied to test inputs. This can enable early test
design and selection, prior to even having a software system to test, and
complement other types of test automation and analysis. We argue that this
quantification of test set diversity creates a number of opportunities to
better understand software quality and provides practical ways to increase it."
David Clark,Clark_David,arXiv:1502.07661,https://arxiv.org/abs/1502.07661,"Abstract:  This work focuses on a specific front of the malware detection arms-race,
namely the detection of persistent, disk-resident malware. We exploit
normalised compression distance (NCD), an information theoretic measure,
applied directly to binaries. Given a zoo of labelled malware and benign-ware,
we ask whether a suspect program is more similar to our malware or to our
benign-ware. Our approach classifies malware with 97.1% accuracy and a false
positive rate of 3%. We achieve our results with off-the-shelf compressors and
a standard machine learning classifier and without any specialised knowledge.
An end-user need only collect a zoo of malware and benign-ware and then can
immediately apply our techniques.
We apply statistical rigour to our experiments and our selection of data. We
demonstrate that accuracy can be optimised by combining NCD with the
compressibility rates of the executables. We demonstrate that malware reported
within a more narrow time frame of a few days is more homogenous than malware
reported over a longer one of two years but that our method still classifies
the latter with 95.2% accuracy and a 5% false positive rate. Due to the use of
compression, the time and computation cost of our method is non-trivial. We
show that simple approximation techniques can improve the time complexity of
our approach by up to 63%.
We compare our results to the results of applying the 59 anti-malware
programs used on the VirusTotal web site to our malware. Our approach does
better than any single one of them as well as the 59 used collectively."
David Clark,Clark_David,arXiv:1306.5339,https://arxiv.org/abs/1306.5339,"Abstract:  We give a new solution to the famous Gion shrine geometry problem from
eighteenth-century Japan. Like the classical Japanese solution, ours is given
in the form of a degree ten equation. However, our polynomial has the advantage
of being much easier to write down. We also provide some additional analysis,
including a discussion of existence and uniqueness."
David Clark,Clark_David,arXiv:1008.4747,https://arxiv.org/abs/1008.4747,"Abstract:  This paper develops a general method for constructing entanglement-assisted
quantum low-density parity-check (LDPC) codes, which is based on combinatorial
design theory. Explicit constructions are given for entanglement-assisted
quantum error-correcting codes (EAQECCs) with many desirable properties. These
properties include the requirement of only one initial entanglement bit, high
error correction performance, high rates, and low decoding complexity. The
proposed method produces infinitely many new codes with a wide variety of
parameters and entanglement requirements. Our framework encompasses various
codes including the previously known entanglement-assisted quantum LDPC codes
having the best error correction performance and many new codes with better
block error rates in simulations over the depolarizing channel. We also
determine important parameters of several well-known classes of quantum and
classical LDPC codes for previously unsettled cases."
David Clark,Clark_David,arXiv:0911.0665,https://arxiv.org/abs/0911.0665,"Abstract:  We present high sensitivity H91$\alpha$ and 3.5 cm radio continuum
observations toward the planetary nebula NGC 3242. The electron temperature
determined assuming local thermodynamic equilibrium is consistent within
$\sim$10% with that derived from optical lines and the Balmer discontinuity.
The line emission and the continuum emission have very similar spatial
distribution, suggesting that at this wavelength there is no other continuum
process present in a significant manner. In particular, we conclude that
emission from spinning dust is not important at this wavelength. In this radio
recombination line the nebula presents a radial velocity structure consistent
with that obtained from observations of optical lines."
David Clark,Clark_David,arXiv:0806.0601,https://arxiv.org/abs/0806.0601,"Abstract:  We prove that Morrison and Nieh's categorification of the su(3) quantum knot
invariant is functorial with respect to tangle cobordisms. This is in contrast
to the categorified su(2) theory, which was not functorial as originally
defined. We use methods of Bar-Natan to construct explicit chain maps for each
variation of the third Reidemeister move. Then, to show functoriality, we
modify arguments used by Clark, Morrison, and Walker to show that induced chain
maps are invariant under Carter and Saito's movie moves."
David Clark,Clark_David,arXiv:0707.0040,https://arxiv.org/abs/0707.0040,"Abstract:  Neutron resonance spectrometry (NRS) has been used to measure the temperature
inside Mo samples during shock loading. The temperatures obtained were
significantly higher than predicted assuming ideal hydrodynamic loading. The
effect of plastic flow and non-ideal projectile behavior were assessed. Plastic
flow was calculated self-consistently with the shock jump conditions: this is
necessary for a rigorous estimate of the locus of shock states accessible.
Plastic flow was estimated to contribute a temperature rise of 53K compared
with hydrodynamic flow. Simulations were performed of the operation of the
explosively-driven projectile system used to induce the shock in the Mo sample.
The simulations predicted that the projectile was significantly curved on
impact, and still accelerating. The resulting spatial variations in load,
including radial components of velocity, were predicted to increase the
apparent temperature that would be deduced from the width of the neutron
resonance by 160K. These corrections are sufficient to reconcile the apparent
temperatures deduced using NRS with the accepted properties of Mo, in
particular its equation of state."
David Clark,Clark_David,arXiv:0704.1850,https://arxiv.org/abs/0704.1850,"Abstract:  Shock and release temperatures in Mo were calculated, taking account of
heating from plastic flow predicted using the Steinberg-Guinan model. Plastic
flow was calculated self-consistently with the shock jump conditions: this is
necessary for a rigorous estimate of the locus of shock states accessible. The
temperatures obtained were significantly higher than predicted assuming ideal
hydrodynamic loading. The temperatures were compared with surface emission
spectrometry measurements for Mo shocked to around 60GPa and then released into
vacuum or into a LiF window. Shock loading was induced by the impact of a
planar projectile, accelerated by high explosive or in a gas gun. Surface
velocimetry showed an elastic wave at the start of release from the shocked
state; the amplitude of the elastic wave matched the prediction to around 10%,
indicating that the predicted flow stress in the shocked state was reasonable.
The measured temperatures were consistent with the simulations, indicating that
the fraction of plastic work converted to heat was in the range 70-100% for
these loading conditions."
David Clark,Clark_David,arXiv:cond-mat/0702693,https://arxiv.org/abs/cond-mat/0702693,"Abstract:  The hydrodynamic operation of the `Forest Flyer' type of explosive launching
system for shock physics projectiles was investigated in detail using one- and
two-dimensional continuum dynamics simulations. The simulations were
insensitive to uncertainties in the material properties, and reproduced
measurements of the projectile. The most commonly-used variant, with an Al
alloy case, was predicted to produce a slightly curved projectile, subjected to
some shock heating, and likely exhibiting some porosity from tensile damage.
The flatness can be improved by using a case of lower shock impedance, such as
polymethyl methacrylate. High-impedance cases, including Al alloys but with
denser materials improving the launching efficiency, can be used if designed
according to the physics of oblique shock reflection. The tensile stress
induced in the projectile depends on the relative thickness of the explosive,
expansion gap, and projectile. The thinner the projectile with respect to the
explosive, the smaller the tensile stress. If the explosive is initiated with a
plane wave lens, the tensile stress is lower than for initiation with multiple
detonators over a plane. The previous plane wave lens designs did however
induce a tensile stress close to the spall strength of the projectile. The
tensile stress can be reduced by changes in the component thicknesses.
Experiments to verify the operation of explosively-launched projectiles should
attempt to measure porosity induced in the projectile: arrival time
measurements may be insensitive to porous regions caused by damaged or
recollected material."
David Clark,Clark_David,arXiv:math/0701339,https://arxiv.org/abs/math/0701339,"Abstract:  We describe a modification of Khovanov homology (math.QA/9908171), in the
spirit of Bar-Natan (math.GT/0410495), which makes the theory properly
functorial with respect to link cobordisms.
This requires introducing `disorientations' in the category of smoothings and
abstract cobordisms between them used in Bar-Natan's definition.
Disorientations have `seams' separating oppositely oriented regions, coming
with a preferred normal direction. The seams satisfy certain relations (just as
the underlying cobordisms satisfy relations such as the neck cutting relation).
We construct explicit chain maps for the various Reidemeister moves, then
prove that the compositions of chain maps associated to each side of each of
Carter and Saito's movie moves (MR1238875, MR1445361) always agree. These
calculations are greatly simplified by following arguments due to Bar-Natan and
Khovanov, which ensure that the two compositions must agree, up to a sign. We
set up this argument in our context by proving a result about duality in
Khovanov homology, generalising previous results about mirror images of knots
to a `local' result about tangles. Along the way, we reproduce Jacobsson's sign
table (math.GT/0206303) for the original `unoriented theory', with a few
disagreements."
David Clark,Clark_David,arXiv:cs/9901011,https://arxiv.org/abs/cs/9901011,"Abstract:  The Internet has revolutionized the computer and communications world like
nothing before. The invention of the telegraph, telephone, radio, and computer
set the stage for this unprecedented integration of capabilities. The Internet
is at once a world-wide broadcasting capability, a mechanism for information
dissemination, and a medium for collaboration and interaction between
individuals and their computers without regard for geographic location.
In this paper, several of us involved in the development and evolution of the
Internet share our views of its origins and history. This is intended to be a
brief, necessarily cursory and incomplete history. This history revolves around
four distinct aspects. There is the technological evolution that began with
early research on packet switching and the ARPANET (and related technologies),
and where current research continues to expand the horizons of the
infrastructure along several dimensions, such as scale, performance, and higher
level functionality. There is the operations and management aspect of a global
and complex operational infrastructure. There is the social aspect, which
resulted in a broad community of Internauts working together to create and
evolve the technology. And there is the commercialization aspect, resulting in
an extremely effective transition of research results into a broadly deployed
and available information infrastructure."
Munther Dahleh,Dahleh_Munther,arXiv:1902.01848,https://arxiv.org/abs/1902.01848,"Abstract:  We address the problem of learning the parameters of a stable linear time
invariant (LTI) system with unknown latent space dimension, or \textit{order},
from its noisy input-output data. In particular, we focus on learning the
parameters of the best lower order approximation allowed by the finite data.
This is achieved by constructing a Hankel-like representation of the underlying
system using ordinary least squares. Such a representation circumvents the
non-convexities that typically arise in system identification, and it allows
accurate estimation of the underlying LTI system. Our results rely on a careful
analysis of a self-normalized martingale difference term that helps bound
identification error up to logarithmic factors of the lower bound. We provide a
data-dependent scheme for order selection and find a realization of system
parameters, corresponding to that order, by an approach that is closely related
to the celebrated Kalman-Ho subspace algorithm. We show that this realization
is a good approximation of the underlying LTI system with high probability.
Finally, we demonstrate that the proposed model order selection procedure is
minimax optimal, i.e., for the given data length it is not always possible to
estimate higher order models or find higher order approximations with
reasonable accuracy."
Munther Dahleh,Dahleh_Munther,arXiv:1810.10513,https://arxiv.org/abs/1810.10513,"Abstract:  Spurred by the growth of transportation network companies and increasing data
capabilities, vehicle routing and ride-matching algorithms can improve the
efficiency of private transportation services. However, existing routing
solutions do not address where drivers should travel after dropping off a
passenger and before receiving the next passenger ride request, i.e., during
the between-ride period. We address this problem by developing an efficient
algorithm to find the optimal policy for drivers between rides in order to
maximize driver profits. We model the road network as a graph, and we show that
the between-ride routing problem is equivalent to a stochastic shortest path
problem, an infinite dynamic program with no discounting. We prove under
reasonable assumptions that an optimal routing policy exists that avoids
cycles; policies of this type can be efficiently found. We present an iterative
approach to find an optimal routing policy. Our approach can account for
various factors, including the frequency of passenger ride requests at
different locations, traffic conditions, and surge pricing. We demonstrate the
effectiveness of the approach by implementing it on road network data from
Boston and New York City."
Munther Dahleh,Dahleh_Munther,arXiv:1809.05948,https://arxiv.org/abs/1809.05948,"Abstract:  This paper addresses two fundamental problems in the context of jump linear
systems (JLS). The first problem is concerned with characterizing the minimal
state space dimension solely from input-output pairs and without any knowledge
of the number of mode switches. The second problem is concerned with
characterizing the number of discrete modes of the JLS. For the first problem,
we develop a linear system theory based approach and construct an appropriate
Hankel-like matrix. The rank of this matrix gives us the state space dimension.
For the second problem we show that minimal number of modes corresponds to the
minimal rank of a positive semi-definite matrix obtained via a non--convex
formulation."
Munther Dahleh,Dahleh_Munther,arXiv:1805.08125,https://arxiv.org/abs/1805.08125,"Abstract:  In this work, we aim to create a data marketplace; a robust real-time
matching mechanism to efficiently buy and sell training data for Machine
Learning tasks. While the monetization of data and pre-trained models is an
essential focus of industry today, there does not exist a market mechanism to
price training data and match buyers to vendors while still addressing the
associated (computational and other) complexity. The challenge in creating such
a market stems from the very nature of data as an asset: (i) it is freely
replicable; (ii) its value is inherently combinatorial due to correlation with
signal in other data; (iii) prediction tasks and the value of accuracy vary
widely; (iv) usefulness of training data is difficult to verify a priori
without first applying it to a prediction task. As our main contributions we:
(i) propose a mathematical model for a two-sided data market and formally
define the key associated challenges; (ii) construct algorithms for such a
market to function and rigorously prove how they meet the challenges defined.
We highlight two technical contributions: (i) a new notion of ""fairness""
required for cooperative games with freely replicable goods; (ii) a truthful,
zero regret mechanism for auctioning a particular class of combinatorial goods
based on utilizing Myerson's payment function and the Multiplicative Weights
algorithm. These might be of independent interest."
Munther Dahleh,Dahleh_Munther,arXiv:1802.07321,https://arxiv.org/abs/1802.07321,"Abstract:  We consider the problem of robustness in large consensus networks that occur
in many areas such as distributed optimization. Robustness, in this context, is
the scaling of performance measures, e.g. H2-norm, as a function of network
dimension. We provide a formal framework to quantify the relation between such
performance scaling and the convergence speed of the network. Specifically, we
provide upper and lower bounds for the convergence speed in terms of robustness
and discuss how these bounds scale with the network topology. The main
contribution of this work is that we obtain tight bounds, that hold regardless
of network topology. The work here also encompasses some results in convergence
time analysis in previous literature."
Munther Dahleh,Dahleh_Munther,arXiv:1712.05273,https://arxiv.org/abs/1712.05273,"Abstract:  This paper examines the dependence of network performance measures on network
size and considers scaling results for large networks. We connect two
performance measures that are well studied, but appear to be unrelated. The
first measure is concerned with energy metrics, namely the $\Hcal_2$--norm of a
network, which arises in control theory applications. The second measure is
concerned with the notion of ""tail risk"" which arises in economic and financial
networks. We study the question of why such performance measures may
deteriorate at a faster rate than the growth rate of the network. We first
focus on the energy metric and its well known connection to controllability
Gramian of the underlying dynamical system. We show that undirected networks
exhibit the most graceful energy growth rates as network size grows. This rate
is quantified completely by the proximity of spectral radius to unity or
distance to instability. In contrast, we show that the simple characterization
of energy in terms of network spectrum does not exist for directed networks. We
demonstrate that, for any fixed distance to instability, energy of a directed
network can grow at an exponentially faster rate. We provide general methods
for manipulating networks to reduce energy. In particular, we prove that
certain operations that increase the symmetry in a network cannot increase
energy (in an order sense). Secondly, we focus on tail risk in economic and
financial networks. In contrast to $\Hcal_2$--norm which arises from computing
the expectation of energy in the network, tail risk focuses on tail probability
behavior of network variables. Although the two measures differ substantially
we show that they are precisely connected through the system Gramian. This
surprising result explains why topology considerations rather than specific
performance measures dictate the large scale behavior of networks."
Munther Dahleh,Dahleh_Munther,arXiv:1709.01432,https://arxiv.org/abs/1709.01432,"Abstract:  In coalitional games, traditional coalitional game theory does not apply if
different participants hold different opinions about the payoff function that
corresponds to each subset of the coalition. In this paper, we propose a
framework in which players can exchange opinions about their views of payoff
functions and then decide the distribution of the value of the grand coalition.
When all players are truth-telling, the problem of opinion consensus is
decoupled from the coalitional game, but interesting dynamics will arise when
players are strategic in the consensus phase. Assuming that all players are
rational, the model implies that, if influential players are risk-averse, an
efficient fusion of the distributed data is achieved at pure strategy Nash
equilibrium, meaning that the average opinion will not drift. Also, without the
assumption that all players are rational, each player can use an algorithmic
R-learning process, which gives the same result as the pure strategy Nash
equilibrium with rational players."
Munther Dahleh,Dahleh_Munther,arXiv:1703.00980,https://arxiv.org/abs/1703.00980,"Abstract:  This paper analyzes the impact of peer effects on electricity consumption of
a network of rational, utility-maximizing users. Users derive utility from
consuming electricity as well as consuming less energy than their neighbors.
However, a disutility is incurred for consuming more than their neighbors. To
maximize the profit of the load-serving entity that provides electricity to
such users, we develop a two-stage game-theoretic model, where the entity sets
the prices in the first stage. In the second stage, consumers decide on their
demand in response to the observed price set in the first stage so as to
maximize their utility. To this end, we derive theoretical statements under
which such peer effects reduce aggregate user consumption. Further, we obtain
expressions for the resulting electricity consumption and profit of the load
serving entity for the case of perfect price discrimination and a single price
under complete information, and approximations under incomplete information.
Simulations suggest that exposing only a selected subset of all users to peer
effects maximizes the entity's profit."
Munther Dahleh,Dahleh_Munther,arXiv:1703.00976,https://arxiv.org/abs/1703.00976,"Abstract:  Load-serving entities which procure electricity from the wholesale
electricity market to service end-users face significant quantity and price
risks due to the volatile nature of electricity demand and quasi-fixed
residential tariffs at which electricity is sold. This paper investigates
strategies for load serving entities to hedge against such price risks.
Specifically, we compute profit-maximizing portfolios of forward contract and
call options as a function of the uncertain aggregate user demand. We compare
the profit to the case of Demand Response, where users are offered monetary
incentives to temporarily reduce their consumption during periods of supply
shortages. Using smart meter data of residential customers in California, we
simulate optimal portfolios and derive conditions under which Demand Response
outperforms call options and forward contracts."
Munther Dahleh,Dahleh_Munther,arXiv:1703.00972,https://arxiv.org/abs/1703.00972,"Abstract:  Residential Demand Response has emerged as a viable tool to alleviate supply
and demand imbalances of electricity, particularly during times when the
electric grid is strained due a shortage of supply. Demand Response providers
bid reduction capacity into the wholesale electricity market by asking their
customers under contract to temporarily reduce their consumption in exchange
for a monetary incentive. To contribute to the analysis of consumer behavior in
response to such incentives, this paper formulates Demand Response as a
Mechanism Design problem, where a Demand Response Provider elicits private
information of its rational, profit-maximizing customers who derive positive
expected utility by participating in reduction events. By designing an
incentive compatible and individually rational mechanism to collect users'
price elasticities of demand, the Demand Response provider can target the most
susceptible users to incentives. We measure reductions by comparing the
materialized consumption to the projected consumption, which we model as the
""10-in-10""-baseline, the regulatory standard set by the California Independent
System Operator. Due to the suboptimal performance of this baseline, we show,
using consumption data of residential customers in California, that Demand
Response Providers receive payments for ""virtual reductions"", which exist due
to the inaccuracies of the baseline rather than actual reductions. Improving
the accuracy of the baseline diminishes the contribution of these virtual
reductions."
Munther Dahleh,Dahleh_Munther,arXiv:1611.03765,https://arxiv.org/abs/1611.03765,"Abstract:  We investigate the ability of a homogeneous collection of deferrable energy
loads to behave as a battery; that is, to absorb and release energy in a
controllable fashion up to fixed and predetermined limits on volume, charge
rate and discharge rate. We derive explicit bounds on the battery capacity that
can be offered, and show that there is a fundamental trade-off between the
abilities of collective load to absorb and release energy at high aggregate
rates. Finally, we introduce a new class of dynamic priority-driven feedback
policies that balance these abilities, and characterize the batteries that they
can emulate."
Munther Dahleh,Dahleh_Munther,arXiv:1610.01973,https://arxiv.org/abs/1610.01973,"Abstract:  We investigate the ability of a homogeneous collection of deferrable energy
loads to behave as a battery; that is, to absorb and release energy in a
controllable fashion up to fixed and predetermined limits on volume, charge
rate and discharge rate. We derive bounds on the battery capacity that can be
realized and show that there are fundamental trade-offs between battery
parameters. By characterizing the state trajectories under scheduling policies
that emulate two illustrative batteries, we show that the trade-offs occur
because the states that allow the loads to absorb and release energy at high
aggregate rates are conflicting."
Munther Dahleh,Dahleh_Munther,arXiv:1609.06700,https://arxiv.org/abs/1609.06700,"Abstract:  In this paper, we investigate the use of variable speed limits for resilient
operation of transportation networks, which are modeled as dynamical flow
networks under local routing decisions. In such systems, some external inflow
is injected to the so-called origin nodes of the network. The total inflow
arriving at each node is routed to its operational outgoing links based on
their current particle densities. The density on each link has first order
dynamics driven by the difference of its incoming and outgoing flows. A link
irreversibly fails if it reaches its jam density. Such failures may propagate
in the network and cause a systemic failure. We show that larger link
capacities do not necessarily help in preventing systemic failures under local
routing. Accordingly, we propose the use of variable speed limits to operate
the links below their capacities, when necessary, to compensate for the lack of
global information and coordination in routing decisions. Our main result shows
that systemic failures under feasible external inflows can always be averted
through a proper selection of speed limits if the routing decisions are
sufficiently responsive to local congestion and the network is initially
uncongested. This is an attractive feature as it is much easier in practice to
adjust the speed limits than to build more physical capacity or to alter
routing decisions that are determined by social behavior."
Munther Dahleh,Dahleh_Munther,arXiv:1609.06193,https://arxiv.org/abs/1609.06193,"Abstract:  This paper analyzes stability conditions for wholesale electricity markets
under real-time retail pricing and realistic consumption models with memory,
which explicitly take into account previous electricity prices and consumption
levels. By passing on the current retail price of electricity from supplier to
consumer and feeding the observed consumption back to the supplier, a
closed-loop dynamical system for electricity prices and consumption arises
whose stability is to be investigated. Under mild assumptions on the generation
cost of electricity and consumers' backlog disutility functions, we show that,
for consumer models with price memory only, market stability is achieved if the
ratio between the consumers' marginal backlog disutility and the suppliers'
marginal cost of supply remains below a fixed threshold. Further, consumer
models with price and consumption memory can result in greater stability
regions and faster convergence to the equilibrium compared to models with price
memory alone, if consumption deviations from nominal demand are adequately
penalized."
Munther Dahleh,Dahleh_Munther,arXiv:1608.04155,https://arxiv.org/abs/1608.04155,"Abstract:  In this paper, we are concerned with the resilience of locally routed network
flows with finite link capacities. In this setting, an external inflow is
injected to the so-called origin nodes. The total inflow arriving at each node
is routed locally such that none of the outgoing links are overloaded unless
the node receives an inflow greater than its total outgoing capacity. A link
irreversibly fails if it is overloaded or if there is no operational link in
its immediate downstream to carry its flow. For such systems, resilience is
defined as the minimum amount of reduction in the link capacities that would
result in the failure of all the outgoing links of an origin node. We show that
such networks do not necessarily become more resilient as additional capacity
is built in the network. Moreover, when the external inflow does not exceed the
network capacity, selective reductions of capacity at certain links can
actually help averting the cascading failures, without requiring any change in
the local routing policies. This is an attractive feature as it is often easier
in practice to reduce the available capacity of some critical links than to add
physical capacity or to alter routing policies, e.g., when such policies are
determined by social behavior, as in the case of road traffic networks. The
results can thus be used for real-time monitoring of distance-to-failure in
such networks and devising a feasible course of actions to avert systemic
failures."
Munther Dahleh,Dahleh_Munther,arXiv:1606.08101,https://arxiv.org/abs/1606.08101,"Abstract:  Voltage control plays an important role in the operation of electricity
distribution networks, especially when there is a large penetration of
renewable energy resources. In this paper, we focus on voltage control through
reactive power compensation and study how different information structures
affect the control performance. In particular, we first show that using only
voltage measurements to determine reactive power compensation is insufficient
to maintain voltage in the acceptable range. Then we propose two fully
decentralized and robust algorithms by adding additional information, which can
stabilize the voltage in the acceptable range. The one with higher complexity
can further minimize a cost of reactive power compensation in a particular
form. Both of the two algorithms use only local measurements and local
variables and require no communication. In addition, the two algorithms are
robust against heterogeneous update rates and delays."
Munther Dahleh,Dahleh_Munther,arXiv:1506.06394,https://arxiv.org/abs/1506.06394,"Abstract:  We introduce a new class of (dynamical) systems that inherently capture
cascading effects (viewed as consequential effects) and are naturally amenable
to combinations. We develop an axiomatic general theory around those systems,
and guide the endeavor towards an understanding of cascading failure. The
theory evolves as an interplay of lattices and fixed points, and its results
may be instantiated to commonly studied models of cascade effects.
We characterize the systems through their fixed points, and equip them with
two operators. We uncover properties of the operators, and express global
systems through combinations of local systems. We enhance the theory with a
notion of failure, and understand the class of shocks inducing a system to
failure. We develop a notion of mu-rank to capture the energy of a system, and
understand the minimal amount of effort required to fail a system, termed
resilience. We deduce a dual notion of fragility and show that the combination
of systems sets a limit on the amount of fragility inherited."
Munther Dahleh,Dahleh_Munther,arXiv:1411.3698,https://arxiv.org/abs/1411.3698,"Abstract:  Consider a stationary discrete random process with alphabet size d, which is
assumed to be the output process of an unknown stationary Hidden Markov Model
(HMM). Given the joint probabilities of finite length strings of the process,
we are interested in finding a finite state generative model to describe the
entire process. In particular, we focus on two classes of models: HMMs and
quasi-HMMs, which is a strictly larger class of models containing HMMs. In the
main theorem, we show that if the random process is generated by an HMM of
order less or equal than k, and whose transition and observation probability
matrix are in general position, namely almost everywhere on the parameter
space, both the minimal quasi-HMM realization and the minimal HMM realization
can be efficiently computed based on the joint probabilities of all the length
N strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to
compare and connect the two lines of literature: realization theory of HMMs,
and the recent development in learning latent variable models with tensor
decomposition techniques."
Munther Dahleh,Dahleh_Munther,arXiv:1407.3518,https://arxiv.org/abs/1407.3518,"Abstract:  We propose a dynamical model for cascading failures in single-commodity
network flows. In the proposed model, the network state consists of flows and
activation status of the links. Network dynamics is determined by a, possibly
state-dependent and adversarial, disturbance process that reduces flow capacity
on the links, and routing policies at the nodes that have access to the network
state, but are oblivious to the presence of disturbance. Under the proposed
dynamics, a link becomes irreversibly inactive either due to overload condition
on itself or on all of its immediate downstream links. The coupling between
link activation and flow dynamics implies that links to become inactive
successively are not necessarily adjacent to each other, and hence the pattern
of cascading failure under our model is qualitatively different than standard
cascade models. The magnitude of a disturbance process is defined as the sum of
cumulative capacity reductions across time and links of the network, and the
margin of resilience of the network is defined as the infimum over the
magnitude of all disturbance processes under which the links at the origin node
become inactive. We propose an algorithm to compute an upper bound on the
margin of resilience for the setting where the routing policy only has access
to information about the local state of the network. For the limiting case when
the routing policies update their action as fast as network dynamics, we
identify sufficient conditions on network parameters under which the upper
bound is tight under an appropriate routing policy. Our analysis relies on
making connections between network parameters and monotonicity in network state
evolution under proposed dynamics."
Munther Dahleh,Dahleh_Munther,arXiv:1211.1696,https://arxiv.org/abs/1211.1696,"Abstract:  The primary concerns of this paper are twofold: to understand the economic
value of storage in the presence of ramp constraints and exogenous electricity
prices, and to understand the implications of the associated optimal storage
management policy on qualitative and quantitative characteristics of storage
response to real-time prices. We present an analytic characterization of the
optimal policy, along with the associated finite-horizon time-averaged value of
storage. We also derive an analytical upperbound on the infinite-horizon
time-averaged value of storage. This bound is valid for any achievable
realization of prices when the support of the distribution is fixed, and
highlights the dependence of the value of storage on ramp constraints and
storage capacity. While the value of storage is a non-decreasing function of
price volatility, due to the finite ramp rate, the value of storage saturates
quickly as the capacity increases, regardless of volatility. To study the
implications of the optimal policy, we first present computational experiments
that suggest that optimal utilization of storage can, in expectation, induce a
considerable amount of price elasticity near the average price, but little or
no elasticity far from it. We then present a computational framework for
understanding the behavior of storage as a function of price and the amount of
stored energy, and for characterization of the buy/sell phase transition region
in the price-state plane. Finally, we study the impact of market-based
operation of storage on the required reserves, and show that the reserves may
need to be expanded to accommodate market-based storage."
Munther Dahleh,Dahleh_Munther,arXiv:1211.0654,https://arxiv.org/abs/1211.0654,"Abstract:  We study a model for cascade effects over finite networks based on a
deterministic binary linear threshold model. Our starting point is a networked
coordination game where each agent's payoff is the sum of the payoffs coming
from pairwise interactions with each of the neighbors. We first establish that
the best response dynamics in this networked game is equivalent to the linear
threshold dynamics with heterogeneous thresholds over the agents. While the
previous literature has studied such linear threshold models under the
assumption that each agent may change actions at most once, a study of best
response dynamics in such networked games necessitates an analysis that allows
for multiple switches in actions. In this paper, we develop such an analysis
and construct a combinatorial framework to understand the behavior of the
model. To this end, we establish that the agents behavior cycles among
different actions in the limit and provide three sets of results.
We first characterize the limiting behavioral properties of the dynamics. We
determine the length of the limit cycles and reveal bounds on the time steps
required to reach such cycles for different network structures. We then study
the complexity of decision/counting problems that arise within the context.
Specifically, we consider the tractability of counting the number of limit
cycles and fixed-points, and deciding the reachability of action profiles. We
finally propose a measure of network resilience that captures the nature of the
involved dynamics. We prove bounds and investigate the resilience of different
network structures under this measure."
Munther Dahleh,Dahleh_Munther,arXiv:1209.0229,https://arxiv.org/abs/1209.0229,"Abstract:  In this paper, we examine in an abstract framework, how a tradeoff between
efficiency and robustness arises in different dynamic oligopolistic market
architectures. We consider a market in which there is a monopolistic resource
provider and agents that enter and exit the market following a random process.
Self-interested and fully rational agents dynamically update their resource
consumption decisions over a finite time horizon, under the constraint that the
total resource consumption requirements are met before each individual's
deadline. We then compare the statistics of the stationary aggregate demand
processes induced by the non-cooperative and cooperative load scheduling
schemes. We show that although the non-cooperative load scheduling scheme leads
to an efficiency loss - widely known as the ""price of anarchy"" - the stationary
distribution of the corresponding aggregate demand process has a smaller tail.
This tail, which corresponds to rare and undesirable demand spikes, is
important in many applications of interest. On the other hand, when the agents
can cooperate with each other in optimizing their total cost, a higher market
efficiency is achieved at the cost of a higher probability of demand spikes. We
thus posit that the origins of endogenous risk in such systems may lie in the
market architecture, which is an inherent characteristic of the system."
Munther Dahleh,Dahleh_Munther,arXiv:1205.0076,https://arxiv.org/abs/1205.0076,"Abstract:  Robustness of routing policies for networks is a central problem which is
gaining increased attention with a growing awareness to safeguard critical
infrastructure networks against natural and man-induced disruptions. Routing
under limited information and the possibility of cascades through the network
adds serious challenges to this problem. This abstract considers the framework
of dynamical networks introduced in our earlier work [1,2], where the network
is modeled by a system of ordinary differential equations derived from mass
conservation laws on directed acyclic graphs with a single origin-destination
pair and a constant inflow at the origin. The rate of change of the particle
density on each link of the network equals the difference between the inflow
and the outflow on that link. The latter is modeled to depend on the current
particle density on that link through a flow function. The novel modeling
element in this paper is that every link is assumed to have finite capacity for
particle density and that the flow function is modeled to be strictly
increasing as density increases from zero up to the maximum density capacity,
and is discontinuous at the maximum density capacity, with the flow function
value being zero at that point. This feature, in particular, allows for the
possibility of spill-backs in our model. In this paper, we present our results
on resilience of such networks under distributed routing, towards perturbations
that reduce link-wise flow functions."
Munther Dahleh,Dahleh_Munther,arXiv:1109.6505,https://arxiv.org/abs/1109.6505,"Abstract:  This paper examines the value of storage in securing reliability of a system
with uncertain supply and demand, and supply friction. The storage is
frictionless as a supply source, but once used, it cannot be filled up
instantaneously. The focus application is a power supply network in which the
base supply and demand are assumed to match perfectly, while deviations from
the base are modeled as random shocks with stochastic arrivals. Due to
friction, the random surge shocks cannot be tracked by the main supply sources.
Storage, when available, can be used to compensate, fully or partially, for the
surge in demand or loss of supply. The problem of optimal utilization of
storage with the objective of maximizing system reliability is formulated as
minimization of the expected discounted cost of blackouts over an infinite
horizon. It is shown that when the stage cost is linear in the size of the
blackout, the optimal policy is myopic in the sense that all shocks are
compensated by storage up to the available level of storage. However, when the
stage cost is strictly convex, it may be optimal to curtail some of the demand
and allow a small current blackout in the interest of maintaining a higher
level of reserve to avoid a large blackout in the future. The value of storage
capacity in improving system's reliability, as well as the effects of the
associated optimal policies under different stage costs on the probability
distribution of blackouts are examined."
Munther Dahleh,Dahleh_Munther,arXiv:1109.4564,https://arxiv.org/abs/1109.4564,"Abstract:  We propose a general methodology for performing statistical inference within
a `rare-events regime' that was recently suggested by Wagner, Viswanath and
Kulkarni. Our approach allows one to easily establish consistent estimators for
a very large class of canonical estimation problems, in a large alphabet
setting. These include the problems studied in the original paper, such as
entropy and probability estimation, in addition to many other interesting ones.
We particularly illustrate this approach by consistently estimating the size of
the alphabet and the range of the probabilities. We start by proposing an
abstract methodology based on constructing a probability measure with the
desired asymptotic properties. We then demonstrate two concrete constructions
by casting the Good-Turing estimator as a pseudo-empirical measure, and by
using the theory of mixture model estimation."
Munther Dahleh,Dahleh_Munther,arXiv:1106.1401,https://arxiv.org/abs/1106.1401,"Abstract:  The paper proposes a framework for modeling and analysis of the dynamics of
supply, demand, and clearing prices in power system with real-time retail
pricing and information asymmetry. Real-time retail pricing is characterized by
passing on the real-time wholesale electricity prices to the end consumers, and
is shown to create a closed-loop feedback system between the physical layer and
the market layer of the power system. In the absence of a carefully designed
control law, such direct feedback between the two layers could increase
volatility and lower the system's robustness to uncertainty in demand and
generation. A new notion of generalized price-elasticity is introduced, and it
is shown that price volatility can be characterized in terms of the system's
maximal relative price elasticity, defined as the maximal ratio of the
generalized price-elasticity of consumers to that of the producers. As this
ratio increases, the system becomes more volatile, and eventually, unstable. As
new demand response technologies and distributed storage increase the
price-elasticity of demand, the architecture under examination is likely to
lead to increased volatility and possibly instability. This highlights the need
for assessing architecture systematically and in advance, in order to optimally
strike the trade-offs between volatility, economic efficiency, and system
reliability."
Munther Dahleh,Dahleh_Munther,arXiv:1103.4893,https://arxiv.org/abs/1103.4893,"Abstract:  Strong resilience properties of dynamical flow networks are analyzed for
distributed routing policies. The latter are characterized by the property that
the way the inflow at a non-destination node gets split among its outgoing
links is allowed to depend only on local information about the current particle
densities on the outgoing links. The strong resilience of the network is
defined as the infimum sum of link-wise flow capacity reductions under which
the network cannot maintain the asymptotic total inflow to the destination node
to be equal to the inflow at the origin. A class of distributed routing
policies that are locally responsive to local information is shown to yield the
maximum possible strong resilience under such local information constraints for
an acyclic dynamical flow network with a single origin-destination pair. The
maximal strong resilience achievable is shown to be equal to the minimum node
residual capacity of the network. The latter depends on the limit flow of the
unperturbed network and is defined as the minimum, among all the
non-destination nodes, of the sum, over all the links outgoing from the node,
of the differences between the maximum flow capacity and the limit flow of the
unperturbed network. We propose a simple convex optimization problem to solve
for equilibrium limit flows of the unperturbed network that minimize average
delay subject to strong resilience guarantees, and discuss the use of tolls to
induce such an equilibrium limit flow in transportation networks. Finally, we
present illustrative simulations to discuss the connection between cascaded
failures and the resilience properties of the network."
Munther Dahleh,Dahleh_Munther,arXiv:1102.1107,https://arxiv.org/abs/1102.1107,"Abstract:  Robustness of distributed routing policies is studied for dynamical flow
networks, with respect to adversarial disturbances that reduce the link flow
capacities. A dynamical flow network is modeled as a system of ordinary
differential equations derived from mass conservation laws on a directed
acyclic graph with a single origin-destination pair and a constant inflow at
the origin. Routing policies regulate the way the inflow at a non-destination
node gets split among its outgoing links as a function of the current particle
density, while the outflow of a link is modeled to depend on the current
particle density on that link through a flow function. The dynamical flow
network is called partially transferring if the total inflow at the destination
node is asymptotically bounded away from zero, and its weak resilience is
measured as the minimum sum of the link-wise magnitude of all disturbances that
make it not partially transferring. The weak resilience of a dynamical flow
network with arbitrary routing policy is shown to be upper-bounded by the
network's min-cut capacity, independently of the initial flow conditions.
Moreover, a class of distributed routing policies that rely exclusively on
local information on the particle densities, and are locally responsive to
that, is shown to yield such maximal weak resilience. These results imply that
locality constraints on the information available to the routing policies do
not cause loss of weak resilience. Some fundamental properties of dynamical
flow networks driven by locally responsive distributed policies are analyzed in
detail, including global convergence to a unique limit flow."
Munther Dahleh,Dahleh_Munther,arXiv:1101.2220,https://arxiv.org/abs/1101.2220,"Abstract:  Stability of Wardrop equilibria is analyzed for dynamical transportation
networks in which the drivers' route choices are influenced by information at
multiple temporal and spatial scales. The considered model involves a continuum
of indistinguishable drivers commuting between a common origin/destination pair
in an acyclic transportation network. The drivers' route choices are affected
by their, relatively infrequent, perturbed best responses to global information
about the current network congestion levels, as well as their instantaneous
local observation of the immediate surroundings as they transit through the
network. A novel model is proposed for the drivers' route choice behavior,
exhibiting local consistency with their preference toward globally less
congested paths as well as myopic decisions in favor of locally less congested
paths. The simultaneous evolution of the traffic congestion on the network and
of the aggregate path preference is modeled by a system of coupled ordinary
differential equations. The main result shows that, if the frequency of updates
of path preferences is sufficiently small as compared to the frequency of the
traffic flow dynamics, then the state of the transportation network ultimately
approaches a neighborhood of the Wardrop equilibrium. The presented results may
be read as a further evidence in support of Wardrop's postulate of equilibrium,
showing robustness of it with respect to non-persistent perturbations. The
proposed analysis combines techniques from singular perturbation theory,
evolutionary game theory, and cooperative dynamical systems."
Munther Dahleh,Dahleh_Munther,arXiv:0903.5535,https://arxiv.org/abs/0903.5535,"Abstract:  We demonstrate the use of a new, control-oriented notion of finite state
approximation for a particular class of hybrid systems. Specifically, we
consider the problem of designing a stabilizing binary output feedback
switching controller for a pair of unstable homogeneous second order systems.
The constructive approach presented in this note, in addition to yielding an
explicit construction of a deterministic finite state approximate model of the
hybrid plant, allows us to efficiently establish a useable upper bound on the
quality of approximation, and leads to a discrete optimization problem whose
solution immediately provides a certifiably correct-by-design controller for
the original system. The resulting controller consists of a finite state
observer for the plant and a corresponding full state feedback switching
control law."
Munther Dahleh,Dahleh_Munther,arXiv:0810.5148,https://arxiv.org/abs/0810.5148,"Abstract:  A set of N independent Gaussian linear time invariant systems is observed by
M sensors whose task is to provide the best possible steady-state causal
minimum mean square estimate of the state of the systems, in addition to
minimizing a steady-state measurement cost. The sensors can switch between
systems instantaneously, and there are additional resource constraints, for
example on the number of sensors which can observe a given system
simultaneously. We first derive a tractable relaxation of the problem, which
provides a bound on the achievable performance. This bound can be computed by
solving a convex program involving linear matrix inequalities. Exploiting the
additional structure of the sites evolving independently, we can decompose this
program into coupled smaller dimensional problems. In the scalar case with
identical sensors, we give an analytical expression of an index policy proposed
in a more general context by Whittle. In the general case, we develop open-loop
periodic switching policies whose performance matches the bound arbitrarily
closely."
Munther Dahleh,Dahleh_Munther,arXiv:0805.1563,https://arxiv.org/abs/0805.1563,"Abstract:  We extend a relaxation technique due to Bertsimas and Nino-Mora for the
restless bandit problem to the case where arbitrary costs penalize switching
between the bandits. We also construct a one-step lookahead policy using the
solution of the relaxation. Computational experiments and a bound for
approximate dynamic programming provide some empirical support for the
heuristic."
Luca Daniel,Daniel_Luca,arXiv:1812.08329,https://arxiv.org/abs/1812.08329,"Abstract:  With deep neural networks providing state-of-the-art machine learning models
for numerous machine learning tasks, quantifying the robustness of these models
has become an important area of research. However, most of the research
literature merely focuses on the \textit{worst-case} setting where the input of
the neural network is perturbed with noises that are constrained within an
$\ell_p$ ball; and several algorithms have been proposed to compute certified
lower bounds of minimum adversarial distortion based on such worst-case
analysis. In this paper, we address these limitations and extend the approach
to a \textit{probabilistic} setting where the additive noises can follow a
given distributional characterization. We propose a novel probabilistic
framework PROVEN to PRObabilistically VErify Neural networks with statistical
guarantees -- i.e., PROVEN certifies the probability that the classifier's
top-1 prediction cannot be altered under any constrained $\ell_p$ norm
perturbation to a given input. Importantly, we show that it is possible to
derive closed-form probabilistic certificates based on current state-of-the-art
neural network robustness verification frameworks. Hence, the probabilistic
certificates provided by PROVEN come naturally and with almost no overhead when
obtaining the worst-case certified lower bounds from existing methods such as
Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR
neural network models demonstrate our probabilistic approach can achieve up to
around $75\%$ improvement in the robustness certification with at least a
$99.99\%$ confidence compared with the worst-case robustness certificate
delivered by CROWN."
Luca Daniel,Daniel_Luca,arXiv:1811.12395,https://arxiv.org/abs/1811.12395,"Abstract:  Verifying robustness of neural network classifiers has attracted great
interests and attention due to the success of deep neural networks and their
unexpected vulnerability to adversarial perturbations. Although finding minimum
adversarial distortion of neural networks (with ReLU activations) has been
shown to be an NP-complete problem, obtaining a non-trivial lower bound of
minimum distortion as a provable robustness guarantee is possible. However,
most previous works only focused on simple fully-connected layers (multilayer
perceptrons) and were limited to ReLU activations. This motivates us to propose
a general and efficient framework, CNN-Cert, that is capable of certifying
robustness on general convolutional neural networks. Our framework is general
-- we can handle various architectures including convolutional layers,
max-pooling layers, batch normalization layer, residual blocks, as well as
general activation functions; our approach is efficient -- by exploiting the
special structure of convolutional layers, we achieve up to 17 and 11 times of
speed-up compared to the state-of-the-art certification algorithms (e.g.
Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach
while our algorithm obtains similar or even better verification bounds. In
addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and
CROWN. We demonstrate by extensive experiments that our method outperforms
state-of-the-art lower-bound-based certification algorithms in terms of both
bound quality and speed."
Luca Daniel,Daniel_Luca,arXiv:1811.00866,https://arxiv.org/abs/1811.00866,"Abstract:  Finding minimum distortion of adversarial examples and thus certifying
robustness in neural network classifiers for given data points is known to be a
challenging problem. Nevertheless, recently it has been shown to be possible to
give a non-trivial certified lower bound of minimum adversarial distortion, and
some recent progress has been made towards this direction by exploiting the
piece-wise linear nature of ReLU activations. However, a generic robustness
certification for general activation functions still remains largely
unexplored. To address this issue, in this paper we introduce CROWN, a general
framework to certify robustness of neural networks with general activation
functions for given input data points. The novelty in our algorithm consists of
bounding a given activation function with linear and quadratic functions, hence
allowing it to tackle general activation functions including but not limited to
four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we
facilitate the search for a tighter certified lower bound by adaptively
selecting appropriate surrogates for each neuron activation. Experimental
results show that CROWN on ReLU networks can notably improve the certified
lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while
having comparable computational efficiency. Furthermore, CROWN also
demonstrates its effectiveness and flexibility on networks with general
activation functions, including tanh, sigmoid and arctan."
Luca Daniel,Daniel_Luca,arXiv:1810.08640,https://arxiv.org/abs/1810.08640,"Abstract:  CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme
Value Theory (EVT) based robustness score for large-scale deep neural networks
(DNNs). In this paper, we propose two extensions on this robustness score.
First, we provide a new formal robustness guarantee for classifier functions
that are twice differentiable. We apply extreme value theory on the new formal
robustness guarantee and the estimated robustness is called second-order CLEVER
score. Second, we discuss how to handle gradient masking, a common defensive
technique, using CLEVER with Backward Pass Differentiable Approximation (BPDA).
With BPDA applied, CLEVER can evaluate the intrinsic robustness of neural
networks of a broader class -- networks with non-differentiable input
transformations. We demonstrate the effectiveness of CLEVER with BPDA in
experiments on a 121-layer Densenet model trained on the ImageNet dataset."
Luca Daniel,Daniel_Luca,arXiv:1804.09699,https://arxiv.org/abs/1804.09699,"Abstract:  Verifying the robustness property of a general Rectified Linear Unit (ReLU)
network is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer
CAV17]. Although finding the exact minimum adversarial distortion is hard,
giving a certified lower bound of the minimum distortion is possible. Current
available methods of computing such a bound are either time-consuming or
delivering low quality bounds that are too loose to be useful. In this paper,
we exploit the special structure of ReLU networks and provide two
computationally efficient algorithms Fast-Lin and Fast-Lip that are able to
certify non-trivial lower bounds of minimum distortions, by bounding the ReLU
units with appropriate linear functions Fast-Lin, or by bounding the local
Lipschitz constant Fast-Lip. Experiments show that (1) our proposed methods
deliver bounds close to (the gap is 2-3X) exact minimum distortion found by
Reluplex in small MNIST networks while our algorithms are more than 10,000
times faster; (2) our methods deliver similar quality of bounds (the gap is
within 35% and usually around 10%; sometimes our bounds are even better) for
larger networks compared to the methods based on solving linear programming
problems but our algorithms are 33-14,000 times faster; (3) our method is
capable of solving large MNIST and CIFAR networks up to 7 layers with more than
10,000 neurons within tens of seconds on a single CPU core.
In addition, we show that, in fact, there is no polynomial time algorithm
that can approximately find the minimum $\ell_1$ adversarial distortion of a
ReLU network with a $0.99\ln n$ approximation ratio unless
$\mathsf{NP}$=$\mathsf{P}$, where $n$ is the number of neurons in the network."
Luca Daniel,Daniel_Luca,arXiv:1801.10578,https://arxiv.org/abs/1801.10578,"Abstract:  The robustness of neural networks to adversarial examples has received great
attention due to security implications. Despite various attack approaches to
crafting visually imperceptible adversarial examples, little has been developed
towards a comprehensive measure of robustness. In this paper, we provide a
theoretical justification for converting robustness analysis into a local
Lipschitz constant estimation problem, and propose to use the Extreme Value
Theory for efficient evaluation. Our analysis yields a novel robustness metric
called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork
Robustness. The proposed CLEVER score is attack-agnostic and computationally
feasible for large neural networks. Experimental results on various networks,
including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned
with the robustness indication measured by the $\ell_2$ and $\ell_\infty$ norms
of adversarial examples from powerful attacks, and (ii) defended networks using
defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To
the best of our knowledge, CLEVER is the first attack-independent robustness
metric that can be applied to any neural network classifier."
Luca Daniel,Daniel_Luca,arXiv:1707.07803,https://arxiv.org/abs/1707.07803,"Abstract:  We propose a new algorithm for the computation of a singular value
decomposition (SVD) low-rank approximation of a matrix in the Matrix Product
Operator (MPO) format, also called the Tensor Train Matrix format. Our tensor
network randomized SVD (TNrSVD) algorithm is an MPO implementation of the
randomized SVD algorithm that is able to compute dominant singular values and
their corresponding singular vectors. In contrast to the state-of-the-art
tensor-based alternating least squares SVD (ALS-SVD) and modified alternating
least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to
17 times faster while achieving the same accuracy. In addition, our TNrSVD
algorithm also produces accurate approximations in particular cases where both
ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the
fast conversion of a sparse matrix into its corresponding MPO form, which is up
to 509 times faster than the standard Tensor Train SVD (TT-SVD) method while
achieving machine precision accuracy. The efficiency and accuracy of both
algorithms are demonstrated in numerical experiments."
Luca Daniel,Daniel_Luca,arXiv:1611.02256,https://arxiv.org/abs/1611.02256,"Abstract:  Fabrication process variations are a major source of yield degradation in the
nano-scale design of integrated circuits (IC), microelectromechanical systems
(MEMS) and photonic circuits. Stochastic spectral methods are a promising
technique to quantify the uncertainties caused by process variations. Despite
their superior efficiency over Monte Carlo for many design cases, these
algorithms suffer from the curse of dimensionality; i.e., their computational
cost grows very fast as the number of random parameters increases. In order to
solve this challenging problem, this paper presents a high-dimensional
uncertainty quantification algorithm from a big-data perspective. Specifically,
we show that the huge number of (e.g., $1.5 \times 10^{27}$) simulation samples
in standard stochastic collocation can be reduced to a very small one (e.g.,
$500$) by exploiting some hidden structures of a high-dimensional data array.
This idea is formulated as a tensor recovery problem with sparse and low-rank
constraints; and it is solved with an alternating minimization approach.
Numerical results show that our approach can simulate efficiently some ICs, as
well as MEMS and photonic problems with over 50 independent random parameters,
whereas the traditional algorithm can only handle several random parameters."
Luca Daniel,Daniel_Luca,arXiv:1610.04272,https://arxiv.org/abs/1610.04272,"Abstract:  Many critical EDA problems suffer from the curse of dimensionality, i.e. the
very fast-scaling computational burden produced by large number of parameters
and/or unknown variables. This phenomenon may be caused by multiple spatial or
temporal factors (e.g. 3-D field solvers discretizations and multi-rate circuit
simulation), nonlinearity of devices and circuits, large number of design or
optimization parameters (e.g. full-chip routing/placement and circuit sizing),
or extensive process variations (e.g. variability/reliability analysis and
design for manufacturability). The computational challenges generated by such
high dimensional problems are generally hard to handle efficiently with
traditional EDA core algorithms that are based on matrix and vector
computation. This paper presents ""tensor computation"" as an alternative general
framework for the development of efficient EDA algorithms and tools. A tensor
is a high-dimensional generalization of a matrix and a vector, and is a natural
choice for both storing and solving efficiently high-dimensional EDA problems.
This paper gives a basic tutorial on tensors, demonstrates some recent examples
of EDA applications (e.g., nonlinear circuit modeling and high-dimensional
uncertainty quantification), and suggests further open EDA problems where the
use of tensor computation could be of advantage."
Luca Daniel,Daniel_Luca,arXiv:1603.06119,https://arxiv.org/abs/1603.06119,"Abstract:  Stochastic spectral methods have become a popular technique to quantify the
uncertainties of nano-scale devices and circuits. They are much more efficient
than Monte Carlo for certain design cases with a small number of random
parameters. However, their computational cost significantly increases as the
number of random parameters increases. This paper presents a big-data approach
to solve high-dimensional uncertainty quantification problems. Specifically, we
simulate integrated circuits and MEMS at only a small number of quadrature
samples, then, a huge number of (e.g., $1.5 \times 10^{27}$) solution samples
are estimated from the available small-size (e.g., $500$) solution samples via
a low-rank and tensor-recovery method. Numerical results show that our
algorithm can easily extend the applicability of tensor-product stochastic
collocation to IC and MEMS problems with over 50 random parameters, whereas the
traditional algorithm can only handle several random parameters."
Luca Daniel,Daniel_Luca,arXiv:1508.02489,https://arxiv.org/abs/1508.02489,"Abstract:  This paper presents a tensor-recovery method to solve probabilistic power
flow problems. Our approach generates a high-dimensional and sparse generalized
polynomial-chaos expansion that provides useful statistical information. The
result can also speed up other essential routines in power systems (e.g.,
stochastic planning, operations and controls).
Instead of simulating a power flow equation at all quadrature points, our
approach only simulates an extremely small subset of samples. We suggest a
model to exploit the underlying low-rank and sparse structure of
high-dimensional simulation data arrays, making our technique applicable to
power systems with many random parameters. We also present a numerical method
to solve the resulting nonlinear optimization problem.
Our algorithm is implemented in MATLAB and is verified by several benchmarks
in MATPOWER $5.1$. Accurate results are obtained for power systems with up to
$50$ independent random parameters, with a speedup factor up to $9\times
10^{20}$."
Luca Daniel,Daniel_Luca,arXiv:1409.4831,https://arxiv.org/abs/1409.4831,"Abstract:  Uncertainties have become a major concern in integrated circuit design. In
order to avoid the huge number of repeated simulations in conventional Monte
Carlo flows, this paper presents an intrusive spectral simulator for
statistical circuit analysis. Our simulator employs the recently developed
generalized polynomial chaos expansion to perform uncertainty quantification of
nonlinear transistor circuits with both Gaussian and non-Gaussian random
parameters. We modify the nonintrusive stochastic collocation (SC) method and
develop an intrusive variant called stochastic testing (ST) method to
accelerate the numerical simulation. Compared with the stochastic Galerkin (SG)
method, the resulting coupled deterministic equations from our proposed ST
method can be solved in a decoupled manner at each time point. At the same
time, ST uses fewer samples and allows more flexible time step size controls
than directly using a nonintrusive SC solver. These two properties make ST more
efficient than SG and than existing SC methods, and more suitable for
time-domain circuit simulation. Simulation results of several digital, analog
and RF circuits are reported. Since our algorithm is based on generic
mathematical models, the proposed ST algorithm can be applied to many other
engineering problems."
Luca Daniel,Daniel_Luca,arXiv:1409.4829,https://arxiv.org/abs/1409.4829,"Abstract:  Stochastic spectral methods are efficient techniques for uncertainty
quantification. Recently they have shown excellent performance in the
statistical analysis of integrated circuits. In stochastic spectral methods,
one needs to determine a set of orthonormal polynomials and a proper numerical
quadrature rule. The former are used as the basis functions in a generalized
polynomial chaos expansion. The latter is used to compute the integrals
involved in stochastic spectral methods. Obtaining such information requires
knowing the density function of the random input {\it a-priori}. However,
individual system components are often described by surrogate models rather
than density functions. In order to apply stochastic spectral methods in
hierarchical uncertainty quantification, we first propose to construct
physically consistent closed-form density functions by two monotone
interpolation schemes. Then, by exploiting the special forms of the obtained
density functions, we determine the generalized polynomial-chaos basis
functions and the Gauss quadrature rules that are required by a stochastic
spectral simulator. The effectiveness of our proposed algorithm is verified by
both synthetic and practical circuit examples."
Luca Daniel,Daniel_Luca,arXiv:1409.4826,https://arxiv.org/abs/1409.4826,"Abstract:  This brief paper proposes an uncertainty quantification method for the
periodic steady-state (PSS) analysis with both Gaussian and non-Gaussian
variations. Our stochastic testing formulation for the PSS problem provides
superior efficiency over both Monte Carlo methods and existing spectral
methods. The numerical implementation of a stochastic shooting Newton solver is
presented for both forced and autonomous circuits. Simulation results on some
analog/RF circuits are reported to show the effectiveness of our proposed
algorithms."
Luca Daniel,Daniel_Luca,arXiv:1409.4824,https://arxiv.org/abs/1409.4824,"Abstract:  Due to significant manufacturing process variations, the performance of
integrated circuits (ICs) has become increasingly uncertain. Such uncertainties
must be carefully quantified with efficient stochastic circuit simulators. This
paper discusses the recent advances of stochastic spectral circuit simulators
based on generalized polynomial chaos (gPC). Such techniques can handle both
Gaussian and non-Gaussian random parameters, showing remarkable speedup over
Monte Carlo for circuits with a small or medium number of parameters. We focus
on the recently developed stochastic testing and the application of
conventional stochastic Galerkin and stochastic collocation schemes to
nonlinear circuit problems. The uncertainty quantification algorithms for
static, transient and periodic steady-state simulations are presented along
with some practical simulation results. Some open problems in this field are
discussed."
Luca Daniel,Daniel_Luca,arXiv:1409.4822,https://arxiv.org/abs/1409.4822,"Abstract:  Process variations are a major concern in today's chip design since they can
significantly degrade chip performance. To predict such degradation, existing
circuit and MEMS simulators rely on Monte Carlo algorithms, which are typically
too slow. Therefore, novel fast stochastic simulators are highly desired. This
paper first reviews our recently developed stochastic testing simulator that
can achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we
develop a fast hierarchical stochastic spectral simulator to simulate a complex
circuit or system consisting of several blocks. We further present a fast
simulation approach based on anchored ANOVA (analysis of variance) for some
design problems with many process variations. This approach can reduce the
simulation cost and can identify which variation sources have strong impacts on
the circuit's performance. The simulation results of some circuit and MEMS
examples are reported to show the effectiveness of our simulator"
Luca Daniel,Daniel_Luca,arXiv:1407.3023,https://arxiv.org/abs/1407.3023,"Abstract:  Hierarchical uncertainty quantification can reduce the computational cost of
stochastic circuit simulation by employing spectral methods at different
levels. This paper presents an efficient framework to simulate hierarchically
some challenging stochastic circuits/systems that include high-dimensional
subsystems. Due to the high parameter dimensionality, it is challenging to both
extract surrogate models at the low level of the design hierarchy and to handle
them in the high-level simulation. In this paper, we develop an efficient
ANOVA-based stochastic circuit/MEMS simulator to extract efficiently the
surrogate models at the low level. In order to avoid the curse of
dimensionality, we employ tensor-train decomposition at the high level to
construct the basis functions and Gauss quadrature points. As a demonstration,
we verify our algorithm on a stochastic oscillator with four MEMS capacitors
and 184 random parameters. This challenging example is simulated efficiently by
our simulator at the cost of only 10 minutes in MATLAB on a regular personal
computer."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1811.10581,https://arxiv.org/abs/1811.10581,"Abstract:  Asynchronous Gibbs sampling has been recently shown to be fast-mixing and an
accurate method for estimating probabilities of events on a small number of
variables of a graphical model satisfying Dobrushin's
condition~\cite{DeSaOR16}. We investigate whether it can be used to accurately
estimate expectations of functions of {\em all the variables} of the model.
Under the same condition, we show that the synchronous (sequential) and
asynchronous Gibbs samplers can be coupled so that the expected Hamming
distance between their (multivariate) samples remains bounded by $O(\tau \log
n),$ where $n$ is the number of variables in the graphical model, and $\tau$ is
a measure of the asynchronicity. A similar bound holds for any constant power
of the Hamming distance. Hence, the expectation of any function that is
Lipschitz with respect to a power of the Hamming distance, can be estimated
with a bias that grows logarithmically in $n$. Going beyond Lipschitz
functions, we consider the bias arising from asynchronicity in estimating the
expectation of polynomial functions of all variables in the model. Using recent
concentration of measure results, we show that the bias introduced by the
asynchronicity is of smaller order than the standard deviation of the function
value already present in the true model. We perform experiments on a
multi-processor machine to empirically illustrate our theoretical findings."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1810.11896,https://arxiv.org/abs/1810.11896,"Abstract:  We analyze linear independence of rank one tensors produced by tensor powers
of randomly perturbed vectors. This enables efficient decomposition of sums of
high-order tensors. Our analysis builds upon [BCMV14] but allows for a wider
range of perturbation models, including discrete ones. We give an application
to recovering assemblies of neurons.
Assemblies are large sets of neurons representing specific memories or
concepts. The size of the intersection of two assemblies has been shown in
experiments to represent the extent to which these memories co-occur or these
concepts are related; the phenomenon is called association of assemblies. This
suggests that an animal's memory is a complex web of associations, and poses
the problem of recovering this representation from cognitive data. Motivated by
this problem, we study the following more general question: Can we reconstruct
the Venn diagram of a family of sets, given the sizes of their $\ell$-wise
intersections? We show that as long as the family of sets is randomly
perturbed, it is enough for the number of measurements to be polynomially
larger than the number of nonempty regions of the Venn diagram to fully
reconstruct the diagram."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1809.03986,https://arxiv.org/abs/1809.03986,"Abstract:  We provide an efficient algorithm for the classical problem, going back to
Galton, Pearson, and Fisher, of estimating, with arbitrary accuracy the
parameters of a multivariate normal distribution from truncated samples.
Truncated samples from a $d$-variate normal ${\cal
N}(\mathbf{\mu},\mathbf{\Sigma})$ means a samples is only revealed if it falls
in some subset $S \subseteq \mathbb{R}^d$; otherwise the samples are hidden and
their count in proportion to the revealed samples is also hidden. We show that
the mean $\mathbf{\mu}$ and covariance matrix $\mathbf{\Sigma}$ can be
estimated with arbitrary accuracy in polynomial-time, as long as we have oracle
access to $S$, and $S$ has non-trivial measure under the unknown $d$-variate
normal distribution. Additionally we show that without oracle access to $S$,
any non-trivial estimation is impossible."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1807.04252,https://arxiv.org/abs/1807.04252,"Abstract:  Motivated by applications in Game Theory, Optimization, and Generative
Adversarial Networks, recent work of Daskalakis et al~\cite{DISZ17} and
follow-up work of Liang and Stokes~\cite{LiangS18} have established that a
variant of the widely used Gradient Descent/Ascent procedure, called
""Optimistic Gradient Descent/Ascent (OGDA)"", exhibits last-iterate convergence
to saddle points in {\em unconstrained} convex-concave min-max optimization
problems. We show that the same holds true in the more general problem of {\em
constrained} min-max optimization under a variant of the no-regret
Multiplicative-Weights-Update method called ""Optimistic Multiplicative-Weights
Update (OMWU)"". This answers an open question of Syrgkanis et al~\cite{SALS15}.
The proof of our result requires fundamentally different techniques from
those that exist in no-regret learning literature and the aforementioned
papers. We show that OMWU monotonically improves the Kullback-Leibler
divergence of the current iterate to the (appropriately normalized) min-max
solution until it enters a neighborhood of the solution. Inside that
neighborhood we show that OMWU becomes a contracting map converging to the
exact solution. We believe that our techniques will be useful in the analysis
of the last iterate of other learning algorithms."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1807.03907,https://arxiv.org/abs/1807.03907,"Abstract:  Motivated by applications in Optimization, Game Theory, and the training of
Generative Adversarial Networks, the convergence properties of first order
methods in min-max problems have received extensive study. It has been
recognized that they may cycle, and there is no good understanding of their
limit points when they do not. When they converge, do they converge to local
min-max solutions? We characterize the limit points of two basic first order
methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent
Ascent (OGDA). We show that both dynamics avoid unstable critical points for
almost all initializations. Moreover, for small step sizes and under mild
assumptions, the set of \{OGDA\}-stable critical points is a superset of
\{GDA\}-stable critical points, which is a superset of local min-max solutions
(strict in some cases). The connecting thread is that the behavior of these
dynamics can be studied from a dynamical systems perspective."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1805.09697,https://arxiv.org/abs/1805.09697,"Abstract:  We consider testing and learning problems on causal Bayesian networks as
defined by Pearl (Pearl, 2009). Given a causal Bayesian network $\mathcal{M}$
on a graph with $n$ discrete variables and bounded in-degree and bounded
`confounded components', we show that $O(\log n)$ interventions on an unknown
causal Bayesian network $\mathcal{X}$ on the same graph, and
$\tilde{O}(n/\epsilon^2)$ samples per intervention, suffice to efficiently
distinguish whether $\mathcal{X}=\mathcal{M}$ or whether there exists some
intervention under which $\mathcal{X}$ and $\mathcal{M}$ are farther than
$\epsilon$ in total variation distance. We also obtain sample/time/intervention
efficient algorithms for: (i) testing the identity of two unknown causal
Bayesian networks on the same graph; and (ii) learning a causal Bayesian
network on a given graph. Although our algorithms are non-adaptive, we show
that adaptivity does not help in general: $\Omega(\log n)$ interventions are
necessary for testing the identity of two unknown causal Bayesian networks on
the same graph, even adaptively. Our algorithms are enabled by a new
subadditivity inequality for the squared Hellinger distance between two causal
Bayesian networks."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1803.00494,https://arxiv.org/abs/1803.00494,"Abstract:  We study revenue optimization in a repeated auction between a single seller
and a single buyer. Traditionally, the design of repeated auctions requires
strong modeling assumptions about the bidder behavior, such as it being myopic,
infinite lookahead, or some specific form of learning behavior. Is it possible
to design mechanisms which are simultaneously optimal against a multitude of
possible buyer behaviors? We answer this question by designing a simple
state-based mechanism that is simultaneously approximately optimal against a
$k$-lookahead buyer for all $k$, a buyer who is a no-regret learner, and a
buyer who is a policy-regret learner. Against each type of buyer our mechanism
attains a constant fraction of the optimal revenue attainable against that type
of buyer. We complement our positive results with almost tight impossibility
results, showing that the revenue approximation tradeoffs achieved by our
mechanism for different lookahead attitudes are near-optimal."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1712.09196,https://arxiv.org/abs/1712.09196,"Abstract:  Deep neural networks are demonstrating excellent performance on several
classical vision problems. However, these networks are vulnerable to
adversarial examples, minutely modified images that induce arbitrary
attacker-chosen output from the network. We propose a mechanism to protect
against these adversarial inputs based on a generative model of the data. We
introduce a pre-processing step that projects on the range of a generative
model using gradient descent before feeding an input into a classifier. We show
that this step provides the classifier with robustness against first-order,
substitute model, and combined adversarial attacks. Using a min-max
formulation, we show that there may exist adversarial examples even in the
range of the generator, natural-looking images extremely close to the decision
boundary for which the classifier has unjustifiedly high confidence. We show
that adversarial training on the generative manifold can be used to make a
classifier that is robust to these attacks.
Finally, we show how our method can be applied even without a pre-trained
generative model using a recent method called the deep image prior. We evaluate
our method on MNIST, CelebA and Imagenet and show robustness against the
current state of the art attacks."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1711.00141,https://arxiv.org/abs/1711.00141,"Abstract:  We address the issue of limit cycling behavior in training Generative
Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for
training Wasserstein GANs. Recent theoretical results have shown that
optimistic mirror decent (OMD) can enjoy faster regret rates in the context of
zero-sum games. WGANs is exactly a context of solving a zero-sum game with
simultaneous no-regret dynamics. Moreover, we show that optimistic mirror
decent addresses the limit cycling problem in training WGANs. We formally show
that in the case of bi-linear zero-sum games the last iterate of OMD dynamics
converges to an equilibrium, in contrast to GD dynamics which are bound to
cycle. We also portray the huge qualitative difference between GD and OMD
dynamics with toy examples, even when GD is modified with many adaptations
proposed in the recent literature, such as gradient penalty or momentum. We
apply OMD WGAN training to a bioinformatics problem of generating DNA
sequences. We observe that models trained with OMD achieve consistently smaller
KL divergence with respect to the true underlying distribution, than models
trained with GD variants. Finally, we introduce a new algorithm, Optimistic
Adam, which is an optimistic variant of Adam. We apply it to WGAN training on
CIFAR10 and observe improved performance in terms of inception score as
compared to Adam."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1710.04170,https://arxiv.org/abs/1710.04170,"Abstract:  We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree $d$, we show that a
degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that
scale as $\exp(-r^{2/d})$ at radius $r=\tilde{\Omega}_d(n^{d/2})$. Our
concentration radius is optimal up to logarithmic factors for constant $d$,
improving known results by polynomial factors in the number of spins. We
demonstrate the efficacy of polynomial functions as statistics for testing the
strength of interactions in social networks in both synthetic and real world
data."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1709.00228,https://arxiv.org/abs/1709.00228,"Abstract:  We provide algorithms that learn simple auctions whose revenue is
approximately optimal in multi-item multi-bidder settings, for a wide range of
valuations including unit-demand, additive, constrained additive, XOS, and
subadditive. We obtain our learning results in two settings. The first is the
commonly studied setting where sample access to the bidders' distributions over
valuations is given, for both regular distributions and arbitrary distributions
with bounded support. Our algorithms require polynomially many samples in the
number of items and bidders. The second is a more general max-min learning
setting that we introduce, where we are given ""approximate distributions,"" and
we seek to compute an auction whose revenue is approximately optimal
simultaneously for all ""true distributions"" that are close to the given ones.
These results are more general in that they imply the sample-based results, and
are also applicable in settings where we have no sample access to the
underlying distributions but have estimated them indirectly via market research
or by observation of previously run, potentially non-truthful auctions.
Our results hold for valuation distributions satisfying the standard (and
necessary) independence-across-items property. They also generalize and improve
upon recent works, which have provided algorithms that learn approximately
optimal auctions in more restricted settings with additive, subadditive and
unit-demand valuations using sample access to distributions. We generalize
these results to the complete unit-demand, additive, and XOS setting, to i.i.d.
subadditive bidders, and to the max-min setting.
Our results are enabled by new uniform convergence bounds for hypotheses
classes under product measures. Our bounds result in exponential savings in
sample complexity compared to bounds derived by bounding the VC dimension, and
are of independent interest."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1708.00002,https://arxiv.org/abs/1708.00002,"Abstract:  Given samples from an unknown distribution $p$ and a description of a
distribution $q$, are $p$ and $q$ close or far? This question of ""identity
testing"" has received significant attention in the case of testing whether $p$
and $q$ are equal or far in total variation distance. However, in recent work,
the following questions have been been critical to solving problems at the
frontiers of distribution testing:
-Alternative Distances: Can we test whether $p$ and $q$ are far in other
distances, say Hellinger?
-Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if
so, close in which distances?
Motivated by these questions, we characterize the complexity of distribution
testing under a variety of distances, including total variation, $\ell_2$,
Hellinger, Kullback-Leibler, and $\chi^2$. For each pair of distances $d_1$ and
$d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$
versus far in $d_2$, with a focus on identifying which problems allow strongly
sublinear testers (i.e., those with complexity $O(n^{1 - \gamma})$ for some
$\gamma > 0$ where $n$ is the size of the support of the distributions $p$ and
$q$). We provide matching upper and lower bounds for each case. We also study
these questions in the case where we only have samples from $q$ (equivalence
testing), showing qualitative differences from identity testing in terms of
when tolerance can be achieved. Our algorithms fall into the classical paradigm
of $\chi^2$-statistics, but require crucial changes to handle the challenges
introduced by each distance we consider. Finally, we survey other recent
results in an attempt to serve as a reference for the complexity of various
distribution testing problems."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1704.06850,https://arxiv.org/abs/1704.06850,"Abstract:  Classical distribution testing assumes access to i.i.d. samples from the
distribution that is being tested. We initiate the study of Markov chain
testing, assuming access to a single trajectory of a Markov Chain. In
particular, we observe a single trajectory X0,...,Xt,... of an unknown,
symmetric, and finite state Markov Chain M. We do not control the starting
state X0, and we cannot restart the chain. Given our single trajectory, the
goal is to test whether M is identical to a model Markov Chain M0 , or far from
it under an appropriate notion of difference. We propose a measure of
difference between two Markov chains, motivated by the early work of Kazakos
[Kaz78], which captures the scaling behavior of the total variation distance
between trajectories sampled from the Markov chains as the length of these
trajectories grows. We provide efficient testers and information-theoretic
lower bounds for testing identity of symmetric Markov chains under our proposed
measure of difference, which are tight up to logarithmic factors if the hitting
times of the model chain M0 is O(n) in the size of the state space n."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1703.10127,https://arxiv.org/abs/1703.10127,"Abstract:  We develop differentially private hypothesis testing methods for the small
sample regime. Given a sample $\cal D$ from a categorical distribution $p$ over
some domain $\Sigma$, an explicitly described distribution $q$ over $\Sigma$,
some privacy parameter $\varepsilon$, accuracy parameter $\alpha$, and
requirements $\beta_{\rm I}$ and $\beta_{\rm II}$ for the type I and type II
errors of our test, the goal is to distinguish between $p=q$ and
$d_{\rm{TV}}(p,q) \geq \alpha$.
We provide theoretical bounds for the sample size $|{\cal D}|$ so that our
method both satisfies $(\varepsilon,0)$-differential privacy, and guarantees
$\beta_{\rm I}$ and $\beta_{\rm II}$ type I and type II errors. We show that
differential privacy may come for free in some regimes of parameters, and we
always beat the sample complexity resulting from running the $\chi^2$-test with
noisy counts, or standard approaches such as repetition for endowing
non-private $\chi^2$-style statistics with differential privacy guarantees. We
experimentally compare the sample complexity of our method to that of recently
proposed methods for private hypothesis testing."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1702.07339,https://arxiv.org/abs/1702.07339,"Abstract:  Banach's fixed point theorem for contraction maps has been widely used to
analyze the convergence of iterative methods in non-convex problems. It is a
common experience, however, that iterative maps fail to be globally contracting
under the natural metric in their domain, making the applicability of Banach's
theorem limited. We explore how generally we can apply Banach's fixed point
theorem to establish the convergence of iterative methods when pairing it with
carefully designed metrics.
Our first result is a strong converse of Banach's theorem, showing that it is
a universal analysis tool for establishing global convergence of iterative
methods to unique fixed points, and for bounding their convergence rate. In
other words, we show that, whenever an iterative map globally converges to a
unique fixed point, there exists a metric under which the iterative map is
contracting and which can be used to bound the number of iterations until
convergence. We illustrate our approach in the widely used power method,
providing a new way of bounding its convergence rate through contraction
arguments.
We next consider the computational complexity of Banach's fixed point
theorem. Making the proof of our converse theorem constructive, we show that
computing a fixed point whose existence is guaranteed by Banach's fixed point
theorem is CLS-complete. We thus provide the first natural complete problem for
the class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capture
the complexity of problems such as P-matrix LCP, computing KKT-points, and
finding mixed Nash equilibria in congestion and network coordination games."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1612.03164,https://arxiv.org/abs/1612.03164,"Abstract:  We show that the square Hellinger distance between two Bayesian networks on
the same directed graph, $G$, is subadditive with respect to the neighborhoods
of $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two
Bayesian networks on the same DAG, our inequality states that the square
Hellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the
sum, $\sum_v H^2(P_{\{v\} \cup \Pi_v}, Q_{\{v\} \cup \Pi_v})$, of the square
Hellinger distances between the marginals of $P$ and $Q$ on every node $v$ and
its parents $\Pi_v$ in the DAG. Importantly, our bound does not involve the
conditionals but the marginals of $P$ and $Q$. We derive a similar inequality
for more general Markov Random Fields.
As an application of our inequality, we show that distinguishing whether two
Bayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy
$P=Q$ vs $d_{\rm TV}(P,Q)>\epsilon$ can be performed from
$\tilde{O}(|\Sigma|^{3/4(d+1)} \cdot n/\epsilon^2)$ samples, where $d$ is the
maximum in-degree of the DAG and $\Sigma$ the domain of each variable of the
Bayesian networks. If $P$ and $Q$ are defined on potentially different and
potentially unknown trees, the sample complexity becomes
$\tilde{O}(|\Sigma|^{4.5} n/\epsilon^2)$, whose dependence on $n, \epsilon$ is
optimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product
distributions over $\{0,1\}^n$ and $Q$ is known, the sample complexity becomes
$O(\sqrt{n}/\epsilon^2)$, which is optimal up to constant factors."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1612.03147,https://arxiv.org/abs/1612.03147,"Abstract:  Given samples from an unknown multivariate distribution $p$, is it possible
to distinguish whether $p$ is the product of its marginals versus $p$ being far
from every product distribution? Similarly, is it possible to distinguish
whether $p$ equals a given distribution $q$ versus $p$ and $q$ being far from
each other? These problems of testing independence and goodness-of-fit have
received enormous attention in statistics, information theory, and theoretical
computer science, with sample-optimal algorithms known in several interesting
regimes of parameters. Unfortunately, it has also been understood that these
problems become intractable in large dimensions, necessitating exponential
sample complexity.
Motivated by the exponential lower bounds for general distributions as well
as the ubiquity of Markov Random Fields (MRFs) in the modeling of
high-dimensional distributions, we initiate the study of distribution testing
on structured multivariate distributions, and in particular the prototypical
example of MRFs: the Ising Model. We demonstrate that, in this structured
setting, we can avoid the curse of dimensionality, obtaining sample and time
efficient testers for independence and goodness-of-fit. One of the key
technical challenges we face along the way is bounding the variance of
functions of the Ising model."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1609.00368,https://arxiv.org/abs/1609.00368,"Abstract:  The Expectation-Maximization (EM) algorithm is a widely used method for
maximum likelihood estimation in models with latent variables. For estimating
mixtures of Gaussians, its iteration can be viewed as a soft version of the
k-means clustering algorithm. Despite its wide use and applications, there are
essentially no known convergence guarantees for this method. We provide global
convergence guarantees for mixtures of two Gaussians with known covariance
matrices. We show that the population version of EM, where the algorithm is
given access to infinitely many samples from the mixture, converges
geometrically to the correct mean vectors, and provide simple, closed-form
expressions for the convergence rate. As a simple illustration, we show that,
in one dimension, ten steps of the EM algorithm initialized at infinity result
in less than 1\% error estimation of the means. In the finite sample regime, we
show that, under a random initialization, $\tilde{O}(d/\epsilon^2)$ samples
suffice to compute the unknown vectors to within $\epsilon$ in Mahalanobis
distance, where $d$ is the dimension. In particular, the error rate of the EM
based estimator is $\tilde{O}\left(\sqrt{d \over n}\right)$ where $n$ is the
number of samples, which is optimal up to logarithmic factors."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1605.02054,https://arxiv.org/abs/1605.02054,"Abstract:  We consider the problem of a revenue-maximizing seller with m items for sale
to n additive bidders with hard budget constraints, assuming that the seller
has some prior distribution over bidder values and budgets. The prior may be
correlated across items and budgets of the same bidder, but is assumed
independent across bidders. We target mechanisms that are Bayesian Incentive
Compatible, but that are ex-post Individually Rational and ex-post budget
respecting. Virtually no such mechanisms are known that satisfy all these
conditions and guarantee any revenue approximation, even with just a single
item. We provide a computationally efficient mechanism that is a
$3$-approximation with respect to all BIC, ex-post IR, and ex-post budget
respecting mechanisms. Note that the problem is NP-hard to approximate better
than a factor of 16/15, even in the case where the prior is a point mass
\cite{ChakrabartyGoel}. We further characterize the optimal mechanism in this
setting, showing that it can be interpreted as a distribution over virtual
welfare maximizers.
We prove our results by making use of a black-box reduction from mechanism to
algorithm design developed by \cite{CaiDW13b}. Our main technical contribution
is a computationally efficient $3$-approximation algorithm for the algorithmic
problem that results by an application of their framework to this problem. The
algorithmic problem has a mixed-sign objective and is NP-hard to optimize
exactly, so it is surprising that a computationally efficient approximation is
possible at all. In the case of a single item ($m=1$), the algorithmic problem
can be solved exactly via exhaustive search, leading to a computationally
efficient exact algorithm and a stronger characterization of the optimal
mechanism as a distribution over virtual value maximizers."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1603.07229,https://arxiv.org/abs/1603.07229,"Abstract:  We provide a characterization of revenue-optimal dynamic mechanisms in
settings where a monopolist sells k items over k periods to a buyer who
realizes his value for item i in the beginning of period i. We require that the
mechanism satisfies a strong individual rationality constraint, requiring that
the stage utility of each agent be positive during each period. We show that
the optimum mechanism can be computed by solving a nested sequence of static
(single-period) mechanisms that optimize a tradeoff between the surplus of the
allocation and the buyer's utility. We also provide a simple dynamic mechanism
that obtains at least half of the optimal revenue. The mechanism either ignores
history and posts the optimal monopoly price in each period, or allocates with
a probability that is independent of the current report of the agent and is
based only on previous reports. Our characterization extends to multi-agent
auctions. We also formulate a discounted infinite horizon version of the
problem, where we study the performance of ""Markov mechanisms."""
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1511.03641,https://arxiv.org/abs/1511.03641,"Abstract:  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the
sum of $n$ independent random vectors supported on the set ${\cal
B}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We show
that any $(n,k)$-PMD is ${\rm poly}\left({k\over \sigma}\right)$-close in total
variation distance to the (appropriately discretized) multi-dimensional
Gaussian with the same first two moments, removing the dependence on $n$ from
the Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is
obtained by bootstrapping the Valiant-Valiant CLT itself through the structural
characterization of PMDs shown in recent work by Daskalakis, Kamath, and
Tzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS
for approximate Nash equilibria in anonymous games, significantly improving the
state of the art, and matching qualitatively the running time dependence on $n$
and $1/\varepsilon$ of the best known algorithm for two-strategy anonymous
games. Our new CLT also enables the construction of covers for the set of
$(n,k)$-PMDs, which are proper and whose size is shown to be essentially
optimal. Our cover construction combines our CLT with the Shapley-Folkman
theorem and recent sparsification results for Laplacian matrices by Batson,
Spielman, and Srivastava. Our cover size lower bound is based on an algebraic
geometric construction. Finally, leveraging the structural properties of the
Fourier spectrum of PMDs we show that these distributions can be learned from
$O_k(1/\varepsilon^2)$ samples in ${\rm poly}_k(1/\varepsilon)$-time, removing
the quasi-polynomial dependence of the running time on $1/\varepsilon$ from the
algorithm of Daskalakis, Kamath, and Tzamos."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1511.01411,https://arxiv.org/abs/1511.01411,"Abstract:  A line of recent work provides welfare guarantees of simple combinatorial
auction formats, such as selling m items via simultaneous second price auctions
(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et
al. 2013). These guarantees hold even when the auctions are repeatedly executed
and players use no-regret learning algorithms. Unfortunately, off-the-shelf
no-regret algorithms for these auctions are computationally inefficient as the
number of actions is exponential. We show that this obstacle is insurmountable:
there are no polynomial-time no-regret algorithms for SiSPAs, unless
RP$\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises
the question of how good outcomes polynomially-bounded bidders may discover in
such auctions.
To answer this question, we propose a novel concept of learning in auctions,
termed ""no-envy learning."" This notion is founded upon Walrasian equilibrium,
and we show that it is both efficiently implementable and results in
approximately optimal welfare, even when the bidders have fractionally
subadditive (XOS) valuations (assuming demand oracles) or coverage valuations
(without demand oracles). No-envy learning outcomes are a relaxation of
no-regret outcomes, which maintain their approximate welfare optimality while
endowing them with computational tractability. Our results extend to other
auction formats that have been studied in the literature via the smoothness
paradigm.
Our results for XOS valuations are enabled by a novel
Follow-The-Perturbed-Leader algorithm for settings where the number of experts
is infinite, and the payoff function of the learner is non-linear. This
algorithm has applications outside of auction settings, such as in security
games. Our result for coverage valuations is based on a novel use of convex
rounding schemes and a reduction to online convex optimization."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1508.01962,https://arxiv.org/abs/1508.01962,"Abstract:  Reconstructing the tree of life from molecular sequences is a fundamental
problem in computational biology. Modern data sets often contain a large number
of genes, which can complicate the reconstruction problem due to the fact that
different genes may undergo different evolutionary histories. This is the case
in particular in the presence of horizontal genetic transfer (HGT), where a
gene is inherited from a distant species rather than an immediate ancestor.
Such an event produces a gene tree which is distinct from, but related to, the
species phylogeny.
In previous work, a natural stochastic models of HGT was introduced and
studied. It was shown, both in simulation and theoretical studies, that a
species phylogeny can be reconstructed from gene trees despite surprisingly
high rates of HGT under this model. Rigorous lower and upper bounds on this
achievable rate were also obtained, but a large gap remained. Here we close
this gap, up to a constant. Specifically we show that a species phylogeny can
be reconstructed correctly from gene trees even when, on each gene, each edge
of the species tree has a constant probability of being the location of an HGT
event. Our new reconstruction algorithm, which relies only on unrooted gene
tree topologies, builds the tree recursively from the leaves and runs in
polynomial time.
We also provide a matching bound in the negative direction (up to a constant)
and extend our results to some cases where gene trees are not perfectly known."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1507.05952,https://arxiv.org/abs/1507.05952,"Abstract:  Given samples from an unknown distribution $p$, is it possible to distinguish
whether $p$ belongs to some class of distributions $\mathcal{C}$ versus $p$
being far from every distribution in $\mathcal{C}$? This fundamental question
has received tremendous attention in statistics, focusing primarily on
asymptotic analysis, and more recently in information theory and theoretical
computer science, where the emphasis has been on small sample size and
computational complexity. Nevertheless, even for basic properties of
distributions such as monotonicity, log-concavity, unimodality, independence,
and monotone-hazard rate, the optimal sample complexity is unknown.
We provide a general approach via which we obtain sample-optimal and
computationally efficient testers for all these distribution families. At the
core of our approach is an algorithm which solves the following problem: Given
samples from an unknown distribution $p$, and a known distribution $q$, are $p$
and $q$ close in $\chi^2$-distance, or far in total variation distance?
The optimality of our testers is established by providing matching lower
bounds with respect to both $n$ and $\varepsilon$. Finally, a necessary
building block for our testers and an important byproduct of our work are the
first known computationally efficient proper learners for discrete log-concave
and monotone hazard rate distributions."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1504.08363,https://arxiv.org/abs/1504.08363,"Abstract:  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the
sum of $n$ independent random vectors supported on the set ${\cal
B}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We prove
a structural characterization of these distributions, showing that, for all
$\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is
$\varepsilon$-close, in total variation distance, to the sum of a discretized
multidimensional Gaussian and an independent $(\text{poly}(k/\varepsilon),
k)$-Poisson multinomial random vector. Our structural characterization extends
the multi-dimensional CLT of Valiant and Valiant, by simultaneously applying to
all approximation requirements $\varepsilon$. In particular, it overcomes
factors depending on $\log n$ and, importantly, the minimum eigenvalue of the
PMD's covariance matrix from the distance to a multidimensional Gaussian random
variable.
We use our structural characterization to obtain an $\varepsilon$-cover, in
total variation distance, of the set of all $(n, k)$-PMDs, significantly
improving the cover size of Daskalakis and Papadimitriou, and obtaining the
same qualitative dependence of the cover size on $n$ and $\varepsilon$ as the
$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structure
to show that $(n,k)$-PMDs can be learned to within $\varepsilon$ in total
variation distance from $\tilde{O}_k(1/\varepsilon^2)$ samples, which is
near-optimal in terms of dependence on $\varepsilon$ and independent of $n$. In
particular, our result generalizes the single-dimensional result of Daskalakis,
Diakonikolas, and Servedio for Poisson Binomials to arbitrary dimension."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1503.02516,https://arxiv.org/abs/1503.02516,"Abstract:  We show that computing the revenue-optimal deterministic auction in
unit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, is
computationally hard even in single-item settings where the buyer's value
distribution is a sum of independently distributed attributes, or multi-item
settings where the buyer's values for the items are independent. We also show
that it is intractable to optimally price the grand bundle of multiple items
for an additive bidder whose values for the items are independent. These
difficulties stem from implicit definitions of a value distribution. We provide
three instances of how different properties of implicit distributions can lead
to intractability: the first is a #P-hardness proof, while the remaining two
are reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. While
simple pricing schemes can oftentimes approximate the best scheme in revenue,
they can have drastically different underlying structure. We argue therefore
that either the specification of the input distribution must be highly
restricted in format, or it is necessary for the goal to be mere approximation
to the optimal scheme's revenue instead of computing properties of the scheme
itself."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1503.01958,https://arxiv.org/abs/1503.01958,"Abstract:  Optimal mechanisms have been provided in quite general multi-item settings,
as long as each bidder's type distribution is given explicitly by listing every
type in the support along with its associated probability. In the implicit
setting, e.g. when the bidders have additive valuations with independent and/or
continuous values for the items, these results do not apply, and it was
recently shown that exact revenue optimization is intractable, even when there
is only one bidder. Even for item distributions with special structure, optimal
mechanisms have been surprisingly rare and the problem is challenging even in
the two-item case. In this paper, we provide a framework for designing optimal
mechanisms using optimal transport theory and duality theory. We instantiate
our framework to obtain conditions under which only pricing the grand bundle is
optimal in multi-item settings (complementing the work of [Manelli and Vincent
2006], as well as to characterize optimal two-item mechanisms. We use our
results to derive closed-form descriptions of the optimal mechanism in several
two-item settings, exhibiting also a setting where a continuum of lotteries is
necessary for revenue optimization but a closed-form representation of the
mechanism can still be found efficiently using our framework."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1412.4840,https://arxiv.org/abs/1412.4840,"Abstract:  Fictitious play is a natural dynamic for equilibrium play in zero-sum games,
proposed by [Brown 1949], and shown to converge by [Robinson 1951]. Samuel
Karlin conjectured in 1959 that fictitious play converges at rate
$O(1/\sqrt{t})$ with the number of steps $t$. We disprove this conjecture
showing that, when the payoff matrix of the row player is the $n \times n$
identity matrix, fictitious play may converge with rate as slow as
$\Omega(t^{-1/n})$."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1410.3386,https://arxiv.org/abs/1410.3386,"Abstract:  A Poisson Binomial distribution over $n$ variables is the distribution of the
sum of $n$ independent Bernoullis. We provide a sample near-optimal algorithm
for testing whether a distribution $P$ supported on $\{0,...,n\}$ to which we
have sample access is a Poisson Binomial distribution, or far from all Poisson
Binomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$
to which we provide a matching lower bound. We note that our sample complexity
improves quadratically upon that of the naive ""learn followed by tolerant-test""
approach, while instance optimal identity testing [VV14] is not applicable
since we are looking to simultaneously test against a whole family of
distributions."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1409.4150,https://arxiv.org/abs/1409.4150,"Abstract:  We characterize optimal mechanisms for the multiple-good monopoly problem and
provide a framework to find them. We show that a mechanism is optimal if and
only if a measure $\mu$ derived from the buyer's type distribution satisfies
certain stochastic dominance conditions. This measure expresses the marginal
change in the seller's revenue under marginal changes in the rent paid to
subsets of buyer types. As a corollary, we characterize the optimality of
grand-bundling mechanisms, strengthening several results in the literature,
where only sufficient optimality conditions have been derived. As an
application, we show that the optimal mechanism for $n$ independent uniform
items each supported on $[c,c+1]$ is a grand-bundling mechanism, as long as $c$
is sufficiently large, extending Pavlov's result for $2$ items [Pavlov'11]. At
the same time, our characterization also implies that, for all $c$ and for all
sufficiently large $n$, the optimal mechanism for $n$ independent uniform items
supported on $[c,c+1]$ is not a grand bundling mechanism."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1408.2539,https://arxiv.org/abs/1408.2539,"Abstract:  We propose an optimum mechanism for providing monetary incentives to the data
sources of a statistical estimator such as linear regression, so that high
quality data is provided at low cost, in the sense that the sum of payments and
estimation error is minimized. The mechanism applies to a broad range of
estimators, including linear and polynomial regression, kernel regression, and,
under some additional assumptions, ridge regression. It also generalizes to
several objectives, including minimizing estimation error subject to budget
constraints. Besides our concrete results for regression problems, we
contribute a mechanism design framework through which to design and analyze
statistical estimators whose examples are supplied by workers with cost for
labeling said examples."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1405.5940,https://arxiv.org/abs/1405.5940,"Abstract:  We provide polynomial-time approximately optimal Bayesian mechanisms for
makespan minimization on unrelated machines as well as for max-min fair
allocations of indivisible goods, with approximation factors of $2$ and
$\min\{m-k+1, \tilde{O}(\sqrt{k})\}$ respectively, matching the approximation
ratios of best known polynomial-time \emph{algorithms} (for max-min fairness,
the latter claim is true for certain ratios of the number of goods $m$ to
people $k$). Our mechanisms are obtained by establishing a polynomial-time
approximation-sensitive reduction from the problem of designing approximately
optimal {\em mechanisms} for some arbitrary objective ${\cal O}$ to that of
designing bi-criterion approximation {\em algorithms} for the same objective
${\cal O}$ plus a linear allocation cost term. Our reduction is itself enabled
by extending the celebrated ""equivalence of separation and
optimization""[GLSS81,KP80] to also accommodate bi-criterion approximations.
Moreover, to apply the reduction to the specific problems of makespan and
max-min fairness we develop polynomial-time bi-criterion approximation
algorithms for makespan minimization with costs and max-min fairness with
costs, adapting the algorithms of [ST93], [BD05] and [AS07] to the type of
bi-criterion approximation that is required by the reduction."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1312.1054,https://arxiv.org/abs/1312.1054,"Abstract:  We provide an algorithm for properly learning mixtures of two
single-dimensional Gaussians without any separability assumptions. Given
$\tilde{O}(1/\varepsilon^2)$ samples from an unknown mixture, our algorithm
outputs a mixture that is $\varepsilon$-close in total variation distance, in
time $\tilde{O}(1/\varepsilon^5)$. Our sample complexity is optimal up to
logarithmic factors, and significantly improves upon both Kalai et al., whose
algorithm has a prohibitive dependence on $1/\varepsilon$, and Feldman et al.,
whose algorithm requires bounds on the mixture parameters and depends
pseudo-polynomially in these parameters.
One of our main contributions is an improved and generalized algorithm for
selecting a good candidate distribution from among competing hypotheses.
Namely, given a collection of $N$ hypotheses containing at least one candidate
that is $\varepsilon$-close to an unknown distribution, our algorithm outputs a
candidate which is $O(\varepsilon)$-close to the distribution. The algorithm
requires ${O}(\log{N}/\varepsilon^2)$ samples from the unknown distribution and
${O}(N \log N/\varepsilon^2)$ time, which improves previous such results (such
as the Scheffé estimator) from a quadratic dependence of the running time on
$N$ to quasilinear. Given the wide use of such results for the purpose of
hypothesis selection, our improved algorithm implies immediate improvements to
any such use."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1309.7084,https://arxiv.org/abs/1309.7084,"Abstract:  The Chord algorithm is a popular, simple method for the succinct
approximation of curves, which is widely used, under different names, in a
variety of areas, such as, multiobjective and parametric optimization,
computational geometry, and graphics. We analyze the performance of the Chord
algorithm, as compared to the optimal approximation that achieves a desired
accuracy with the minimum number of points. We prove sharp upper and lower
bounds, both in the worst case and average case setting."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1307.3621,https://arxiv.org/abs/1307.3621,"Abstract:  We consider a problem which has received considerable attention in systems
literature because of its applications to routing in delay tolerant networks
and replica placement in distributed storage systems. In abstract terms the
problem can be stated as follows: Given a random variable $X$ generated by a
known product distribution over $\{0,1\}^n$ and a target value $0 \leq \theta
\leq 1$, output a non-negative vector $w$, with $\|w\|_1 \le 1$, which
maximizes the probability of the event $w \cdot X \ge \theta$. This is a
challenging non-convex optimization problem for which even computing the value
$\Pr[w \cdot X \ge \theta]$ of a proposed solution vector $w$ is #P-hard.
We provide an additive EPTAS for this problem which, for constant-bounded
product distributions, runs in $ \poly(n) \cdot 2^{\poly(1/\eps)}$ time and
outputs an $\eps$-approximately optimal solution vector $w$ for this problem.
Our approach is inspired by, and extends, recent structural results from the
complexity-theoretic study of linear threshold functions. Furthermore, in spite
of the objective function being non-smooth, we give a \emph{unicriterion} PTAS
while previous work for such objective functions has typically led to a
\emph{bicriterion} PTAS. We believe our techniques may be applicable to get
unicriterion PTAS for other non-smooth objective functions."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1306.1265,https://arxiv.org/abs/1306.1265,"Abstract:  For all $n, \epsilon >0$, we show that the set of Poisson Binomial
distributions on $n$ variables admits a proper $\epsilon$-cover in total
variation distance of size $n^2+n \cdot (1/\epsilon)^{O(\log^2 (1/\epsilon))}$,
which can also be computed in polynomial time. We discuss the implications of
our construction for approximation algorithms and the computation of
approximate Nash equilibria in anonymous games."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1305.4002,https://arxiv.org/abs/1305.4002,"Abstract:  We provide a computationally efficient black-box reduction from mechanism
design to algorithm design in very general settings. Specifically, we give an
approximation-preserving reduction from truthfully maximizing \emph{any}
objective under \emph{arbitrary} feasibility constraints with \emph{arbitrary}
bidder types to (not necessarily truthfully) maximizing the same objective plus
virtual welfare (under the same feasibility constraints). Our reduction is
based on a fundamentally new approach: we describe a mechanism's behavior
indirectly only in terms of the expected value it awards bidders for certain
behavior, and never directly access the allocation rule at all.
Applying our new approach to revenue, we exhibit settings where our reduction
holds \emph{both ways}. That is, we also provide an approximation-sensitive
reduction from (non-truthfully) maximizing virtual welfare to (truthfully)
maximizing revenue, and therefore the two problems are computationally
equivalent. With this equivalence in hand, we show that both problems are
NP-hard to approximate within any polynomial factor, even for a single monotone
submodular bidder.
We further demonstrate the applicability of our reduction by providing a
truthful mechanism maximizing fractional max-min fairness. This is the first
instance of a truthful mechanism that optimizes a non-linear objective."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1305.4000,https://arxiv.org/abs/1305.4000,"Abstract:  It was recently shown in [this http URL] that revenue
optimization can be computationally efficiently reduced to welfare optimization
in all multi-dimensional Bayesian auction problems with arbitrary (possibly
combinatorial) feasibility constraints and independent additive bidders with
arbitrary (possibly combinatorial) demand constraints. This reduction provides
a poly-time solution to the optimal mechanism design problem in all auction
settings where welfare optimization can be solved efficiently, but it is
fragile to approximation and cannot provide solutions to settings where welfare
maximization can only be tractably approximated. In this paper, we extend the
reduction to accommodate approximation algorithms, providing an approximation
preserving reduction from (truthful) revenue maximization to (not necessarily
truthful) welfare maximization. The mechanisms output by our reduction choose
allocations via black-box calls to welfare approximation on randomly selected
inputs, thereby generalizing also our earlier structural results on optimal
multi-dimensional mechanisms to approximately optimal mechanisms. Unlike
[this http URL], our results here are obtained through novel
uses of the Ellipsoid algorithm and other optimization techniques over {\em
non-convex regions}."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1211.1703,https://arxiv.org/abs/1211.1703,"Abstract:  Myerson's seminal work provides a computationally efficient revenue-optimal
auction for selling one item to multiple bidders. Generalizing this work to
selling multiple items at once has been a central question in economics and
algorithmic game theory, but its complexity has remained poorly understood. We
answer this question by showing that a revenue-optimal auction in multi-item
settings cannot be found and implemented computationally efficiently, unless
ZPP contains P^#P. This is true even for a single additive bidder whose values
for the items are independently distributed on two rational numbers with
rational probabilities. Our result is very general: we show that it is hard to
compute any encoding of an optimal auction of any format (direct or indirect,
truthful or non-truthful) that can be implemented in expected polynomial time.
In particular, under well-believed complexity-theoretic assumptions,
revenue-optimization in very simple multi-item settings can only be tractably
approximated.
We note that our hardness result applies to randomized mechanisms in a very
simple setting, and is not an artifact of introducing combinatorial structure
to the problem by allowing correlation among item values, introducing
combinatorial valuations, or requiring the mechanism to be deterministic (whose
structure is readily combinatorial). Our proof is enabled by a
flow-interpretation of the solutions of an exponential-size linear program for
revenue maximization with an additional supermodularity constraint."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1207.5518,https://arxiv.org/abs/1207.5518,"Abstract:  We provide a reduction from revenue maximization to welfare maximization in
multi-dimensional Bayesian auctions with arbitrary (possibly combinatorial)
feasibility constraints and independent bidders with arbitrary (possibly
combinatorial) demand constraints, appropriately extending Myerson's result to
this setting. We also show that every feasible Bayesian auction can be
implemented as a distribution over virtual VCG allocation rules. A virtual VCG
allocation rule has the following simple form: Every bidder's type t_i is
transformed into a virtual type f_i(t_i), via a bidder-specific function. Then,
the allocation maximizing virtual welfare is chosen. Using this
characterization, we show how to find and run the revenue-optimal auction given
only black box access to an implementation of the VCG allocation rule. We
generalize this result to arbitrarily correlated bidders, introducing the
notion of a second-order VCG allocation rule.
We obtain our reduction from revenue to welfare optimization via two
algorithmic results on reduced forms in settings with arbitrary feasibility and
demand constraints. First, we provide a separation oracle for determining
feasibility of a reduced form. Second, we provide a geometric algorithm to
decompose any feasible reduced form into a distribution over virtual VCG
allocation rules. In addition, we show how to execute both algorithms given
only black box access to an implementation of the VCG allocation rule.
Our results are computationally efficient for all multi-dimensional settings
where the bidders are additive. In this case, our mechanisms run in time
polynomial in the total number of bidder types, but not type profiles. For
generic correlated distributions, this is the natural description complexity of
the problem. The runtime can be further improved to poly(#items, #bidders) in
item-symmetric settings by making use of recent techniques."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.5659,https://arxiv.org/abs/1112.5659,"Abstract:  We give highly efficient algorithms, and almost matching lower bounds, for a
range of basic statistical problems that involve testing and estimating the L_1
distance between two k-modal distributions $p$ and $q$ over the discrete domain
$\{1,\dots,n\}$. More precisely, we consider the following four problems: given
sample access to an unknown k-modal distribution $p$,
Testing identity to a known or unknown distribution:
1. Determine whether $p = q$ (for an explicitly given k-modal distribution
$q$) versus $p$ is $\eps$-far from $q$;
2. Determine whether $p=q$ (where $q$ is available via sample access) versus
$p$ is $\eps$-far from $q$;
Estimating $L_1$ distance (""tolerant testing'') against a known or unknown
distribution:
3. Approximate $d_{TV}(p,q)$ to within additive $\eps$ where $q$ is an
explicitly given k-modal distribution $q$;
4. Approximate $d_{TV}(p,q)$ to within additive $\eps$ where $q$ is available
via sample access.
For each of these four problems we give sub-logarithmic sample algorithms,
that we show are tight up to additive $\poly(k)$ and multiplicative
$\polylog\log n+\polylog k$ factors. Thus our bounds significantly improve the
previous results of \cite{BKR:04}, which were for testing identity of
distributions (items (1) and (2) above) in the special cases k=0 (monotone
distributions) and k=1 (unimodal distributions) and required $O((\log n)^3)$
samples.
As our main conceptual contribution, we introduce a new reduction-based
approach for distribution-testing problems that lets us obtain all the above
results in a unified way. Roughly speaking, this approach enables us to
transform various distribution testing problems for k-modal distributions over
$\{1,\dots,n\}$ to the corresponding distribution testing problems for
unrestricted distributions over a much smaller domain $\{1,\dots,\ell\}$ where
$\ell = O(k \log n).$"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.4572,https://arxiv.org/abs/1112.4572,"Abstract:  We provide a constructive proof of Border's theorem [Bor91, HR15a] and its
generalization to reduced-form auctions with asymmetric bidders [Bor07, MV10,
CKM13]. Given a reduced form, we identify a subset of Border constraints that
are necessary and sufficient to determine its feasibility. Importantly, the
number of these constraints is linear in the total number of bidder types. In
addition, we provide a characterization result showing that every feasible
reduced form can be induced by an ex-post allocation rule that is a
distribution over ironings of the same total ordering of the union of all
bidders' types.
We show how to leverage our results for single-item reduced forms to design
auctions with heterogeneous items and asymmetric bidders with valuations that
are additive over items. Appealing to our constructive Border's theorem, we
obtain polynomial-time algorithms for computing the revenue-optimal auction.
Appealing to our characterization of feasible reduced forms, we characterize
feasible multi-item allocation rules."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1112.4006,https://arxiv.org/abs/1112.4006,"Abstract:  We efficiently solve the optimal multi-dimensional mechanism design problem
for independent bidders with arbitrary demand constraints when either the
number of bidders is a constant or the number of items is a constant. In the
first setting, we need that each bidder's values for the items are sampled from
a possibly correlated, item-symmetric distribution, allowing different
distributions for each bidder. In the second setting, we allow the values of
each bidder for the items to be arbitrarily correlated, but assume that the
distribution of bidder types is bidder-symmetric.
For all eps>0, we obtain an additive eps-approximation, when the value
distributions are bounded, or a multiplicative (1-eps)-approximation when the
value distributions are unbounded, but satisfy the Monotone Hazard Rate
condition, covering a widely studied class of distributions in Economics. Our
runtime is polynomial in max{#items,#bidders}, and not the size of the support
of the joint distribution of all bidders' values for all items, which is
typically exponential in both the number of items and the number of bidders.
Our mechanisms are randomized, explicitly price bundles, and can sometimes
accommodate budget constraints.
Our results are enabled by establishing several new tools and structural
properties of Bayesian mechanisms. We provide a symmetrization technique
turning any truthful mechanism into one that has the same revenue and respects
all symmetries in the underlying value distributions. We also prove that
item-symmetric mechanisms satisfy a natural monotonicity property which, unlike
cyclic-monotonicity, can be harnessed algorithmically. Finally, we provide a
technique that turns any given eps-BIC mechanism (i.e. one where incentive
constraints are violated by eps) into a truly-BIC mechanism at the cost of
O(sqrt{eps}) revenue. We expect our tools to be used beyond the settings we
consider here."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1109.5002,https://arxiv.org/abs/1109.5002,"Abstract:  We present an efficient phylogenetic reconstruction algorithm allowing
insertions and deletions which provably achieves a sequence-length requirement
(or sample complexity) growing polynomially in the number of taxa. Our
algorithm is distance-based, that is, it relies on pairwise sequence
comparisons. More importantly, our approach largely bypasses the difficult
problem of multiple sequence alignment."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1107.2702,https://arxiv.org/abs/1107.2702,"Abstract:  We consider a basic problem in unsupervised learning: learning an unknown
\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)
over $\{0,1,\dots,n\}$ is the distribution of a sum of $n$ independent
Bernoulli random variables which may have arbitrary, potentially non-equal,
expectations. These distributions were first studied by S. Poisson in 1837
\cite{Poisson:37} and are a natural $n$-parameter generalization of the
familiar Binomial Distribution. Surprisingly, prior to our work this basic
learning problem was poorly understood, and known results for it were far from
optimal.
We essentially settle the complexity of the learning problem for this basic
class of distributions. As our first main result we give a highly efficient
algorithm which learns to $\eps$-accuracy (with respect to the total variation
distance) using $\tilde{O}(1/\eps^3)$ samples \emph{independent of $n$}. The
running time of the algorithm is \emph{quasilinear} in the size of its input
data, i.e., $\tilde{O}(\log(n)/\eps^3)$ bit-operations. (Observe that each draw
from the distribution is a $\log(n)$-bit string.) Our second main result is a
{\em proper} learning algorithm that learns to $\eps$-accuracy using
$\tilde{O}(1/\eps^2)$ samples, and runs in time $(1/\eps)^{\poly (\log
(1/\eps))} \cdot \log n$. This is nearly optimal, since any algorithm {for this
problem} must use $\Omega(1/\eps^2)$ samples. We also give positive and
negative results for some extensions of this learning problem to weighted sums
of independent Bernoulli random variables."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1107.2700,https://arxiv.org/abs/1107.2700,"Abstract:  A $k$-modal probability distribution over the discrete domain $\{1,...,n\}$
is one whose histogram has at most $k$ ""peaks"" and ""valleys."" Such
distributions are natural generalizations of monotone ($k=0$) and unimodal
($k=1$) probability distributions, which have been intensively studied in
probability theory and statistics.
In this paper we consider the problem of \emph{learning} (i.e., performing
density estimation of) an unknown $k$-modal distribution with respect to the
$L_1$ distance. The learning algorithm is given access to independent samples
drawn from an unknown $k$-modal distribution $p$, and it must output a
hypothesis distribution $\widehat{p}$ such that with high probability the total
variation distance between $p$ and $\widehat{p}$ is at most $\epsilon.$ Our
main goal is to obtain \emph{computationally efficient} algorithms for this
problem that use (close to) an information-theoretically optimal number of
samples.
We give an efficient algorithm for this problem that runs in time
$\mathrm{poly}(k,\log(n),1/\epsilon)$. For $k \leq \tilde{O}(\log n)$, the
number of samples used by our algorithm is very close (within an
$\tilde{O}(\log(1/\epsilon))$ factor) to being information-theoretically
optimal. Prior to this work computationally efficient algorithms were known
only for the cases $k=0,1$ \cite{Birge:87b,Birge:97}.
A novel feature of our approach is that our learning algorithm crucially uses
a new algorithm for \emph{property testing of probability distributions} as a
key subroutine. The learning algorithm uses the property tester to efficiently
decompose the $k$-modal distribution into $k$ (near-)monotone distributions,
which are easier to learn."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1106.0519,https://arxiv.org/abs/1106.0519,"Abstract:  We provide a near-optimal, computationally efficient algorithm for the
unit-demand pricing problem, where a seller wants to price n items to optimize
revenue against a unit-demand buyer whose values for the items are
independently drawn from known distributions. For any chosen accuracy eps>0 and
item values bounded in [0,1], our algorithm achieves revenue that is optimal up
to an additive error of at most eps, in polynomial time. For values sampled
from Monotone Hazard Rate (MHR) distributions, we achieve a (1-eps)-fraction of
the optimal revenue in polynomial time, while for values sampled from regular
distributions the same revenue guarantees are achieved in quasi-polynomial
time.
Our algorithm for bounded distributions applies probabilistic techniques to
understand the statistical properties of revenue distributions, obtaining a
reduction in the search space of the algorithm via dynamic programming.
Adapting this approach to MHR and regular distributions requires the proof of
novel extreme value theorems for such distributions.
As a byproduct, our techniques establish structural properties of
approximately-optimal and near-optimal solutions. We show that, for values
independently distributed according to MHR distributions, pricing all items at
the same price achieves a constant fraction of the optimal revenue. Moreover,
for all eps >0, g(1/eps) distinct prices suffice to obtain a (1-eps)-fraction
of the optimal revenue, where g(1/eps) is quadratic in 1/eps and independent of
n. Similarly, for all eps>0 and n>0, at most g(1/(eps log n)) distinct prices
suffice if the values are independently distributed according to regular
distributions, where g() is a polynomial function. Finally, when the values are
i.i.d. from some MHR distribution, we show that, if n is a sufficiently large
function of 1/eps, a single price suffices to achieve a (1-eps)-fraction of the
optimal revenue."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1103.0598,https://arxiv.org/abs/1103.0598,"Abstract:  We consider the problem of learning an unknown product distribution $X$ over
$\{0,1\}^n$ using samples $f(X)$ where $f$ is a \emph{known} transformation
function. Each choice of a transformation function $f$ specifies a learning
problem in this framework.
Information-theoretic arguments show that for every transformation function
$f$ the corresponding learning problem can be solved to accuracy $\eps$, using
$\tilde{O}(n/\eps^2)$ examples, by a generic algorithm whose running time may
be exponential in $n.$ We show that this learning problem can be
computationally intractable even for constant $\eps$ and rather simple
transformation functions. Moreover, the above sample complexity bound is nearly
optimal for the general problem, as we give a simple explicit linear
transformation function $f(x)=w \cdot x$ with integer weights $w_i \leq n$ and
prove that the corresponding learning problem requires $\Omega(n)$ samples.
As our main positive result we give a highly efficient algorithm for learning
a sum of independent unknown Bernoulli random variables, corresponding to the
transformation function $f(x)= \sum_{i=1}^n x_i$. Our algorithm learns to
$\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\eps)$ number of
samples that is independent of $n.$ We also give an efficient algorithm that
uses $\log n \cdot \poly(1/\eps)$ samples but has running time that is only
$\poly(\log n, 1/\eps).$"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:1102.2280,https://arxiv.org/abs/1102.2280,"Abstract:  If a game has a Nash equilibrium with probability values that are either zero
or Omega(1) then this equilibrium can be found exhaustively in polynomial time.
Somewhat surprisingly, we show that there is a PTAS for the games whose
equilibria are guaranteed to have small-O(1/n)-values, and therefore
large-Omega(n)-supports. We also point out that there is a PTAS for games with
sparse payoff matrices, which are known to be PPAD-complete to solve exactly.
Both algorithms are of a special kind that we call oblivious: The algorithm
just samples a fixed distribution on pairs of mixed strategies, and the game is
only used to determine whether the sampled strategies comprise an eps-Nash
equilibrium; the answer is yes with inverse polynomial probability. These
results bring about the question: Is there an oblivious PTAS for Nash
equilibrium in general games? We answer this question in the negative; our
lower bound comes close to the quasi-polynomial upper bound of [Lipton,
Markakis, Mehta 2003].
Another recent PTAS for anonymous games is also oblivious in a weaker sense
appropriate for this class of games (it samples from a fixed distribution on
unordered collections of mixed strategies), but its runtime is exponential in
1/eps. We prove that any oblivious PTAS for anonymous games with two strategies
and three player types must have 1/eps^c in the exponent of the running time
for some c>1/3, rendering the algorithm in [Daskalakis 2008] essentially
optimal within oblivious algorithms. In contrast, we devise a poly(n)
(1/eps)^O(log^2(1/eps)) non-oblivious PTAS for anonymous games with 2
strategies and any bounded number of player types.
Our algorithm is based on the construction of a sparse (and efficiently
computable) eps-cover of the set of all possible sums of n independent
indicators, under the total variation distance. The size of the cover is
poly(n) (1/ eps^{O(log^2 (1/eps))}."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0912.2577,https://arxiv.org/abs/0912.2577,"Abstract:  Molecular phylogenetic techniques do not generally account for such common
evolutionary events as site insertions and deletions (known as indels). Instead
tree building algorithms and ancestral state inference procedures typically
rely on substitution-only models of sequence evolution. In practice these
methods are extended beyond this simplified setting with the use of heuristics
that produce global alignments of the input sequences--an important problem
which has no rigorous model-based solution. In this paper we consider a new
version of the multiple sequence alignment in the context of stochastic indel
models. More precisely, we introduce the following {\em trace reconstruction
problem on a tree} (TRPT): a binary sequence is broadcast through a tree
channel where we allow substitutions, deletions, and insertions; we seek to
reconstruct the original sequence from the sequences received at the leaves of
the tree. We give a recursive procedure for this problem with strong
reconstruction guarantees at low mutation rates, providing also an alignment of
the sequences at the leaves of the tree. The TRPT problem without indels has
been studied in previous work (Mossel 2004, Daskalakis et al. 2006) as a
bootstrapping step towards obtaining optimal phylogenetic reconstruction
methods. The present work sets up a framework for extending these works to
evolutionary models with indels."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0812.2277,https://arxiv.org/abs/0812.2277,"Abstract:  We present a novel polynomial time approximation scheme for two-strategy
anonymous games, in which the players' utility functions, although potentially
different, do not differentiate among the identities of the other players. Our
algorithm computes an $eps$-approximate Nash equilibrium of an $n$-player
2-strategy anonymous game in time $poly(n) (1/eps)^{O(1/eps^2)}$, which
significantly improves upon the running time $n^{O(1/eps^2)}$ required by the
algorithm of Daskalakis & Papadimitriou, 2007. The improved running time is
based on a new structural understanding of approximate Nash equilibria: We show
that, for any $eps$, there exists an $eps$-approximate Nash equilibrium in
which either only $O(1/eps^3)$ players randomize, or all players who randomize
use the same mixed strategy. To show this result we employ tools from the
literature on Stein's Method."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0808.2801,https://arxiv.org/abs/0808.2801,"Abstract:  We show that there is a polynomial-time approximation scheme for computing
Nash equilibria in anonymous games with any fixed number of strategies (a very
broad and important class of games), extending the two-strategy result of
Daskalakis and Papadimitriou 2007. The approximation guarantee follows from a
probabilistic result of more general interest: The distribution of the sum of n
independent unit vectors with values ranging over {e1, e2, ...,ek}, where ei is
the unit vector along dimension i of the k-dimensional Euclidean space, can be
approximated by the distribution of the sum of another set of independent unit
vectors whose probabilities of obtaining each value are multiples of 1/z for
some integer z, and so that the variational distance of the two distributions
is at most eps, where eps is bounded by an inverse polynomial in z and a
function of k, but with no dependence on n. Our probabilistic result specifies
the construction of a surprisingly sparse eps-cover -- under the total
variation distance -- of the set of distributions of sums of independent unit
vectors, which is of interest on its own right."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0802.1604,https://arxiv.org/abs/0802.1604,"Abstract:  We consider the problem of computing Nash Equilibria of action-graph games
(AGGs). AGGs, introduced by Bhat and Leyton-Brown, is a succinct representation
of games that encapsulates both ""local"" dependencies as in graphical games, and
partial indifference to other agents' identities as in anonymous games, which
occur in many natural settings. This is achieved by specifying a graph on the
set of actions, so that the payoff of an agent for selecting a strategy depends
only on the number of agents playing each of the neighboring strategies in the
action graph. We present a Polynomial Time Approximation Scheme for computing
mixed Nash equilibria of AGGs with constant treewidth and a constant number of
agent types (and an arbitrary number of strategies), together with hardness
results for the cases when either the treewidth or the number of agent types is
unconstrained. In particular, we show that even if the action graph is a tree,
but the number of agent-types is unconstrained, it is NP-complete to decide the
existence of a pure-strategy Nash equilibrium and PPAD-complete to compute a
mixed Nash equilibrium (even an approximate one); similarly for symmetric AGGs
(all agents belong to a single type), if we allow arbitrary treewidth. These
hardness results suggest that, in some sense, our PTAS is as strong of a
positive result as one can expect."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0801.4190,https://arxiv.org/abs/0801.4190,"Abstract:  We introduce a new phylogenetic reconstruction algorithm which, unlike most
previous rigorous inference techniques, does not rely on assumptions regarding
the branch lengths or the depth of the tree. The algorithm returns a forest
which is guaranteed to contain all edges that are: 1) sufficiently long and 2)
sufficiently close to the leaves. How much of the true tree is recovered
depends on the sequence length provided. The algorithm is distance-based and
runs in polynomial time."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0710.5582,https://arxiv.org/abs/0710.5582,"Abstract:  We present efficient approximation algorithms for finding Nash equilibria in
anonymous games, that is, games in which the players utilities, though
different, do not differentiate between other players. Our results pertain to
such games with many players but few strategies. We show that any such game has
an approximate pure Nash equilibrium, computable in polynomial time, with
approximation O(s^2 L), where s is the number of strategies and L is the
Lipschitz constant of the utilities. Finally, we show that there is a PTAS for
finding an epsilon"
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0710.4982,https://arxiv.org/abs/0710.4982,"Abstract:  In this paper, we provide a rigorous analysis of preferential attachment with
fitness, a random graph model introduced by Bianconi and Barabasi. Depending on
the shape of the fitness distribution, we observe three distinct phases: a
first-mover-advantage phase, a fit-get-richer phase and an innovation-pays-off
phase."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:0707.1532,https://arxiv.org/abs/0707.1532,"Abstract:  Classical problems of sorting and searching assume an underlying linear
ordering of the objects being compared. In this paper, we study a more general
setting, in which some pairs of objects are incomparable. This generalization
is relevant in applications related to rankings in sports, college admissions,
or conference submissions. It also has potential applications in biology, such
as comparing the evolutionary fitness of different strains of bacteria, or
understanding input-output relations among a set of metabolic reactions or the
causal influences among a set of interacting genes or proteins. Our results
improve and extend results from two decades ago of Faigle and Turán.
A measure of complexity of a partially ordered set (poset) is its width. Our
algorithms obtain information about a poset by queries that compare two
elements. We present an algorithm that sorts, i.e. completely identifies, a
width w poset of size n and has query complexity O(wn + nlog(n)), which is
within a constant factor of the information-theoretic lower bound. We also show
that a variant of Mergesort has query complexity O(wn(log(n/w))) and total
complexity O((w^2)nlog(n/w)). Faigle and Turán have shown that the sorting
problem has query complexity O(wn(log(n/w))) but did not address its total
complexity.
For the related problem of determining the minimal elements of a poset, we
give efficient deterministic and randomized algorithms with O(wn) query and
total complexity, along with matching lower bounds for the query complexity up
to a factor of 2. We generalize these results to the k-selection problem of
determining the elements of height at most k. We also derive upper bounds on
the total complexity of some other problems of a similar flavor."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:math/0703902,https://arxiv.org/abs/math/0703902,"Abstract:  We study how the structure of the interaction graph of a game affects the
existence of pure Nash equilibria. In particular, for a fixed interaction
graph, we are interested in whether there are pure Nash equilibria arising when
random utility tables are assigned to the players. We provide conditions for
the structure of the graph under which equilibria are likely to exist and
complementary conditions which make the existence of equilibria highly
unlikely. Our results have immediate implications for many deterministic graphs
and generalize known results for random games on the complete graph. In
particular, our results imply that the probability that bounded degree graphs
have pure Nash equilibria is exponentially small in the size of the graph and
yield a simple algorithm that finds small nonexistence certificates for a large
family of graphs. Then we show that in any strongly connected graph of n
vertices with expansion $(1+\Omega(1))\log_2(n)$ the distribution of the number
of equilibria approaches the Poisson distribution with parameter 1,
asymptotically as $n \to +\infty$."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:cs/0702014,https://arxiv.org/abs/cs/0702014,"Abstract:  We initiate the probabilistic analysis of linear programming (LP) decoding of
low-density parity-check (LDPC) codes. Specifically, we show that for a random
LDPC code ensemble, the linear programming decoder of Feldman et al. succeeds
in correcting a constant fraction of errors with high probability. The fraction
of correctable errors guaranteed by our analysis surpasses previous
non-asymptotic results for LDPC codes, and in particular exceeds the best
previous finite-length result on LP decoding by a factor greater than ten. This
improvement stems in part from our analysis of probabilistic bit-flipping
channels, as opposed to adversarial channels. At the core of our analysis is a
novel combinatorial characterization of LP decoding success, based on the
notion of a generalized matching. An interesting by-product of our analysis is
to establish the existence of ``probabilistic expansion'' in random bipartite
graphs, in which one requires only that almost every (as opposed to every) set
of a certain size expands, for sets much larger than in the classical
worst-case setting."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:cs/0510031,https://arxiv.org/abs/cs/0510031,"Abstract:  In this paper we present a novel generic mapping between Graphical Games and
Markov Random Fields so that pure Nash equilibria in the former can be found by
statistical inference on the latter. Thus, the problem of deciding whether a
graphical game has a pure Nash equilibrium, a well-known intractable problem,
can be attacked by well-established algorithms such as Belief Propagation,
Junction Trees, Markov Chain Monte Carlo and Simulated Annealing. Large classes
of graphical games become thus tractable, including all classes already known,
but also new classes such as the games with O(log n) treewidth."
Constantinos Daskalakis,Daskalakis_Constantinos,arXiv:math/0509575,https://arxiv.org/abs/math/0509575,"Abstract:  A major task of evolutionary biology is the reconstruction of phylogenetic
trees from molecular data. The evolutionary model is given by a Markov chain on
a tree. Given samples from the leaves of the Markov chain, the goal is to
reconstruct the leaf-labelled tree.
It is well known that in order to reconstruct a tree on $n$ leaves, sample
sequences of length $\Omega(\log n)$ are needed. It was conjectured by M. Steel
that for the CFN/Ising evolutionary model, if the mutation probability on all
edges of the tree is less than $p^{\ast} = (\sqrt{2}-1)/2^{3/2}$, then the tree
can be recovered from sequences of length $O(\log n)$. The value $p^{\ast}$ is
given by the transition point for the extremality of the free Gibbs measure for
the Ising model on the binary tree. Steel's conjecture was proven by the second
author in the special case where the tree is ""balanced."" The second author also
proved that if all edges have mutation probability larger than $p^{\ast}$ then
the length needed is $n^{\Omega(1)}$. Here we show that Steel's conjecture
holds true for general trees by giving a reconstruction algorithm that recovers
the tree from $O(\log n)$-length sequences when the mutation probabilities are
discretized and less than $p^\ast$. Our proof and results demonstrate that
extremality of the free Gibbs measure on the infinite binary tree, which has
been studied before in probability, statistical physics and computer science,
determines how distinguishable are Gibbs measures on finite binary trees."
Randall Davis,Davis_Randall,arXiv:1606.07163,https://arxiv.org/abs/1606.07163,"Abstract:  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular
neuropsychological screening tool for cognitive conditions. The Digital Clock
Drawing Test (dCDT) uses novel software to analyze data from a digitizing
ballpoint pen that reports its position with considerable spatial and temporal
precision, making possible the analysis of both the drawing process and final
product. We developed methodology to analyze pen stroke data from these
drawings, and computed a large collection of features which were then analyzed
with a variety of machine learning techniques. The resulting scoring systems
were designed to be more accurate than the systems currently used by
clinicians, but just as interpretable and easy to use. The systems also allow
us to quantify the tradeoff between accuracy and interpretability. We created
automated versions of the CDT scoring systems currently used by clinicians,
allowing us to benchmark our models, which indicated that our machine learning
models substantially outperformed the existing scoring systems."
Randall Davis,Davis_Randall,arXiv:1604.07429,https://arxiv.org/abs/1604.07429,"Abstract:  We describe a sketch interpretation system that detects and classifies clock
numerals created by subjects taking the Clock Drawing Test, a clinical tool
widely used to screen for cognitive impairments (e.g., dementia). We describe
how it balances appearance and context, and document its performance on some
2,000 drawings (about 24K clock numerals) produced by a wide spectrum of
patients. We calibrate the utility of different forms of context, describing
experiments with Conditional Random Fields trained and tested using a variety
of features. We identify context that contributes to interpreting otherwise
ambiguous or incomprehensible strokes. We describe ST-slices, a novel
representation that enables ""unpeeling"" the layers of ink that result when
people overwrite, which often produces ink impossible to analyze if only the
final drawing is examined. We characterize when ST-slices work, calibrate their
impact on performance, and consider their breadth of applicability."
Erik Demaine,Demaine_Erik,arXiv:1901.08564,https://arxiv.org/abs/1901.08564,"Abstract:  We study the problem of deciding whether a crease pattern can be folded by
simple folds (folding along one line at a time) under the infinite all-layers
model introduced by [Akitaya et al., 2017], in which each simple fold is
defined by an infinite line and must fold all layers of paper that intersect
this line. This model is motivated by folding in manufacturing such as
sheet-metal bending. We improve on [Arkin et al., 2004] by giving a
deterministic $O(n)$-time algorithm to decide simple foldability of 1D crease
patterns in the all-layers model. Then we extend this 1D result to 2D, showing
that simple foldability in this model can be decided in linear time for
unassigned axis-aligned orthogonal crease patterns on axis-aligned 2D
orthogonal paper. On the other hand, we show that simple foldability is
strongly NP-complete if a subset of the creases have a mountain-valley
assignment, even for an axis-aligned rectangle of paper."
Erik Demaine,Demaine_Erik,arXiv:1812.03592,https://arxiv.org/abs/1812.03592,"Abstract:  We build a general theory for characterizing the computational complexity of
motion planning of robot(s) through a graph of ""gadgets"", where each gadget has
its own state defining a set of allowed traversals which in turn modify the
gadget's state. We study two families of such gadgets, one which naturally
leads to motion planning problems with polynomially bounded solutions, and
another which leads to polynomially unbounded (potentially exponential)
solutions. We also study a range of competitive game-theoretic scenarios, from
one player controlling one robot to teams of players each controlling their own
robot and racing to achieve their team's goal. Under small restrictions on
these gadgets, we fully characterize the complexity of bounded 1-player motion
planning (NL vs. NP-complete), unbounded 1-player motion planning (NL vs.
PSPACE-complete), and bounded 2-player motion planning (P vs. PSPACE-complete),
and we partially characterize the complexity of unbounded 2-player motion
planning (P vs. EXPTIME-complete), bounded 2-team motion planning (P vs.
NEXPTIME-complete), and unbounded 2-team motion planning (P vs. undecidable).
These results can be seen as an alternative to Constraint Logic (which has
already proved useful as a basis for hardness reductions), providing a wide
variety of agent-based gadgets, any one of which suffices to prove a problem
hard."
Erik Demaine,Demaine_Erik,arXiv:1812.01167,https://arxiv.org/abs/1812.01167,"Abstract:  We characterize when two conic curved creases are compatible with each other,
when the rule lines must converge to conic foci and reflect at the crease.
Namely, two conics are compatible (can be connected by rule segments in a
foldable curved crease pattern) if and only if they have equal or reciprocal
eccentricity. Thus, circles (eccentricity 0) and parabolas (eccentricity 1) are
compatible with only themselves (when scaled from a focus), and ellipses
(eccentricity strictly between 0 and 1) and hyperbolas (eccentricity above 1)
are compatible with themselves and each other (but only in specific pairings).
The foundation of this result is a general condition relating any two curved
creases connected by rule segments. We also use our characterization to analyze
several curved crease designs."
Erik Demaine,Demaine_Erik,arXiv:1812.01160,https://arxiv.org/abs/1812.01160,"Abstract:  In this paper, we show that deciding rigid foldability of a given crease
pattern using all creases is weakly NP-hard by a reduction from Partition, and
that deciding rigid foldability with optional creases is strongly NP-hard by a
reduction from 1-in-3 SAT. Unlike flat foldability of origami or flexibility of
other kinematic linkages, whose complexity originates in the complexity of the
layer ordering and possible self-intersection of the material, rigid
foldability from a planar state is hard even though there is no potential
self-intersection. In fact, the complexity comes from the combinatorial
behavior of the different possible rigid folding configurations at each vertex.
The results underpin the fact that it is harder to fold from an unfolded sheet
of paper than to unfold a folded state back to a plane, frequently encountered
problem when realizing folding-based systems such as self-folding matter and
reconfigurable robots."
Erik Demaine,Demaine_Erik,arXiv:1808.07540,https://arxiv.org/abs/1808.07540,"Abstract:  Cookie Clicker is a popular online incremental game where the goal of the
game is to generate as many cookies as possible. In the game you start with an
initial cookie generation rate, and you can use cookies as currency to purchase
various items that increase your cookie generation rate. In this paper, we
analyze strategies for playing Cookie Clicker optimally. While simple to state,
the game gives rise to interesting analysis involving ideas from NP-hardness,
approximation algorithms, and dynamic programming."
Erik Demaine,Demaine_Erik,arXiv:1807.04682,https://arxiv.org/abs/1807.04682,"Abstract:  An oritatami system (OS) is a theoretical model of self-assembly via
co-transcriptional folding. It consists of a growing chain of beads which can
form bonds with each other as they are transcribed. During the transcription
process, the $\delta$ most recently produced beads dynamically fold so as to
maximize the number of bonds formed, self-assemblying into a shape
incrementally. The parameter $\delta$ is called the delay and is related to the
transcription rate in nature.
This article initiates the study of shape self-assembly using oritatami. A
shape is a connected set of points in the triangular lattice. We first show
that oritatami systems differ fundamentally from tile-assembly systems by
exhibiting a family of infinite shapes that can be tile-assembled but cannot be
folded by any OS. As it is NP-hard in general to determine whether there is an
OS that folds into (self-assembles) a given finite shape, we explore the
folding of upscaled versions of finite shapes. We show that any shape can be
folded from a constant size seed, at any scale n >= 3, by an OS with delay 1.
We also show that any shape can be folded at the smaller scale 2 by an OS with
unbounded delay. This leads us to investigate the influence of delay and to
prove that, for all {\delta} > 2, there are shapes that can be folded (at scale
1) with delay {\delta} but not with delay {\delta}'<{\delta}. These results
serve as a foundation for the study of shape-building in this new model of
self-assembly, and have the potential to provide better understanding of
cotranscriptional folding in biology, as well as improved abilities of
experimentalists to design artificial systems that self-assemble via this
complex dynamical process."
Erik Demaine,Demaine_Erik,arXiv:1806.05657,https://arxiv.org/abs/1806.05657,"Abstract:  We prove computational intractability of variants of checkers: (1) deciding
whether there is a move that forces the other player to win in one move is
NP-complete; (2) checkers where players must always be able to jump on their
turn is PSPACE-complete; and (3) cooperative versions of (1) and (2) are
NP-complete. We also give cooperative checkers puzzles whose solutions are the
letters of the alphabet."
Erik Demaine,Demaine_Erik,arXiv:1806.03539,https://arxiv.org/abs/1806.03539,"Abstract:  We initiate a general theory for analyzing the complexity of motion planning
of a single robot through a graph of ""gadgets"", each with their own state, set
of locations, and allowed traversals between locations that can depend on and
change the state. This type of setup is common to many robot motion planning
hardness proofs. We characterize the complexity for a natural simple case: each
gadget connects up to four locations in a perfect matching (but each direction
can be traversable or not in the current state), has one or two states, every
gadget traversal is immediately undoable, and that gadget locations are
connected by an always-traversable forest, possibly restricted to avoid
crossings in the plane. Specifically, we show that any single nontrivial
four-location two-state gadget type is enough for motion planning to become
PSPACE-complete, while any set of simpler gadgets (effectively two-location or
one-state) has a polynomial-time motion planning algorithm. As a sample
application, our results show that motion planning games with ""spinners"" are
PSPACE-complete, establishing a new hard aspect of Zelda: Oracle of Seasons."
Erik Demaine,Demaine_Erik,arXiv:1806.02771,https://arxiv.org/abs/1806.02771,"Abstract:  We develop a new framework for generalizing approximation algorithms from the
structural graph algorithm literature so that they apply to graphs somewhat
close to that class (a scenario we expect is common when working with
real-world networks) while still guaranteeing approximation ratios. The idea is
to $\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph
into an algorithmically tractable class, apply known approximation algorithms
for that class, and then $\textit{lift}$ the solution to apply to the original
graph. We give a general characterization of when an optimization problem is
amenable to this approach, and show that it includes many well-studied graph
problems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum
Maximal Matching, Chromatic Number, ($\ell$-)Dominating Set, Edge
($\ell$-)Dominating Set, and Connected Dominating Set.
To enable this framework, we develop new editing algorithms that find the
approximately-fewest edits required to bring a given graph into one of several
important graph classes (in some cases, also approximating the target parameter
of the family). For bounded degeneracy, we obtain a bicriteria
$(4,4)$-approximation which also extends to a smoother bicriteria trade-off.
For bounded treewidth, we obtain a bicriteria $(O(\log^{1.5} n), O(\sqrt{\log
w}))$-approximation, and for bounded pathwidth, we obtain a bicriteria
$(O(\log^{1.5} n), O(\sqrt{\log w} \cdot \log n))$-approximation. For treedepth
$2$ (also related to bounded expansion), we obtain a $4$-approximation. We also
prove complementary hardness-of-approximation results assuming $\mathrm{P} \neq
\mathrm{NP}$: in particular, these problems are all log-factor inapproximable,
except the last which is not approximable below some constant factor ($2$
assuming UGC)."
Erik Demaine,Demaine_Erik,arXiv:1805.04055,https://arxiv.org/abs/1805.04055,"Abstract:  We consider the computational complexity of reconfiguration problems, in
which one is given two combinatorial configurations satisfying some
constraints, and is asked to transform one into the other using elementary
transformations, while satisfying the constraints at all times. Such problems
appear naturally in many contexts, such as model checking, motion planning,
enumeration and sampling, and recreational mathematics. We provide hardness
results for problems in this family, in which the constraints and operations
are particularly simple. More precisely, we prove the PSPACE-completeness of
the following decision problems:
$\bullet$ Given two satisfying assignments to a planar monotone instance of
Not-All-Equal 3-SAT, can one assignment be transformed into the other by single
variable `flips' (assignment changes), preserving satisfiability at every step?
$\bullet$ Given two subsets of a set S of integers with the same sum, can one
subset be transformed into the other by adding or removing at most three
elements of S at a time, such that the intermediate subsets also have the same
sum?
$\bullet$ Given two points in $\{0,1\}^n$ contained in a polytope P specified
by a constant number of linear inequalities, is there a path in the n-hypercube
connecting the two points and contained in P?
These problems can be interpreted as reconfiguration analogues of standard
problems in NP. Interestingly, the instances of the NP problems that appear as
input to the reconfiguration problems in our reductions can be shown to lie in
P. In particular, the elements of S and the coefficients of the inequalities
defining P can be restricted to have logarithmic bit-length."
Erik Demaine,Demaine_Erik,arXiv:1804.10193,https://arxiv.org/abs/1804.10193,"Abstract:  We analyze the computational complexity of the many types of
pencil-and-paper-style puzzles featured in the 2016 puzzle video game The
Witness. In all puzzles, the goal is to draw a simple path in a rectangular
grid graph from a start vertex to a destination vertex. The different puzzle
types place different constraints on the path: preventing some edges from being
visited (broken edges); forcing some edges or vertices to be visited
(hexagons); forcing some cells to have certain numbers of incident path edges
(triangles); or forcing the regions formed by the path to be partially
monochromatic (squares), have exactly two special cells (stars), or be singly
covered by given shapes (polyominoes) and/or negatively counting shapes
(antipolyominoes). We show that any one of these clue types (except the first)
is enough to make path finding NP-complete (""witnesses exist but are hard to
find""), even for rectangular boards. Furthermore, we show that a final clue
type (antibody), which necessarily ""cancels"" the effect of another clue in the
same region, makes path finding $\Sigma_2$-complete (""witnesses do not exist""),
even with a single antibody (combined with many anti/polyominoes), and the
problem gets no harder with many antibodies. On the positive side, we give a
polynomial-time algorithm for monomino clues, by reducing to hexagon clues on
the boundary of the puzzle, even in the presence of broken edges, and solving
""subset Hamiltonian path"" for terminals on the boundary of an embedded planar
graph in polynomial time."
Erik Demaine,Demaine_Erik,arXiv:1804.06932,https://arxiv.org/abs/1804.06932,"Abstract:  Since the introduction of retroactive data structures at SODA 2004, a major
unsolved problem has been to bound the gap between the best partially
retroactive data structure (where changes can be made to the past, but only the
present can be queried) and the best fully retroactive data structure (where
the past can also be queried) for any problem. It was proved in 2004 that any
partially retroactive data structure with operation time $T(n,m)$ can be
transformed into a fully retroactive data structure with operation time
$O(\sqrt{m} \cdot T(n,m))$, where $n$ is the size of the data structure and $m$
is the number of operations in the timeline [Demaine 2004], but it has been
open for 14 years whether such a gap is necessary.
In this paper, we prove nearly matching upper and lower bounds on this gap
for all $n$ and $m$. We improve the upper bound for $n \ll \sqrt m$ by showing
a new transformation with multiplicative overhead $n \log m$. We then prove a
lower bound of $\min\{n \log m, \sqrt m\}^{1-o(1)}$ assuming any of the
following conjectures:
- Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input
circuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH
conjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses
already requires $2^{n-o(n)}$ time.)
- Conjecture II: Online $(\min,+)$ product between an integer $n\times n$
matrix and $n$ vectors requires $n^{3 - o(1)}$ time.
- Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers,
each of size $n$, deciding whether there exist $a \in A, b \in B, c \in C$ such
that $a + b + c = 0$ requires $n^{2 - o(1)}$ time.
Our lower bound construction illustrates an interesting power of fully
retroactive queries: they can be used to quickly solve batched pair evaluation.
We believe this technique can prove useful for other data structure lower
bounds, especially dynamic ones."
Erik Demaine,Demaine_Erik,arXiv:1803.03708,https://arxiv.org/abs/1803.03708,"Abstract:  We analyze the computational complexity of optimally playing the two-player
board game Push Fight, generalized to an arbitrary board and number of pieces.
We prove that the game is PSPACE-hard to decide who will win from a given
position, even for simple (almost rectangular) hole-free boards. We also
analyze the mate-in-1 problem: can the player win in a single turn? One turn in
Push Fight consists of up to two ""moves"" followed by a mandatory ""push"". With
these rules, or generalizing the number of allowed moves to any constant, we
show mate-in-1 can be solved in polynomial time. If, however, the number of
moves per turn is part of the input, the problem becomes NP-complete. On the
other hand, without any limit on the number of moves per turn, the problem
becomes polynomially solvable again."
Erik Demaine,Demaine_Erik,arXiv:1803.01176,https://arxiv.org/abs/1803.01176,"Abstract:  We prove that path puzzles with complete row and column information--or
equivalently, 2D orthogonal discrete tomography with Hamiltonicity
constraint--are strongly NP-complete, ASP-complete, and #P-complete. Along the
way, we newly establish ASP-completeness and #P-completeness for 3-Dimensional
Matching and Numerical 3-Dimensional Matching."
Erik Demaine,Demaine_Erik,arXiv:1803.01172,https://arxiv.org/abs/1803.01172,"Abstract:  We prove that two polygons $A$ and $B$ have a reversible hinged dissection (a
chain hinged dissection that reverses inside and outside boundaries when
folding between $A$ and $B$) if and only if $A$ and $B$ are two non-crossing
nets of a common polyhedron. Furthermore, monotone hinged dissections (where
all hinges rotate in the same direction when changing from $A$ to $B$
correspond exactly to non-crossing nets of a common convex polyhedron. By
envelope/parcel magic, it becomes easy to design many hinged dissections."
Erik Demaine,Demaine_Erik,arXiv:1801.01689,https://arxiv.org/abs/1801.01689,"Abstract:  We present a number of breakthroughs for coordinated motion planning, in
which the objective is to reconfigure a swarm of labeled convex objects by a
combination of parallel, continuous, collision-free translations into a given
target arrangement. Problems of this type can be traced back to the classic
work of Schwartz and Sharir (1983), who gave a method for deciding the
existence of a coordinated motion for a set of disks between obstacles; their
approach is polynomial in the complexity of the obstacles, but exponential in
the number of disks. Other previous work has largely focused on {\em
sequential} schedules, in which one robot moves at a time.
We provide constant-factor approximation algorithms for minimizing the
execution time of a coordinated, {\em parallel} motion plan for a swarm of
robots in the absence of obstacles, provided some amount of separability.
Our algorithm achieves {\em constant stretch factor}: If all robots are at
most $d$ units from their respective starting positions, the total duration of
the overall schedule is $O(d)$. Extensions include unlabeled robots and
different classes of robots. We also prove that finding a plan with minimal
execution time is NP-hard, even for a grid arrangement without any stationary
obstacles. On the other hand, we show that for densely packed disks that cannot
be well separated, a stretch factor $\Omega(N^{1/4})$ may be required. On the
positive side, we establish a stretch factor of $O(N^{1/2})$ even in this case."
Erik Demaine,Demaine_Erik,arXiv:1712.09317,https://arxiv.org/abs/1712.09317,"Abstract:  We study the problem of folding a polyomino $P$ into a polycube $Q$, allowing
faces of $Q$ to be covered multiple times. First, we define a variety of
folding models according to whether the folds (a) must be along grid lines of
$P$ or can divide squares in half (diagonally and/or orthogonally), (b) must be
mountain or can be both mountain and valley, (c) can remain flat (forming an
angle of $180^\circ$), and (d) must lie on just the polycube surface or can
have interior faces as well. Second, we give all the inclusion relations among
all models that fold on the grid lines of $P$. Third, we characterize all
polyominoes that can fold into a unit cube, in some models. Fourth, we give a
linear-time dynamic programming algorithm to fold a tree-shaped polyomino into
a constant-size polycube, in some models. Finally, we consider the triangular
version of the problem, characterizing which polyiamonds fold into a regular
tetrahedron."
Erik Demaine,Demaine_Erik,arXiv:1712.01197,https://arxiv.org/abs/1712.01197,"Abstract:  We investigate algorithmic control of a large swarm of mobile particles (such
as robots, sensors, or building material) that move in a 2D workspace using a
global input signal (such as gravity or a magnetic field). We show that a maze
of obstacles to the environment can be used to create complex systems. We
provide a wide range of results for a wide range of questions. These can be
subdivided into external algorithmic problems, in which particle configurations
serve as input for computations that are performed elsewhere, and internal
logic problems, in which the particle configurations themselves are used for
carrying out computations. For external algorithms, we give both negative and
positive results. If we are given a set of stationary obstacles, we prove that
it is NP-hard to decide whether a given initial configuration of unit-sized
particles can be transformed into a desired target configuration. Moreover, we
show that finding a control sequence of minimum length is PSPACE-complete. We
also work on the inverse problem, providing constructive algorithms to design
workspaces that efficiently implement arbitrary permutations between different
configurations. For internal logic, we investigate how arbitrary computations
can be implemented. We demonstrate how to encode dual-rail logic to build a
universal logic gate that concurrently evaluates and, nand, nor, and or
operations. Using many of these gates and appropriate interconnects, we can
evaluate any logical expression. However, we establish that simulating the full
range of complex interactions present in arbitrary digital circuits encounters
a fundamental difficulty: a fan-out gate cannot be generated. We resolve this
missing component with the help of 2x1 particles, which can create fan-out
gates that produce multiple copies of the inputs. Using these gates we provide
rules for replicating arbitrary digital circuits."
Erik Demaine,Demaine_Erik,arXiv:1711.07960,https://arxiv.org/abs/1711.07960,"Abstract:  This paper initiates the study of I/O algorithms (minimizing cache misses)
from the perspective of fine-grained complexity (conditional polynomial lower
bounds). Specifically, we aim to answer why sparse graph problems are so hard,
and why the Longest Common Subsequence problem gets a savings of a factor of
the size of cache times the length of a cache line, but no more. We take the
reductions and techniques from complexity and fine-grained complexity and apply
them to the I/O model to generate new (conditional) lower bounds as well as
faster algorithms. We also prove the existence of a time hierarchy for the I/O
model, which motivates the fine-grained reductions.
Using fine-grained reductions, we give an algorithm for distinguishing 2 vs.
3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for
sparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new
reductions from radius and diameter to Wiener index and median. We show
meaningful reductions between problems that have linear-time solutions in the
RAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus
help to finely capture the relationship between ""I/O linear time"" $\Theta(n/B)$
and RAM linear time $\Theta(n)$. We generate new I/O assumptions based on the
difficulty of improving sparse graph problem running times in the I/O model. We
create conjectures that the current best known algorithms for Single Source
Shortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model
assumptions, we show that many of the known reductions in the word-RAM model
can naturally extend to hold in the I/O model as well (e.g., a lower bound on
the I/O complexity of Longest Common Subsequence that matches the best known
running time). Finally, we prove an analog of the Time Hierarchy Theorem in the
I/O model."
Erik Demaine,Demaine_Erik,arXiv:1709.01241,https://arxiv.org/abs/1709.01241,"Abstract:  This paper proves that push-pull block puzzles in 3D are PSPACE-complete to
solve, and push-pull block puzzles in 2D with thin walls are NP-hard to solve,
settling an open question by Zubaran and Ritt. Push-pull block puzzles are a
type of recreational motion planning problem, similar to Sokoban, that involve
moving a `robot' on a square grid with $1 \times 1$ obstacles. The obstacles
cannot be traversed by the robot, but some can be pushed and pulled by the
robot into adjacent squares. Thin walls prevent movement between two adjacent
squares. This work follows in a long line of algorithms and complexity work on
similar problems. The 2D push-pull block puzzle shows up in the video games
Pukoban as well as The Legend of Zelda: A Link to the Past, giving another
proof of hardness for the latter. This variant of block-pushing puzzles is of
particular interest because of its connections to reversibility, since any
action (e.g., push or pull) can be inverted by another valid action (e.g., pull
or push)."
Erik Demaine,Demaine_Erik,arXiv:1708.06730,https://arxiv.org/abs/1708.06730,"Abstract:  We analyze a directed variation of the book embedding problem when the page
partition is prespecified and the nodes on the spine must be in topological
order (upward book embedding). Given a directed acyclic graph and a partition
of its edges into $k$ pages, can we linearly order the vertices such that the
drawing is upward (a topological sort) and each page avoids crossings? We prove
that the problem is NP-complete for $k\ge 3$, and for $k\ge 4$ even in the
special case when each page is a matching. By contrast, the problem can be
solved in linear time for $k=2$ pages when pages are restricted to matchings.
The problem comes from Jack Edmonds (1997), motivated as a generalization of
the map folding problem from computational origami."
Erik Demaine,Demaine_Erik,arXiv:1707.06343,https://arxiv.org/abs/1707.06343,"Abstract:  Pebble games are single-player games on DAGs involving placing and moving
pebbles on nodes of the graph according to a certain set of rules. The goal is
to pebble a set of target nodes using a minimum number of pebbles. In this
paper, we present a possibly simpler proof of the result in [CLNV15] and
strengthen the result to show that it is PSPACE-hard to determine the minimum
number of pebbles to an additive $n^{1/3-\epsilon}$ term for all $\epsilon >
0$, which improves upon the currently known additive constant hardness of
approximation [CLNV15] in the standard pebble game. We also introduce a family
of explicit, constant indegree graphs with $n$ nodes where there exists a graph
in the family such that using constant $k$ pebbles requires $\Omega(n^k)$ moves
to pebble in both the standard and black-white pebble games. This independently
answers an open question summarized in [Nor15] of whether a family of DAGs
exists that meets the upper bound of $O(n^k)$ moves using constant $k$ pebbles
with a different construction than that presented in [AdRNV17]."
Erik Demaine,Demaine_Erik,arXiv:1707.03146,https://arxiv.org/abs/1707.03146,"Abstract:  The 15 puzzle is a classic reconfiguration puzzle with fifteen uniquely
labeled unit squares within a $4 \times 4$ board in which the goal is to slide
the squares (without ever overlapping) into a target configuration. By
generalizing the puzzle to an $n \times n$ board with $n^2-1$ squares, we can
study the computational complexity of problems related to the puzzle; in
particular, we consider the problem of determining whether a given end
configuration can be reached from a given start configuration via at most a
given number of moves. This problem was shown NP-complete in Ratner and Warmuth
(1990). We provide an alternative simpler proof of this fact by reduction from
the rectilinear Steiner tree problem."
Erik Demaine,Demaine_Erik,arXiv:1706.10046,https://arxiv.org/abs/1706.10046,"Abstract:  In 2007, Arkin et al. initiated a systematic study of the complexity of the
Hamiltonian cycle problem on square, triangular, or hexagonal grid graphs,
restricted to polygonal, thin, superthin, degree-bounded, or solid grid graphs.
They solved many combinations of these problems, proving them either
polynomially solvable or NP-complete, but left three combinations open. In this
paper, we prove two of these unsolved combinations to be NP-complete:
Hamiltonicity of Square Polygonal Grid Graphs and Hamiltonicity of Hexagonal
Thin Grid Graphs. We also consider a new restriction, where the grid graph is
both thin and polygonal, and prove that Hamiltonicity then becomes polynomially
solvable for square, triangular, and hexagonal grid graphs."
Erik Demaine,Demaine_Erik,arXiv:1706.07900,https://arxiv.org/abs/1706.07900,"Abstract:  In this paper, we introduce a new problem called Tree-Residue Vertex-Breaking
(TRVB): given a multigraph $G$ some of whose vertices are marked ""breakable,""
is it possible to convert $G$ into a tree via a sequence of ""vertex-breaking""
operations (replacing a degree-$k$ breakable vertex by $k$ degree-$1$ vertices,
disconnecting the $k$ incident edges)?
We characterize the computational complexity of TRVB with any combination of
the following additional constraints: $G$ must be planar, $G$ must be a simple
graph, the degree of every breakable vertex must belong to an allowed list $B$,
and the degree of every unbreakable vertex must belong to an allowed list $U$.
The two results which we expect to be most generally applicable are that (1)
TRVB is polynomially solvable when breakable vertices are restricted to have
degree at most $3$; and (2) for any $k \ge 4$, TRVB is NP-complete when the
given multigraph is restricted to be planar and to consist entirely of
degree-$k$ breakable vertices. To demonstrate the use of TRVB, we give a simple
proof of the known result that Hamiltonicity in max-degree-$3$ square grid
graphs is NP-hard.
We also demonstrate a connection between TRVB and the Hypergraph Spanning
Tree problem. This connection allows us to show that the Hypergraph Spanning
Tree problem in $k$-uniform $2$-regular hypergraphs is NP-complete for any $k
\ge 4$, even when the incidence graph of the hypergraph is planar."
Erik Demaine,Demaine_Erik,arXiv:1706.06708,https://arxiv.org/abs/1706.06708,"Abstract:  In this paper, we prove that optimally solving an $n \times n \times n$
Rubik's Cube is NP-complete by reducing from the Hamiltonian Cycle problem in
square grid graphs. This improves the previous result that optimally solving an
$n \times n \times n$ Rubik's Cube with missing stickers is NP-complete. We
prove this result first for the simpler case of the Rubik's Square---an $n
\times n \times 1$ generalization of the Rubik's Cube---and then proceed with a
similar but more complicated proof for the Rubik's Cube case."
Erik Demaine,Demaine_Erik,arXiv:1703.06373,https://arxiv.org/abs/1703.06373,"Abstract:  This paper addresses the problem of finding minimum forcing sets in origami.
The origami material folds flat along straight lines called creases that can be
labeled as mountains or valleys. A forcing set is a subset of creases that
force all the other creases to fold according to their labels. The result is a
flat folding of the origami material. In this paper we develop a linear time
algorithm that finds minimum forcing sets in one dimensional origami."
Erik Demaine,Demaine_Erik,arXiv:1703.02671,https://arxiv.org/abs/1703.02671,"Abstract:  We study the complexity of symmetric assembly puzzles: given a collection of
simple polygons, can we translate, rotate, and possibly flip them so that their
interior-disjoint union is line symmetric? On the negative side, we show that
the problem is strongly NP-complete even if the pieces are all polyominos. On
the positive side, we show that the problem can be solved in polynomial time if
the number of pieces is a fixed constant."
Erik Demaine,Demaine_Erik,arXiv:1701.05999,https://arxiv.org/abs/1701.05999,"Abstract:  A conflict-free k-coloring of a graph assigns one of k different colors to
some of the vertices such that, for every vertex v, there is a color that is
assigned to exactly one vertex among v and v's neighbors. Such colorings have
applications in wireless networking, robotics, and geometry, and are
well-studied in graph theory. Here we study the natural problem of the
conflict-free chromatic number chi_CF(G) (the smallest k for which
conflict-free k-colorings exist). We provide results both for closed
neighborhoods N[v], for which a vertex v is a member of its neighborhood, and
for open neighborhoods N(v), for which vertex v is not a member of its
neighborhood.
For closed neighborhoods, we prove the conflict-free variant of the famous
Hadwiger Conjecture: If an arbitrary graph G does not contain K_{k+1} as a
minor, then chi_CF(G) <= k. For planar graphs, we obtain a tight worst-case
bound: three colors are sometimes necessary and always sufficient. We also give
a complete characterization of the computational complexity of conflict-free
coloring. Deciding whether chi_CF(G)<= 1 is NP-complete for planar graphs G,
but polynomial for outerplanar graphs. Furthermore, deciding whether
chi_CF(G)<= 2 is NP-complete for planar graphs G, but always true for
outerplanar graphs. For the bicriteria problem of minimizing the number of
colored vertices subject to a given bound k on the number of colors, we give a
full algorithmic characterization in terms of complexity and approximation for
outerplanar and planar graphs.
For open neighborhoods, we show that every planar bipartite graph has a
conflict-free coloring with at most four colors; on the other hand, we prove
that for k in {1,2,3}, it is NP-complete to decide whether a planar bipartite
graph has a conflict-free k-coloring. Moreover, we establish that any general}
planar graph has a conflict-free coloring with at most eight colors."
Erik Demaine,Demaine_Erik,arXiv:1701.00146,https://arxiv.org/abs/1701.00146,"Abstract:  We prove the computational intractability of rotating and placing $n$ square
tiles into a $1 \times n$ array such that adjacent tiles are compatible--either
equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes,
as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard
even to approximately maximize the number of placed tiles (allowing blanks),
while satisfying the compatibility constraint between nonblank tiles, within a
factor of 0.9999999851. (On the other hand, there is an easy $1 \over
2$-approximation.) This is the first (correct) proof of inapproximability for
edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of
distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian
path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a
vertex-disjoint union of paths. We use this gap hardness and gap-preserving
reductions to establish similar gap hardness for $1 \times n$ jigsaw and
edge-matching puzzles."
Erik Demaine,Demaine_Erik,arXiv:1611.10319,https://arxiv.org/abs/1611.10319,"Abstract:  We classify the computational complexity of the popular video games Portal
and Portal 2. We isolate individual mechanics of the game and prove
NP-hardness, PSPACE-completeness, or (pseudo)polynomiality depending on the
specific game mechanics allowed. One of our proofs generalizes to prove
NP-hardness of many other video games such as Half-Life 2, Halo, Doom, Elder
Scrolls, Fallout, Grand Theft Auto, Left 4 Dead, Mass Effect, Deus Ex, Metal
Gear Solid, and Resident Evil.
These results build on the established literature on the complexity of video
games."
Erik Demaine,Demaine_Erik,arXiv:1611.03187,https://arxiv.org/abs/1611.03187,"Abstract:  We present two universal hinge patterns that enable a strip of material to
fold into any connected surface made up of unit squares on the 3D cube
grid--for example, the surface of any polycube. The folding is efficient: for
target surfaces topologically equivalent to a sphere, the strip needs to have
only twice the target surface area, and the folding stacks at most two layers
of material anywhere. These geometric results offer a new way to build
programmable matter that is substantially more efficient than what is possible
with a square $N \times N$ sheet of material, which can fold into all polycubes
only of surface area $O(N)$ and may stack $\Theta(N^2)$ layers at one point. We
also show how our strip foldings can be executed by a rigid motion without
collisions, which is not possible in general with 2D sheet folding.
To achieve these results, we develop new approximation algorithms for milling
the surface of a grid polyhedron, which simultaneously give a 2-approximation
in tour length and an 8/3-approximation in the number of turns. Both length and
turns consume area when folding a strip, so we build on past approximation
algorithms for these two objectives from 2D milling."
Erik Demaine,Demaine_Erik,arXiv:1611.00106,https://arxiv.org/abs/1611.00106,"Abstract:  We show that every orthogonal polyhedron of genus at most 2 can be unfolded
without overlap while using only a linear number of orthogonal cuts (parallel
to the polyhedron edges). This is the first result on unfolding general
orthogonal polyhedra beyond genus-0. Our unfolding algorithm relies on the
existence of at most 2 special leaves in what we call the ""unfolding tree""
(which ties back to the genus), so unfolding polyhedra of genus 3 and beyond
requires new techniques."
Erik Demaine,Demaine_Erik,arXiv:1608.00477,https://arxiv.org/abs/1608.00477,"Abstract:  We show how to design a universal shape replicator in a self-assembly system
with both attractive and repulsive forces. More precisely, we show that there
is a universal set of constant-size objects that, when added to any unknown
hole-free polyomino shape, produces an unbounded number of copies of that shape
(plus constant-size garbage objects). The constant-size objects can be easily
constructed from a constant number of individual tile types using a constant
number of preprocessing self-assembly steps. Our construction uses the
well-studied 2-Handed Assembly Model (2HAM) of tile self-assembly, in the
simple model where glues interact only with identical glues, allowing glue
strengths that are either positive (attractive) or negative (repulsive), and
constant temperature (required glue strength for parts to hold together). We
also require that the given shape has specified glue types on its surface, and
that the feature size (smallest distance between nonincident edges) is bounded
below by a constant. Shape replication necessarily requires a self-assembly
model where parts can both attach and detach, and this construction is the
first to do so using the natural model of negative/repulsive glues (also
studied before for other problems such as fuel-efficient computation); previous
replication constructions require more powerful global operations such as an
""enzyme"" that destroys a subset of the tile types."
Erik Demaine,Demaine_Erik,arXiv:1607.04220,https://arxiv.org/abs/1607.04220,"Abstract:  This paper proves that arrangement of music is NP-hard when subject to
various constraints: avoiding musical dissonance, limiting how many notes can
be played simultaneously, and limiting transition speed between chords. These
results imply the computational complexity of related musical problems,
including musical choreography and rhythm games."
Erik Demaine,Demaine_Erik,arXiv:1607.01826,https://arxiv.org/abs/1607.01826,"Abstract:  We study the computational complexity of the Buttons \& Scissors game and
obtain sharp thresholds with respect to several parameters. Specifically we
show that the game is NP-complete for $C = 2$ colors but polytime solvable for
$C = 1$. Similarly the game is NP-complete if every color is used by at most $F
= 4$ buttons but polytime solvable for $F \leq 3$. We also consider
restrictions on the board size, cut directions, and cut sizes. Finally, we
introduce several natural two-player versions of the game and show that they
are PSPACE-complete."
Erik Demaine,Demaine_Erik,arXiv:1605.08475,https://arxiv.org/abs/1605.08475,"Abstract:  We introduce a new programming language for expressing reversibility,
Energy-Efficient Language (Eel), geared toward algorithm design and
implementation. Eel is the first language to take advantage of a partially
reversible computation model, where programs can be composed of both reversible
and irreversible operations. In this model, irreversible operations cost energy
for every bit of information created or destroyed. To handle programs of
varying degrees of reversibility, Eel supports a log stack to automatically
trade energy costs for space costs, and introduces many powerful control logic
operators including protected conditional, general conditional, protected
loops, and general loops. In this paper, we present the design and compiler for
the three language levels of Eel along with an interpreter to simulate and
annotate incurred energy costs of a program."
Erik Demaine,Demaine_Erik,arXiv:1605.08448,https://arxiv.org/abs/1605.08448,"Abstract:  We initiate the systematic study of the energy complexity of algorithms (in
addition to time and space complexity) based on Landauer's Principle in
physics, which gives a lower bound on the amount of energy a system must
dissipate if it destroys information. We propose energy-aware variations of
three standard models of computation: circuit RAM, word RAM, and
transdichotomous RAM. On top of these models, we build familiar high-level
primitives such as control logic, memory allocation, and garbage collection
with zero energy complexity and only constant-factor overheads in space and
time complexity, enabling simple expression of energy-efficient algorithms. We
analyze several classic algorithms in our models and develop low-energy
variations: comparison sort, insertion sort, counting sort, breadth-first
search, Bellman-Ford, Floyd-Warshall, matrix all-pairs shortest paths, AVL
trees, binary heaps, and dynamic arrays. We explore the time/space/energy
trade-off and develop several general techniques for analyzing algorithms and
reducing their energy complexity. These results lay a theoretical foundation
for a new field of semi-reversible computing and provide a new framework for
the investigation of algorithms."
Erik Demaine,Demaine_Erik,arXiv:1601.05747,https://arxiv.org/abs/1601.05747,"Abstract:  Modeling folding surfaces with nonzero thickness is of practical interest for
mechanical engineering. There are many existing approaches that account for
material thickness in folding applications. We propose a new systematic and
broadly applicable algorithm to transform certain flat-foldable crease patterns
into new crease patterns with similar folded structure but with a
facet-separated folded state. We provide conditions on input crease patterns
for the algorithm to produce a thickened crease pattern avoiding local self
intersection, and provide bounds for the maximum thickness that the algorithm
can produce for a given input. We demonstrate these results in parameterized
numerical simulations and physical models."
Erik Demaine,Demaine_Erik,arXiv:1601.05706,https://arxiv.org/abs/1601.05706,"Abstract:  Inspired by the Japanese game Pachinko, we study simple (perfectly
""inelastic"" collisions) dynamics of a unit ball falling amidst point obstacles
(pins) in the plane. A classic example is that a checkerboard grid of pins
produces the binomial distribution, but what probability distributions result
from different pin placements? In the 50-50 model, where the pins form a subset
of this grid, not all probability distributions are possible, but surprisingly
the uniform distribution is possible for $\{1,2,4,8,16\}$ possible drop
locations. Furthermore, every probability distribution can be approximated
arbitrarily closely, and every dyadic probability distribution can be divided
by a suitable power of $2$ and then constructed exactly (along with extra
""junk"" outputs). In a more general model, if a ball hits a pin off center, it
falls left or right accordingly. Then we prove a universality result: any
distribution of $n$ dyadic probabilities, each specified by $k$ bits, can be
constructed using $O(n k^2)$ pins, which is close to the information-theoretic
lower bound of $\Omega(n k)$."
Erik Demaine,Demaine_Erik,arXiv:1512.06706,https://arxiv.org/abs/1512.06706,"Abstract:  We prove that it is NP-hard to dissect one simple orthogonal polygon into
another using a given number of pieces, as is approximating the fewest pieces
to within a factor of $1+1/1080-\varepsilon$."
Erik Demaine,Demaine_Erik,arXiv:1507.01644,https://arxiv.org/abs/1507.01644,"Abstract:  We develop an intrinsic necessary and sufficient condition for single-vertex
origami crease patterns to be able to fold rigidly. We classify such patterns
in the case where the creases are pre-assigned to be mountains and valleys as
well as in the unassigned case. We also illustrate the utility of this result
by applying it to the new concept of minimal forcing sets for rigid origami
models, which are the smallest collection of creases that, when folded, will
force all the other creases to fold in a prescribed way."
Erik Demaine,Demaine_Erik,arXiv:1506.08409,https://arxiv.org/abs/1506.08409,"Abstract:  We prove that the classic 1994 Taito video game, known as Puzzle Bobble or
Bust-a-Move, is NP-complete. Our proof applies to the perfect-information
version where the bubble sequence is known in advance, and it uses just three
bubble colors."
Erik Demaine,Demaine_Erik,arXiv:1505.07862,https://arxiv.org/abs/1505.07862,"Abstract:  We consider staged self-assembly systems, in which square-shaped tiles can be
added to bins in several stages. Within these bins, the tiles may connect to
each other, depending on the glue types of their edges. Previous work by
Demaine et al. showed that a relatively small number of tile types suffices to
produce arbitrary shapes in this model. However, these constructions were only
based on a spanning tree of the geometric shape, so they did not produce full
connectivity of the underlying grid graph in the case of shapes with holes;
designing fully connected assemblies with a polylogarithmic number of stages
was left as a major open problem. We resolve this challenge by presenting new
systems for staged assembly that produce fully connected polyominoes in O(log^2
n) stages, for various scale factors and temperature {\tau} = 2 as well as
{\tau} = 1. Our constructions work even for shapes with holes and uses only a
constant number of glues and tiles. Moreover, the underlying approach is more
geometric in nature, implying that it promised to be more feasible for shapes
with compact geometric description."
Erik Demaine,Demaine_Erik,arXiv:1502.03191,https://arxiv.org/abs/1502.03191,"Abstract:  We describe a general family of curved-crease folding tessellations
consisting of a repeating ""lens"" motif formed by two convex curved arcs. The
third author invented the first such design in 1992, when he made both a sketch
of the crease pattern and a vinyl model (pictured below). Curve fitting
suggests that this initial design used circular arcs. We show that in fact the
curve can be chosen to be any smooth convex curve without inflection point. We
identify the ruling configuration through qualitative properties that a curved
folding satisfies, and prove that the folded form exists with no additional
creases, through the use of differential geometry."
Erik Demaine,Demaine_Erik,arXiv:1411.6371,https://arxiv.org/abs/1411.6371,"Abstract:  In this paper, we study how to fold a specified origami crease pattern in
order to minimize the impact of paper thickness. Specifically, origami designs
are often expressed by a mountain-valley pattern (plane graph of creases with
relative fold orientations), but in general this specification is consistent
with exponentially many possible folded states. We analyze the complexity of
finding the best consistent folded state according to two metrics: minimizing
the total number of layers in the folded state (so that a ""flat folding"" is
indeed close to flat), and minimizing the total amount of paper required to
execute the folding (where ""thicker"" creases consume more paper). We prove both
problems strongly NP-complete even for 1D folding. On the other hand, we prove
the first problem fixed-parameter tractable in 1D with respect to the number of
layers."
Erik Demaine,Demaine_Erik,arXiv:1410.6520,https://arxiv.org/abs/1410.6520,"Abstract:  Given a sheet of paper and a prescribed folding of its boundary, is there a
way to fold the paper's interior without stretching so that the boundary lines
up with the prescribed boundary folding? For polygonal boundaries
nonexpansively folded at finitely many points, we prove that a consistent
isometric mapping of the polygon interior always exists and is computable in
polynomial time."
Erik Demaine,Demaine_Erik,arXiv:1410.5845,https://arxiv.org/abs/1410.5845,"Abstract:  When can $t$ terminal pairs in an $m \times n$ grid be connected by $t$
vertex-disjoint paths that cover all vertices of the grid? We prove that this
problem is NP-complete. Our hardness result can be compared to two previous
NP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices''
constraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted
to have the fewest possible corners within their homotopy class. The latter
restriction is a common form of the famous Nikoli puzzle \emph{Numberlink}; our
problem is another common form of Numberlink, sometimes called \emph{Zig-Zag
Numberlink} and popularized by the smartphone app \emph{Flow Free}."
Erik Demaine,Demaine_Erik,arXiv:1408.6771,https://arxiv.org/abs/1408.6771,"Abstract:  When can a plane graph with prescribed edge lengths and prescribed angles
(from among $\{0,180^\circ, 360^\circ$\}) be folded flat to lie in an
infinitesimally thin line, without crossings? This problem generalizes the
classic theory of single-vertex flat origami with prescribed mountain-valley
assignment, which corresponds to the case of a cycle graph. We characterize
such flat-foldable plane graphs by two obviously necessary but also sufficient
conditions, proving a conjecture made in 2001: the angles at each vertex should
sum to $360^\circ$, and every face of the graph must itself be flat foldable.
This characterization leads to a linear-time algorithm for testing flat
foldability of plane graphs with prescribed edge lengths and angles, and a
polynomial-time algorithm for counting the number of distinct folded states."
Erik Demaine,Demaine_Erik,arXiv:1406.6576,https://arxiv.org/abs/1406.6576,"Abstract:  Suppose that we are given two independent sets $I_b$ and $I_r$ of a graph
such that $|I_b|=|I_r|$, and imagine that a token is placed on each vertex in
$I_b$. Then, the sliding token problem is to determine whether there exists a
sequence of independent sets which transforms $I_b$ into $I_r$ so that each
independent set in the sequence results from the previous one by sliding
exactly one token along an edge in the graph. This problem is known to be
PSPACE-complete even for planar graphs, and also for bounded treewidth graphs.
In this paper, we thus study the problem restricted to trees, and give the
following three results: (1) the decision problem is solvable in linear time;
(2) for a yes-instance, we can find in quadratic time an actual sequence of
independent sets between $I_b$ and $I_r$ whose length (i.e., the number of
token-slides) is quadratic; and (3) there exists an infinite family of
instances on paths for which any sequence requires quadratic length."
Erik Demaine,Demaine_Erik,arXiv:1406.2587,https://arxiv.org/abs/1406.2587,"Abstract:  This research establishes that many real-world networks exhibit bounded
expansion, a strong notion of structural sparsity, and demonstrates that it can
be leveraged to design efficient algorithms for network analysis. We analyze
several common network models regarding their structural sparsity. We show
that, with high probability, (1) graphs sampled with a prescribed s parse
degree sequence; (2) perturbed bounded-degree graphs; (3) stochastic block
models with small probabilities; result in graphs of bounded expansion.
In contrast, we show that the Kleinberg and the Barabasi-Albert model have
unbounded expansion. We support our findings with empirical measurements on a
corpus of real-world networks."
Erik Demaine,Demaine_Erik,arXiv:1405.3739,https://arxiv.org/abs/1405.3739,"Abstract:  In this paper, we study the problem of fast dynamic pointer following: given
a directed graph $G$ where each vertex has outdegree $1$, efficiently support
the operations of i) changing the outgoing edge of any vertex, and ii) find the
vertex $k$ vertices `after' a given vertex. We exhibit a solution to this
problem based on link-cut trees that requires $O(\lg n)$ time per operation,
and prove that this is optimal in the cell-probe complexity model."
Erik Demaine,Demaine_Erik,arXiv:1405.2378,https://arxiv.org/abs/1405.2378,"Abstract:  Can folding a piece of paper flat make it larger? We explore whether a shape
$S$ must be scaled to cover a flat-folded copy of itself. We consider both
single folds and arbitrary folds (continuous piecewise isometries $S\rightarrow
R^2$). The underlying problem is motivated by computational origami, and is
related to other covering and fixturing problems, such as Lebesgue's universal
cover problem and force closure grasps. In addition to considering special
shapes (squares, equilateral triangles, polygons and disks), we give upper and
lower bounds on scale factors for single folds of convex objects and arbitrary
folds of simply connected objects."
Erik Demaine,Demaine_Erik,arXiv:1404.1775,https://arxiv.org/abs/1404.1775,"Abstract:  Over the past decade, we have designed six typefaces based on mathematical
theorems and open problems, specifically computational geometry. These
typefaces expose the general public in a unique way to intriguing results and
hard problems in hinged dissections, geometric tours, origami design,
computer-aided glass design, physical simulation, and protein folding. In
particular, most of these typefaces include puzzle fonts, where reading the
intended message requires solving a series of puzzles which illustrate the
challenge of the underlying algorithmic problem."
Erik Demaine,Demaine_Erik,arXiv:1403.7980,https://arxiv.org/abs/1403.7980,"Abstract:  A stacking operation adds a $d$-simplex on top of a facet of a simplicial
$d$-polytope while maintaining the convexity of the polytope. A stacked
$d$-polytope is a polytope that is obtained from a $d$-simplex and a series of
stacking operations. We show that for a fixed $d$ every stacked $d$-polytope
with $n$ vertices can be realized with nonnegative integer coordinates. The
coordinates are bounded by $O(n^{2\log(2d)})$, except for one axis, where the
coordinates are bounded by $O(n^{3\log(2d)})$. The described realization can be
computed with an easy algorithm.
The realization of the polytopes is obtained with a lifting technique which
produces an embedding on a large grid. We establish a rounding scheme that
places the vertices on a sparser grid, while maintaining the convexity of the
embedding."
Erik Demaine,Demaine_Erik,arXiv:1402.3749,https://arxiv.org/abs/1402.3749,"Abstract:  Micro- and nanorobots are often controlled by global input signals, such as
an electromagnetic or gravitational field. These fields move each robot
maximally until it hits a stationary obstacle or another stationary robot. This
paper investigates 2D motion-planning complexity for large swarms of simple
mobile robots (such as bacteria, sensors, or smart building material).
In previous work we proved it is NP-hard to decide whether a given initial
configuration can be transformed into a desired target configuration; in this
paper we prove a stronger result: the problem of finding an optimal control
sequence is PSPACE-complete. On the positive side, we show we can build useful
systems by designing obstacles. We present a reconfigurable hardware platform
and demonstrate how to form arbitrary permutations and build a compact absolute
encoder. We then take the same platform and use dual-rail logic to build a
universal logic gate that concurrently evaluates AND, NAND, NOR and OR
operations. Using many of these gates and appropriate interconnects we can
evaluate any logical expression."
Erik Demaine,Demaine_Erik,arXiv:1401.7970,https://arxiv.org/abs/1401.7970,"Abstract:  We study the power of fractional allocations of resources to maximize
influence in a network. This work extends in a natural way the well-studied
model by Kempe, Kleinberg, and Tardos (2003), where a designer selects a
(small) seed set of nodes in a social network to influence directly, this
influence cascades when other nodes reach certain thresholds of neighbor
influence, and the goal is to maximize the final number of influenced nodes.
Despite extensive study from both practical and theoretical viewpoints, this
model limits the designer to a binary choice for each node, with no way to
apply intermediate levels of influence. This model captures some settings
precisely, e.g. exposure to an idea or pathogen, but it fails to capture very
relevant concerns in others, for example, a manufacturer promoting a new
product by distributing five ""20% off"" coupons instead of giving away one free
product.
While fractional versions of problems tend to be easier to solve than
integral versions, for influence maximization, we show that the two versions
have essentially the same computational complexity. On the other hand, the two
versions can have vastly different solutions: the added flexibility of
fractional allocation can lead to significantly improved influence. Our main
theoretical contribution is to show how to adapt the major positive results
from the integral case to the fractional case. Specifically, Mossel and Roch
(2006) used the submodularity of influence to obtain their integral results; we
introduce a new notion of continuous submodularity, and use this to obtain
matching fractional results. We conclude that we can achieve the same greedy
$(1-1/e-\epsilon)$-approximation for the fractional case as the integral case.
In practice, we find that the fractional model performs substantially better
than the integral model, according to simulations on real-world social network
data."
Erik Demaine,Demaine_Erik,arXiv:1310.4561,https://arxiv.org/abs/1310.4561,"Abstract:  We define a new class of orthogonal polyhedra, called orthogrids, that can be
unfolded without overlap with constant refinement of the gridded surface."
Erik Demaine,Demaine_Erik,arXiv:1306.6710,https://arxiv.org/abs/1306.6710,"Abstract:  The well-studied Two-Handed Tile Assembly Model (2HAM) is a model of tile
assembly in which pairs of large assemblies can bind, or self-assemble,
together. In order to bind, two assemblies must have matching glues that can
simultaneously touch each other, and stick together with strength that is at
least the temperature $\tau$, where $\tau$ is some fixed positive integer. We
ask whether the 2HAM is intrinsically universal, in other words we ask: is
there a single universal 2HAM tile set $U$ which can be used to simulate any
instance of the model? Our main result is a negative answer to this question.
We show that for all $\tau' < \tau$, each temperature-$\tau'$ 2HAM tile system
does not simulate at least one temperature-$\tau$ 2HAM tile system. This
impossibility result proves that the 2HAM is not intrinsically universal, in
stark contrast to the simpler (single-tile addition only) abstract Tile
Assembly Model which is intrinsically universal (""The tile assembly model is
intrinsically universal"", FOCS 2012). However, on the positive side, we prove
that, for every fixed temperature $\tau \geq 2$, temperature-$\tau$ 2HAM tile
systems are indeed intrinsically universal: in other words, for each $\tau$
there is a single universal 2HAM tile set $U$ that, when appropriately
initialized, is capable of simulating the behavior of any temperature-$\tau$
2HAM tile system. As a corollary of these results we find an infinite set of
infinite hierarchies of 2HAM systems with strictly increasing simulation power
within each hierarchy. Finally, we show that for each $\tau$, there is a
temperature-$\tau$ 2HAM system that simultaneously simulates all
temperature-$\tau$ 2HAM systems."
Erik Demaine,Demaine_Erik,arXiv:1304.7604,https://arxiv.org/abs/1304.7604,"Abstract:  We present a general transformation for combining a constant number of binary
search tree data structures (BSTs) into a single BST whose running time is
within a constant factor of the minimum of any ""well-behaved"" bound on the
running time of the given BSTs, for any online access sequence.
(A BST has a well behaved bound with $f(n)$ overhead if it spends at most
\bigoh{f(n)} time per access and its bound satisfies a weak sense of closure
under subsequences.) In particular, we obtain a BST data structure that is
\bigoh{\log\log n} competitive, satisfies the working set bound (and thus
satisfies the static finger bound and the static optimality bound), satisfies
the dynamic finger bound, satisfies the unified bound with an additive
\bigoh{\log\log n} factor, and performs each access in worst-case \bigoh{\log
n} time."
Erik Demaine,Demaine_Erik,arXiv:1212.4771,https://arxiv.org/abs/1212.4771,"Abstract:  We give subquadratic algorithms that, given two necklaces each with n beads
at arbitrary positions, compute the optimal rotation of the necklaces to best
align the beads. Here alignment is measured according to the p norm of the
vector of distances between pairs of beads from opposite necklaces in the best
perfect matching. We show surprisingly different results for p = 1, p even, and
p = \infty. For p even, we reduce the problem to standard convolution, while
for p = \infty and p = 1, we reduce the problem to (min, +) convolution and
(median, +) convolution. Then we solve the latter two convolution problems in
subquadratic time, which are interesting results in their own right. These
results shed some light on the classic sorting X + Y problem, because the
convolutions can be viewed as computing order statistics on the antidiagonals
of the X + Y matrix. All of our algorithms run in o(n^2) time, whereas the
obvious algorithms for these problems run in \Theta(n^2) time."
Erik Demaine,Demaine_Erik,arXiv:1212.4756,https://arxiv.org/abs/1212.4756,"Abstract:  In this paper we explore the power of tile self-assembly models that extend
the well-studied abstract Tile Assembly Model (aTAM) by permitting tiles of
shapes beyond unit squares. Our main result shows the surprising fact that any
aTAM system, consisting of many different tile types, can be simulated by a
single tile type of a general shape. As a consequence, we obtain a single
universal tile type of a single (constant-size) shape that serves as a
""universal tile machine"": the single universal tile type can simulate any
desired aTAM system when given a single seed assembly that encodes the desired
aTAM system. We also show how to adapt this result to convert any of a variety
of plane tiling systems (such as Wang tiles) into a ""nearly"" plane tiling
system with a single tile (but with small gaps between the tiles). All of these
results rely on the ability to both rotate and translate tiles; by contrast, we
show that a single nonrotatable tile, of arbitrary shape, can produce
assemblies which either grow infinitely or cannot grow at all, implying
drastically limited computational power.
On the positive side, we show how to simulate arbitrary cellular automata for
a limited number of steps using a single nonrotatable tile and a linear-size
seed assembly."
Erik Demaine,Demaine_Erik,arXiv:1205.6960,https://arxiv.org/abs/1205.6960,"Abstract:  We study an extensive class of movement minimization problems which arise
from many practical scenarios but so far have little theoretical study. In
general, these problems involve planning the coordinated motion of a collection
of agents (representing robots, people, map labels, network messages, etc.) to
achieve a global property in the network while minimizing the maximum or
average movement (expended energy). The only previous theoretical results about
this class of problems are about approximation, and mainly negative: many
movement problems of interest have polynomial inapproximability. Given that the
number of mobile agents is typically much smaller than the complexity of the
environment, we turn to fixed-parameter tractability. We characterize the
boundary between tractable and intractable movement problems in a very general
set up: it turns out the complexity of the problem fundamentally depends on the
treewidth of the minimal configurations. Thus the complexity of a particular
problem can be determined by answering a purely combinatorial question. Using
our general tools, we determine the complexity of several concrete problems and
fortunately show that many movement problems of interest can be solved
efficiently."
Erik Demaine,Demaine_Erik,arXiv:1203.3602,https://arxiv.org/abs/1203.3602,"Abstract:  We show how to hang a picture by wrapping rope around n nails, making a
polynomial number of twists, such that the picture falls whenever any k out of
the n nails get removed, and the picture remains hanging when fewer than k
nails get removed. This construction makes for some fun mathematical magic
performances. More generally, we characterize the possible Boolean functions
characterizing when the picture falls in terms of which nails get removed as
all monotone Boolean functions. This construction requires an exponential
number of twists in the worst case, but exponential complexity is almost always
necessary for general functions."
Erik Demaine,Demaine_Erik,arXiv:1203.1895,https://arxiv.org/abs/1203.1895,"Abstract:  We prove NP-hardness results for five of Nintendo's largest video game
franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokemon. Our
results apply to generalized versions of Super Mario Bros. 1-3, The Lost
Levels, and Super Mario World; Donkey Kong Country 1-3; all Legend of Zelda
games; all Metroid games; and all Pokemon role-playing games. In addition, we
prove PSPACE-completeness of the Donkey Kong Country games and several Legend
of Zelda games."
Erik Demaine,Demaine_Erik,arXiv:1201.1650,https://arxiv.org/abs/1201.1650,"Abstract:  We study the difference between the standard seeded model of tile
self-assembly, and the ""seedless"" two-handed model of tile self-assembly. Most
of our results suggest that the two-handed model is more powerful. In
particular, we show how to simulate any seeded system with a two-handed system
that is essentially just a constant factor larger. We exhibit finite shapes
with a busy-beaver separation in the number of distinct tiles required by
seeded versus two-handed, and exhibit an infinite shape that can be constructed
two-handed but not seeded. Finally, we show that verifying whether a given
system uniquely assembles a desired supertile is co-NP-complete in the
two-handed model, while it was known to be polynomially solvable in the seeded
model."
Erik Demaine,Demaine_Erik,arXiv:1112.4791,https://arxiv.org/abs/1112.4791,"Abstract:  We show that every orthogonal polyhedron homeomorphic to a sphere can be
unfolded without overlap while using only polynomially many (orthogonal) cuts.
By contrast, the best previous such result used exponentially many cuts. More
precisely, given an orthogonal polyhedron with n vertices, the algorithm cuts
the polyhedron only where it is met by the grid of coordinate planes passing
through the vertices, together with Theta(n^2) additional coordinate planes
between every two such grid planes."
Erik Demaine,Demaine_Erik,arXiv:1106.5736,https://arxiv.org/abs/1106.5736,"Abstract:  The Rubik's Cube is perhaps the world's most famous and iconic puzzle,
well-known to have a rich underlying mathematical structure (group theory). In
this paper, we show that the Rubik's Cube also has a rich underlying
algorithmic structure. Specifically, we show that the n x n x n Rubik's Cube,
as well as the n x n x 1 variant, has a ""God's Number"" (diameter of the
configuration space) of Theta(n^2/log n). The upper bound comes from
effectively parallelizing standard Theta(n^2) solution algorithms, while the
lower bound follows from a counting argument. The upper bound gives an
asymptotically optimal algorithm for solving a general Rubik's Cube in the
worst case. Given a specific starting state, we show how to find the shortest
solution in an n x O(1) x O(1) Rubik's Cube. Finally, we show that finding this
optimal solution becomes NP-hard in an n x n x 1 Rubik's Cube when the
positions and colors of some of the cubies are ignored (not used in determining
whether the cube is solved)."
Erik Demaine,Demaine_Erik,arXiv:1103.4513,https://arxiv.org/abs/1103.4513,"Abstract:  The separating words problem asks for the size of the smallest DFA needed to
distinguish between two words of length <= n (by accepting one and rejecting
the other). In this paper we survey what is known and unknown about the
problem, consider some variations, and prove several new results."
Erik Demaine,Demaine_Erik,arXiv:1009.5628,https://arxiv.org/abs/1009.5628,"Abstract:  An n-town, for a natural number n, is a group of n buildings, each occupying
a distinct position on a 2-dimensional integer grid. If we measure the distance
between two buildings along the axis-parallel street grid, then an n-town has
optimal shape if the sum of all pairwise Manhattan distances is minimized. This
problem has been studied for cities, i.e., the limiting case of very large n.
For cities, it is known that the optimal shape can be described by a
differential equation, for which no closed-form is known. We show that optimal
n-towns can be computed in O(n^7.5) time. This is also practically useful, as
it allows us to compute optimal solutions up to n=80."
Erik Demaine,Demaine_Erik,arXiv:1008.1224,https://arxiv.org/abs/1008.1224,"Abstract:  We show that deciding whether a given set of circles can be packed into a
rectangle, an equilateral triangle, or a unit square are NP-hard problems,
settling the complexity of these natural packing problems. On the positive
side, we show that any set of circles of total area 1 can be packed into a
square of size 4/\sqrt{pi}=2.2567... These results are motivated by problems
arising in the context of origami design."
Erik Demaine,Demaine_Erik,arXiv:1007.3607,https://arxiv.org/abs/1007.3607,"Abstract:  We introduce a notion of $k$-convexity and explore polygons in the plane that
have this property. Polygons which are \mbox{$k$-convex} can be triangulated
with fast yet simple algorithms. However, recognizing them in general is a
3SUM-hard problem. We give a characterization of \mbox{$2$-convex} polygons, a
particularly interesting class, and show how to recognize them in \mbox{$O(n
\log n)$} time. A description of their shape is given as well, which leads to
Erdős-Szekeres type results regarding subconfigurations of their vertex
sets. Finally, we introduce the concept of generalized geometric permutations,
and show that their number can be exponential in the number of
\mbox{$2$-convex} objects considered."
Erik Demaine,Demaine_Erik,arXiv:1004.4383,https://arxiv.org/abs/1004.4383,"Abstract:  We consider a model of algorithmic self-assembly of geometric shapes out of
square Wang tiles studied in SODA 2010, in which there are two types of tiles
(e.g., constructed out of DNA and RNA material) and one operation that destroys
all tiles of a particular type (e.g., an RNAse enzyme destroys all RNA tiles).
We show that a single use of this destruction operation enables much more
efficient construction of arbitrary shapes. In particular, an arbitrary shape
can be constructed using an asymptotically optimal number of distinct tile
types (related to the shape's Kolmogorov complexity), after scaling the shape
by only a logarithmic factor. By contrast, without the destruction operation,
the best such result has a scale factor at least linear in the size of the
shape, and is connected only by a spanning tree of the scaled tiles. We also
characterize a large collection of shapes that can be constructed efficiently
without any scaling."
Erik Demaine,Demaine_Erik,arXiv:1003.2851,https://arxiv.org/abs/1003.2851,"Abstract:  This paper investigates the popular card game UNO from the viewpoint of
algorithmic combinatorial game theory. We define simple and concise
mathematical models for the game, including both cooperative and uncooperative
versions, and analyze their computational complexity. In particular, we prove
that even a single-player version of UNO is NP-complete, although some
restricted cases are in P. Surprisingly, we show that the uncooperative
two-player version is also in P."
Erik Demaine,Demaine_Erik,arXiv:0910.1643,https://arxiv.org/abs/0910.1643,"Abstract:  For a set of n points in the plane, we consider the axis--aligned (p,k)-Box
Covering problem: Find p axis-aligned, pairwise-disjoint boxes that together
contain n-k points. In this paper, we consider the boxes to be either squares
or rectangles, and we want to minimize the area of the largest box. For general
p we show that the problem is NP-hard for both squares and rectangles. For a
small, fixed number p, we give algorithms that find the solution in the
following running times:
For squares we have O(n+k log k) time for p=1, and O(n log n+k^p log^p k time
for p = 2,3. For rectangles we get O(n + k^3) for p = 1 and O(n log n+k^{2+p}
log^{p-1} k) time for p = 2,3.
In all cases, our algorithms use O(n) space."
Erik Demaine,Demaine_Erik,arXiv:0909.5388,https://arxiv.org/abs/0909.5388,"Abstract:  We present a universal crease pattern--known in geometry as the tetrakis
tiling and in origami as box pleating--that can fold into any object made up of
unit cubes joined face-to-face (polycubes). More precisely, there is one
universal finite crease pattern for each number n of unit cubes that need to be
folded. This result contrasts previous universality results for origami, which
require a different crease pattern for each target object, and confirms
intuition in the origami community that box pleating is a powerful design
technique."
Erik Demaine,Demaine_Erik,arXiv:0909.3221,https://arxiv.org/abs/0909.3221,"Abstract:  The Stackelberg Minimum Spanning Tree Game is a two-level combinatorial
pricing problem played on a graph representing a network. Its edges are colored
either red or blue, and the red edges have a given fixed cost, representing the
competitor's prices. The first player chooses an assignment of prices to the
blue edges, and the second player then buys the cheapest spanning tree, using
any combination of red and blue edges. The goal of the first player is to
maximize the total price of purchased blue edges.
We study this problem in the cases of planar and bounded-treewidth graphs. We
show that the problem is NP-hard on planar graphs but can be solved in
polynomial time on graphs of bounded treewidth."
Erik Demaine,Demaine_Erik,arXiv:0908.2493,https://arxiv.org/abs/0908.2493,"Abstract:  The minimum feature size of a crossing-free straight line drawing is the
minimum distance between a vertex and a non-incident edge. This quantity
measures the resolution needed to display a figure or the tool size needed to
mill the figure. The spread is the ratio of the diameter to the minimum feature
size. While many algorithms (particularly in meshing) depend on the spread of
the input, none explicitly consider finding a mesh whose spread is similar to
the input. When a polygon is partitioned into smaller regions, such as
triangles or quadrangles, the degradation is the ratio of original to final
spread (the final spread is always greater).
Here we present an algorithm to quadrangulate a simple n-gon, while achieving
constant degradation. Note that although all faces have a quadrangular shape,
the number of edges bounding each face may be larger. This method uses Theta(n)
Steiner points and produces Theta(n) quadrangles. In fact to obtain constant
degradation, Omega(n) Steiner points are required by any algorithm.
We also show that, for some polygons, a constant factor cannot be achieved by
any triangulation, even with an unbounded number of Steiner points. The
specific lower bounds depend on whether Steiner vertices are used or not."
Erik Demaine,Demaine_Erik,arXiv:0908.2440,https://arxiv.org/abs/0908.2440,"Abstract:  We consider the theoretical model of Crystalline robots, which have been
introduced and prototyped by the robotics community. These robots consist of
independently manipulable unit-square atoms that can extend/contract arms on
each side and attach/detach from neighbors. These operations suffice to
reconfigure between any two given (connected) shapes. The worst-case number of
sequential moves required to transform one connected configuration to another
is known to be Theta(n). However, in principle, atoms can all move
simultaneously. We develop a parallel algorithm for reconfiguration that runs
in only O(log n) parallel steps, although the total number of operations
increases slightly to Theta(nlogn). The result is the first (theoretically)
almost-instantaneous universally reconfigurable robot built from simple units."
Erik Demaine,Demaine_Erik,arXiv:0906.4747,https://arxiv.org/abs/0906.4747,"Abstract:  We prove that the pleated hyperbolic paraboloid, a familiar origami model
known since 1927, in fact cannot be folded with the standard crease pattern in
the standard mathematical model of zero-thickness paper. In contrast, we show
that the model can be folded with additional creases, suggesting that real
paper ""folds"" into this model via small such creases. We conjecture that the
circular version of this model, consisting simply of concentric circular
creases, also folds without extra creases.
At the heart of our results is a new structural theorem characterizing
uncreased intrinsically flat surfaces--the portions of paper between the
creases. Differential geometry has much to say about the local behavior of such
surfaces when they are sufficiently smooth, e.g., that they are torsal ruled.
But this classic result is simply false in the context of the whole surface.
Our structural characterization tells the whole story, and even applies to
surfaces with discontinuities in the second derivative. We use our theorem to
prove fundamental properties about how paper folds, for example, that straight
creases on the piece of paper must remain piecewise-straight (polygonal) by
folding."
Erik Demaine,Demaine_Erik,arXiv:0906.2461,https://arxiv.org/abs/0906.2461,"Abstract:  We construct the first two continuous bloomings of all convex polyhedra.
First, the source unfolding can be continuously bloomed. Second, any unfolding
of a convex polyhedron can be refined (further cut, by a linear number of cuts)
to have a continuous blooming."
Erik Demaine,Demaine_Erik,arXiv:0902.1400,https://arxiv.org/abs/0902.1400,"Abstract:  In general, the games are played on a host graph, where each node is a
selfish independent agent (player) and each edge has a fixed link creation cost
\alpha. Together the agents create a network (a subgraph of the host graph)
while selfishly minimizing the link creation costs plus the sum of the
distances to all other players (usage cost). In this paper, we pursue two
important facets of the network creation game. First, we study extensively a
natural version of the game, called the cooperative model, where nodes can
collaborate and share the cost of creating any edge in the host graph. We prove
the first nontrivial bounds in this model, establishing that the price of
anarchy is polylogarithmic in n for all values of &#945; in complete host
graphs. This bound is the first result of this type for any version of the
network creation game; most previous general upper bounds are polynomial in n.
Interestingly, we also show that equilibrium graphs have polylogarithmic
diameter for the most natural range of \alpha (at most n polylg n). Second, we
study the impact of the natural assumption that the host graph is a general
graph, not necessarily complete. This model is a simple example of nonuniform
creation costs among the edges (effectively allowing weights of \alpha and
\infty). We prove the first assemblage of upper and lower bounds for this
context, stablishing nontrivial tight bounds for many ranges of \alpha, for
both the unilateral and cooperative versions of network creation. In
particular, we establish polynomial lower bounds for both versions and many
ranges of \alpha, even for this simple nonuniform cost model, which sharply
contrasts the conjectured constant bounds for these games in complete (uniform)
graphs."
Erik Demaine,Demaine_Erik,arXiv:0902.1043,https://arxiv.org/abs/0902.1043,"Abstract:  We present the first polynomial-time approximation schemes (PTASes) for the
following subset-connectivity problems in edge-weighted graphs of bounded
genus: Steiner tree, low-connectivity survivable-network design, and subset
TSP. The schemes run in O(n log n) time for graphs embedded on both orientable
and non-orientable surfaces. This work generalizes the PTAS frameworks of
Borradaile, Klein, and Mathieu from planar graphs to bounded-genus graphs: any
future problems shown to admit the required structure theorem for planar graphs
will similarly extend to bounded-genus graphs."
Erik Demaine,Demaine_Erik,arXiv:0901.1322,https://arxiv.org/abs/0901.1322,"Abstract:  The Carpenter's Rule Theorem states that any chain linkage in the plane can
be folded continuously between any two configurations while preserving the bar
lengths and without the bars crossing. However, this theorem applies only to
strictly simple configurations, where bars intersect only at their common
endpoints. We generalize the theorem to self-touching configurations, where
bars can touch but not properly cross. At the heart of our proof is a new
definition of self-touching configurations of planar linkages, based on an
annotated configuration space and limits of nontouching configurations. We show
that this definition is equivalent to the previously proposed definition of
self-touching configurations, which is based on a combinatorial description of
overlapping features. Using our new definition, we prove the generalized
Carpenter's Rule Theorem using a topological argument. We believe that our
topological methodology provides a powerful tool for manipulating many kinds of
self-touching objects, such as 3D hinged assemblies of polygons and rigid
origami. In particular, we show how to apply our methodology to extend to
self-touching configurations universal reconfigurability results for open
chains with slender polygonal adornments, and single-vertex rigid origami with
convex cones."
Erik Demaine,Demaine_Erik,arXiv:0812.5030,https://arxiv.org/abs/0812.5030,"Abstract:  Alexandrov's Theorem states that every metric with the global topology and
local geometry required of a convex polyhedron is in fact the intrinsic metric
of a unique convex polyhedron. Recent work by Bobenko and Izmestiev describes a
differential equation whose solution leads to the polyhedron corresponding to a
given metric. We describe an algorithm based on this differential equation to
compute the polyhedron to arbitrary precision given the metric, and prove a
pseudopolynomial bound on its running time. Along the way, we develop
pseudopolynomial algorithms for computing shortest paths and weighted Delaunay
triangulations on a polyhedral surface, even when the surface edges are not
shortest paths."
Erik Demaine,Demaine_Erik,arXiv:0804.0986,https://arxiv.org/abs/0804.0986,"Abstract:  We propose a variant of Cauchy's Lemma, proving that when a convex chain on
one sphere is redrawn (with the same lengths and angles) on a larger sphere,
the distance between its endpoints increases. The main focus of this work is a
comparison of three alternate proofs, to show the links between Toponogov's
Comparison Theorem, Legendre's Theorem and Cauchy's Arm Lemma."
Erik Demaine,Demaine_Erik,arXiv:0803.0316,https://arxiv.org/abs/0803.0316,"Abstract:  We introduce staged self-assembly of Wang tiles, where tiles can be added
dynamically in sequence and where intermediate constructions can be stored for
later mixing. This model and its various constraints and performance measures
are motivated by a practical nanofabrication scenario through protein-based
bioengineering. Staging allows us to break through the traditional lower bounds
in tile self-assembly by encoding the shape in the staging algorithm instead of
the tiles. All of our results are based on the practical assumption that only a
constant number of glues, and thus only a constant number of tiles, can be
engineered, as each new glue type requires significant biochemical research and
experiments. Under this assumption, traditional tile self-assembly cannot even
manufacture an n*n square; in contrast, we show how staged assembly enables
manufacture of arbitrary orthogonal shapes in a variety of precise formulations
of the model."
Erik Demaine,Demaine_Erik,arXiv:0801.4405,https://arxiv.org/abs/0801.4405,"Abstract:  We give a counterexample to a conjecture of Poon [Poo06] that any orthogonal
tree in two dimensions can always be flattened by a continuous motion that
preserves edge lengths and avoids self-intersection. We show our example is
locked by extending results on strongly locked self-touching linkages due to
Connelly, Demaine and Rote [CDR02] to allow zero-length edges as defined in
[ADG07], which may be of independent interest. Our results also yield a locked
tree with only eleven edges, which is the smallest known example of a locked
tree."
Erik Demaine,Demaine_Erik,arXiv:0712.2094,https://arxiv.org/abs/0712.2094,"Abstract:  We prove that any finite collection of polygons of equal area has a common
hinged dissection. That is, for any such collection of polygons there exists a
chain of polygons hinged at vertices that can be folded in the plane
continuously without self-intersection to form any polygon in the collection.
This result settles the open problem about the existence of hinged dissections
between pairs of polygons that goes back implicitly to 1864 and has been
studied extensively in the past ten years. Our result generalizes and indeed
builds upon the result from 1814 that polygons have common dissections (without
hinges). We also extend our common dissection result to edge-hinged dissections
of solid 3D polyhedra that have a common (unhinged) dissection, as determined
by Dehn's 1900 solution to Hilbert's Third Problem. Our proofs are
constructive, giving explicit algorithms in all cases. For a constant number of
planar polygons, both the number of pieces and running time required by our
construction are pseudopolynomial. This bound is the best possible, even for
unhinged dissections. Hinged dissections have possible applications to
reconfigurable robotics, programmable matter, and nanomanufacturing."
Erik Demaine,Demaine_Erik,arXiv:0711.2605,https://arxiv.org/abs/0711.2605,"Abstract:  A convex surface that is flat everywhere but on finitely many smooth curves
(or ""seams"") and points is a seam form. We show that the only creases through
the flat components of a seam form are either between vertices or tangent to
the seams. As corollaries we resolve open problems about certain special seam
forms: the flat components of a D-form have no creases at all, and the flat
component of a pita-form has at most one crease, between the seam's endpoints."
Erik Demaine,Demaine_Erik,arXiv:0705.4085,https://arxiv.org/abs/0705.4085,"Abstract:  We demonstrate relationships between the classic Euclidean algorithm and many
other fields of study, particularly in the context of music and distance
geometry. Specifically, we show how the structure of the Euclidean algorithm
defines a family of rhythms which encompass over forty timelines
(\emph{ostinatos}) from traditional world music. We prove that these
\emph{Euclidean rhythms} have the mathematical property that their onset
patterns are distributed as evenly as possible: they maximize the sum of the
Euclidean distances between all pairs of onsets, viewing onsets as points on a
circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this
notion of \emph{evenness}. We also show that essentially all Euclidean rhythms
are \emph{deep}: each distinct distance between onsets occurs with a unique
multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally,
we characterize all deep rhythms, showing that they form a subclass of
generated rhythms, which in turn proves a useful property called shelling. All
of our results for musical rhythms apply equally well to musical scales. In
addition, many of the problems we explore are interesting in their own right as
distance geometry problems on the circle; some of the same problems were
explored by Erdős in the plane."
Erik Demaine,Demaine_Erik,arXiv:cs/0703019,https://arxiv.org/abs/cs/0703019,"Abstract:  We consider a one-round two-player network pricing game, the Stackelberg
Minimum Spanning Tree game or StackMST.
The game is played on a graph (representing a network), whose edges are
colored either red or blue, and where the red edges have a given fixed cost
(representing the competitor's prices). The first player chooses an assignment
of prices to the blue edges, and the second player then buys the cheapest
possible minimum spanning tree, using any combination of red and blue edges.
The goal of the first player is to maximize the total price of purchased blue
edges. This game is the minimum spanning tree analog of the well-studied
Stackelberg shortest-path game.
We analyze the complexity and approximability of the first player's best
strategy in StackMST. In particular, we prove that the problem is APX-hard even
if there are only two different red costs, and give an approximation algorithm
whose approximation ratio is at most $\min \{k,1+\ln b,1+\ln W\}$, where $k$ is
the number of distinct red costs, $b$ is the number of blue edges, and $W$ is
the maximum ratio between red costs. We also give a natural integer linear
programming formulation of the problem, and show that the integrality gap of
the fractional relaxation asymptotically matches the approximation guarantee of
our algorithm."
Erik Demaine,Demaine_Erik,arXiv:cs/0604037,https://arxiv.org/abs/cs/0604037,"Abstract:  The {\em edit distance} between two ordered trees with vertex labels is the
minimum cost of transforming one tree into the other by a sequence of
elementary operations consisting of deleting and relabeling existing nodes, as
well as inserting new nodes. In this paper, we present a worst-case
$O(n^3)$-time algorithm for this problem, improving the previous best
$O(n^3\log n)$-time algorithm~\cite{Klein}. Our result requires a novel
adaptive strategy for deciding how a dynamic program divides into subproblems
(which is interesting in its own right), together with a deeper understanding
of the previous algorithms for the problem. We also prove the optimality of our
algorithm among the family of \emph{decomposition strategy} algorithms--which
also includes the previous fastest algorithms--by tightening the known lower
bound of $\Omega(n^2\log^2 n)$~\cite{Touzet} to $\Omega(n^3)$, matching our
algorithm's running time. Furthermore, we obtain matching upper and lower
bounds of $\Theta(n m^2 (1 + \log \frac{n}{m}))$ when the two trees have
different sizes $m$ and~$n$, where $m < n$."
Erik Demaine,Demaine_Erik,arXiv:cs/0604022,https://arxiv.org/abs/cs/0604022,"Abstract:  We extend linkage unfolding results from the well-studied case of polygonal
linkages to the more general case of linkages of polygons. More precisely, we
consider chains of nonoverlapping rigid planar shapes (Jordan regions) that are
hinged together sequentially at rotatable joints. Our goal is to characterize
the families of planar shapes that admit locked chains, where some
configurations cannot be reached by continuous reconfiguration without
self-intersection, and which families of planar shapes guarantee universal
foldability, where every chain is guaranteed to have a connected configuration
space. Previously, only obtuse triangles were known to admit locked shapes, and
only line segments were known to guarantee universal foldability. We show that
a surprisingly general family of planar shapes, called slender adornments,
guarantees universal foldability: roughly, the distance from each edge along
the path along the boundary of the slender adornment to each hinge should be
monotone. In contrast, we show that isosceles triangles with any desired apex
angle less than 90 degrees admit locked chains, which is precisely the
threshold beyond which the inward-normal property no longer holds."
Erik Demaine,Demaine_Erik,arXiv:cs/0512091,https://arxiv.org/abs/cs/0512091,"Abstract:  We consider preprocessing a set $S$ of $n$ points in convex position in the
plane into a data structure supporting queries of the following form: given a
point $q$ and a directed line $\ell$ in the plane, report the point of $S$ that
is farthest from (or, alternatively, nearest to) the point $q$ among all points
to the left of line $\ell$. We present two data structures for this problem.
The first data structure uses $O(n^{1+\varepsilon})$ space and preprocessing
time, and answers queries in $O(2^{1/\varepsilon} \log n)$ time, for any $0 <
\varepsilon < 1$. The second data structure uses $O(n \log^3 n)$ space and
polynomial preprocessing time, and answers queries in $O(\log n)$ time. These
are the first solutions to the problem with $O(\log n)$ query time and $o(n^2)$
space.
The second data structure uses a new representation of nearest- and
farthest-point Voronoi diagrams of points in convex position. This
representation supports the insertion of new points in clockwise order using
only $O(\log n)$ amortized pointer changes, in addition to $O(\log n)$-time
point-location queries, even though every such update may make $\Theta(n)$
combinatorial changes to the Voronoi diagram. This data structure is the first
demonstration that deterministically and incrementally constructed Voronoi
diagrams can be maintained in $o(n)$ amortized pointer changes per operation
while keeping $O(\log n)$-time point-location queries."
Erik Demaine,Demaine_Erik,arXiv:cs/0512081,https://arxiv.org/abs/cs/0512081,"Abstract:  We develop dynamic dictionaries on the word RAM that use asymptotically
optimal space, up to constant factors, subject to insertions and deletions, and
subject to supporting perfect-hashing queries and/or membership queries, each
operation in constant time with high probability. When supporting only
membership queries, we attain the optimal space bound of Theta(n lg(u/n)) bits,
where n and u are the sizes of the dictionary and the universe, respectively.
Previous dictionaries either did not achieve this space bound or had time
bounds that were only expected and amortized. When supporting perfect-hashing
queries, the optimal space bound depends on the range {1,2,...,n+t} of
hashcodes allowed as output. We prove that the optimal space bound is Theta(n
lglg(u/n) + n lg(n/(t+1))) bits when supporting only perfect-hashing queries,
and it is Theta(n lg(u/n) + n lg(n/(t+1))) bits when also supporting membership
queries. All upper bounds are new, as is the Omega(n lg(n/(t+1))) lower bound."
Erik Demaine,Demaine_Erik,arXiv:cs/0502070,https://arxiv.org/abs/cs/0502070,"Abstract:  In this paper we extend the theory of bidimensionality to two families of
graphs that do not exclude fixed minors: map graphs and power graphs. In both
cases we prove a polynomial relation between the treewidth of a graph in the
family and the size of the largest grid minor. These bounds improve the running
times of a broad class of fixed-parameter algorithms. Our novel technique of
using approximate max-min relations between treewidth and size of grid minors
is powerful, and we show how it can also be used, e.g., to prove a linear
relation between the treewidth of a bounded-genus graph and the treewidth of
its dual."
Erik Demaine,Demaine_Erik,arXiv:cs/0502041,https://arxiv.org/abs/cs/0502041,"Abstract:  We develop a new technique for proving cell-probe lower bounds on dynamic
data structures. This technique enables us to prove an amortized randomized
Omega(lg n) lower bound per operation for several data structural problems on n
elements, including partial sums, dynamic connectivity among disjoint paths (or
a forest or a graph), and several other dynamic graph problems (by simple
reductions). Such a lower bound breaks a long-standing barrier of Omega(lg n /
lglg n) for any dynamic language membership problem. It also establishes the
optimality of several existing data structures, such as Sleator and Tarjan's
dynamic trees. We also prove the first Omega(log_B n) lower bound in the
external-memory model without assumptions on the data structure (such as the
comparison model). Our lower bounds also give a query-update trade-off curve
matched, e.g., by several data structures for dynamic connectivity in graphs.
We also prove matching upper and lower bounds for partial sums when
parameterized by the word size and the maximum additive change in an update."
Erik Demaine,Demaine_Erik,arXiv:cs/0410048,https://arxiv.org/abs/cs/0410048,"Abstract:  Consider laying out a fixed-topology tree of N nodes into external memory
with block size B so as to minimize the worst-case number of block memory
transfers required to traverse a path from the root to a node of depth D. We
prove that the optimal number of memory transfers is $$ \cases{
\displaystyle
\Theta\left( {D \over \lg (1{+}B)} \right)
& when $D = O(\lg N)$, \cr
\displaystyle
\Theta\left( {\lg N \over \lg \left(1{+}{B \lg N \over D}\right)} \right)
& when $D = \Omega(\lg N)$ and $D = O(B \lg N)$, \cr
\displaystyle
\Theta\left( {D \over B} \right)
& when $D = \Omega(B \lg N)$.
} $$"
Erik Demaine,Demaine_Erik,arXiv:cs/0407058,https://arxiv.org/abs/cs/0407058,"Abstract:  This paper gives processor-allocation algorithms for minimizing the average
number of communication hops between the assigned processors for grid
architectures, in the presence of occupied cells. The simpler problem of
assigning processors on a free grid has been studied by Karp, McKellar, and
Wong who show that the solutions have nontrivial structure; they left open the
complexity of the problem.
The associated clustering problem is as follows: Given n points in Re^d, find
k points that minimize their average pairwise L1 distance. We present a natural
approximation algorithm and show that it is a 7/4-approximation for 2D grids.
For d-dimensional space, the approximation guarantee is 2-(1/2d), which is
tight. We also give a polynomial-time approximation scheme (PTAS) for constant
dimension d, and report on experimental results."
Fredo Durand,Durand_Fredo,arXiv:1810.09718,https://arxiv.org/abs/1810.09718,"Abstract:  Texture, highlights, and shading are some of many visual cues that allow
humans to perceive material appearance in single pictures. Yet, recovering
spatially-varying bi-directional reflectance distribution functions (SVBRDFs)
from a single image based on such cues has challenged researchers in computer
graphics for decades. We tackle lightweight appearance capture by training a
deep neural network to automatically extract and make sense of these visual
cues. Once trained, our network is capable of recovering per-pixel normal,
diffuse albedo, specular albedo and specular roughness from a single picture of
a flat surface lit by a hand-held flash. We achieve this goal by introducing
several innovations on training data acquisition and network design. For
training, we leverage a large dataset of artist-created, procedural SVBRDFs
which we sample and render under multiple lighting directions. We further
amplify the data by material mixing to cover a wide diversity of shading
effects, which allows our network to work across many material classes.
Motivated by the observation that distant regions of a material sample often
offer complementary visual cues, we design a network that combines an
encoder-decoder convolutional track for local feature extraction with a
fully-connected track for global feature extraction and propagation. Many
important material effects are view-dependent, and as such ambiguous when
observed in a single image. We tackle this challenge by defining the loss as a
differentiable SVBRDF similarity metric that compares the renderings of the
predicted maps against renderings of the ground truth from several lighting and
viewing directions. Combined together, these novel ingredients bring clear
improvement over state of the art methods for single-shot capture of spatially
varying BRDFs."
Fredo Durand,Durand_Fredo,arXiv:1809.07851,https://arxiv.org/abs/1809.07851,"Abstract:  Winograd-based convolution has quickly gained traction as a preferred
approach to implement convolutional neural networks (ConvNet) on various
hardware platforms because it requires fewer floating point operations than
FFT-based or direct convolutions.
This paper compares three highly optimized implementations (regular FFT--,
Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and
many--core CPUs. Although all three implementations employed the same
optimizations for modern CPUs, our experimental results with two popular
ConvNets (VGG and AlexNet) show that the FFT--based implementations generally
outperform the Winograd--based approach, contrary to the popular belief.
To understand the results, we use a Roofline performance model to analyze the
three implementations in detail, by looking at each of their computation phases
and by considering not only the number of floating point operations, but also
the memory bandwidth and the cache sizes. The performance analysis explains
why, and under what conditions, the FFT--based implementations outperform the
Winograd--based one, on modern CPUs."
Fredo Durand,Durand_Fredo,arXiv:1807.10441,https://arxiv.org/abs/1807.10441,"Abstract:  Widely used in news, business, and educational media, infographics are
handcrafted to effectively communicate messages about complex and often
abstract topics including `ways to conserve the environment' and `understanding
the financial crisis'. Composed of stylistically and semantically diverse
visual and textual elements, infographics pose new challenges for computer
vision. While automatic text extraction works well on infographics, computer
vision approaches trained on natural images fail to identify the stand-alone
visual elements in infographics, or `icons'. To bridge this representation gap,
we propose a synthetic data generation strategy: we augment background patches
in infographics from our Visually29K dataset with Internet-scraped icons which
we use as training data for an icon proposal mechanism. On a test set of 1K
annotated infographics, icons are located with 38% precision and 34% recall
(the best model trained with natural images achieves 14% precision and 7%
recall). Combining our icon proposals with icon classification and text
extraction, we present a multi-modal summarization application. Our application
takes an infographic as input and automatically produces text tags and visual
hashtags that are textually and visually representative of the infographic's
topics respectively."
Fredo Durand,Durand_Fredo,arXiv:1804.07739,https://arxiv.org/abs/1804.07739,"Abstract:  We address the computational problem of novel human pose synthesis. Given an
image of a person and a desired pose, we produce a depiction of that person in
that pose, retaining the appearance of both the person and background. We
present a modular generative neural network that synthesizes unseen poses using
training pairs of images and poses taken from human action videos. Our network
separates a scene into different body part and background layers, moves body
parts to new locations and refines their appearances, and composites the new
foreground with a hole-filled background. These subtasks, implemented with
separate modules, are trained jointly using only a single target image as a
supervised label. We use an adversarial discriminator to force our network to
synthesize realistic details conditioned on pose. We demonstrate image
synthesis results on three action classes: golf, yoga/workouts and tennis, and
show that our method produces accurate results within action classes as well as
across action classes. Given a sequence of desired poses, we also produce
coherent videos of actions."
Fredo Durand,Durand_Fredo,arXiv:1804.02684,https://arxiv.org/abs/1804.02684,"Abstract:  Video motion magnification techniques allow us to see small motions
previously invisible to the naked eyes, such as those of vibrating airplane
wings, or swaying buildings under the influence of the wind. Because the motion
is small, the magnification results are prone to noise or excessive blurring.
The state of the art relies on hand-designed filters to extract representations
that may not be optimal. In this paper, we seek to learn the filters directly
from examples using deep convolutional neural networks. To make training
tractable, we carefully design a synthetic dataset that captures small motion
well, and use two-frame input for training. We show that the learned filters
achieve high-quality results on real videos, with less ringing artifacts and
better noise characteristics than previous methods. While our model is not
trained with temporal filters, we found that the temporal filters can be used
with our extracted representations up to a moderate magnification, enabling a
frequency-based motion selection. Finally, we analyze the learned filters and
show that they behave similarly to the derivative filters used in previous
works. Our code, trained model, and datasets will be available online."
Fredo Durand,Durand_Fredo,arXiv:1709.09215,https://arxiv.org/abs/1709.09215,"Abstract:  We introduce the problem of visual hashtag discovery for infographics:
extracting visual elements from an infographic that are diagnostic of its
topic. Given an infographic as input, our computational approach automatically
outputs textual and visual elements predicted to be representative of the
infographic content. Concretely, from a curated dataset of 29K large
infographic images sampled across 26 categories and 391 tags, we present an
automated two step approach. First, we extract the text from an infographic and
use it to predict text tags indicative of the infographic content. And second,
we use these predicted text tags as a supervisory signal to localize the most
diagnostic visual elements from within the infographic i.e. visual hashtags. We
report performances on a categorization and multi-label tag prediction problem
and compare our proposed visual hashtags to human annotations."
Fredo Durand,Durand_Fredo,arXiv:1708.02660,https://arxiv.org/abs/1708.02660,"Abstract:  Knowing where people look and click on visual designs can provide clues about
how the designs are perceived, and where the most important or relevant content
lies. The most important content of a visual design can be used for effective
summarization or to facilitate retrieval from a database. We present automated
models that predict the relative importance of different elements in data
visualizations and graphic designs. Our models are neural networks trained on
human clicks and importance annotations on hundreds of designs. We collected a
new dataset of crowdsourced importance, and analyzed the predictions of our
models with respect to ground truth importance and human eye movements. We
demonstrate how such predictions of importance can be used for automatic design
retargeting and thumbnailing. User studies with hundreds of MTurk participants
validate that, with limited post-processing, our importance-driven applications
are on par with, or outperform, current state-of-the-art methods, including
natural image saliency. We also provide a demonstration of how our importance
predictions can be built into interactive design tools to offer immediate
feedback during the design process."
Fredo Durand,Durand_Fredo,arXiv:1707.02880,https://arxiv.org/abs/1707.02880,"Abstract:  Performance is a critical challenge in mobile image processing. Given a
reference imaging pipeline, or even human-adjusted pairs of images, we seek to
reproduce the enhancements and enable real-time evaluation. For this, we
introduce a new neural network architecture inspired by bilateral grid
processing and local affine color transforms. Using pairs of input/output
images, we train a convolutional neural network to predict the coefficients of
a locally-affine model in bilateral space. Our architecture learns to make
local, global, and content-dependent decisions to approximate the desired image
transformation. At runtime, the neural network consumes a low-resolution
version of the input image, produces a set of affine transformations in
bilateral space, upsamples those transformations in an edge-preserving fashion
using a new slicing node, and then applies those upsampled transformations to
the full-resolution image. Our algorithm processes high-resolution images on a
smartphone in milliseconds, provides a real-time viewfinder at 1080p
resolution, and matches the quality of state-of-the-art approximation
techniques on a large class of image operators. Unlike previous work, our model
is trained off-line from data and therefore does not require access to the
original operator at runtime. This allows our model to learn complex,
scene-dependent transformations for which no reference implementation is
available, such as the photographic edits of a human retoucher."
Fredo Durand,Durand_Fredo,arXiv:1702.05150,https://arxiv.org/abs/1702.05150,"Abstract:  In this paper, we present BubbleView, an alternative methodology for eye
tracking using discrete mouse clicks to measure which information people
consciously choose to examine. BubbleView is a mouse-contingent, moving-window
interface in which participants are presented with a series of blurred images
and click to reveal ""bubbles"" - small, circular areas of the image at original
resolution, similar to having a confined area of focus like the eye fovea.
Across 10 experiments with 28 different parameter combinations, we evaluated
BubbleView on a variety of image types: information visualizations, natural
images, static webpages, and graphic designs, and compared the clicks to eye
fixations collected with eye-trackers in controlled lab settings. We found that
BubbleView clicks can both (i) successfully approximate eye fixations on
different images, and (ii) be used to rank image and design elements by
importance. BubbleView is designed to collect clicks on static images, and
works best for defined tasks such as describing the content of an information
visualization or measuring image importance. BubbleView data is cleaner and
more consistent than related methodologies that use continuous mouse movements.
Our analyses validate the use of mouse-contingent, moving-window methodologies
as approximating eye fixations for different image and task types."
Fredo Durand,Durand_Fredo,arXiv:1612.04007,https://arxiv.org/abs/1612.04007,"Abstract:  For many movement disorders, such as Parkinson's disease and ataxia, disease
progression is visually assessed by a clinician using a numerical disease
rating scale. These tests are subjective, time-consuming, and must be
administered by a professional. This can be problematic where specialists are
not available, or when a patient is not consistently evaluated by the same
clinician. We present an automated method for quantifying the severity of
motion impairment in patients with ataxia, using only video recordings. We
consider videos of the finger-to-nose test, a common movement task used as part
of the assessment of ataxia progression during the course of routine clinical
checkups.
Our method uses neural network-based pose estimation and optical flow
techniques to track the motion of the patient's hand in a video recording. We
extract features that describe qualities of the motion such as speed and
variation in performance. Using labels provided by an expert clinician, we
train a supervised learning model that predicts severity according to the Brief
Ataxia Rating Scale (BARS). The performance of our system is comparable to that
of a group of ataxia specialists in terms of mean error and correlation, and
our system's predictions were consistently within the range of inter-rater
variability. This work demonstrates the feasibility of using computer vision
and machine learning to produce consistent and clinically useful measures of
motor impairment."
Fredo Durand,Durand_Fredo,arXiv:1610.02769,https://arxiv.org/abs/1610.02769,"Abstract:  The inverse diffusion curve problem focuses on automatic creation of
diffusion curve images that resemble user provided color fields. This problem
is challenging since the 1D curves have a nonlinear and global impact on
resulting color fields via a partial differential equation (PDE). We introduce
a new approach complementary to previous methods by optimizing curve geometry.
In particular, we propose a novel iterative algorithm based on the theory of
shape derivatives. The resulting diffusion curves are clean and well-shaped,
and the final image closely approximates the input. Our method provides a
user-controlled parameter to regularize curve complexity, and generalizes to
handle input color fields represented in a variety of formats."
Fredo Durand,Durand_Fredo,arXiv:1604.03605,https://arxiv.org/abs/1604.03605,"Abstract:  How best to evaluate a saliency model's ability to predict where humans look
in images is an open research question. The choice of evaluation metric depends
on how saliency is defined and how the ground truth is represented. Metrics
differ in how they rank saliency models, and this results from how false
positives and false negatives are treated, whether viewing biases are accounted
for, whether spatial deviations are factored in, and how the saliency maps are
pre-processed. In this paper, we provide an analysis of 8 different evaluation
metrics and their properties. With the help of systematic experiments and
visualizations of metric computations, we add interpretability to saliency
scores and more transparency to the evaluation of saliency models. Building off
the differences in metric properties and behaviors, we make recommendations for
metric selections under specific assumptions and for specific applications."
Joel Emer,Emer_Joel,arXiv:1807.07928,https://arxiv.org/abs/1807.07928,"Abstract:  The design of DNNs has increasingly focused on reducing the computational
complexity in addition to improving accuracy. While emerging DNNs tend to have
fewer weights and operations, they also reduce the amount of data reuse with
more widely varying layer shapes and sizes. This leads to a diverse set of
DNNs, ranging from large ones with high reuse (e.g., AlexNet) to compact ones
with high bandwidth requirements (e.g., MobileNet). However, many existing DNN
processors depend on certain DNN properties, e.g., a large number of channels,
to achieve high performance and energy efficiency and do not have sufficient
flexibility to efficiently process a diverse set of DNNs. In this work, we
present Eyexam, a performance analysis framework that quantitatively identifies
the sources of performance loss in DNN processors. It highlights two
architectural bottlenecks in many existing designs. First, their dataflows are
not flexible enough to adapt to the varying layer shapes and sizes of different
DNNs. Second, their network-on-chip (NoC) can't adapt to support both high data
reuse and high bandwidth scenarios. Based on this analysis, we present Eyeriss
v2, a high-performance DNN accelerator that adapts to a wide range of DNNs.
Eyeriss v2 has a new dataflow, called Row-Stationary Plus (RS+), that enables
the spatial tiling of data from all dimensions to fully utilize the parallelism
for high performance. To support RS+, it has a low-cost and scalable NoC
design, called hierarchical mesh, that connects the high-bandwidth global
buffer to the array of processing elements (PEs) in a two-level hierarchy. This
enables high-bandwidth data delivery while still being able to harness any
available data reuse. Compared with Eyeriss, Eyeriss v2 has a performance
increase of 10.4x-17.9x for 256 PEs, 37.7x-71.5x for 1024 PEs, and
448.8x-1086.7x for 16384 PEs on DNNs with widely varying amounts of data reuse."
Joel Emer,Emer_Joel,arXiv:1708.04485,https://arxiv.org/abs/1708.04485,"Abstract:  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology
for machine learning. High performance and extreme energy efficiency are
critical for deployments of CNNs in a wide range of situations, especially
mobile platforms such as autonomous vehicles, cameras, and electronic personal
assistants. This paper introduces the Sparse CNN (SCNN) accelerator
architecture, which improves performance and energy efficiency by exploiting
the zero-valued weights that stem from network pruning during training and
zero-valued activations that arise from the common ReLU operator applied during
inference. Specifically, SCNN employs a novel dataflow that enables maintaining
the sparse weights and activations in a compressed encoding, which eliminates
unnecessary data transfers and reduces storage requirements. Furthermore, the
SCNN dataflow facilitates efficient delivery of those weights and activations
to the multiplier array, where they are extensively reused. In addition, the
accumulation of multiplication products are performed in a novel accumulator
array. Our results show that on contemporary neural networks, SCNN can improve
both performance and energy by a factor of 2.7x and 2.3x, respectively, over a
comparably provisioned dense CNN accelerator."
Joel Emer,Emer_Joel,arXiv:1703.09039,https://arxiv.org/abs/1703.09039,"Abstract:  Deep neural networks (DNNs) are currently widely used for many artificial
intelligence (AI) applications including computer vision, speech recognition,
and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it
comes at the cost of high computational complexity. Accordingly, techniques
that enable efficient processing of DNNs to improve energy efficiency and
throughput without sacrificing application accuracy or increasing hardware cost
are critical to the wide deployment of DNNs in AI systems.
This article aims to provide a comprehensive tutorial and survey about the
recent advances towards the goal of enabling efficient processing of DNNs.
Specifically, it will provide an overview of DNNs, discuss various hardware
platforms and architectures that support DNNs, and highlight key trends in
reducing the computation cost of DNNs either solely via hardware design changes
or via joint hardware design and DNN algorithm changes. It will also summarize
various development resources that enable researchers and practitioners to
quickly get started in this field, and highlight important benchmarking metrics
and design considerations that should be used for evaluating the rapidly
growing number of DNN hardware designs, optionally including algorithmic
co-designs, being proposed in academia and industry.
The reader will take away the following concepts from this article:
understand the key design considerations for DNNs; be able to evaluate
different DNN hardware implementations with benchmarks and comparison metrics;
understand the trade-offs between various hardware architectures and platforms;
be able to evaluate the utility of various DNN design techniques for efficient
processing; and understand recent implementation trends and opportunities."
Joel Emer,Emer_Joel,arXiv:1703.05853,https://arxiv.org/abs/1703.05853,"Abstract:  Computer vision enables a wide range of applications in robotics/drones,
self-driving cars, smart Internet of Things, and portable/wearable electronics.
For many of these applications, local embedded processing is preferred due to
privacy and/or latency concerns. Accordingly, energy-efficient embedded vision
hardware delivering real-time and robust performance is crucial. While deep
learning is gaining popularity in several computer vision algorithms, a
significant energy consumption difference exists compared to traditional
hand-crafted approaches. In this paper, we provide an in-depth analysis of the
computation, energy and accuracy trade-offs between learned features such as
deep Convolutional Neural Networks (CNN) and hand-crafted features such as
Histogram of Oriented Gradients (HOG). This analysis is supported by
measurements from two chips that implement these algorithms. Our goal is to
understand the source of the energy discrepancy between the two approaches and
to provide insight about the potential areas where CNNs can be improved and
eventually approach the energy-efficiency of HOG while maintaining its
outstanding performance accuracy."
Joel Emer,Emer_Joel,arXiv:1612.07625,https://arxiv.org/abs/1612.07625,"Abstract:  Machine learning plays a critical role in extracting meaningful information
out of the zetabytes of sensor data collected every day. For some applications,
the goal is to analyze and understand the data to identify trends (e.g.,
surveillance, portable/wearable electronics); in other applications, the goal
is to take immediate action based the data (e.g., robotics/drones, self-driving
cars, smart Internet of Things). For many of these applications, local embedded
processing near the sensor is preferred over the cloud due to privacy or
latency concerns, or limitations in the communication bandwidth. However, at
the sensor there are often stringent constraints on energy consumption and cost
in addition to throughput and accuracy requirements. Furthermore, flexibility
is often required such that the processing can be adapted for different
applications or environments (e.g., update the weights and model in the
classifier). In many applications, machine learning often involves transforming
the input data into a higher dimensional space, which, along with programmable
weights, increases data movement and consequently energy consumption. In this
paper, we will discuss how these challenges can be addressed at various levels
of hardware design ranging from architecture, hardware-friendly algorithms,
mixed-signal circuits, and advanced technologies (including memories and
sensors)."
William Freeman,Freeman_William,arXiv:1901.09887,https://arxiv.org/abs/1901.09887,"Abstract:  Generative Adversarial Networks (GANs) have achieved impressive results for
many real-world applications. As an active research topic, many GAN variants
have emerged with improvements in sample quality and training stability.
However, visualization and understanding of GANs is largely missing. How does a
GAN represent our visual world internally? What causes the artifacts in GAN
results? How do architectural choices affect GAN learning? Answering such
questions could enable us to develop new insights and better models. In this
work, we present an analytic framework to visualize and understand GANs at the
unit-, object-, and scene-level. We first identify a group of interpretable
units that are closely related to concepts with a segmentation-based network
dissection method. We quantify the causal effect of interpretable units by
measuring the ability of interventions to control objects in the output.
Finally, we examine the contextual relationship between these units and their
surrounding by inserting the discovered concepts into new images. We show
several practical applications enabled by our framework, from comparing
internal representations across different layers, models, and datasets, to
improving GANs by locating and removing artifact-causing units, to
interactively manipulating objects in the scene. We will open source our
interactive tools to help researchers and practitioners better understand their
models."
William Freeman,Freeman_William,arXiv:1901.02875,https://arxiv.org/abs/1901.02875,"Abstract:  Human perception of 3D shapes goes beyond reconstructing them as a set of
points or a composition of geometric primitives: we also effortlessly
understand higher-level shape structure such as the repetition and reflective
symmetry of object parts. In contrast, recent advances in 3D shape sensing
focus more on low-level geometry but less on these higher-level relationships.
In this paper, we propose 3D shape programs, integrating bottom-up recognition
systems with top-down, symbolic program structure to capture both low-level
geometry and high-level structural priors for 3D shapes. Because there are no
annotations of shape programs for real shapes, we develop neural modules that
not only learn to infer 3D shape programs from raw, unannotated shapes, but
also to execute these programs for shape reconstruction. After initial
bootstrapping, our end-to-end differentiable model learns 3D shape programs by
reconstructing shapes in a self-supervised manner. Experiments demonstrate that
our model accurately infers and executes 3D shape programs for highly complex
shapes from various categories. It can also be integrated with an
image-to-shape module to infer 3D shape programs directly from an RGB image,
leading to 3D shape reconstructions that are both more accurate and more
physically plausible."
William Freeman,Freeman_William,arXiv:1812.11166,https://arxiv.org/abs/1812.11166,"Abstract:  From a single image, humans are able to perceive the full 3D shape of an
object by exploiting learned shape priors from everyday life. Contemporary
single-image 3D reconstruction algorithms aim to solve this task in a similar
fashion, but often end up with priors that are highly biased by training
classes. Here we present an algorithm, Generalizable Reconstruction (GenRe),
designed to capture more generic, class-agnostic shape priors. We achieve this
with an inference network and training procedure that combine 2.5D
representations of visible surfaces (depth and silhouette), spherical shape
representations of both visible and non-visible surfaces, and 3D voxel-based
representations, in a principled manner that exploits the causal structure of
how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe
performs well on single-view shape reconstruction, and generalizes to diverse
novel objects from categories not seen during training."
William Freeman,Freeman_William,arXiv:1812.10972,https://arxiv.org/abs/1812.10972,"Abstract:  Object-based factorizations provide a useful level of abstraction for
interacting with the world. Building explicit object representations, however,
often requires supervisory signals that are difficult to obtain in practice. We
present a paradigm for learning object-centric representations for physical
scene understanding without direct supervision of object properties. Our model,
Object-Oriented Prediction and Planning (O2P2), jointly learns a perception
function to map from image observations to object representations, a pairwise
physics interaction function to predict the time evolution of a collection of
objects, and a rendering function to map objects back to pixels. For
evaluation, we consider not only the accuracy of the physical predictions of
the model, but also its utility for downstream tasks that require an actionable
representation of intuitive physics. After training our model on an image
prediction task, we can use its learned representations to build block towers
more complicated than those observed during training."
William Freeman,Freeman_William,arXiv:1812.02725,https://arxiv.org/abs/1812.02725,"Abstract:  Recent progress in deep generative models has led to tremendous breakthroughs
in image generation. However, while existing models can synthesize
photorealistic images, they lack an understanding of our underlying 3D world.
We present a new generative model, Visual Object Networks (VON), synthesizing
natural images of objects with a disentangled 3D representation. Inspired by
classic graphics rendering pipelines, we unravel our image formation process
into three conditionally independent factors---shape, viewpoint, and
texture---and present an end-to-end adversarial learning framework that jointly
models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes
that are indistinguishable from real shapes. It then renders the object's 2.5D
sketches (i.e., silhouette and depth map) from its shape under a sampled
viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches
to generate natural images. The VON not only generates images that are more
realistic than state-of-the-art 2D image synthesis methods, but also enables
many 3D operations such as changing the viewpoint of a generated image, editing
of shape and texture, linear interpolation in texture and shape space, and
transferring appearance across different objects and viewpoints."
William Freeman,Freeman_William,arXiv:1811.11767,https://arxiv.org/abs/1811.11767,"Abstract:  We use extensive spectroscopy from the MOSFIRE Deep Evolution Field (MOSDEF)
survey to investigate the relationships between rest-frame optical emission
line equivalent widths ($W$) and a number of galaxy and ISM characteristics for
a sample of $1134$ star-forming galaxies at redshifts $1.4\lesssim z\lesssim
3.8$. We examine how the equivalent widths of [OII]$\lambda\lambda 3727, 3730$,
H$\beta$, [OIII]$\lambda\lambda 4960, 5008$, [OIII]$+$H$\beta$, H$\alpha$, and
H$\alpha$+[NII]$\lambda\lambda 6550, 6585$, depend on stellar mass, UV slope,
age, star-formation rate (SFR) and specific SFR (sSFR), ionization parameter
and excitation conditions (O32 and [OIII]/H$\beta$), gas-phase metallicity, and
ionizing photon production efficiency ($\xi_{\rm ion}$). The trend of
increasing $W$ with decreasing stellar mass is strongest for [OIII] (and
[OIII]+H$\beta$). More generally, the equivalent widths of all the lines
increase with redshift at a fixed stellar mass or fixed gas-phase metallicity,
suggesting that high equivalent width galaxies are common at high redshift.
This redshift evolution in equivalent widths can be explained by the increase
in SFR and decrease in metallicity with redshift at a fixed stellar mass.
Consequently, the dependence of $W$ on sSFR is largely invariant with redshift,
particularly when examined for galaxies of a given metallicity. Our results
show that high equivalent width galaxies, specifically those with high $W({\rm
[OIII]})$, have low stellar masses, blue UV slopes, young ages, high sSFRs, ISM
line ratios indicative of high ionization parameters, high $\xi_{\rm ion}$, and
low metallicities. As these characteristics are often attributed to galaxies
with high ionizing escape fractions, galaxies with high $W$ are likely
candidates for the population that dominates cosmic reionization."
William Freeman,Freeman_William,arXiv:1811.10597,https://arxiv.org/abs/1811.10597,"Abstract:  Generative Adversarial Networks (GANs) have recently achieved impressive
results for many real-world applications, and many GAN variants have emerged
with improvements in sample quality and training stability. However, they have
not been well visualized or understood. How does a GAN represent our visual
world internally? What causes the artifacts in GAN results? How do
architectural choices affect GAN learning? Answering such questions could
enable us to develop new insights and better models.
In this work, we present an analytic framework to visualize and understand
GANs at the unit-, object-, and scene-level. We first identify a group of
interpretable units that are closely related to object concepts using a
segmentation-based network dissection method. Then, we quantify the causal
effect of interpretable units by measuring the ability of interventions to
control objects in the output. We examine the contextual relationship between
these units and their surroundings by inserting the discovered object concepts
into new images. We show several practical applications enabled by our
framework, from comparing internal representations across different layers,
models, and datasets, to improving GANs by locating and removing
artifact-causing units, to interactively manipulating objects in a scene. We
provide open source interpretation tools to help researchers and practitioners
better understand their GAN models."
William Freeman,Freeman_William,arXiv:1811.05443,https://arxiv.org/abs/1811.05443,"Abstract:  Deep neural networks, trained with large amount of labeled data, can fail to
generalize well when tested with examples from a \emph{target domain} whose
distribution differs from the training data distribution, referred as the
\emph{source domain}. It can be expensive or even infeasible to obtain required
amount of labeled data in all possible domains. Unsupervised domain adaptation
sets out to address this problem, aiming to learn a good predictive model for
the target domain using labeled examples from the source domain but only
unlabeled examples from the target domain. Domain alignment approaches this
problem by matching the source and target feature distributions, and has been
used as a key component in many state-of-the-art domain adaptation methods.
However, matching the marginal feature distributions does not guarantee that
the corresponding class conditional distributions will be aligned across the
two domains. We propose co-regularized domain alignment for unsupervised domain
adaptation, which constructs multiple diverse feature spaces and aligns source
and target distributions in each of them individually, while encouraging that
alignments agree with each other with regard to the class predictions on the
unlabeled target examples. The proposed method is generic and can be used to
improve any domain adaptation method which uses domain alignment. We
instantiate it in the context of a recent state-of-the-art method and observe
that it provides significant performance improvements on several domain
adaptation benchmarks."
William Freeman,Freeman_William,arXiv:1810.07204,https://arxiv.org/abs/1810.07204,"Abstract:  Using the MOSDEF rest-frame optical spectroscopic survey, we investigate the
star-formation histories (SFHs) of different galaxy types, ranging from
actively star forming to quiescent at $1.4\leq~z\leq2.6$. SFHs are constrained
utilizing stellar continuum spectroscopy, specifically through a combination of
Balmer absorption lines, the 4000$\AA$ break, and the equivalent width of the
H$\alpha$ emission line. To attain a sufficiently high signal-to-noise ratio
(S/N) to conduct these measurements we stack spectra of galaxies with similar
spectral types, as determined from their rest-frame $U-V$ and $V-J$ colors. We
bin the MOSDEF sample into five spectral types, subdividing the quiescent and
star-forming bins to better explore galaxies transitioning between the two. We
constrain the average SFHs for each type, finding that quiescent and
transitional galaxies in the MOSDEF sample are dominated by an SFH with an
average star-formation timescale of $\tau\sim0.1-0.2$~Gyr. These findings
contrast with measurements from the low-redshift Universe where, on average,
galaxies form their stars over a more extended time period ($\tau>1$~Gyr).
Furthermore, our spectral index measurements correlate with mass surface
density for all spectral types. Finally, we compare the average properties of
the galaxies in our transitional bins to investigate possible paths to
quiescence, and speculate on the viability of a dusty post-starburst phase."
William Freeman,Freeman_William,arXiv:1810.01054,https://arxiv.org/abs/1810.01054,"Abstract:  Physical simulators have been widely used in robot planning and control.
Among them, differentiable simulators are particularly favored, as they can be
incorporated into gradient-based optimization algorithms that are efficient in
solving inverse problems such as optimal control and motion planning.
Simulating deformable objects is, however, more challenging compared to rigid
body dynamics. The underlying physical laws of deformable objects are more
complex, and the resulting systems have orders of magnitude more degrees of
freedom and therefore they are significantly more computationally expensive to
simulate. Computing gradients with respect to physical design or controller
parameters is typically even more computationally challenging. In this paper,
we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical
simulator for deformable objects, ChainQueen, based on the Moving Least Squares
Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects
including contact and can be seamlessly incorporated into inference, control
and co-design systems. We demonstrate that our simulator achieves high
precision in both forward simulation and backward gradient computation. We have
successfully employed it in a diverse set of control tasks for soft robots,
including problems with nearly 3,000 decision variables."
William Freeman,Freeman_William,arXiv:1809.05491,https://arxiv.org/abs/1809.05491,"Abstract:  We present a system that allows users to visualize complex human motion via
3D motion sculptures---a representation that conveys the 3D structure swept by
a human body as it moves through space. Given an input video, our system
computes the motion sculptures and provides a user interface for rendering it
in different styles, including the options to insert the sculpture back into
the original video, render it in a synthetic scene or physically print it.
To provide this end-to-end workflow, we introduce an algorithm that estimates
that human's 3D geometry over time from a set of 2D images and develop a
3D-aware image-based rendering approach that embeds the sculpture back into the
scene. By automating the process, our system takes motion sculpture creation
out of the realm of professional artists, and makes it applicable to a wide
range of existing video material.
By providing viewers with 3D information, motion sculptures reveal space-time
motion information that is difficult to perceive with the naked eye, and allow
viewers to interpret how different parts of the object interact over time. We
validate the effectiveness of this approach with user studies, finding that our
motion sculpture visualizations are significantly more informative about motion
than existing stroboscopic and space-time visualization methods."
William Freeman,Freeman_William,arXiv:1809.05070,https://arxiv.org/abs/1809.05070,"Abstract:  Objects are made of parts, each with distinct geometry, physics,
functionality, and affordances. Developing such a distributed, physical,
interpretable representation of objects will facilitate intelligent agents to
better explore and interact with the world. In this paper, we study physical
primitive decomposition---understanding an object through its components, each
with physical and geometric attributes. As annotated data for object parts and
physics are rare, we propose a novel formulation that learns physical
primitives by explaining both an object's appearance and its behaviors in
physical events. Our model performs well on block towers and tools in both
synthetic and real scenarios; we also demonstrate that visual and physical
observations often provide complementary signals. We further present ablation
and behavioral studies to better understand our model and contrast it with
human performance."
William Freeman,Freeman_William,arXiv:1809.05068,https://arxiv.org/abs/1809.05068,"Abstract:  The problem of single-view 3D shape completion or reconstruction is
challenging, because among the many possible shapes that explain an
observation, most are implausible and do not correspond to natural objects.
Recent research in the field has tackled this problem by exploiting the
expressiveness of deep convolutional networks. In fact, there is another level
of ambiguity that is often overlooked: among plausible shapes, there are still
multiple shapes that fit the 2D image equally well; i.e., the ground truth
shape is non-deterministic given a single-view input. Existing fully supervised
approaches fail to address this issue, and often produce blurry mean shapes
with smooth surfaces but no fine details.
In this paper, we propose ShapeHD, pushing the limit of single-view shape
completion and reconstruction by integrating deep generative models with
adversarially learned shape priors. The learned priors serve as a regularizer,
penalizing the model only if its output is unrealistic, not if it deviates from
the ground truth. Our design thus overcomes both levels of ambiguity
aforementioned. Experiments demonstrate that ShapeHD outperforms state of the
art by a large margin in both shape completion and shape reconstruction on
multiple real datasets."
William Freeman,Freeman_William,arXiv:1809.05067,https://arxiv.org/abs/1809.05067,"Abstract:  Humans recognize object structure from both their appearance and motion;
often, motion helps to resolve ambiguities in object structure that arise when
we observe object appearance only. There are particular scenarios, however,
where neither appearance nor spatial-temporal motion signals are informative:
occluding twigs may look connected and have almost identical movements, though
they belong to different, possibly disconnected branches. We propose to tackle
this problem through spectrum analysis of motion signals, because vibrations of
disconnected branches, though visually similar, often have distinctive natural
frequencies. We propose a novel formulation of tree structure based on a
physics-based link model, and validate its effectiveness by theoretical
analysis, numerical simulation, and empirical experiments. With this
formulation, we use nonparametric Bayesian inference to reconstruct tree
structure from both spectral vibration signals and appearance cues. Our model
performs well in recognizing hierarchical tree structure from real-world videos
of trees and vessels."
William Freeman,Freeman_William,arXiv:1808.09978,https://arxiv.org/abs/1808.09978,"Abstract:  We study the properties of 32 spectroscopically-identified pairs of galaxies
observed during the peak epoch of star formation in the universe. These systems
are drawn from the MOSFIRE Deep Evolution Field (MOSDEF) Survey at $1.4 \leq z
\leq 3.8$, and are interpreted as early-stage galaxy mergers. Galaxy pairs in
our sample are identified as two objects whose spectra were collected on the
same Keck/MOSFIRE spectroscopic slit. Accordingly, all pairs in the sample have
projected separations $R_{\rm proj}\leq 60$~kpc. The redshift separation for
pairs was required to be $\Delta z \leq 0.01$ ($\Delta v \lesssim 1000 \mbox{
km s}^{-1}$), but, in practice, all but two pairs have velocity separations
consistent with $\Delta v \leq 500 \mbox{ km s}^{-1}$, which is the standard
threshold for defining interacting galaxy pairs at low redshift. Stellar mass
ratios in our sample range from 1.1 to 550, with 13 ratios closer than 3:1, the
common definition of a ""major merger."" Studies of merging pairs in the local
universe indicate an enhancement in star-formation activity and deficit in
gas-phase oxygen abundance relative to isolated galaxies of the same mass. We
compare the MOSDEF pairs sample to a control sample of isolated galaxies at the
same redshift, finding no measurable SFR enhancement or metallicity deficit at
fixed stellar mass for the pairs sample, in contrast to low-redshift studies.
These results are consistent with some theoretical works suggesting a reduced
differential effect of pre-coalescence mergers on galaxy properties at high
redshift -- specifically that pre-coalescence mergers do not drive strong
starbursts."
William Freeman,Freeman_William,arXiv:1808.09351,https://arxiv.org/abs/1808.09351,"Abstract:  We aim to obtain an interpretable, expressive, and disentangled scene
representation that contains comprehensive structural and textural information
for each object. Previous scene representations learned by neural networks are
often uninterpretable, limited to a single object, or lacking 3D knowledge. In
this work, we propose 3D scene de-rendering networks (3D-SDN) to address the
above issues by integrating disentangled representations for semantics,
geometry, and appearance into a deep generative model. Our scene encoder
performs inverse graphics, translating a scene into a structured object-wise
representation. Our decoder has two components: a differentiable shape renderer
and a neural texture generator. The disentanglement of semantics, geometry, and
appearance supports 3D-aware scene manipulation, e.g., rotating and moving
objects freely while keeping the consistent shape and texture, and changing the
object appearance without affecting its shape. Experiments demonstrate that our
editing scheme based on 3D-SDN is superior to its 2D counterpart."
William Freeman,Freeman_William,arXiv:1808.05732,https://arxiv.org/abs/1808.05732,"Abstract:  We present an algorithm for creating high resolution anatomically plausible
images consistent with acquired clinical brain MRI scans with large inter-slice
spacing. Although large data sets of clinical images contain a wealth of
information, time constraints during acquisition result in sparse scans that
fail to capture much of the anatomy. These characteristics often render
computational analysis impractical as many image analysis algorithms tend to
fail when applied to such images. Highly specialized algorithms that explicitly
handle sparse slice spacing do not generalize well across problem domains. In
contrast, we aim to enable application of existing algorithms that were
originally developed for high resolution research scans to significantly
undersampled scans. We introduce a generative model that captures fine-scale
anatomical structure across subjects in clinical image collections and derive
an algorithm for filling in the missing data in scans with large inter-slice
spacing. Our experimental results demonstrate that the resulting method
outperforms state-of-the-art upsampling super-resolution techniques, and
promises to facilitate subsequent analysis not previously possible with scans
of this quality. Our implementation is freely available at
this https URL ."
William Freeman,Freeman_William,arXiv:1808.03247,https://arxiv.org/abs/1808.03247,"Abstract:  Perceiving accurate 3D object shape is important for robots to interact with
the physical world. Current research along this direction has been primarily
relying on visual observations. Vision, however useful, has inherent
limitations due to occlusions and the 2D-3D ambiguities, especially for
perception with a monocular camera. In contrast, touch gets precise local shape
information, though its efficiency for reconstructing the entire shape could be
low. In this paper, we propose a novel paradigm that efficiently perceives
accurate 3D object shape by incorporating visual and tactile observations, as
well as prior knowledge of common object shapes learned from large-scale shape
repositories. We use vision first, applying neural networks with learned shape
priors to predict an object's 3D shape from a single-view color image. We then
use tactile sensing to refine the shape; the robot actively touches the object
regions where the visual prediction has high uncertainty. Our method
efficiently builds the 3D shape of common objects from a color image and a
small number of tactile explorations (around 10). Our setup is easy to apply
and has potentials to help robots better perform grasping or manipulation tasks
on real-world objects."
William Freeman,Freeman_William,arXiv:1807.09245,https://arxiv.org/abs/1807.09245,"Abstract:  We study the problem of synthesizing a number of likely future frames from a
single input image. In contrast to traditional methods that have tackled this
problem in a deterministic or non-parametric way, we propose to model future
frames in a probabilistic manner. Our probabilistic model makes it possible for
us to sample and synthesize many possible future frames from a single input
image. To synthesize realistic movement of objects, we propose a novel network
structure, namely a Cross Convolutional Network; this network encodes image and
motion information as feature maps and convolutional kernels, respectively. In
experiments, our model performs well on synthetic data, such as 2D shapes and
animated game sprites, and on real-world video frames. We present analyses of
the learned network representations, showing it is implicitly learning a
compact encoding of object appearance and motion. We also demonstrate a few of
its applications, including visual analogy-making and video extrapolation."
William Freeman,Freeman_William,arXiv:1806.08989,https://arxiv.org/abs/1806.08989,"Abstract:  We present an analysis using the MOSFIRE Deep Evolution Field (MOSDEF) survey
on the nature of ""MIR-excess"" galaxies, which have star formation rates (SFR)
inferred from mid-infrared (MIR) data that is substantially elevated relative
to that estimated from dust-corrected UV data. We use a sample of $\sim$200
galaxies and AGN at $1.40<z<2.61$ with 24 $\mu$m detections (rest-frame
8$\mu$m) from MIPS/\textit{Spitzer}. We find that the identification of
MIR-excess galaxies strongly depends on the methodologies used to estimate IR
luminosity ($\rm L_{IR}$) and to correct the UV light for dust attenuation. We
find that extrapolations of the SFR from the observed 24 $\mu$m flux, using
luminosity-dependent templates based on local galaxies, substantially
overestimate $\rm L_{IR}$ in $z\sim2$ galaxies. By including \textit{Herschel}
observations and using a stellar mass-dependent, luminosity-independent $\rm
L_{IR}$, we obtain more reliable estimates of the SFR and a lower fraction of
MIR-excess galaxies. Once stellar mass selection biases are taken into account,
we identify $\sim24\%$ of our galaxies as MIR-excess. However, $\rm
SFR_{H\alpha}$ is not elevated in MIR-excess galaxies compared to MIR-normal
galaxies, indicating that the intrinsic fraction of MIR-excess may be lower.
Using X-ray, IR, and optically-selected AGN in MOSDEF, we do not find a higher
prevalence for AGN in MIR-excess galaxies relative to MIR-normal galaxies. A
stacking analysis of X-ray undetected galaxies does not reveal a harder
spectrum in MIR-excess galaxies relative to MIR-normal galaxies. Our analysis
indicates that AGN activity does not contribute substantially to the MIR excess
and instead implies that it is likely due to the enhanced PAH emission."
William Freeman,Freeman_William,arXiv:1806.06098,https://arxiv.org/abs/1806.06098,"Abstract:  We present a method for training a regression network from image pixels to 3D
morphable model coordinates using only unlabeled photographs. The training loss
is based on features from a facial recognition network, computed on-the-fly by
rendering the predicted faces with a differentiable renderer. To make training
from features feasible and avoid network fooling effects, we introduce three
objectives: a batch distribution loss that encourages the output distribution
to match the distribution of the morphable model, a loopback loss that ensures
the network can correctly reinterpret its own output, and a multi-view identity
loss that compares the features of the predicted 3D face and the input
photograph from multiple viewing angles. We train a regression network using
these objectives, a set of unlabeled photographs, and the morphable model
itself, and demonstrate state-of-the-art results."
William Freeman,Freeman_William,arXiv:1804.04610,https://arxiv.org/abs/1804.04610,"Abstract:  We study 3D shape modeling from a single image and make contributions to it
in three aspects. First, we present Pix3D, a large-scale benchmark of diverse
image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications
in shape-related tasks including reconstruction, retrieval, viewpoint
estimation, etc. Building such a large-scale dataset, however, is highly
challenging; existing datasets either contain only synthetic data, or lack
precise alignment between 2D images and 3D shapes, or only have a small number
of images. Second, we calibrate the evaluation criteria for 3D shape
reconstruction through behavioral studies, and use them to objectively and
systematically benchmark cutting-edge reconstruction algorithms on Pix3D.
Third, we design a novel model that simultaneously performs 3D reconstruction
and pose estimation; our multi-task learning approach achieves state-of-the-art
performance on both tasks."
William Freeman,Freeman_William,arXiv:1804.03619,https://arxiv.org/abs/1804.03619,"Abstract:  We present a joint audio-visual model for isolating a single speech signal
from a mixture of sounds such as other speakers and background noise. Solving
this task using only audio as input is extremely challenging and does not
provide an association of the separated speech signals with speakers in the
video. In this paper, we present a deep network-based model that incorporates
both visual and auditory signals to solve this task. The visual features are
used to ""focus"" the audio on desired speakers in a scene and to improve the
speech separation quality. To train our joint audio-visual model, we introduce
AVSpeech, a new dataset comprised of thousands of hours of video segments from
the Web. We demonstrate the applicability of our method to classic speech
separation tasks, as well as real-world scenarios involving heated interviews,
noisy bars, and screaming children, only requiring the user to specify the face
of the person in the video whose speech they want to isolate. Our method shows
clear advantage over state-of-the-art audio-only speech separation in cases of
mixed speech. In addition, our model, which is speaker-independent (trained
once, applicable to any speaker), produces better results than recent
audio-visual speech separation methods that are speaker-dependent (require
training a separate model for each speaker of interest)."
William Freeman,Freeman_William,arXiv:1804.02684,https://arxiv.org/abs/1804.02684,"Abstract:  Video motion magnification techniques allow us to see small motions
previously invisible to the naked eyes, such as those of vibrating airplane
wings, or swaying buildings under the influence of the wind. Because the motion
is small, the magnification results are prone to noise or excessive blurring.
The state of the art relies on hand-designed filters to extract representations
that may not be optimal. In this paper, we seek to learn the filters directly
from examples using deep convolutional neural networks. To make training
tractable, we carefully design a synthetic dataset that captures small motion
well, and use two-frame input for training. We show that the learned filters
achieve high-quality results on real videos, with less ringing artifacts and
better noise characteristics than previous methods. While our model is not
trained with temporal filters, we found that the temporal filters can be used
with our extracted representations up to a moderate magnification, enabling a
frequency-based motion selection. Finally, we analyze the learned filters and
show that they behave similarly to the derivative filters used in previous
works. Our code, trained model, and datasets will be available online."
William Freeman,Freeman_William,arXiv:1804.00782,https://arxiv.org/abs/1804.00782,"Abstract:  Understanding 3D object structure from a single image is an important but
challenging task in computer vision, mostly due to the lack of 3D object
annotations to real images. Previous research tackled this problem by either
searching for a 3D shape that best explains 2D annotations, or training purely
on synthetic data with ground truth 3D information. In this work, we propose 3D
INterpreter Networks (3D-INN), an end-to-end trainable framework that
sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses.
Our system learns from both 2D-annotated real images and synthetic 3D data.
This is made possible mainly by two technical innovations. First, heatmaps of
2D keypoints serve as an intermediate representation to connect real and
synthetic data. 3D-INN is trained on real images to estimate 2D keypoint
heatmaps from an input image; it then predicts 3D object structure from
heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN
benefits from the variation and abundance of synthetic 3D objects, without
suffering from the domain difference between real and synthesized images, often
due to imperfect rendering. Second, we propose a Projection Layer, mapping
estimated 3D structure back to 2D. During training, it ensures 3D-INN to
predict 3D structure whose projection is consistent with the 2D annotations to
real images. Experiments show that the proposed system performs well on both 2D
keypoint estimation and 3D structure recovery. We also demonstrate that the
recovered 3D information has wide vision applications, such as image retrieval."
William Freeman,Freeman_William,arXiv:1712.08232,https://arxiv.org/abs/1712.08232,"Abstract:  We study the problem of reconstructing an image from information stored at
contour locations. We show that high-quality reconstructions with high fidelity
to the source image can be obtained from sparse input, e.g., comprising less
than $6\%$ of image pixels. This is a significant improvement over existing
contour-based reconstruction methods that require much denser input to capture
subtle texture information and to ensure image quality. Our model, based on
generative adversarial networks, synthesizes texture and details in regions
where no input information is provided. The semantic knowledge encoded into our
model and the sparsity of the input allows to use contours as an intuitive
interface for semantically-aware image manipulation: local edits in contour
domain translate to long-range and coherent changes in pixel space. We can
perform complex structural changes such as changing facial expression by simple
edits of contours. Our experiments demonstrate that humans as well as a face
recognition system mostly cannot distinguish between our reconstructions and
the source images."
William Freeman,Freeman_William,arXiv:1712.07271,https://arxiv.org/abs/1712.07271,"Abstract:  The sound of crashing waves, the roar of fast-moving cars -- sound conveys
important information about the objects in our surroundings. In this work, we
show that ambient sounds can be used as a supervisory signal for learning
visual models. To demonstrate this, we train a convolutional neural network to
predict a statistical summary of the sound associated with a video frame. We
show that, through this process, the network learns a representation that
conveys information about objects and scenes. We evaluate this representation
on several recognition tasks, finding that its performance is comparable to
that of other state-of-the-art unsupervised learning methods. Finally, we show
through visualizations that the network learns units that are selective to
objects that are often associated with characteristic sounds. This paper
extends an earlier conference paper, Owens et al. 2016, with additional
experiments and discussion."
William Freeman,Freeman_William,arXiv:1711.09078,https://arxiv.org/abs/1711.09078,"Abstract:  Many video processing algorithms rely on optical flow to register different
frames within a sequence. However, a precise estimation of optical flow is
often neither tractable nor optimal for a particular task. In this paper, we
propose task-oriented flow (TOFlow), a flow representation tailored for
specific video processing tasks. We design a neural network with a motion
estimation component and a video processing component. These two parts can be
jointly trained in a self-supervised manner to facilitate learning of the
proposed TOFlow. We demonstrate that TOFlow outperforms the traditional optical
flow on three different video processing tasks: frame interpolation, video
denoising/deblocking, and video super-resolution. We also introduce Vimeo-90K,
a large-scale, high-quality video dataset for video processing to better
evaluate the proposed algorithm."
William Freeman,Freeman_William,arXiv:1711.06297,https://arxiv.org/abs/1711.06297,"Abstract:  Active non-line-of-sight imaging systems are of growing interest for diverse
applications. The most commonly proposed approaches to date rely on exploiting
time-resolved measurements, i.e., measuring the time it takes for short light
pulses to transit the scene. This typically requires expensive, specialized,
ultrafast lasers and detectors that must be carefully calibrated. We develop an
alternative approach that exploits the valuable role that natural occluders in
a scene play in enabling accurate and practical image formation in such
settings without such hardware complexity. In particular, we demonstrate that
the presence of occluders in the hidden scene can obviate the need for
collecting time-resolved measurements, and develop an accompanying analysis for
such systems and their generalizations. Ultimately, the results suggest the
potential to develop increasingly sophisticated future systems that are able to
identify and exploit diverse structural features of the environment to
reconstruct scenes hidden from view."
William Freeman,Freeman_William,arXiv:1711.03129,https://arxiv.org/abs/1711.03129,"Abstract:  3D object reconstruction from a single image is a highly under-determined
problem, requiring strong prior knowledge of plausible 3D shapes. This
introduces challenges for learning-based approaches, as 3D object annotations
are scarce in real images. Previous work chose to train on synthetic data with
ground truth 3D information, but suffered from domain adaptation when tested on
real data. In this work, we propose MarrNet, an end-to-end trainable model that
sequentially estimates 2.5D sketches and 3D object shape. Our disentangled,
two-step formulation has three advantages. First, compared to full 3D shape,
2.5D sketches are much easier to be recovered from a 2D image; models that
recover 2.5D sketches are also more likely to transfer from synthetic to real
data. Second, for 3D reconstruction from 2.5D sketches, systems can learn
purely from synthetic data. This is because we can easily render realistic 2.5D
sketches without modeling object appearance variations in real images,
including lighting, texture, etc. This further relieves the domain adaptation
problem. Third, we derive differentiable projective functions from 3D shape to
2.5D sketches; the framework is therefore end-to-end trainable on real images,
requiring no human annotations. Our model achieves state-of-the-art performance
on 3D shape reconstruction."
William Freeman,Freeman_William,arXiv:1711.01357,https://arxiv.org/abs/1711.01357,"Abstract:  Very long baseline interferometry (VLBI) makes it possible to recover images
of astronomical sources with extremely high angular resolution. Most recently,
the Event Horizon Telescope (EHT) has extended VLBI to short millimeter
wavelengths with a goal of achieving angular resolution sufficient for imaging
the event horizons of nearby supermassive black holes. VLBI provides
measurements related to the underlying source image through a sparse set
spatial frequencies. An image can then be recovered from these measurements by
making assumptions about the underlying image. One of the most important
assumptions made by conventional imaging methods is that over the course of a
night's observation the image is static. However, for quickly evolving sources,
such as the galactic center's supermassive black hole (Sgr A*) targeted by the
EHT, this assumption is violated and these conventional imaging approaches
fail. In this work we propose a new way to model VLBI measurements that allows
us to recover both the appearance and dynamics of an evolving source by
reconstructing a video rather than a static image. By modeling VLBI
measurements using a Gaussian Markov Model, we are able to propagate
information across observations in time to reconstruct a video, while
simultaneously learning about the dynamics of the source's emission region. We
demonstrate our proposed Expectation-Maximization (EM) algorithm, StarWarps, on
realistic synthetic observations of black holes, and show how it substantially
improves results compared to conventional imaging algorithms. Additionally, we
demonstrate StarWarps on real VLBI data of the M87 Jet from the VLBA."
William Freeman,Freeman_William,arXiv:1711.00224,https://arxiv.org/abs/1711.00224,"Abstract:  We investigate the nature of the relation among stellar mass, star-formation
rate, and gas-phase metallicity (the M$_*$-SFR-Z relation) at high redshifts
using a sample of 260 star-forming galaxies at $z\sim2.3$ from the MOSDEF
survey. We present an analysis of the high-redshift M$_*$-SFR-Z relation based
on several emission-line ratios for the first time. We show that a M$_*$-SFR-Z
relation clearly exists at $z\sim2.3$. The strength of this relation is similar
to predictions from cosmological hydrodynamical simulations. By performing a
direct comparison of stacks of $z\sim0$ and $z\sim2.3$ galaxies, we find that
$z\sim2.3$ galaxies have $\sim0.1$ dex lower metallicity at fixed M$_*$ and
SFR. In the context of chemical evolution models, this evolution of the
M$_*$-SFR-Z relation suggests an increase with redshift of the mass-loading
factor at fixed M$_*$, as well as a decrease in the metallicity of infalling
gas that is likely due to a lower importance of gas recycling relative to
accretion from the intergalactic medium at high redshifts. Performing this
analysis simultaneously with multiple metallicity-sensitive line ratios allows
us to rule out the evolution in physical conditions (e.g., N/O ratio,
ionization parameter, and hardness of the ionizing spectrum) at fixed
metallicity as the source of the observed trends with redshift and with SFR at
fixed M$_*$ at $z\sim2.3$. While this study highlights the promise of
performing high-order tests of chemical evolution models at high redshifts,
detailed quantitative comparisons ultimately await a full understanding of the
evolution of metallicity calibrations with redshift."
William Freeman,Freeman_William,arXiv:1711.00013,https://arxiv.org/abs/1711.00013,"Abstract:  We combine spectroscopic measurements of H$\alpha$ and H$\beta$ and UV
continuum photometry for a sample of 673 galaxies from the MOSFIRE Deep
Evolution Field survey to constrain hydrogen ionizing photon production
efficiencies ($\xi_{\rm ion}$, xi_ion) at z=1.4-2.6. We find average
log(xi_ion/[Hz erg$^{-1}$])=25.06 (25.34), assuming the Calzetti (SMC) curve
for the UV dust correction and a scatter of 0.28 dex in xi_ion distribution.
After accounting for observational uncertainties and variations in dust
attenuation, we conclude that the remaining scatter in xi_ion is likely
dominated by galaxy-to-galaxy variations in stellar populations, including the
slope and upper-mass cutoff of the initial mass function, stellar metallicity,
star-formation burstiness, and stellar evolution (e.g., single/binary star
evolution). Moreover, xi_ion is elevated in galaxies with high ionization
states (high [OIII]/[OII]) and low oxygen abundances (low [NII]/H$\alpha$ and
high [OIII]/H$\beta$) in the ionized ISM. However, xi_ion does not correlate
with the offset from the z~0 star-forming locus in the BPT diagram, suggesting
no change in the hardness of ionizing radiation accompanying the offset from
the z~0 sequence. We also find that galaxies with blue UV spectral slopes
($\langle\beta\rangle$=-2.1) have elevated xi_ion by a factor of ~2 relative to
the average xi_ion of the sample ($\langle\beta\rangle$=-1.4). If these blue
galaxies are similar to those at z > 6, our results suggest that a lower Lyman
continuum escape fraction is required for galaxies to maintain reionization,
compared to the canonical xi_ion predictions from stellar population models.
Furthermore, we demonstrate that even with robustly dust-corrected H$\alpha$,
the UV dust attenuation can cause on average a ~0.3dex systematic uncertainty
in xi_ion calculations."
William Freeman,Freeman_William,arXiv:1710.03230,https://arxiv.org/abs/1710.03230,"Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on
broad flux from the nebular emission lines H$\alpha$, [NII], [OIII], H$\beta$,
and [SII]. The sample consists of 127 star-forming galaxies at $1.37 < z <
2.61$ and 84 galaxies at $2.95 < z < 3.80$. We decompose the emission lines
using narrow ($\text{FWHM} < 275 \ \text{km s}^{-1}$) and broad ($\text{FWHM} >
300 \ \text{km s}^{-1}$) Gaussian components for individual galaxies and
stacks. Broad emission is detected at $>3\sigma$ in $<10$% of galaxies and the
broad flux accounts for 10-70% of the total flux. We find a slight increase in
broad to narrow flux ratio with mass but note that we cannot reliably detect
broad emission with $\text{FWHM} < 275 \ \text{km s}^{-1}$, which may be
significant at low masses. Notably, there is a correlation between higher
signal-to-noise (S/N) spectra and a broad component detection indicating a S/N
dependence in our ability to detect broad flux. When placed on the N2-BPT
diagram ([OIII]/H$\beta$ vs. [NII]/H$\alpha$) the broad components of the
stacks are shifted towards higher [OIII]/H$\beta$ and [NII]/$\alpha$ ratios
compared to the narrow component. We compare the location of the broad
components to shock models and find that the broad component could be due to
shocks, but we do not rule out other possibilities such as the presence of an
AGN. We estimate the mass loading factor (mass outflow rate/star formation
rate) assuming the broad component is a photoionized outflow and find that the
mass loading factor increases as a function of mass which agrees with previous
studies. We show that adding emission from shocked gas to $z\sim0$ SDSS spectra
shifts galaxies towards the location of $z\sim2$ galaxies on several emission
line diagnostic diagrams."
William Freeman,Freeman_William,arXiv:1707.05331,https://arxiv.org/abs/1707.05331,"Abstract:  We present the first spectroscopic measurement of multiple rest-frame optical
emission lines at $z>4$. During the MOSFIRE Deep Evolution Field (MOSDEF)
survey, we observed the galaxy GOODSN-17940 with the Keck I/MOSFIRE
spectrograph. The K-band spectrum of GOODSN-17940 includes significant
detections of the [OII]$\lambda\lambda 3726,3729$, [NeIII]$\lambda3869$, and
H$\gamma$ emission lines and a tentative detection of H$\delta$, indicating
$z_{\rm{spec}}=4.4121$. GOODSN-17940 is an actively star-forming $z>4$ galaxy
based on its K-band spectrum and broadband spectral energy distribution. A
significant excess relative to the surrounding continuum is present in the
Spitzer/IRAC channel 1 photometry of GOODSN-17940, due primarily to strong
H$\alpha$ emission with a rest-frame equivalent width of
$\mbox{EW(H}\alpha)=1200$ \AA. Based on the assumption of $0.5 Z_{\odot}$
models and the Calzetti attenuation curve, GOODSN-17940 is characterized by
$M_*=5.0^{+4.3}_{-0.2}\times 10^9 M_{\odot}$. The Balmer decrement inferred
from H$\alpha$/H$\gamma$ is used to dust correct the H$\alpha$ emission,
yielding $\mbox{SFR(H}\alpha)=320^{+190}_{-140} M_{\odot}\mbox{ yr}^{-1}$.
These $M_*$ and SFR values place GOODSN-17940 an order of magnitude in SFR
above the $z\sim 4$ star-forming ""main sequence."" Finally, we use the observed
ratio of [NeIII]/[OII] to estimate the nebular oxygen abundance in
GOODSN-17940, finding $\mbox{O/H}\sim 0.2 \mbox{ (O/H)}_{\odot}$. Combining our
new [NeIII]/[OII] measurement with those from stacked spectra at $z\sim 0, 2,
\mbox{ and } 3$, we show that GOODSN-17940 represents an extension to $z>4$ of
the evolution towards higher [NeIII]/[OII] (i.e., lower $\mbox{O/H}$) at fixed
stellar mass. It will be possible to perform the measurements presented here
out to $z\sim 10$ using the James Webb Space Telescope."
William Freeman,Freeman_William,arXiv:1703.10255,https://arxiv.org/abs/1703.10255,"Abstract:  Using observations from the first two years of the MOSFIRE Deep Evolution
Field (MOSDEF) survey, we study 13 AGN-driven outflows detected from a sample
of 67 X-ray, IR and/or optically-selected AGN at $z \sim 2$. The AGN have
bolometric luminosities of $\sim10^{44}-10^{46} ~\mathrm{erg~s^{-1}}$,
including both quasars and moderate-luminosity AGN. We detect blueshifted,
ionized gas outflows in the H$\beta$ , [OIII], H$\alpha$ ~and/or [NII] emission
lines of $19\%$ of the AGN, while only 1.8\% of the MOSDEF galaxies have
similarly-detected outflows. The outflow velocities span $\sim$300 to 1000 km
s$^{-1}$. Eight of the 13 outflows are spatially extended on similar scales as
the host galaxies, with spatial extents of 2.5 to 11.0 kpc. Outflows are
detected uniformly across the star-forming main sequence, showing little trend
with the host galaxy SFR. Line ratio diagnostics indicate that the outflowing
gas is photoionized by the AGN. We do not find evidence for positive AGN
feedback, in either our small MOSDEF sample or a much larger SDSS sample, using
the BPT diagram. Given that a galaxy with an AGN is ten times more likely to
have a detected outflow, the outflowing gas is photoionzed by the AGN, and
estimates of the mass and energy outflow rates indicate that stellar feedback
is insufficient to drive at least some of these outflows, they are very likely
to be AGN-driven. The outflows have mass-loading factors of the order of unity,
suggesting that they help regulate star formation in their host galaxies,
though they may be insufficient to fully quench it."
William Freeman,Freeman_William,arXiv:1701.04851,https://arxiv.org/abs/1701.04851,"Abstract:  We present a method for synthesizing a frontal, neutral-expression image of a
person's face given an input face photograph. This is achieved by learning to
generate facial landmarks and textures from features extracted from a
facial-recognition network. Unlike previous approaches, our encoding feature
vector is largely invariant to lighting, pose, and facial expression.
Exploiting this invariance, we train our decoder network using only frontal,
neutral-expression photographs. Since these photographs are well aligned, we
can decompose them into a sparse set of landmark points and aligned texture
maps. The decoder then predicts landmarks and textures independently and
combines them using a differentiable image warping operation. The resulting
images can be used for a number of applications, such as analyzing facial
attributes, exposure and white balance adjustment, or creating a 3-D avatar."
William Freeman,Freeman_William,arXiv:1610.07584,https://arxiv.org/abs/1610.07584,"Abstract:  We study the problem of 3D object generation. We propose a novel framework,
namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects
from a probabilistic space by leveraging recent advances in volumetric
convolutional networks and generative adversarial nets. The benefits of our
model are three-fold: first, the use of an adversarial criterion, instead of
traditional heuristic criteria, enables the generator to capture object
structure implicitly and to synthesize high-quality 3D objects; second, the
generator establishes a mapping from a low-dimensional probabilistic space to
the space of 3D objects, so that we can sample objects without a reference
image or CAD models, and explore the 3D object manifold; third, the adversarial
discriminator provides a powerful 3D shape descriptor which, learned without
supervision, has wide applications in 3D object recognition. Experiments
demonstrate that our method generates high-quality 3D objects, and our
unsupervisedly learned features achieve impressive performance on 3D object
recognition, comparable with those of supervised learning methods."
William Freeman,Freeman_William,arXiv:1609.04814,https://arxiv.org/abs/1609.04814,"Abstract:  We present results on the variation of 7.7 micron Polycyclic Aromatic
Hydrocarbon (PAH) emission in galaxies spanning a wide range in metallicity at
z ~ 2. For this analysis, we use rest-frame optical spectra of 476 galaxies at
1.37 < z < 2.61 from the MOSFIRE Deep Evolution Field (MOSDEF) survey to infer
metallicities and ionization states. Spitzer/MIPS 24 micron and Herschel/PACS
100 and 160 micron observations are used to derive rest-frame 7.7 micron
luminosities (L(7.7)) and total IR luminosities (L(IR)), respectively. We find
significant trends between the ratio of L(7.7) to L(IR) (and to dust-corrected
SFR) and both metallicity and [OIII]/[OII] (O32) emission-line ratio. The
latter is an empirical proxy for the ionization parameter. These trends
indicate a paucity of PAH emission in low metallicity environments with harder
and more intense radiation fields. Additionally, L(7.7)/L(IR) is significantly
lower in the youngest quartile of our sample (ages of 500 Myr) compared to
older galaxies, which may be a result of the delayed production of PAHs by AGB
stars. The relative strength of L(7.7) to L(IR) is also lower by a factor of ~
2 for galaxies with masses $M_* < 10^{10}M_{\odot}$, compared to the more
massive ones. We demonstrate that commonly-used conversions of L(7.7) (or 24
micron flux density; f(24)) to L(IR) underestimate the IR luminosity by more
than a factor of 2 at $M_*$ ~ $10^{9.6-10.0} M_{\odot}$. We adopt a
mass-dependent conversion of L(7.7) to L(IR) with L(7.7)/L(IR)= 0.09 and 0.22
for $M_* < 10^{10}$ and $> 10^{10} M_{\odot}$, respectively. Based on the new
scaling, the SFR-$M_*$ relation has a shallower slope than previously derived.
Our results also suggest a higher IR luminosity density at z ~ 2 than
previously measured, corresponding to a ~ 30% increase in the SFR density."
William Freeman,Freeman_William,arXiv:1609.01571,https://arxiv.org/abs/1609.01571,"Abstract:  We propose a novel method for template matching in unconstrained
environments. Its essence is the Best-Buddies Similarity (BBS), a useful,
robust, and parameter-free similarity measure between two sets of points. BBS
is based on counting the number of Best-Buddies Pairs (BBPs)--pairs of points
in source and target sets, where each point is the nearest neighbor of the
other. BBS has several key features that make it robust against complex
geometric deformations and high levels of outliers, such as those arising from
background clutter and occlusions. We study these properties, provide a
statistical analysis that justifies them, and demonstrate the consistent
success of BBS on a challenging real-world dataset while using different types
of features."
William Freeman,Freeman_William,arXiv:1608.07017,https://arxiv.org/abs/1608.07017,"Abstract:  The sound of crashing waves, the roar of fast-moving cars -- sound conveys
important information about the objects in our surroundings. In this work, we
show that ambient sounds can be used as a supervisory signal for learning
visual models. To demonstrate this, we train a convolutional neural network to
predict a statistical summary of the sound associated with a video frame. We
show that, through this process, the network learns a representation that
conveys information about objects and scenes. We evaluate this representation
on several recognition tasks, finding that its performance is comparable to
that of other state-of-the-art unsupervised learning methods. Finally, we show
through visualizations that the network learns units that are selective to
objects that are often associated with characteristic sounds."
William Freeman,Freeman_William,arXiv:1608.05890,https://arxiv.org/abs/1608.05890,"Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on
the identification, selection biases, and host galaxy properties of 55 X-ray,
IR and optically-selected active galactic nuclei (AGN) at $1.4 < z < 3.8$. We
obtain rest-frame optical spectra of galaxies and AGN and use the BPT diagram
to identify optical AGN. We examine the uniqueness and overlap of the AGN
identified at different wavelengths. There is a strong bias against identifying
AGN at any wavelength in low mass galaxies, and an additional bias against
identifying IR AGN in the most massive galaxies. AGN hosts span a wide range of
star formation rate (SFR), similar to inactive galaxies once stellar mass
selection effects are accounted for. However, we find (at $\sim 2-3\sigma$
significance) that IR AGN are in less dusty galaxies with relatively higher SFR
and optical AGN in dusty galaxies with relatively lower SFR. X-ray AGN
selection does not display a bias with host galaxy SFR. These results are
consistent with those from larger studies at lower redshifts. Within
star-forming galaxies, once selection biases are accounted for, we find AGN in
galaxies with similar physical properties as inactive galaxies, with no
evidence for AGN activity in particular types of galaxies. This is consistent
with AGN being fueled stochastically in any star-forming host galaxy. We do not
detect a significant correlation between SFR and AGN luminosity for individual
AGN hosts, which may indicate the timescale difference between the growth of
galaxies and their supermassive black holes."
William Freeman,Freeman_William,arXiv:1607.03034,https://arxiv.org/abs/1607.03034,"Abstract:  Originally developed to image the shadow region of the central black hole in
Sagittarius A* and in the nearby galaxy M87, the Event Horizon Telescope (EHT)
provides deep, very high angular resolution data on other AGN sources too. The
challenges of working with EHT data have spurred the development of new image
reconstruction algorithms. This work briefly reviews the status of the EHT and
its utility for observing AGN sources, with emphasis on novel imaging
techniques that offer the promise of better reconstructions at 1.3 mm and other
wavelengths."
William Freeman,Freeman_William,arXiv:1607.02586,https://arxiv.org/abs/1607.02586,"Abstract:  We study the problem of synthesizing a number of likely future frames from a
single input image. In contrast to traditional methods, which have tackled this
problem in a deterministic or non-parametric way, we propose a novel approach
that models future frames in a probabilistic manner. Our probabilistic model
makes it possible for us to sample and synthesize many possible future frames
from a single input image. Future frame synthesis is challenging, as it
involves low- and high-level image and motion understanding. We propose a novel
network structure, namely a Cross Convolutional Network to aid in synthesizing
future frames; this network structure encodes image and motion information as
feature maps and convolutional kernels, respectively. In experiments, our model
performs well on synthetic data, such as 2D shapes and animated game sprites,
as well as on real-wold videos. We also show that our model can be applied to
tasks such as visual analogy-making, and present an analysis of the learned
network representations."
William Freeman,Freeman_William,arXiv:1606.04107,https://arxiv.org/abs/1606.04107,"Abstract:  We present measurements of the electron-temperature based oxygen abundance
for a highly star-forming galaxy at z=3.08, COSMOS-1908. This is the highest
redshift at which [OIII]$\lambda$4363 has been detected, and the first time
that this line has been measured at z>2. We estimate an oxygen abundance of
12+log(O/H)$=8.00^{+0.13}_{-0.14}$. This galaxy is a low-mass ($10^{9.3}$
M$_{\odot}$), highly star-forming ($\sim50$ M$_{\odot}$ yr$^{-1}$) system that
hosts a young stellar population ($\sim160$ Myr). We investigate the physical
conditions of the ionized gas in COSMOS-1908 and find that this galaxy has a
high ionization parameter, little nebular reddening ($E(B-V)_{\rm gas}<0.14$),
and a high electron density ($n_e\sim500$ cm$^{-3}$). We compare the ratios of
strong oxygen, neon, and hydrogen lines to the direct-method oxygen abundance
for COSMOS-1908 and additional star-forming galaxies at z=0-1.8 with
[OIII]$\lambda$4363 measurements, and show that galaxies at z$\sim$1-3 follow
the same strong-line correlations as galaxies in the local universe. This
agreement suggests that the relationship between ionization parameter and O/H
is similar for z$\sim$0 and high-redshift galaxies. These results imply that
metallicity calibrations based on lines of oxygen, neon, and hydrogen do not
strongly evolve with redshift and can reliably estimate abundances out to
z$\sim$3, paving the way for robust measurements of the evolution of the
mass-metallicity relation to high redshift."
William Freeman,Freeman_William,arXiv:1606.00469,https://arxiv.org/abs/1606.00469,"Abstract:  [Abridged] We present a robust measurement of the rest-frame UV luminosity
function (LF) and its evolution during the peak epoch of cosmic star formation
at 1<z<3. We use our deep near ultraviolet imaging from WFC3/UVIS on the Hubble
Space Telescope (HST) and existing ACS/WFC and WFC3/IR imaging of three lensing
galaxy clusters, Abell 2744 and MACSJ0717 from the Hubble Frontier Field survey
and Abell 1689. We use photometric redshifts to identify 780 ultra-faint
galaxies with $M_{UV}$<-12.5 AB mag at 1<z<3. From these samples, we identified
5 new, faint, multiply imaged systems in A1689. We compute the rest-frame UV LF
and find the best-fit faint-end slopes of $\alpha=-1.56\pm0.04$,
$\alpha=-1.72\pm0.04$ and $\alpha=-1.94\pm0.06$ at 1.0<z<1.6, 1.6<z<2.2 and
2.2<z<3.0, respectively. Our results demonstrate that the UV LF becomes steeper
from z\sim1.3 to z\sim2.6 with no sign of a turnover down to $M_{UV}=-14$ AB
mag. We further derive the UV LFs using the Lyman break ""dropout"" selection and
confirm the robustness of our conclusions against different selection
methodologies. Because the sample sizes are so large, and extend to such faint
luminosities, the statistical uncertainties are quite small, and systematic
uncertainties (due to the assumed size distribution, for example), likely
dominate. If we restrict our analysis to galaxies and volumes above > 50%
completeness in order to minimize these systematics, we still find that the
faint-end slope is steep and getting steeper with redshift, though with
slightly shallower (less negative) values ($\alpha=-1.55\pm0.06$,
$-1.69\pm0.07$ and $-1.79\pm0.08$ for $z\sim1.3$, 1.9 and 2.6, respectively).
Finally, we conclude that the faint star-forming galaxies with UV magnitudes of
$-18.5<M_{UV}<-12.5$ covered in this study, produce the majority (55%-60%) of
the unobscured UV luminosity density at 1<z<3."
William Freeman,Freeman_William,arXiv:1605.01138,https://arxiv.org/abs/1605.01138,"Abstract:  Humans demonstrate remarkable abilities to predict physical events in complex
scenes. Two classes of models for physical scene understanding have recently
been proposed: ""Intuitive Physics Engines"", or IPEs, which posit that people
make predictions by running approximate probabilistic simulations in causal
mental models similar in nature to video-game physics engines, and memory-based
models, which make judgments based on analogies to stored experiences of
previously encountered scenes and physical outcomes. Versions of the latter
have recently been instantiated in convolutional neural network (CNN)
architectures. Here we report four experiments that, to our knowledge, are the
first rigorous comparisons of simulation-based and CNN-based models, where both
approaches are concretely instantiated in algorithms that can run on raw image
inputs and produce as outputs physical judgments such as whether a stack of
blocks will fall. Both approaches can achieve super-human accuracy levels and
can quantitatively predict human judgments to a similar degree, but only the
simulation-based models generalize to novel situations in ways that people do,
and are qualitatively consistent with systematic perceptual illusions and
judgment asymmetries that people show."
William Freeman,Freeman_William,arXiv:1604.08685,https://arxiv.org/abs/1604.08685,"Abstract:  Understanding 3D object structure from a single image is an important but
difficult task in computer vision, mostly due to the lack of 3D object
annotations in real images. Previous work tackles this problem by either
solving an optimization task given 2D keypoint positions, or training on
synthetic data with ground truth 3D information. In this work, we propose 3D
INterpreter Network (3D-INN), an end-to-end framework which sequentially
estimates 2D keypoint heatmaps and 3D object structure, trained on both real
2D-annotated images and synthetic 3D data. This is made possible mainly by two
technical innovations. First, we propose a Projection Layer, which projects
estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D
structural parameters supervised by 2D annotations on real images. Second,
heatmaps of keypoints serve as an intermediate representation connecting real
and synthetic data, enabling 3D-INN to benefit from the variation and abundance
of synthetic 3D objects, without suffering from the difference between the
statistics of real and synthesized images due to imperfect rendering. The
network achieves state-of-the-art performance on both 2D keypoint estimation
and 3D structure recovery. We also show that the recovered 3D information can
be used in other vision applications, such as 3D rendering and image retrieval."
William Freeman,Freeman_William,arXiv:1603.02284,https://arxiv.org/abs/1603.02284,"Abstract:  We present the first direct comparison between Balmer line and panchromatic
SED-based SFRs for z~2 galaxies. For this comparison we used 17 star-forming
galaxies selected from the MOSFIRE Deep Evolution Field (MOSDEF) survey, with
$3\sigma$ detections for H$\alpha$ and at least two IR bands (Spitzer/MIPS
24$\mu$m and Herschel/PACS 100 and 160$\mu$m, and in some cases Herschel/SPIRE
250, 350, and 500$\mu$m). The galaxies have total IR (8-1000$\mu$m)
luminosities of $\sim10^{11.4}-10^{12.4}\,\textrm{L}_\odot$ and star-formation
rates (SFRs) of $\sim30-250\,\textrm{M}_\odot\,\mathrm{yr^{-1}}$. We fit the
UV-to-far-IR SEDs with flexible stellar population synthesis (FSPS) models -
which include both stellar and dust emission - and compare the inferred SFRs
with the SFR(H$\alpha$,H$\beta$) values corrected for dust attenuation using
Balmer decrements. The two SFRs agree with a scatter of 0.17 dex. Our results
imply that the Balmer decrement accurately predicts the obscuration of the
nebular lines and can be used to robustly calculate SFRs for star-forming
galaxies at z~2 with SFRs up to $\sim200\,\textrm{M}_\odot\,\mathrm{yr^{-1}}$.
We also use our data to assess SFR indicators based on modeling the
UV-to-mid-IR SEDs or by adding SFR(UV) and SFR(IR), for which the latter is
based on the mid-IR only or on the full IR SED. All these SFRs show a poorer
agreement with SFR(H$\alpha$,H$\beta$) and in some cases large systematic
biases are observed. Finally, we show that the SFR and dust attenuation derived
from the UV-to-near-IR SED alone are unbiased when assuming a delayed
exponentially declining star-formation history."
William Freeman,Freeman_William,arXiv:1512.08512,https://arxiv.org/abs/1512.08512,"Abstract:  Objects make distinctive sounds when they are hit or scratched. These sounds
reveal aspects of an object's material properties, as well as the actions that
produced them. In this paper, we propose the task of predicting what sound an
object makes when struck as a way of studying physical interactions within a
visual scene. We present an algorithm that synthesizes sound from silent videos
of people hitting and scratching objects with a drumstick. This algorithm uses
a recurrent neural network to predict sound features from videos and then
produces a waveform from these features with an example-based synthesis
procedure. We show that the sounds predicted by our model are realistic enough
to fool participants in a ""real or fake"" psychophysical experiment, and that
they convey significant information about material properties and physical
interactions."
William Freeman,Freeman_William,arXiv:1512.01413,https://arxiv.org/abs/1512.01413,"Abstract:  Very long baseline interferometry (VLBI) is a technique for imaging celestial
radio emissions by simultaneously observing a source from telescopes
distributed across Earth. The challenges in reconstructing images from fine
angular resolution VLBI data are immense. The data is extremely sparse and
noisy, thus requiring statistical image models such as those designed in the
computer vision community. In this paper we present a novel Bayesian approach
for VLBI image reconstruction. While other methods often require careful tuning
and parameter selection for different types of data, our method (CHIRP)
produces good results under different settings such as low SNR or extended
emission. The success of our method is demonstrated on realistic synthetic
experiments as well as publicly available real data. We present this problem in
a way that is accessible to members of the community, and provide a dataset
website (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons
across algorithms."
William Freeman,Freeman_William,arXiv:1511.03272,https://arxiv.org/abs/1511.03272,"Abstract:  We present H$\alpha$ gas kinematics for 178 star-forming galaxies at z~2 from
the MOSFIRE Deep Evolution Field survey. We have developed models to interpret
the kinematic measurements from fixed-angle multi-object spectroscopy, using
structural parameters derived from CANDELS HST/F160W imaging. For 35 galaxies
we measure resolved rotation with a median $(V/\sigma_{V,0})_{R_E}=2.1$. We
derive dynamical masses from the kinematics and sizes and compare them to
baryonic masses, with gas masses estimated from dust-corrected H$\alpha$ star
formation rates (SFRs) and the Kennicutt-Schmidt relation. When assuming that
galaxies with and without observed rotation have the same median
$(V/\sigma_{V,0})_{R_E}$, we find good agreement between the dynamical and
baryonic masses, with a scatter of $\sigma_{RMS}=0.34$dex and a median offset
of $\Delta\log_{10}M=0.04$dex. This comparison implies a low dark matter
fraction (8% within an effective radius) for a Chabrier initial mass function
(IMF), and disfavors a Salpeter IMF. Moreover, the requirement that
$M_{dyn}/M_{baryon}$ should be independent of inclination yields a median value
of $(V/\sigma_{V,0})_{R_E}=2.1$ for galaxies without observed rotation. If
instead we treat the galaxies without detected rotation as early-type galaxies,
the masses are also in reasonable agreement ($\Delta\log_{10}M=-0.07$dex,
$\sigma_{RMS}=0.37$dex). The inclusion of gas masses is critical in this
comparison; if gas masses are excluded there is an increasing trend of
$M_{dyn}/M_{*}$ with higher specific SFR (SSFR). Furthermore, we find
indications that $V/\sigma$ decreases with increasing H$\alpha$ SSFR for our
full sample, which may reflect disk settling. We also study the Tully-Fisher
relation and find that at fixed stellar mass
$S_{0.5}=(0.5V_{2.2}^2+\sigma_{V,0}^2)^{1/2}$ was higher at earlier times. At
fixed baryonic mass, we observe the opposite trend. [abridged]"
William Freeman,Freeman_William,arXiv:1509.03636,https://arxiv.org/abs/1509.03636,"Abstract:  Using observations from the MOSFIRE Deep Evolution Field (MOSDEF) survey, we
investigate the physical conditions of star-forming regions in $z\sim2.3$
galaxies, specifically the electron density and ionization state. From
measurements of the [O II]$\lambda\lambda$3726,3729 and [S
II]$\lambda\lambda$6716,6731 doublets, we find a median electron density of
$\sim250$ cm$^{-3}$ at $z\sim2.3$, an increase of an order of magnitude
compared to measurements of galaxies at $z\sim0$. While $z\sim2.3$ galaxies are
offset towards significantly higher O$_{32}$ values relative to local galaxies
at fixed stellar mass, we find that the high-redshift sample follows a similar
distribution to the low-metallicity tail of the local distribution in the
O$_{32}$ vs. R$_{23}$ and O3N2 diagrams. Based on these results, we propose
that $z\sim2.3$ star-forming galaxies have the same ionization parameter as
local galaxies at fixed metallicity. In combination with simple photoionization
models, the position of local and $z\sim2.3$ galaxies in excitation diagrams
suggests that there is no significant change in the hardness of the ionizing
spectrum at fixed metallicity from $z\sim0$ to $z\sim2.3$. We find that
$z\sim2.3$ galaxies show no offset compared to low-metallicity local galaxies
in emission line ratio diagrams involving only lines of hydrogen, oxygen, and
sulfur, but show a systematic offset in diagrams involving [N II]$\lambda$6584.
We conclude that the offset of $z\sim2.3$ galaxies from the local star-forming
sequence in the [N II] BPT diagram is primarily driven by elevated N/O at fixed
O/H compared to local galaxies. These results suggest that the local gas-phase
and stellar metallicity sets the ionization state of star-forming regions at
$z\sim0$ and $z\sim2$."
William Freeman,Freeman_William,arXiv:1507.03017,https://arxiv.org/abs/1507.03017,"Abstract:  We present results on the star-formation rate (SFR) versus stellar mass
($M_*$) relation (i.e., the ""main sequence"") among star-forming galaxies at
1.37$\leq$$z$$\leq$2.61 using the MOSFIRE Deep Evolution Field (MOSDEF) survey.
Based on a sample of 261 galaxies with H$\alpha$ and H$\beta$ spectroscopy, we
have estimated robust dust-corrected instantaneous SFRs over a large range in
$M_*$ ($\sim10^{9.5}-10^{11.5}M_\odot$). We find a correlation between
log(SFR(H$\alpha$)) and log($M_*$) with a slope of 0.65$\pm$0.08
(0.58$\pm$0.10) at 1.4<z<2.6 (2.1<z<2.6). We find that different assumptions
for the dust correction, such as using the color-excess of the stellar
continuum to correct the nebular lines, sample selection biases against red
star-forming galaxies, and not accounting for Balmer absorption can yield
steeper slopes of the log(SFR)-log($M_*$) relation. Our sample is immune from
these biases as it is rest-frame optically selected, H$\alpha$ and H$\beta$ are
corrected for Balmer absorption, and the H$\alpha$ luminosity is dust-corrected
using the nebular color-excess computed from the Balmer decrement. The scatter
of the log(SFR(H$\alpha$))-log($M_*$) relation, after accounting for the
measurement uncertainties, is 0.31 dex at 2.1<z<2.6, which is 0.05 dex larger
than the scatter in log(SFR(UV))-log($M_*$). Based on comparisons to a
simulated SFR-$M_*$ relation with some intrinsic scatter, we argue that in the
absence of direct measurements of galaxy-to-galaxy variations in the
attenuation/extinction curves and the IMF, one cannot use the difference in the
scatter of the SFR(H$\alpha$)- and SFR(UV)-$M_*$ relations to constrain the
stochasticity of star formation in high-redshift galaxies."
William Freeman,Freeman_William,arXiv:1507.02379,https://arxiv.org/abs/1507.02379,"Abstract:  Convolutional Neural Network (CNN) has been successful in image recognition
tasks, and recent works shed lights on how CNN separates different classes with
the learned inter-class knowledge through visualization. In this work, we
instead visualize the intra-class knowledge inside CNN to better understand how
an object class is represented in the fully-connected layers.
To invert the intra-class knowledge into more interpretable images, we
propose a non-parametric patch prior upon previous CNN visualization models.
With it, we show how different ""styles"" of templates for an object class are
organized by CNN in terms of location and content, and represented in a
hierarchical and ensemble way. Moreover, such intra-class knowledge can be used
in many interesting applications, e.g. style-based image retrieval and
style-based object completion."
William Freeman,Freeman_William,arXiv:1504.02782,https://arxiv.org/abs/1504.02782,"Abstract:  We present results on the dust attenuation curve of z~2 galaxies using early
observations from the MOSFIRE Deep Evolution Field (MOSDEF) survey. Our sample
consists of 224 star-forming galaxies with nebular spectroscopic redshifts in
the range z= 1.36-2.59 and high S/N measurements of, or upper limits on, the
H-alpha and H-beta emission lines obtained with Keck/MOSFIRE. We construct
composite SEDs of galaxies in bins of specific SFR and Balmer optical depth in
order to directly constrain the dust attenuation curve from the UV through
near-IR for typical star-forming galaxies at high redshift. Our results imply
an attenuation curve that is very similar to the SMC extinction curve at
wavelengths redward of 2500 Angstroms. At shorter wavelengths, the shape of the
curve is identical to that of the Calzetti relation, but with a lower
normalization (R_V). Hence, the new attenuation curve results in SFRs that are
~20% lower, and log stellar masses that are 0.16 dex lower, than those obtained
with the Calzetti attenuation curve. Moreover, we find that the difference in
the reddening---and the total attenuation---of the ionized gas and stellar
continuum correlates strongly with SFR, such that for dust-corrected SFRs
larger than 20 Msun/yr assuming a Chabrier IMF, the nebular emission lines
suffer an increasing degree of obscuration relative to the continuum. A simple
model that can account for these trends is one in which the UV through optical
stellar continuum is dominated by a population of less reddened stars, while
the nebular line and bolometric luminosities become increasingly dominated by
dustier stellar populations for galaxies with large SFRs, as a result of the
increased dust enrichment that accompanies such galaxies. Consequently, UV- and
SED-based SFRs may underestimate the total SFR at even modest levels of ~20
Msun/yr. [Abridged]"
William Freeman,Freeman_William,arXiv:1412.1835,https://arxiv.org/abs/1412.1835,"Abstract:  In this paper we present the MOSFIRE Deep Evolution Field (MOSDEF) survey.
The MOSDEF survey aims to obtain moderate-resolution (R=3000-3650) rest-frame
optical spectra (~3700-7000 Angstrom) for ~1500 galaxies at 1.37<z<3.80 in
three well-studied CANDELS fields: AEGIS, COSMOS, and GOODS-N. Targets are
selected in three redshift intervals: 1.37<z<1.70, 2.09<z<2.61, and
2.95<z<3.80, down to fixed H_AB (F160W) magnitudes of 24.0, 24.5 and 25.0,
respectively, using the photometric and spectroscopic catalogs from the 3D-HST
survey. We target both strong nebular emission lines (e.g., [OII], Hbeta,
[OIII], 5008, Halpha, [NII], and [SII]) and stellar continuum and absorption
features (e.g., Balmer lines, Ca-II H and K, Mgb, 4000 Angstrom break). Here we
present an overview of our survey, the observational strategy, the data
reduction and analysis, and the sample characteristics based on spectra
obtained during the first 24 nights. To date, we have completed 21 masks,
obtaining spectra for 591 galaxies. For ~80% of the targets we derive a robust
redshift from either emission or absorption lines. In addition, we confirm 55
additional galaxies, which were serendipitously detected. The MOSDEF galaxy
sample includes unobscured star-forming, dusty star-forming, and quiescent
galaxies and spans a wide range in stellar mass (~10^9-10^11.5 Msol) and star
formation rate (~10^0-10^3 Msol/yr). The spectroscopically confirmed sample is
roughly representative of an H-band limited galaxy sample at these redshifts.
With its large sample size, broad diversity in galaxy properties, and wealth of
available ancillary data, MOSDEF will transform our understanding of the
stellar, gaseous, metal, dust, and black hole content of galaxies during the
time when the universe was most active."
William Freeman,Freeman_William,arXiv:1409.7071,https://arxiv.org/abs/1409.7071,"Abstract:  We present results on the excitation properties of z~2.3 galaxies using early
observations from the MOSFIRE Deep Evolution Field (MOSDEF) Survey. With its
coverage of the full suite of strong rest-frame optical emission lines, MOSDEF
provides an unprecedented view of the rest-frame optical spectra of a
representative sample of distant star-forming galaxies. We investigate the
locations of z~2.3 MOSDEF galaxies in multiple emission-line diagnostic
diagrams. These include the [OIII]/Hb vs. [NII]/Ha and [OIII]/Hb vs. [SII]/Ha
""BPT"" diagrams, as well as the O_32 vs. R_23 excitation diagram. We recover the
well-known offset in the star-forming sequence of high-redshift galaxies in the
[OIII]/Hb vs. [NII]/Ha BPT diagram relative to SDSS star-forming galaxies.
However, the shift for our rest-frame optically selected sample is less
significant than for rest-frame-UV selected and emission-line selected galaxies
at z~2. Furthermore, we find that the offset is mass-dependent, only appearing
within the low-mass half of the z~2.3 MOSDEF sample, where galaxies are shifted
towards higher [NII]/Ha at fixed [OIII]/Hb. Within the [OIII]/Hb vs. [SII]/Ha
and O_32 vs. R_23 diagrams, we find that z~2.3 galaxies are distributed like
local ones, and therefore attribute the shift in the [OIII]/Hb vs. [NII]/Ha BPT
diagram to elevated N/O abundance ratios among lower-mass (M_*<10^10 M_sun)
high-redshift galaxies. The variation in N/O ratios calls into question the use
at high redshift of oxygen abundance indicators based on nitrogen lines, but
the apparent invariance with redshift of the excitation sequence in the O_32
vs. R_23 diagram paves the way for using the combination of O_32 and R_23 as an
unbiased metallicity indicator over a wide range in redshift. This indicator
will allow for an accurate characterization of the shape and normalization of
the mass-metallicity relationship over more than 10 Gyr."
William Freeman,Freeman_William,arXiv:1409.6522,https://arxiv.org/abs/1409.6522,"Abstract:  We present results from the MOSFIRE Deep Evolution Field (MOSDEF) survey on
rest-frame optical AGN identification and completeness at z~2.3. With our
sample of 50 galaxies and 10 X-ray and IR-selected AGN with measured H-beta,
[OIII], H-alpha, and [NII] emission lines, we investigate the location of AGN
in the BPT, MEx (mass-excitation), and CEx (color-excitation) diagrams. We find
that the BPT diagram works well to identify AGN at z~2.3 and that the z~0
AGN/star-forming galaxy classifications do not need to shift substantially at
z~2.3 to robustly separate these populations. However, the MEx diagram fails to
identify all of the AGN identified in the BPT diagram, and the CEx diagram is
substantially contaminated at high redshift. We further show that AGN samples
selected using the BPT diagram have selection biases in terms of both host
stellar mass and stellar population, in that AGN in low mass and/or high
specific star formation rate galaxies are difficult to identify using the BPT
diagram. These selection biases become increasingly severe at high redshift,
such that optically-selected AGN samples at high redshift will necessarily be
incomplete. We also find that the gas in the narrow-line region appears to be
more enriched than gas in the host galaxy for at least some MOSDEF AGN.
However, AGN at z~2 are generally less enriched than local AGN with the same
host stellar mass."
William Freeman,Freeman_William,arXiv:1409.4690,https://arxiv.org/abs/1409.4690,"Abstract:  The image of the emission surrounding the black hole in the center of the
Milky Way is predicted to exhibit the imprint of general relativistic (GR)
effects, including the existence of a shadow feature and a photon ring of
diameter ~50 microarcseconds. Structure on these scales can be resolved by
millimeter-wavelength very long baseline interferometry (VLBI). However,
strong-field GR features of interest will be blurred at lambda >= 1.3 mm due to
scattering by interstellar electrons. The scattering properties are well
understood over most of the relevant range of baseline lengths, suggesting that
the scattering may be (mostly) invertible. We simulate observations of a model
image of Sgr A* and demonstrate that the effects of scattering can indeed be
mitigated by correcting the visibilities before reconstructing the image. This
technique is also applicable to Sgr A* at longer wavelengths."
William Freeman,Freeman_William,arXiv:1408.2521,https://arxiv.org/abs/1408.2521,"Abstract:  We present results on the z~2.3 mass-metallicity relation (MZR) using early
observations from the MOSFIRE Deep Evolution Field (MOSDEF) survey. We use an
initial sample of 87 star-forming galaxies with spectroscopic coverage of
H\beta, [OIII]\lambda 5007, H\alpha, and [NII]\lambda 6584 rest-frame optical
emission lines, and estimate the gas-phase oxygen abundance based on the N2 and
O3N2 strong-line indicators. We find a positive correlation between stellar
mass and metallicity among individual z~2.3 galaxies using both the N2 and O3N2
indicators. We also measure the emission-line ratios and corresponding oxygen
abundances for composite spectra in bins of stellar mass. Among composite
spectra, we find a monotonic increase in metallicity with increasing stellar
mass, offset ~0.15-0.3 dex below the local MZR. When the sample is divided at
the median star-formation rate (SFR), we do not observe significant SFR
dependence of the z~2.3 MZR among either individual galaxies or composite
spectra. We furthermore find that z~2.3 galaxies have metallicities ~0.1 dex
lower at a given stellar mass and SFR than is observed locally. This offset
suggests that high-redshift galaxies do not fall on the local ""fundamental
metallicity relation"" among stellar mass, metallicity, and SFR, and may provide
evidence of a phase of galaxy growth in which the gas reservoir is built up due
to inflow rates that are higher than star-formation and outflow rates. However,
robust conclusions regarding the gas-phase oxygen abundances of high-redshift
galaxies await a systematic reappraisal of the application of locally
calibrated metallicity indicators at high redshift."
William Freeman,Freeman_William,arXiv:1408.1420,https://arxiv.org/abs/1408.1420,"Abstract:  We present deep spectroscopy of 17 very low mass (M* ~ 2.0x10^6 Msun to
1.4x10^9 Msun) and low luminosity (M_UV ~ -13.7 to -19.9) gravitationally
lensed galaxies in the redshift range z~1.5-3.0. Deep rest-frame ultraviolet
spectra reveal large equivalent width emission from numerous lines (NIV],
OIII], CIV, Si III], CIII]) which are rarely seen in individual spectra of more
massive star forming galaxies. CIII] is detected in 16 of 17 low mass star
forming systems with rest-frame equivalent widths as large as 13.5 Angstroms.
Nebular CIV emission is present in the most extreme CIII] emitters, requiring
an ionizing source capable of producing a substantial component of photons with
energies in excess of 47.9 eV. Photoionization models support a picture whereby
the large equivalent widths are driven by the increased electron temperature
and enhanced ionizing output arising from metal poor gas and stars, young
stellar populations, and large ionization parameters. The young ages implied by
the emission lines and continuum SEDs indicate that the extreme line emitters
in our sample are in the midst of a significant upturn in their star formation
activity. The low stellar masses, blue UV colors, and large sSFRs of our sample
are similar to those of typical z>6 galaxies. Given the strong attenuation of
Ly-alpha in z>6 galaxies we suggest that CIII] is likely to provide our best
probe of early star forming galaxies with ground-based spectrographs and one of
the most efficient means of confirming z>10 galaxies with the James Webb Space
Telescope."
William Freeman,Freeman_William,arXiv:1305.2413,https://arxiv.org/abs/1305.2413,"Abstract:  We identified the z~2 Lyman break galaxies using deep HST ultraviolet
(F275W/F336W) imaging of Abell 1689. Because of the imaging depth and the large
magnification provided by the cluster, we detect galaxies 100x fainter (-19.5<
M_1500 <-13) than previous surveys at this redshift. We are able to calculate
the intrinsic sensitivity of the observations as a function of source plane
position, allowing determinations of effective volume as a function of
luminosity. We fit the faint-end slope of the luminosity function to be alpha =
-1.74 +/-0.08, consistent with the values obtained for 2.5 < z < 6. There is no
turnover in the luminosity function down to MUV = -13. The trend of
increasingly redder UV spectral slopes with luminosity at higher redshifts is
observed in our sample, but with redder slopes at all luminosities and average
reddening of < E(B - V) >= 0.15. We assume the stars in these galaxies are
metal poor (0.2Z_sun) compared to their brighter counterparts (Z_sun),
resulting in bluer assumed intrinsic UV slopes and larger derived dust
extinction. The total UV luminosity density at z ~ 2 is 4.31x10^26
erg/s/Hz/Mpc^3, more than 70% of which is emitted by galaxies in the luminosity
range of our sample. We determine the star formation rate density at z ~ 2
(assuming constant dust extinction correction of 4.2 over all luminosities and
a Kroupa IMF) of 0.148 M/yr/Mpc^3, significantly higher than previous
determinations because of the additional population of fainter galaxies and the
larger dust correction factors.[abridged]"
William Freeman,Freeman_William,arXiv:1210.4856,https://arxiv.org/abs/1210.4856,"Abstract:  The recent proliferation of richly structured probabilistic models raises the
question of how to automatically determine an appropriate model for a dataset.
We investigate this question for a space of matrix decomposition models which
can express a variety of widely used models from unsupervised learning. To
enable model selection, we organize these models into a context-free grammar
which generates a wide variety of structures through the compositional
application of a few simple rules. We use our grammar to generically and
efficiently infer latent components and estimate predictive likelihood for
nearly 2500 structures using a small toolbox of reusable algorithms. Using a
greedy search over our grammar, we automatically choose the decomposition
structure from raw data by evaluating only a small fraction of all models. The
proposed method typically finds the correct structure for synthetic data and
backs off gracefully to simpler models under heavy noise. It learns sensible
structures for datasets as diverse as image patches, motion capture, 20
Questions, and U.S. Senate votes, all using exactly the same code."
William Freeman,Freeman_William,arXiv:0901.4275,https://arxiv.org/abs/0901.4275,"Abstract:  Compressed sensing is a recent set of mathematical results showing that
sparse signals can be exactly reconstructed from a small number of linear
measurements. Interestingly, for ideal sparse signals with no measurement
noise, random measurements allow perfect reconstruction while measurements
based on principal component analysis (PCA) or independent component analysis
(ICA) do not. At the same time, for other signal and noise distributions, PCA
and ICA can significantly outperform random projections in terms of enabling
reconstruction from a small number of measurements. In this paper we ask: given
the distribution of signals we wish to measure, what are the optimal set of
linear projections for compressed sensing? We consider the problem of finding a
small number of linear projections that are maximally informative about the
signal. Formally, we use the InfoMax criterion and seek to maximize the mutual
information between the signal, x, and the (possibly noisy) projection y=Wx. We
show that in general the optimal projections are not the principal components
of the data nor random projections, but rather a seemingly novel set of
projections that capture what is still uncertain about the signal, given the
knowledge of distribution. We present analytic solutions for certain special
cases including natural images. In particular, for natural images, the
near-optimal projections are bandwise random, i.e., incoherent to the sparse
bases at a particular frequency band but with more weights on the
low-frequencies, which has a physical relation to the multi-resolution
representation of images."
James Fujimoto,Fujimoto_James,arXiv:1802.03943,https://arxiv.org/abs/1802.03943,"Abstract:  This paper introduces an universal and structure-preserving regularization
term, called quantile sparse image (QuaSI) prior. The prior is suitable for
denoising images from various medical imaging modalities. We demonstrate its
effectiveness on volumetric optical coherence tomography (OCT) and computed
tomography (CT) data, which show different noise and image characteristics. OCT
offers high-resolution scans of the human retina but is inherently impaired by
speckle noise. CT on the other hand has a lower resolution and shows
high-frequency noise. For the purpose of denoising, we propose a variational
framework based on the QuaSI prior and a Huber data fidelity model that can
handle 3-D and 3-D+t data. Efficient optimization is facilitated through the
use of an alternating direction method of multipliers (ADMM) scheme and the
linearization of the quantile filter. Experiments on multiple datasets
emphasize the excellent performance of the proposed method."
James Fujimoto,Fujimoto_James,arXiv:1703.02942,https://arxiv.org/abs/1703.02942,"Abstract:  Optical coherence tomography (OCT) enables high-resolution and non-invasive
3D imaging of the human retina but is inherently impaired by speckle noise.
This paper introduces a spatio-temporal denoising algorithm for OCT data on a
B-scan level using a novel quantile sparse image (QuaSI) prior. To remove
speckle noise while preserving image structures of diagnostic relevance, we
implement our QuaSI prior via median filter regularization coupled with a Huber
data fidelity model in a variational approach. For efficient energy
minimization, we develop an alternating direction method of multipliers (ADMM)
scheme using a linearization of median filtering. Our spatio-temporal method
can handle both, denoising of single B-scans and temporally consecutive
B-scans, to gain volumetric OCT data with enhanced signal-to-noise ratio. Our
algorithm based on 4 B-scans only achieved comparable performance to averaging
13 B-scans and outperformed other current denoising methods."
Robert Gallager,Gallager_Robert,arXiv:0812.2709,https://arxiv.org/abs/0812.2709,"Abstract:  Schalkwijk and Kailath (1966) developed a class of block codes for Gaussian
channels with ideal feedback for which the probability of decoding error
decreases as a second-order exponent in block length for rates below capacity.
This well-known but surprising result is explained and simply derived here in
terms of a result by Elias (1956) concerning the minimum mean-square distortion
achievable in transmitting a single Gaussian random variable over multiple uses
of the same Gaussian channel. A simple modification of the Schalkwijk-Kailath
scheme is then shown to have an error probability that decreases with an
exponential order which is linearly increasing with block length. In the
infinite bandwidth limit, this scheme produces zero error probability using
bounded expected energy at all rates below capacity. A lower bound on error
probability for the finite bandwidth case is then derived in which the error
probability decreases with an exponential order which is linearly increasing in
block length at the same rate as the upper bound."
David Gifford,Gifford_David,arXiv:1810.03805,https://arxiv.org/abs/1810.03805,"Abstract:  Local explanation frameworks aim to rationalize particular decisions made by
a black-box prediction model. Existing techniques are often restricted to a
specific type of predictor or based on input saliency, which may be undesirably
sensitive to factors unrelated to the model's decision making process. We
instead propose sufficient input subsets that identify minimal subsets of
features whose observed values alone suffice for the same decision to be
reached, even if all other input feature values are missing. General principles
that globally govern a model's decision-making can also be revealed by
searching for clusters of such input patterns across many data points. Our
approach is conceptually straightforward, entirely model-agnostic, simply
implemented using instance-wise backward selection, and able to produce more
concise rationales than existing techniques. We demonstrate the utility of our
interpretation method on various neural network models trained on text, image,
and genomic data."
David Gifford,Gifford_David,arXiv:1511.04486,https://arxiv.org/abs/1511.04486,"Abstract:  We present a nonparametric framework to model a short sequence of probability
distributions that vary both due to underlying effects of sequential
progression and confounding noise. To distinguish between these two types of
variation and estimate the sequential-progression effects, our approach
leverages an assumption that these effects follow a persistent trend. This work
is motivated by the recent rise of single-cell RNA-sequencing experiments over
a brief time course, which aim to identify genes relevant to the progression of
a particular biological process across diverse cell populations. While
classical statistical tools focus on scalar-response regression or
order-agnostic differences between distributions, it is desirable in this
setting to consider both the full distributions as well as the structure
imposed by their ordering. We introduce a new regression model for ordinal
covariates where responses are univariate distributions and the underlying
relationship reflects consistent changes in the distributions over increasing
levels of the covariate. This concept is formalized as a ""trend"" in
distributions, which we define as an evolution that is linear under the
Wasserstein metric. Implemented via a fast alternating projections algorithm,
our method exhibits numerous strengths in simulations and analyses of
single-cell gene expression data."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1805.02351,https://arxiv.org/abs/1805.02351,"Abstract:  In this paper we study the fine-grained complexity of finding exact and
approximate solutions to problems in P. Our main contribution is showing
reductions from exact to approximate solution for a host of such problems.
As one (notable) example, we show that the Closest-LCS-Pair problem (Given
two sets of strings $A$ and $B$, compute exactly the maximum $\textsf{LCS}(a,
b)$ with $(a, b) \in A \times B$) is equivalent to its approximation version
(under near-linear time reductions, and with a constant approximation factor).
More generally, we identify a class of problems, which we call BP-Pair-Class,
comprising both exact and approximate solutions, and show that they are all
equivalent under near-linear time reductions.
Exploring this class and its properties, we also show:
$\bullet$ Under the NC-SETH assumption (a significantly more relaxed
assumption than SETH), solving any of the problems in this class requires
essentially quadratic time.
$\bullet$ Modest improvements on the running time of known algorithms
(shaving log factors) would imply that NEXP is not in non-uniform
$\textsf{NC}^1$.
$\bullet$ Finally, we leverage our techniques to show new barriers for
deterministic approximation algorithms for LCS.
At the heart of these new results is a deep connection between interactive
proof systems for bounded-space computations and the fine-grained complexity of
exact and approximate solutions to problems in P. In particular, our results
build on the proof techniques from the classical IP = PSPACE result."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1803.02540,https://arxiv.org/abs/1803.02540,"Abstract:  We introduce a new coordination problem in distributed computing that we call
the population stability problem. A system of agents each with limited memory
and communication, as well as the ability to replicate and self-destruct, is
subjected to attacks by a worst-case adversary that can at a bounded rate (1)
delete agents chosen arbitrarily and (2) insert additional agents with
arbitrary initial state into the system. The goal is perpetually to maintain a
population whose size is within a constant factor of the target size $N$. The
problem is inspired by the ability of complex biological systems composed of a
multitude of memory-limited individual cells to maintain a stable population
size in an adverse environment. Such biological mechanisms allow organisms to
heal after trauma or to recover from excessive cell proliferation caused by
inflammation, disease, or normal development.
We present a population stability protocol in a communication model that is a
synchronous variant of the population model of Angluin et al. In each round,
pairs of agents selected at random meet and exchange messages, where at least a
constant fraction of agents is matched in each round. Our protocol uses
three-bit messages and $\omega(\log^2 N)$ states per agent. We emphasize that
our protocol can handle an adversary that can both insert and delete agents, a
setting in which existing approximate counting techniques do not seem to apply.
The protocol relies on a novel coloring strategy in which the population size
is encoded in the variance of the distribution of colors. Individual agents can
locally obtain a weak estimate of the population size by sampling from the
distribution, and make individual decisions that robustly maintain a stable
global population size."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1706.04641,https://arxiv.org/abs/1706.04641,"Abstract:  We introduce pseudo-deterministic interactive proofs (psdAM): interactive
proof systems for search problems where the verifier is guaranteed with high
probability to output the same output on different executions. As in the case
with classical interactive proofs, the verifier is a probabilistic polynomial
time algorithm interacting with an untrusted powerful prover.
We view pseudo-deterministic interactive proofs as an extension of the study
of pseudo-deterministic randomized polynomial time algorithms: the goal of the
latter is to find canonical solutions to search problems whereas the goal of
the former is to prove that a solution to a search problem is canonical to a
probabilistic polynomial time verifier. Alternatively, one may think of the
powerful prover as aiding the probabilistic polynomial time verifier to find
canonical solutions to search problems, with high probability over the
randomness of the verifier. The challenge is that pseudo-determinism should
hold not only with respect to the randomness, but also with respect to the
prover: a malicious prover should not be able to cause the verifier to output a
solution other than the unique canonical one."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1601.02298,https://arxiv.org/abs/1601.02298,"Abstract:  The availability of vast amounts of data is changing how we can make medical
discoveries, predict global market trends, save energy, and develop educational
strategies. In some settings such as Genome Wide Association Studies or deep
learning, sheer size of data seems critical. When data is held distributedly by
many parties, they must share it to reap its full benefits.
One obstacle to this revolution is the lack of willingness of different
parties to share data, due to reasons such as loss of privacy or competitive
edge. Cryptographic works address privacy aspects, but shed no light on
individual parties' losses/gains when access to data carries tangible rewards.
Even if it is clear that better overall conclusions can be drawn from
collaboration, are individual collaborators better off by collaborating?
Addressing this question is the topic of this paper.
* We formalize a model of n-party collaboration for computing functions over
private inputs in which participants receive their outputs in sequence, and the
order depends on their private inputs. Each output ""improves"" on preceding
outputs according to a score function.
* We say a mechanism for collaboration achieves collaborative equilibrium if
it ensures higher reward for all participants when collaborating (rather than
working alone). We show that in general, computing a collaborative equilibrium
is NP-complete, yet we design efficient algorithms to compute it in a range of
natural model settings.
Our collaboration mechanisms are in the standard model, and thus require a
central trusted party; however, we show this assumption is unnecessary under
standard cryptographic assumptions. We show how to implement the mechanisms in
a decentralized way with new extensions of secure multiparty computation that
impose order/timing constraints on output delivery to different players, as
well as privacy and correctness."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1503.01588,https://arxiv.org/abs/1503.01588,"Abstract:  The full-information model was introduced by Ben-Or and Linial in 1985 to
study collective coin-flipping: the problem of generating a common bounded-bias
bit in a network of $n$ players with $t=t(n)$ faults. They showed that the
majority protocol can tolerate $t=O(\sqrt n)$ adaptive corruptions, and
conjectured that this is optimal in the adaptive setting. Lichtenstein, Linial,
and Saks proved that the conjecture holds for protocols in which each player
sends a single bit. Their result has been the main progress on the conjecture
in the last 30 years.
In this work we revisit this question and ask: what about protocols involving
longer messages? Can increased communication allow for a larger fraction of
faulty players?
We introduce a model of strong adaptive corruptions, where in each round, the
adversary sees all messages sent by honest parties and, based on the message
content, decides whether to corrupt a party (and intercept his message) or not.
We prove that any one-round coin-flipping protocol, regardless of message
length, is secure against at most $\tilde{O}(\sqrt n)$ strong adaptive
corruptions. Thus, increased message length does not help in this setting.
We then shed light on the connection between adaptive and strongly adaptive
adversaries, by proving that for any symmetric one-round coin-flipping protocol
secure against $t$ adaptive corruptions, there is a symmetric one-round
coin-flipping protocol secure against $t$ strongly adaptive corruptions.
Returning to the standard adaptive model, we can now prove that any symmetric
one-round protocol with arbitrarily long messages can tolerate at most
$\tilde{O}(\sqrt n)$ adaptive corruptions.
At the heart of our results lies a novel use of the Minimax Theorem and a new
technique for converting any one-round secure protocol into a protocol with
messages of $polylog(n)$ bits. This technique may be of independent interest."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:1401.0348,https://arxiv.org/abs/1401.0348,"Abstract:  In this paper we show that the existence of general indistinguishability
obfuscators conjectured in a few recent works implies, somewhat
counterintuitively, strong impossibility results for virtual black box
obfuscation. In particular, we show that indistinguishability obfuscation for
all circuits implies:
* The impossibility of average-case virtual black box obfuscation with
auxiliary input for any circuit family with super-polynomial pseudo-entropy.
Such circuit families include all pseudo-random function families, and all
families of encryption algorithms and randomized digital signatures that
generate their required coin flips pseudo-randomly. Impossibility holds even
when the auxiliary input depends only on the public circuit family, and not the
specific circuit in the family being obfuscated.
* The impossibility of average-case virtual black box obfuscation with a
universal simulator (with or without any auxiliary input) for any circuit
family with super-polynomial pseudo-entropy.
These bounds significantly strengthen the impossibility results of Goldwasser
and Kalai (STOC 2005)."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:cs/0212056,https://arxiv.org/abs/cs/0212056,"Abstract:  Madhu Sudan's work spans many areas of computer science theory including
computational complexity theory, the design of efficient algorithms,
algorithmic coding theory, and the theory of program checking and correcting.
Two results of Sudan stand out in the impact they have had on the mathematics
of computation. The first work shows a probabilistic characterization of the
class NP -- those sets for which short and easily checkable proofs of
membership exist, and demonstrates consequences of this characterization to
classifying the complexity of approximation problems. The second work shows a
polynomial time algorithm for list decoding the Reed Solomon error correcting
codes.
This short note will be devoted to describing Sudan's work on
probabilistically checkable proofs -- the so called {\it PCP theorem} and its
implications."
Shafi Goldwasser,Goldwasser_Shafi,arXiv:cs/0212055,https://arxiv.org/abs/cs/0212055,"Abstract:  Theoretical computer science has found fertile ground in many areas of
mathematics. The approach has been to consider classical problems through the
prism of computational complexity, where the number of basic computational
steps taken to solve a problem is the crucial qualitative parameter. This new
approach has led to a sequence of advances, in setting and solving new
mathematical challenges as well as in harnessing discrete mathematics to the
task of solving real-world problems.
In this talk, I will survey the development of modern cryptography -- the
mathematics behind secret communications and protocols -- in this light. I will
describe the complexity theoretic foundations underlying the cryptographic
tasks of encryption, pseudo-randomness number generators and functions, zero
knowledge interactive proofs, and multi-party secure protocols. I will attempt
to highlight the paradigms and proof techniques which unify these foundations,
and which have made their way into the mainstream of complexity theory."
Polina Golland,Golland_Polina,arXiv:1809.04182,https://arxiv.org/abs/1809.04182,"Abstract:  We propose a new iterative segmentation model which can be accurately learned
from a small dataset. A common approach is to train a model to directly segment
an image, requiring a large collection of manually annotated images to capture
the anatomical variability in a cohort. In contrast, we develop a segmentation
model that recursively evolves a segmentation in several steps, and implement
it as a recurrent neural network. We learn model parameters by optimizing the
interme- diate steps of the evolution in addition to the final segmentation. To
this end, we train our segmentation propagation model by presenting incom-
plete and/or inaccurate input segmentations paired with a recommended next
step. Our work aims to alleviate challenges in segmenting heart structures from
cardiac MRI for patients with congenital heart disease (CHD), which encompasses
a range of morphological deformations and topological changes. We demonstrate
the advantages of this approach on a dataset of 20 images from CHD patients,
learning a model that accurately segments individual heart chambers and great
vessels. Com- pared to direct segmentation, the iterative method yields more
accurate segmentation for patients with the most severe CHD malformations."
Polina Golland,Golland_Polina,arXiv:1808.05732,https://arxiv.org/abs/1808.05732,"Abstract:  We present an algorithm for creating high resolution anatomically plausible
images consistent with acquired clinical brain MRI scans with large inter-slice
spacing. Although large data sets of clinical images contain a wealth of
information, time constraints during acquisition result in sparse scans that
fail to capture much of the anatomy. These characteristics often render
computational analysis impractical as many image analysis algorithms tend to
fail when applied to such images. Highly specialized algorithms that explicitly
handle sparse slice spacing do not generalize well across problem domains. In
contrast, we aim to enable application of existing algorithms that were
originally developed for high resolution research scans to significantly
undersampled scans. We introduce a generative model that captures fine-scale
anatomical structure across subjects in clinical image collections and derive
an algorithm for filling in the missing data in scans with large inter-slice
spacing. Our experimental results demonstrate that the resulting method
outperforms state-of-the-art upsampling super-resolution techniques, and
promises to facilitate subsequent analysis not previously possible with scans
of this quality. Our implementation is freely available at
this https URL ."
Polina Golland,Golland_Polina,arXiv:1806.08723,https://arxiv.org/abs/1806.08723,"Abstract:  We introduce an approach for image segmentation based on sparse
correspondences between keypoints in testing and training images. Keypoints
represent automatically identified distinctive image locations, where each
keypoint correspondence suggests a transformation between images. We use these
correspondences to transfer label maps of entire organs from the training
images to the test image. The keypoint transfer algorithm includes three steps:
(i) keypoint matching, (ii) voting-based keypoint labeling, and (iii)
keypoint-based probabilistic transfer of organ segmentations. We report
segmentation results for abdominal organs in whole-body CT and MRI, as well as
in contrast-enhanced CT and MRI. Our method offers a speed-up of about three
orders of magnitude in comparison to common multi-atlas segmentation, while
achieving an accuracy that compares favorably. Moreover, keypoint transfer does
not require the registration to an atlas or a training phase. Finally, the
method allows for the segmentation of scans with highly variable field-of-view."
Polina Golland,Golland_Polina,arXiv:1803.07682,https://arxiv.org/abs/1803.07682,"Abstract:  A reliable Ultrasound (US)-to-US registration method to compensate for brain
shift would substantially improve Image-Guided Neurological Surgery. Developing
such a registration method is very challenging, due to factors such as missing
correspondence in images, the complexity of brain pathology and the demand for
fast computation. We propose a novel feature-driven active framework. Here,
landmarks and their displacement are first estimated from a pair of US images
using corresponding local image features. Subsequently, a Gaussian Process (GP)
model is used to interpolate a dense deformation field from the sparse
landmarks. Kernels of the GP are estimated by using variograms and a discrete
grid search method. If necessary, the user can actively add new landmarks based
on the image context and visualization of the uncertainty measure provided by
the GP to further improve the result. We retrospectively demonstrate our
registration framework as a robust and accurate brain shift compensation
solution on clinical data acquired during neurosurgery."
Polina Golland,Golland_Polina,arXiv:1803.05266,https://arxiv.org/abs/1803.05266,"Abstract:  Estimating the uncertainty in image registration is an area of current
research that is aimed at providing information that will enable surgeons to
assess the operative risk based on registered image data and the estimated
registration uncertainty. If they receive inaccurately calculated registration
uncertainty and misplace confidence in the alignment solutions, severe
consequences may result. For probabilistic image registration (PIR), most
research quantifies the registration uncertainty using summary statistics of
the transformation distributions. In this paper, we study a rarely examined
topic: whether those summary statistics of the transformation distribution
truly represent the registration uncertainty. Using concrete examples, we show
that there are two types of uncertainties: the transformation uncertainty, Ut,
and label uncertainty Ul. Ut indicates the doubt concerning transformation
parameters and can be estimated by conventional uncertainty measures, while Ul
is strongly linked to the goal of registration. Further, we show that using Ut
to quantify Ul is inappropriate and can be misleading. In addition, we present
some potentially critical findings regarding PIR."
Polina Golland,Golland_Polina,arXiv:1608.03907,https://arxiv.org/abs/1608.03907,"Abstract:  We present a robust method to correct for motion and deformations for
in-utero volumetric MRI time series. Spatio-temporal analysis of dynamic MRI
requires robust alignment across time in the presence of substantial and
unpredictable motion. We make a Markov assumption on the nature of deformations
to take advantage of the temporal structure in the image data. Forward message
passing in the corresponding hidden Markov model (HMM) yields an estimation
algorithm that only has to account for relatively small motion between
consecutive frames. We demonstrate the utility of the temporal model by showing
that its use improves the accuracy of the segmentation propagation through
temporal registration. Our results suggest that the proposed model captures
accurately the temporal dynamics of deformations in in-utero MRI time series."
Polina Golland,Golland_Polina,arXiv:1510.01648,https://arxiv.org/abs/1510.01648,"Abstract:  Despite the popularity and empirical success of patch-based nearest-neighbor
and weighted majority voting approaches to medical image segmentation, there
has been no theoretical development on when, why, and how well these
nonparametric methods work. We bridge this gap by providing a theoretical
performance guarantee for nearest-neighbor and weighted majority voting
segmentation under a new probabilistic model for patch-based image
segmentation. Our analysis relies on a new local property for how similar
nearby patches are, and fuses existing lines of work on modeling natural
imagery patches and theory for nonparametric classification. We use the model
to derive a new patch-based segmentation algorithm that iterates between
inferring local label patches and merging these local segmentations to produce
a globally consistent image segmentation. Many existing patch-based algorithms
arise as special cases of the new algorithm."
Polina Golland,Golland_Polina,arXiv:1503.03506,https://arxiv.org/abs/1503.03506,"Abstract:  High computational costs of manifold learning prohibit its application for
large point sets. A common strategy to overcome this problem is to perform
dimensionality reduction on selected landmarks and to successively embed the
entire dataset with the Nyström method. The two main challenges that arise
are: (i) the landmarks selected in non-Euclidean geometries must result in a
low reconstruction error, (ii) the graph constructed from sparsely sampled
landmarks must approximate the manifold well. We propose the sampling of
landmarks from determinantal distributions on non-Euclidean spaces. Since
current determinantal sampling algorithms have the same complexity as those for
manifold learning, we present an efficient approximation running in linear
time. Further, we recover the local geometry after the sparsification by
assigning each landmark a local covariance matrix, estimated from the original
point set. The resulting neighborhood selection based on the Bhattacharyya
distance improves the embedding of sparsely sampled manifolds. Our experiments
show a significant performance improvement compared to state-of-the-art
landmark selection techniques."
Polina Golland,Golland_Polina,arXiv:1411.6307,https://arxiv.org/abs/1411.6307,"Abstract:  We propose a novel diverse feature selection method based on determinantal
point processes (DPPs). Our model enables one to flexibly define diversity
based on the covariance of features (similar to orthogonal matching pursuit) or
alternatively based on side information. We introduce our approach in the
context of Bayesian sparse regression, employing a DPP as a variational
approximation to the true spike and slab posterior distribution. We
subsequently show how this variational DPP approximation generalizes and
extends mean-field approximation, and can be learned efficiently by exploiting
the fast sampling properties of DPPs. Our motivating application comes from
bioinformatics, where we aim to identify a diverse set of genes whose
expression profiles predict a tumor type where the diversity is defined with
respect to a gene-gene interaction network. We also explore an application in
spatial statistics. In both cases, we demonstrate that the proposed method
yields significantly more diverse feature sets than classic sparse methods,
without compromising accuracy."
Polina Golland,Golland_Polina,arXiv:1303.5508,https://arxiv.org/abs/1303.5508,"Abstract:  Manifold learning has been successfully applied to a variety of medical
imaging problems. Its use in real-time applications requires fast projection
onto the low-dimensional space. To this end, out-of-sample extensions are
applied by constructing an interpolation function that maps from the input
space to the low-dimensional manifold. Commonly used approaches such as the
Nyström extension and kernel ridge regression require using all training
points. We propose an interpolation function that only depends on a small
subset of the input training data. Consequently, in the testing phase each new
point only needs to be compared against a small number of input training data
in order to project the point onto the low-dimensional space. We interpret our
method as an out-of-sample extension that approximates kernel ridge regression.
Our method involves solving a simple convex optimization problem and has the
attractive property of guaranteeing an upper bound on the approximation error,
which is crucial for medical applications. Tuning this error bound controls the
sparsity of the resulting interpolation function. We illustrate our method in
two clinical applications that require fast mapping of input images onto a
low-dimensional space."
John Guttag,Guttag_John,arXiv:1901.10002,https://arxiv.org/abs/1901.10002,"Abstract:  As machine learning increasingly affects people and society, it is important
that we strive for a comprehensive and unified understanding of how and why
unwanted consequences arise. For instance, downstream harms to particular
groups are often blamed on ""biased data,"" but this concept encompass too many
issues to be useful in developing solutions. In this paper, we provide a
framework that partitions sources of downstream harm in machine learning into
five distinct categories spanning the data generation and machine learning
pipeline. We describe how these issues arise, how they are relevant to
particular applications, and how they motivate different solutions. In doing
so, we aim to facilitate the development of solutions that stem from an
understanding of application-specific populations and data generation
processes, rather than relying on general claims about what may or may not be
""fair."""
John Guttag,Guttag_John,arXiv:1812.06932,https://arxiv.org/abs/1812.06932,"Abstract:  Deformable registration of clinical scans is a fundamental task for many
applications, such as population studies or the monitoring of long-term disease
progression in individual patients. This task is challenging because, in
contrast to high-resolution research-quality scans, clinical images are often
sparse, missing up to 85% of the slices in comparison. Furthermore, the anatomy
in the acquired slices is not consistent across scans because of variations in
patient orientation with respect to the scanner. In this work, we introduce
Sparse VoxelMorph (SparseVM), which adapts a state-of-the-art learning-based
registration method to improve the registration of sparse clinical images.
SparseVM is a fast, unsupervised method that weights voxel contributions to
registration in proportion to confidence in the voxels. This leads to improved
registration performance on volumes with voxels of varying reliability, such as
interpolated clinical scans. SparseVM registers 3D scans in under a second on
the GPU, which is orders of magnitudes faster than the best performing clinical
registration methods, while still achieving comparable accuracy. Because of its
short runtimes and accurate behavior, SparseVM can enable clinical analyses not
previously possible. The code is publicly available at voxelmorph.mit.edu."
John Guttag,Guttag_John,arXiv:1812.00475,https://arxiv.org/abs/1812.00475,"Abstract:  In this paper, we apply a multiple instance learning paradigm to signal-based
risk stratification for cardiovascular outcomes. In contrast to methods that
require hand-crafted features or domain knowledge, our method learns a
representation with state-of-the-art predictive power from the raw ECG signal.
We accomplish this by leveraging the multiple instance learning framework. This
framework is particularly valuable to learning from biometric signals, where
patient-level labels are available but signal segments are rarely annotated. We
make two contributions in this paper: 1) reframing risk stratification for
cardiovascular death (CVD) as a multiple instance learning problem, and 2)
using this framework to design a new risk score, for which patients in the
highest quartile are 15.9 times more likely to die of CVD within 90 days of
hospital admission for an acute coronary syndrome."
John Guttag,Guttag_John,arXiv:1809.05231,https://arxiv.org/abs/1809.05231,"Abstract:  We present VoxelMorph, a fast learning-based framework for deformable,
pairwise medical image registration. Traditional registration methods optimize
an objective function for each pair of images, which can be time-consuming for
large datasets or rich deformation models. In contrast to this approach, and
building on recent learning-based methods, we formulate registration as a
function that maps an input image pair to a deformation field that aligns these
images. We parameterize the function via a convolutional neural network (CNN),
and optimize the parameters of the neural network on a set of images. Given a
new pair of scans, VoxelMorph rapidly computes a deformation field by directly
evaluating the function. In this work, we explore two different training
strategies. In the first (unsupervised) setting, we train the model to maximize
standard image matching objective functions that are based on the image
intensities. In the second setting, we leverage auxiliary segmentations
available in the training data. We demonstrate that the unsupervised model's
accuracy is comparable to state-of-the-art methods, while operating orders of
magnitude faster. We also show that VoxelMorph trained with auxiliary data
improves registration accuracy at test time, and evaluate the effect of
training set size on registration. Our method promises to speed up medical
image analysis and processing pipelines, while facilitating novel directions in
learning-based registration and its applications. Our code is freely available
at voxelmorph.csail.mit.edu."
John Guttag,Guttag_John,arXiv:1808.02515,https://arxiv.org/abs/1808.02515,"Abstract:  Thanks to the rapid proliferation of connected devices, sensor-generated time
series constitute a large and growing portion of the world's data. Often, this
data is collected from distributed, resource-constrained devices and
centralized at one or more servers. A key challenge in this setup is reducing
the size of the transmitted data without sacrificing its quality. Lower quality
reduces the data's utility, but smaller size enables both reduced network and
storage costs at the servers and reduced power consumption in sensing devices.
A natural solution is to compress the data at the sensing devices.
Unfortunately, existing compression algorithms either violate the memory and
latency constraints common for these devices or, as we show experimentally,
perform poorly on sensor-generated time series.
We introduce a time series compression algorithm that achieves
state-of-the-art compression ratios while requiring less than 1KB of memory and
adding virtually no latency. This method is suitable not only for low-power
devices collecting data, but also for servers storing and querying data; in the
latter context, it can decompress at over 3GB/s in a single thread, even faster
than many algorithms with much lower compression ratios. A key component of our
method is a high-speed forecasting algorithm that can be trained online and
significantly outperforms alternatives such as delta coding.
Extensive experiments on datasets from many domains show that these results
hold not only for sensor data but also across a wide array of other time
series."
John Guttag,Guttag_John,arXiv:1806.02878,https://arxiv.org/abs/1806.02878,"Abstract:  Machine learning approaches have been effective in predicting adverse
outcomes in different clinical settings. These models are often developed and
evaluated on datasets with heterogeneous patient populations. However, good
predictive performance on the aggregate population does not imply good
performance for specific groups.
In this work, we present a two-step framework to 1) learn relevant patient
subgroups, and 2) predict an outcome for separate patient populations in a
multi-task framework, where each population is a separate task. We demonstrate
how to discover relevant groups in an unsupervised way with a
sequence-to-sequence autoencoder. We show that using these groups in a
multi-task framework leads to better predictive performance of in-hospital
mortality both across groups and overall. We also highlight the need for more
granular evaluation of performance when dealing with heterogeneous populations."
John Guttag,Guttag_John,arXiv:1806.00397,https://arxiv.org/abs/1806.00397,"Abstract:  Electronic Health Records (EHRs) contain a large volume of heterogeneous
patient data, which are useful at the point of care and for retrospective
research. These data are typically stored in relational databases. Gaining an
integrated view of these data for a single patient typically requires complex
SQL queries joining multiple tables. In this work, we present a visualization
tool that integrates heterogeneous health care data (e.g., clinical notes,
laboratory test values, vital signs) into a single timeline. We train risk
models offline and dynamically generate and present their predictions alongside
patient data. Our visualization is designed to enable users to understand the
heterogeneous temporal data quickly and comprehensively, and to place the
output of analytic models in the context of the underlying data."
John Guttag,Guttag_John,arXiv:1805.04605,https://arxiv.org/abs/1805.04605,"Abstract:  Traditional deformable registration techniques achieve impressive results and
offer a rigorous theoretical treatment, but are computationally intensive since
they solve an optimization problem for each image pair. Recently,
learning-based methods have facilitated fast registration by learning spatial
deformation functions. However, these approaches use restricted deformation
models, require supervised labels, or do not guarantee a diffeomorphic
(topology-preserving) registration. Furthermore, learning-based registration
tools have not been derived from a probabilistic framework that can offer
uncertainty estimates. In this paper, we present a probabilistic generative
model and derive an unsupervised learning-based inference algorithm that makes
use of recent developments in convolutional neural networks (CNNs). We
demonstrate our method on a 3D brain registration task, and provide an
empirical analysis of the algorithm. Our approach results in state of the art
accuracy and very fast runtimes, while providing diffeomorphic guarantees and
uncertainty estimates. Our implementation is available online at
this http URL ."
John Guttag,Guttag_John,arXiv:1804.07739,https://arxiv.org/abs/1804.07739,"Abstract:  We address the computational problem of novel human pose synthesis. Given an
image of a person and a desired pose, we produce a depiction of that person in
that pose, retaining the appearance of both the person and background. We
present a modular generative neural network that synthesizes unseen poses using
training pairs of images and poses taken from human action videos. Our network
separates a scene into different body part and background layers, moves body
parts to new locations and refines their appearances, and composites the new
foreground with a hole-filled background. These subtasks, implemented with
separate modules, are trained jointly using only a single target image as a
supervised label. We use an adversarial discriminator to force our network to
synthesize realistic details conditioned on pose. We demonstrate image
synthesis results on three action classes: golf, yoga/workouts and tennis, and
show that our method produces accurate results within action classes as well as
across action classes. Given a sequence of desired poses, we also produce
coherent videos of actions."
John Guttag,Guttag_John,arXiv:1802.02604,https://arxiv.org/abs/1802.02604,"Abstract:  We present a fast learning-based algorithm for deformable, pairwise 3D
medical image registration. Current registration methods optimize an objective
function independently for each pair of images, which can be time-consuming for
large data. We define registration as a parametric function, and optimize its
parameters given a set of images from a collection of interest. Given a new
pair of scans, we can quickly compute a registration field by directly
evaluating the function using the learned parameters. We model this function
using a convolutional neural network (CNN), and use a spatial transform layer
to reconstruct one image from another while imposing smoothness constraints on
the registration field. The proposed method does not require supervised
information such as ground truth registration fields or anatomical landmarks.
We demonstrate registration accuracy comparable to state-of-the-art 3D image
registration, while operating orders of magnitude faster in practice. Our
method promises to significantly speed up medical image analysis and processing
pipelines, while facilitating novel directions in learning-based registration
and its applications. Our code is available at
this https URL ."
John Guttag,Guttag_John,arXiv:1712.00643,https://arxiv.org/abs/1712.00643,"Abstract:  When an infection spreads in a community, an individual's probability of
becoming infected depends on both her susceptibility and exposure to the
contagion through contact with others. While one often has knowledge regarding
an individual's susceptibility, in many cases, whether or not an individual's
contacts are contagious is unknown. We study the problem of predicting if an
individual will adopt a contagion in the presence of multiple modes of
infection (exposure/susceptibility) and latent neighbor influence. We present a
generative probabilistic model and a variational inference method to learn the
parameters of our model. Through a series of experiments on synthetic data, we
measure the ability of the proposed model to identify latent spreaders, and
predict the risk of infection. Applied to a real dataset of 20,000 hospital
patients, we demonstrate the utility of our model in predicting the onset of a
healthcare associated infection using patient room-sharing and nurse-sharing
networks. Our model outperforms existing benchmarks and provides actionable
insights for the design and implementation of targeted interventions to curb
the spread of infection."
John Guttag,Guttag_John,arXiv:1706.10283,https://arxiv.org/abs/1706.10283,"Abstract:  Vectors of data are at the heart of machine learning and data mining.
Recently, vector quantization methods have shown great promise in reducing both
the time and space costs of operating on vectors. We introduce a vector
quantization algorithm that can compress vectors over 12x faster than existing
techniques while also accelerating approximate vector operations such as
distance and dot product computations by up to 10x. Because it can encode over
2GB of vectors per second, it makes vector quantization cheap enough to employ
in many more circumstances. For example, using our technique to compute
approximate dot products in a nested loop can multiply matrices faster than a
state-of-the-art BLAS implementation, even when our algorithm must first
compress the matrices.
In addition to showing the above speedups, we demonstrate that our approach
can accelerate nearest neighbor search and maximum inner product search by over
100x compared to floating point operations and up to 10x compared to other
vector quantization methods. Our approximate Euclidean distance and dot product
computations are not only faster than those of related algorithms with slower
encodings, but also faster than Hamming distance computations, which have
direct hardware support on the tested platforms. We also assess the errors of
our algorithm's approximate distances and dot products, and find that it is
competitive with existing, slower vector quantization algorithms."
John Guttag,Guttag_John,arXiv:1612.04007,https://arxiv.org/abs/1612.04007,"Abstract:  For many movement disorders, such as Parkinson's disease and ataxia, disease
progression is visually assessed by a clinician using a numerical disease
rating scale. These tests are subjective, time-consuming, and must be
administered by a professional. This can be problematic where specialists are
not available, or when a patient is not consistently evaluated by the same
clinician. We present an automated method for quantifying the severity of
motion impairment in patients with ataxia, using only video recordings. We
consider videos of the finger-to-nose test, a common movement task used as part
of the assessment of ataxia progression during the course of routine clinical
checkups.
Our method uses neural network-based pose estimation and optical flow
techniques to track the motion of the patient's hand in a video recording. We
extract features that describe qualities of the motion such as speed and
variation in performance. Using labels provided by an expert clinician, we
train a supervised learning model that predicts severity according to the Brief
Ataxia Rating Scale (BARS). The performance of our system is comparable to that
of a group of ataxia specialists in terms of mean error and correlation, and
our system's predictions were consistently within the range of inter-rater
variability. This work demonstrates the feasibility of using computer vision
and machine learning to produce consistent and clinically useful measures of
motor impairment."
John Guttag,Guttag_John,arXiv:1609.09196,https://arxiv.org/abs/1609.09196,"Abstract:  Thanks to the rise of wearable and connected devices, sensor-generated time
series comprise a large and growing fraction of the world's data.
Unfortunately, extracting value from this data can be challenging, since
sensors report low-level signals (e.g., acceleration), not the high-level
events that are typically of interest (e.g., gestures). We introduce a
technique to bridge this gap by automatically extracting examples of real-world
events in low-level data, given only a rough estimate of when these events have
taken place.
By identifying sets of features that repeat in the same temporal arrangement,
we isolate examples of such diverse events as human actions, power consumption
patterns, and spoken words with up to 96% precision and recall. Our method is
fast enough to run in real time and assumes only minimal knowledge of which
variables are relevant or the lengths of events. Our evaluation uses numerous
publicly available datasets and over 1 million samples of manually labeled
sensor data."
John Guttag,Guttag_John,arXiv:1608.02301,https://arxiv.org/abs/1608.02301,"Abstract:  Voice disorders affect an estimated 14 million working-aged Americans, and
many more worldwide. We present the first large scale study of vocal misuse
based on long-term ambulatory data collected by an accelerometer placed on the
neck. We investigate an unsupervised data mining approach to uncovering latent
information about voice misuse.
We segment signals from over 253 days of data from 22 subjects into over a
hundred million single glottal pulses (closures of the vocal folds), cluster
segments into symbols, and use symbolic mismatch to uncover differences between
patients and matched controls, and between patients pre- and post-treatment.
Our results show significant behavioral differences between patients and
controls, as well as between some pre- and post-treatment patients. Our
proposed approach provides an objective basis for helping diagnose behavioral
voice disorders, and is a first step towards a more data-driven understanding
of the impact of voice therapy."
John Guttag,Guttag_John,arXiv:1608.02071,https://arxiv.org/abs/1608.02071,"Abstract:  In many domains such as medicine, training data is in short supply. In such
cases, external knowledge is often helpful in building predictive models. We
propose a novel method to incorporate publicly available domain expertise to
build accurate models. Specifically, we use word2vec models trained on a
domain-specific corpus to estimate the relevance of each feature's text
description to the prediction problem. We use these relevance estimates to
rescale the features, causing more important features to experience weaker
regularization.
We apply our method to predict the onset of five chronic diseases in the next
five years in two genders and two age groups. Our rescaling approach improves
the accuracy of the model, particularly when there are few positive examples.
Furthermore, our method selects 60% fewer features, easing interpretation by
physicians. Our method is applicable to other domains where feature and outcome
descriptions are available."
Peter Hagelstein,Hagelstein_Peter,arXiv:1201.4377,https://arxiv.org/abs/1201.4377,"Abstract:  Motivated by many observations of anomalies in condensed matter systems, we
consider a new fundamental Hamiltonian in which condensed matter and nuclear
systems are described initially on the same footing. Since it may be possible
that the lattice will respond to the mass change associated with a excited
nuclear state, we adopt a relativistic description throughout based on a
many-particle Dirac formalism. This approach has not been used in the past,
perhaps due to the difficulty in separating the center of mass and relative
degrees of freedom of the nuclear system, or perhaps due to an absence of
applications for such a model. In response to some recent ideas about how to
think about the center of mass and relative separation, we obtained from the
Dirac model a new fundamental Hamiltonian in which the lattice couples to
different states within the composite nuclei within the lattice. In this
description the different nuclear states have different mass energies and
kinetic energies, as we had expected. In addition there appear new terms which
provide for nuclear excitation as a result of coupling to the composite
momentum. This new effect comes about because of changes in the composite
nuclear state as a result of the dynamical Lorentz boost in the lattice."
Peter Hagelstein,Hagelstein_Peter,arXiv:1201.1488,https://arxiv.org/abs/1201.1488,"Abstract:  We are interested in the energy-momentum relation for a moving composite in
relativistic quantum mechanics in many-particle Dirac models. For a manifestly
covariant model one can apply the Lorentz transform to go from the rest frame
to a moving frame to establish an energy-momentum relation of the form
$\sqrt{(M^*c^2)^2+c^2|{\bf P}|^2}$ where $M^*$ is the kinematic mass. However,
the many-particle Dirac model is not manifestly covariant, and some other
approach is required. We have found a simple approach that allows for a
separation of relative and center of mass contributions to the energy. We are
able to define the associated kinematic energy and determine the
energy-momentum relation. Our result can be expressed as a modified deBroglie
relation of the form
$$ \hbar \omega ({\bf P}) = <\Phi' | \sum_j {m_j \over M} \beta_j | \Phi' >~
\sqrt{[M^*({\bf P}) c^2]^2 + c^2 |{\bf P}|^2} $$
where the kinematic mass $M^*$ will depend on the total momentum ${\bf P}$
for a general noncovariant potential. The prefactor that occurs we associate
with a time dilation effect, the existence of which has been discussed
previously in the literature."
Peter Hagelstein,Hagelstein_Peter,arXiv:0803.1906,https://arxiv.org/abs/0803.1906,"Abstract:  We consider a generalization of the spin-boson model in which two different
two-level systems are coupled to an oscillator, under conditions where the
oscillator energy is much less than the two-level system energies, and where
the oscillator is highly excited. We find that the two-level system transition
energy is shifted, producing a Bloch-Siegert shift in each two-level system
similar to what would be obtained if the other were absent. At resonances
associated with energy exchange between a two-level system and the oscillator,
the level splitting is about the same as would be obtained in the spin-boson
model at a Bloch-Siegert resonance. However, there occur resonances associated
with the transfer of excitation between one two-level system and the other, an
effect not present in the spin-boson model. We use a unitary transformation
leading to a rotated system in which terms responsible for the shift and
splittings can be identified. The level splittings at the anticrossings
associated with both energy exchange and excitation transfer resonances are
accounted for with simple two-state models and degenerate perturbation theory
using operators that appear in the rotated Hamiltonian."
Peter Hagelstein,Hagelstein_Peter,arXiv:0802.2779,https://arxiv.org/abs/0802.2779,"Abstract:  In previous work we studied the spin-boson model in the multiphoton regime,
using a rotation that provides a separation between terms that contribute most
of the level energies away from resonance, and terms responsible for the level
splittings at the anticrossing. Here, we consider a generalization of the
spin-boson model consisting of a three-level system coupled to an oscillator.
We construct a similar rotation and apply it to the more complicated model. We
find that the rotation provides a useful approximation to the energy levels in
the multiphoton region of the new problem. We find that good results can be
obtained for the level splittings at the anticrossings for resonances involving
the lower two levels in regions away from accidental or low-order resonances of
the upper two levels."
Peter Hagelstein,Hagelstein_Peter,arXiv:0801.3810,https://arxiv.org/abs/0801.3810,"Abstract:  The electron mass is known to be sensitive to local fluctuations in the
electromagnetic field, and undergoes a small shift in a thermal field. It was
claimed recently that a very large electron mass shift should be expected near
the surface of a metal hydride [{\it Eur. Phys. J. C}, {\bf 46} 107 (2006)]. We
examine the shift using a formulation based on the Coulomb gauge, which leads
to a much smaller shift. The maximization of the electron mass shift under
nonequilibrium conditions seems nonetheless to be an interesting problem. We
consider a scheme in which a current in a hollow wire produces a large vector
potential in the wire center. Fluctuations in an LC circuit with nearly matched
loss and gain can produce large current fluctuations; and these can increase
the electron mass shift by orders of magnitude over its room temperature value."
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.3557,https://arxiv.org/abs/0709.3557,"Abstract:  We consider a spin-boson model in which a spin 1 system is coupled to an
oscillator. A unitary transformation is applied which allows a separation of
terms responsible for the Bloch-Siegert shift, and terms responsible for the
level splittings at anticrossings associated with Bloch-Siegert resonances.
When the oscillator is highly excited, the system can maintain resonance for
sequential multiphoton transitions. At lower levels of excitation, resonance
cannot be maintained because energy exchange with the oscillator changes the
level shift. An estimate for the critical excitation level of the oscillator is
developed."
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.1961,https://arxiv.org/abs/0709.1961,"Abstract:  We present a unitary equivalent spin-boson Hamiltonian in which terms can be
identified which contribute to the Bloch-Siegert shift, and to the level
splittings at the anticrossings associated with the Bloch-Siegert resonances.
First-order degenerate perturbation theory is used to develop approximate
results in the case of moderate coupling for the level splitting."
Peter Hagelstein,Hagelstein_Peter,arXiv:0709.1958,https://arxiv.org/abs/0709.1958,"Abstract:  Recently there has been theoretical and experimental interest in
Bloch-Siegert shifts in an intense photon field. A perturbative treatment
becomes difficult in this multiphoton regime. We present a unitary transform
and rotated model, which allows us to get accurate results away from the level
anticrossings. A simple variational energy estimate leads to a new expression
for the dressed two-level system energy which is accurate, and useful over a
wide range of the dimensionless coupling constant."
Peter Hagelstein,Hagelstein_Peter,arXiv:cond-mat/0612306,https://arxiv.org/abs/cond-mat/0612306,"Abstract:  We consider models in which two sets of matched two-level systems are coupled
to a common oscillator in the case where the oscillator energy is small
relative to the two-level transition energies. Since the two sets of two-level
systems are coupled indirectly through the oscillator, excitation transfer from
one set of two-level systems to the other is possible. In addition, the
excitation energy from the two-level systems may be exchanged with the
oscillator coherently, even though the oscillator energy may be orders of
magnitude smaller than the two-level system transition energy. In the lossless
case, we demonstrate these effects numerically, and also use an approximate
diagonalization to show that these effects are expected from the model
Hamiltonian.
We augment the model to include loss effects, and show that loss enhances the
excitation transfer effect by breaking the severe cancelation between different
paths that occurs in the lossless case. We describe a simple approximate model
wavefunction appropriate when the loss increases rapidly with energy. Within
this model approximation, we present numerical and analytical results for
excitation transfer and energy transfer rates, showing that they are greatly
increased.
Our study of these models is motivated in part by claims of excess heat
production in electrochemical experiments in heavy water. We examine the
question of whether the rates associated with this kind of model are
sufficiently large to be relevant to the experimental claims. We find that
consistency is possible given recent experimental results showing strong
screening effects in low energy deuteron-deuteron fusion experiments in metals."
Peter Hagelstein,Hagelstein_Peter,arXiv:cond-mat/0606585,https://arxiv.org/abs/cond-mat/0606585,"Abstract:  Phonon exchange with nuclei in the course of fusion reactions that occur in a
solid has not been analyzed previously. This problem has become of interest in
connection with claims of observations of anomalies in metal deuterides. If the
strong force interaction were dependent only on position (and not spin or
isospin), then the coupling with phonons can be developed directly. Since a
nuclear interaction can change the lattice constituents, the initial and final
state lattices can be different, and we must include this in the formulation.
For more realistic strong force models with spin and isospin dependence, we can
use correlated nuclear wavefunctions which are made up of products of space,
spin and isospin components. In this case, the spin and isospin algebra can be
done analytically, producing channel-dependent potentials that are only space
dependent. The formulation that results can be used for quantitative estimates
of phonon exchange."
Song Han,Han_Song,arXiv:1902.02023,https://arxiv.org/abs/1902.02023,"Abstract:  Along with the rapid growth of Industrial Internet-of-Things (IIoT)
applications and their penetration into many industry sectors, real-time
wireless networks (RTWNs) have been playing a more critical role in providing
real-time, reliable and secure communication services for such applications. A
key challenge in RTWN management is how to ensure real-time Quality of Services
(QoS) especially in the presence of unexpected disturbances and lossy wireless
links. Most prior work takes centralized approaches for handling disturbances,
which are slow and subject to single-point failure, and do not scale. To
overcome these drawbacks, this paper presents a fully distributed packet
scheduling framework called FD-PaS. FD-PaS aims to provide guaranteed fast
response to unexpected disturbances while achieving minimum performance
degradation for meeting the timing and reliability requirements of all critical
tasks. To combat the scalability challenge, FD-PaS incorporates several key
advances in both algorithm design and data link layer protocol design to enable
individual nodes to make on-line decisions locally without any centralized
control. Our extensive simulation and testbed results have validated the
correctness of the FD-PaS design and demonstrated its effectiveness in
providing fast response for handling disturbances while ensuring the designated
QoS requirements."
Song Han,Han_Song,arXiv:1812.02734,https://arxiv.org/abs/1812.02734,"Abstract:  Analog IC design relies on human experts to search for parameters that
satisfy circuit specifications with their experience and intuitions, which is
highly labor intensive, time consuming and suboptimal. Machine learning is a
promising tool to automate this process. However, supervised learning is
difficult for this task due to the low availability of training data: 1)
Circuit simulation is slow, thus generating large-scale dataset is
time-consuming; 2) Most circuit designs are propitiatory IPs within individual
IC companies, making it expensive to collect large-scale datasets. We propose
Learning to Design Circuits (L2DC) to leverage reinforcement learning that
learns to efficiently generate new circuits data and to optimize circuits. We
fix the schematic, and optimize the parameters of the transistors automatically
by training an RL agent with no prior knowledge about optimizing circuits.
After iteratively getting observations, generating a new set of transistor
parameters, getting a reward, and adjusting the model, L2DC is able to optimize
circuits. We evaluate L2DC on two transimpedance amplifiers. Trained for a day,
our RL agent can achieve comparable or better performance than human experts
trained for a quarter. It first learns to meet hard-constraints (eg. gain,
bandwidth), and then learns to optimize good-to-have targets (eg. area, power).
Compared with grid search-aided human design, L2DC can achieve
$\mathbf{250}\boldsymbol{\times}$ higher sample efficiency with comparable
performance. Under the same runtime constraint, the performance of L2DC is also
better than Bayesian Optimization."
Song Han,Han_Song,arXiv:1812.00332,https://arxiv.org/abs/1812.00332,"Abstract:  Neural architecture search (NAS) has a great impact by automatically
designing effective neural network architectures. However, the prohibitive
computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours)
makes it difficult to \emph{directly} search the architectures on large-scale
tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via
a continuous representation of network architecture but suffers from the high
GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a
result, they need to utilize~\emph{proxy} tasks, such as training on a smaller
dataset, or learning with only a few blocks, or training just for a few epochs.
These architectures optimized on proxy tasks are not guaranteed to be optimal
on target task. In this paper, we present \emph{ProxylessNAS} that can
\emph{directly} learn the architectures for large-scale target tasks and target
hardware platforms. We address the high memory consumption issue of
differentiable NAS and reduce the computational cost (GPU hours and GPU memory)
to the same level of regular training while still allowing a large candidate
set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of
directness and specialization. On CIFAR-10, our model achieves 2.08\% test
error with only 5.7M parameters, better than the previous state-of-the-art
architecture AmoebaNet-B, while using 6$\times$ fewer parameters. On ImageNet,
our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being
1.2$\times$ faster with measured GPU latency. We also apply ProxylessNAS to
specialize neural architectures for hardware with direct hardware metrics (e.g.
latency) and provide insights for efficient CNN architecture design."
Song Han,Han_Song,arXiv:1811.08886,https://arxiv.org/abs/1811.08886,"Abstract:  Model quantization is a widely used technique to compress and accelerate deep
neural network (DNN) inference. Emergent DNN hardware accelerators begin to
support flexible bitwidth (1-8 bits) to further improve the computation
efficiency, which raises a great challenge to find the optimal bitwidth for
each layer: it requires domain experts to explore the vast design space trading
off among accuracy, latency, energy, and model size, which is both
time-consuming and sub-optimal. Conventional quantization algorithm ignores the
different hardware architectures and quantizes all the layers in a uniform way.
In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)
framework which leverages the reinforcement learning to automatically determine
the quantization policy, and we take the hardware accelerator's feedback in the
design loop. Rather than relying on proxy signals such as FLOPs and model size,
we employ a hardware simulator to generate direct feedback signals to the RL
agent. Compared with conventional methods, our framework is fully automated and
can specialize the quantization policy for different neural network
architectures and hardware architectures. Our framework effectively reduced the
latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of
accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework
reveals that the optimal policies on different hardware architectures (i.e.,
edge and cloud architectures) under different resource constraints (i.e.,
latency, energy and model size) are drastically different. We interpreted the
implication of different quantization policies, which offer insights for both
neural network architecture design and hardware architecture design."
Song Han,Han_Song,arXiv:1811.08383,https://arxiv.org/abs/1811.08383,"Abstract:  The explosive growth in online video streaming gives rise to challenges on
efficiently extracting the spatial-temporal information to perform video
understanding. Conventional 2D CNNs are computationally cheap but cannot
capture long-term temporal relationships; 3D CNN based methods can achieve good
performance but are computationally intensive, making it expensive to deploy.
In this paper, we propose a generic and effective Temporal Shift Module (TSM)
that enjoys both high efficiency and high performance. Specifically, it can
achieve the performance of 3D CNN but maintain 2D complexity. The central idea
of TSM is to shift part of the channels along the temporal dimension, which
facilitates information exchange among neighboring frames. TSM can be inserted
into 2D CNNs to achieve temporal modeling at the cost of zero FLOPs and zero
parameters. On the Something-Something-V1 dataset which focuses on temporal
modeling, we achieved better results than I3D family and ECO family using 6X
and 2.7X fewer FLOPs respectively. Measured on P100 GPU, our single model
achieved 1.8% higher accuracy at 8X lower latency and 12X higher throughput
compared to I3D. Remarkably, our framework ranks the first on both
Something-Something V1 and V2 leaderboards upon this paper's submission."
Song Han,Han_Song,arXiv:1811.06072,https://arxiv.org/abs/1811.06072,"Abstract:  We consider the problem of clustering graph nodes over large-scale dynamic
graphs, such as citation networks, images and web networks, when graph updates
such as node/edge insertions/deletions are observed distributively. We propose
communication-efficient algorithms for two well-established communication
models namely the message passing and the blackboard models. Given a graph with
$n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two
proposed algorithms have communication costs $\tilde{O}(ns)$ and
$\tilde{O}(n+s)$ ($\tilde{O}$ hides a polylogarithmic factor), almost matching
their lower bounds, $\Omega(ns)$ and $\Omega(n+s)$, respectively, in the
message passing and the blackboard models. More importantly, we prove that at
each time point in $[1,t]$ our algorithms generate clustering quality nearly as
good as that of centralizing all updates up to that time and then applying a
standard centralized clustering algorithm. We conducted extensive experiments
on both synthetic and real-life datasets which confirmed the communication
efficiency of our approach over baseline algorithms while achieving comparable
clustering results."
Song Han,Han_Song,arXiv:1806.02639,https://arxiv.org/abs/1806.02639,"Abstract:  We introduce a new function-preserving transformation for efficient neural
architecture search. This network transformation allows reusing previously
trained networks and existing successful architectures that improves sample
efficiency. We aim to address the limitation of current network transformation
operations that can only perform layer-level architecture modifications, such
as adding (pruning) filters or inserting (removing) a layer, which fails to
change the topology of connection paths. Our proposed path-level transformation
operations enable the meta-controller to modify the path topology of the given
network while keeping the merits of reusing weights, and thus allow efficiently
designing effective structures with complex path topologies like Inception
models. We further propose a bidirectional tree-structured reinforcement
learning meta-controller to explore a simple yet highly expressive
tree-structured architecture space that can be viewed as a generalization of
multi-branch architectures. We experimented on the image classification
datasets with limited computational resources (about 200 GPU-hours), where we
observed improved parameter efficiency and better test results (97.70% test
accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet
in the mobile setting), demonstrating the effectiveness and transferability of
our designed architectures."
Song Han,Han_Song,arXiv:1804.06913,https://arxiv.org/abs/1804.06913,"Abstract:  Recent results at the Large Hadron Collider (LHC) have pointed to enhanced
physics capabilities through the improvement of the real-time event processing
techniques. Machine learning methods are ubiquitous and have proven to be very
powerful in LHC physics, and particle physics as a whole. However, exploration
of the use of such techniques in low-latency, low-power FPGA hardware has only
just begun. FPGA-based trigger and data acquisition (DAQ) systems have
extremely low, sub-microsecond latency requirements that are unique to particle
physics. We present a case study for neural network inference in FPGAs focusing
on a classifier for jet substructure which would enable, among many other
physics scenarios, searches for new dark sector particles and novel
measurements of the Higgs boson. While we focus on a specific example, the
lessons are far-reaching. We develop a package based on High-Level Synthesis
(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS
increases accessibility across a broad user community and allows for a drastic
decrease in firmware development time. We map out FPGA resource usage and
latency versus neural network hyperparameters to identify the problems in
particle physics that would benefit from performing neural network inference
with FPGAs. For our example jet substructure model, we fit well within the
available resources of modern FPGAs with a latency on the scale of 100 ns."
Song Han,Han_Song,arXiv:1803.01992,https://arxiv.org/abs/1803.01992,"Abstract:  Recently emerged dielectric resonators and metasurfaces offer a low-loss
platform for efficient manipulation of electromagnetic waves from microwave to
visible. Such flat meta-optics can focus electromagnetic waves, generate
structured beams and vortices, enhance local fields for sensing as well as
provide additional functionalities for advanced MRI machinery. Recent advances
are associated with exotic optical modes called bound states in the continuum,
which can give rise to extremely large quality factors and supercavity lasing.
Here, we experimentally demonstrate subwavelength active supercavities with
extremely high-Q resonances that could be reconfigured at an ultrafast time
scale. We reveal that such supercavities enable all-optical switching and
modulation of extremely sharp resonances, and thus could have numerous
applications in lasing, mode multiplexing, and biosensing."
Song Han,Han_Song,arXiv:1802.07855,https://arxiv.org/abs/1802.07855,"Abstract:  In most process control systems nowadays, process measurements are
periodically collected and archived in historians. Analytics applications
process the data, and provide results offline or in a time period that is
considerably slow in comparison to the performance of the manufacturing
process. Along with the proliferation of Internet-of-Things (IoT) and the
introduction of ""pervasive sensors"" technology in process industries,
increasing number of sensors and actuators are installed in process plants for
pervasive sensing and control, and the volume of produced process data is
growing exponentially. To digest these data and meet the ever-growing
requirements to increase production efficiency and improve product quality,
there needs to be a way to both improve the performance of the analytics system
and scale the system to closely monitor a much larger set of plant resources.
In this paper, we present a real-time data analytics platform, called RT-DAP,
to support large-scale continuous data analytics in process industries. RT-DAP
is designed to be able to stream, store, process and visualize a large volume
of realtime data flows collected from heterogeneous plant resources, and
feedback to the control system and operators in a realtime manner. A prototype
of the platform is implemented on Microsoft Azure. Our extensive experiments
validate the design methodologies of RT-DAP and demonstrate its efficiency in
both component and system levels."
Song Han,Han_Song,arXiv:1802.06367,https://arxiv.org/abs/1802.06367,"Abstract:  Convolutional Neural Networks (CNNs) are computationally intensive, which
limits their application on mobile devices. Their energy is dominated by the
number of multiplies needed to perform the convolutions. Winograd's minimal
filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can
reduce the operation count, but these two methods cannot be directly combined
$-$ applying the Winograd transform fills in the sparsity in both the weights
and the activations. We propose two modifications to Winograd-based CNNs to
enable these methods to exploit sparsity. First, we move the ReLU operation
into the Winograd domain to increase the sparsity of the transformed
activations. Second, we prune the weights in the Winograd domain to exploit
static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet
datasets, our method reduces the number of multiplications by $10.4\times$,
$6.8\times$ and $10.8\times$ respectively with loss of accuracy less than
$0.1\%$, outperforming previous baselines by $2.0\times$-$3.0\times$. We also
show that moving ReLU to the Winograd domain allows more aggressive pruning."
Song Han,Han_Song,arXiv:1802.03740,https://arxiv.org/abs/1802.03740,"Abstract:  The interaction between microscopic particles has always been a fascinating
and intriguing area of science. Direct interrogation of such interactions is
often difficult or impossible. Structured electromagnetic systems offer a rich
toolkit for mimicking and reproducing the key dynamics that governs the
microscopic interactions, and thus provide an avenue to explore and interpret
the microscopic phenomena. In particular, metamaterials offer the freedom to
artificially tailor light-matter coupling and to control the interaction
between unit cells in the metamaterial array. Here we demonstrate a terahertz
metamaterial that mimics spin-related interactions of microscopic particles in
a 2D lattice via complex electromagnetic multipole interactions within the
metamaterial array. Fano resonances featured by distinct mode properties due to
strong nearest-neighbor interactions are discussed that draw parallels with the
2D Ising model. Interestingly, a hyperfine Fano splitting spectrum is observed
by manipulating the 2D interactions without applying external magnetic or
electric fields, which provides a passive multispectral platform for
applications in super-resolution imaging, biosensing, and selective thermal
emission. The dynamic approach to reproduce the static interaction between
microscopic particles would enable more profound significance in exploring the
unknown physical world by the macroscopic analogues."
Song Han,Han_Song,arXiv:1802.03494,https://arxiv.org/abs/1802.03494,"Abstract:  Model compression is a critical technique to efficiently deploy neural
network models on mobile devices which have limited computation resources and
tight power budgets. Conventional model compression techniques rely on
hand-crafted heuristics and rule-based policies that require domain experts to
explore the large design space trading off among model size, speed, and
accuracy, which is usually sub-optimal and time-consuming. In this paper, we
propose AutoML for Model Compression (AMC) which leverage reinforcement
learning to provide the model compression policy. This learning-based
compression policy outperforms conventional rule-based compression policy by
having higher compression ratio, better preserving the accuracy and freeing
human labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than
the handcrafted model compression policy for VGG-16 on ImageNet. We applied
this automated, push-the-button compression pipeline to MobileNet and achieved
1.81x speedup of measured inference latency on an Android phone and 1.43x
speedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy."
Song Han,Han_Song,arXiv:1712.01887,https://arxiv.org/abs/1712.01887,"Abstract:  Large-scale distributed training requires significant communication bandwidth
for gradient exchange that limits the scalability of multi-node training, and
requires expensive high-bandwidth network infrastructure. The situation gets
even worse with distributed training on mobile devices (federated learning),
which suffers from higher latency, lower throughput, and intermittent poor
connections. In this paper, we find 99.9% of the gradient exchange in
distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to
greatly reduce the communication bandwidth. To preserve accuracy during
compression, DGC employs four methods: momentum correction, local gradient
clipping, momentum factor masking, and warm-up training. We have applied Deep
Gradient Compression to image classification, speech recognition, and language
modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and
Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a
gradient compression ratio from 270x to 600x without losing accuracy, cutting
the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from
488MB to 0.74MB. Deep gradient compression enables large-scale distributed
training on inexpensive commodity 1Gbps Ethernet and facilitates distributed
training on mobile."
Song Han,Han_Song,arXiv:1706.00051,https://arxiv.org/abs/1706.00051,"Abstract:  Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear
inverse task demanding time and resource intensive computations that can
substantially trade off {\it accuracy} for {\it speed} in real-time imaging. In
addition, state-of-the-art compressed sensing (CS) analytics are not cognizant
of the image {\it diagnostic quality}. To cope with these challenges we put
forth a novel CS framework that permeates benefits from generative adversarial
networks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR
images from historical patients. Leveraging a mixture of least-squares (LS)
GANs and pixel-wise $\ell_1$ cost, a deep residual network with skip
connections is trained as the generator that learns to remove the {\it
aliasing} artifacts by projecting onto the manifold. LSGAN learns the texture
details, while $\ell_1$ controls the high-frequency noise. A multilayer
convolutional neural network is then jointly trained based on diagnostic
quality images to discriminate the projection quality. The test phase performs
feed-forward propagation over the generator network that demands a very low
computational overhead. Extensive evaluations are performed on a large
contrast-enhanced MR dataset of pediatric patients. In particular, images rated
based on expert radiologists corroborate that GANCS retrieves high contrast
images with detailed texture relative to conventional CS, and pixel-wise
schemes. In addition, it offers reconstruction under a few milliseconds, two
orders of magnitude faster than state-of-the-art CS-MRI schemes."
Song Han,Han_Song,arXiv:1705.08922,https://arxiv.org/abs/1705.08922,"Abstract:  Sparsity helps reduce the computational complexity of deep neural networks by
skipping zeros. Taking advantage of sparsity is listed as a high priority in
next generation DNN accelerators such as TPU. The structure of sparsity, i.e.,
the granularity of pruning, affects the efficiency of hardware accelerator
design as well as the prediction accuracy. Coarse-grained pruning creates
regular sparsity patterns, making it more amenable for hardware acceleration
but more challenging to maintain the same accuracy. In this paper we
quantitatively measure the trade-off between sparsity regularity and prediction
accuracy, providing insights in how to maintain accuracy while having more a
more structured sparsity pattern. Our experimental results show that
coarse-grained pruning can achieve a sparsity ratio similar to unstructured
pruning without loss of accuracy. Moreover, due to the index saving effect,
coarse-grained pruning is able to obtain a better compression ratio than
fine-grained sparsity at the same accuracy threshold. Based on the recent
sparse convolutional neural network accelerator (SCNN), our experiments further
demonstrate that coarse-grained sparsity saves about 2x the memory references
compared to fine-grained sparsity. Since memory reference is more than two
orders of magnitude more expensive than arithmetic operations, the regularity
of sparse structure leads to more efficient hardware design."
Song Han,Han_Song,arXiv:1612.02562,https://arxiv.org/abs/1612.02562,"Abstract:  As our population ages, neurological impairments and degeneration of the
musculoskeletal system yield gait abnormalities, which can significantly reduce
quality of life. Gait rehabilitative therapy has been widely adopted to help
patients maximize community participation and living independence. To further
improve the precision and efficiency of rehabilitative therapy, more objective
methods need to be developed based on sensory data. In this paper, an
algorithmic framework is proposed to provide classification of gait disorders
caused by two common neurological diseases, stroke and Parkinson's Disease
(PD), from ground contact force (GCF) data. An advanced machine learning
method, multi-task feature learning (MTFL), is used to jointly train
classification models of a subject's gait in three classes, post-stroke, PD and
healthy gait. Gait parameters related to mobility, balance, strength and rhythm
are used as features for the classification. Out of all the features used, the
MTFL models capture the more important ones per disease, which will help
provide better objective assessment and therapy progress tracking. To evaluate
the proposed methodology we use data from a human participant study, which
includes five PD patients, three post-stroke patients, and three healthy
subjects. Despite the diversity of abnormalities, the evaluation shows that the
proposed approach can successfully distinguish post-stroke and PD gait from
healthy gait, as well as post-stroke from PD gait, with Area Under the Curve
(AUC) score of at least 0.96. Moreover, the methodology helps select important
gait features to better understand the key characteristics that distinguish
abnormal gaits and design personalized treatment."
Song Han,Han_Song,arXiv:1612.01064,https://arxiv.org/abs/1612.01064,"Abstract:  Deep neural networks are widely used in machine learning applications.
However, the deployment of large neural networks models can be difficult to
deploy on mobile devices with limited power budgets. To solve this problem, we
propose Trained Ternary Quantization (TTQ), a method that can reduce the
precision of weights in neural networks to ternary values. This method has very
little accuracy degradation and can even improve the accuracy of some models
(32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet
model is trained from scratch, which means it's as easy as to train normal full
precision model. We highlight our trained quantization method that can learn
both ternary values and ternary assignment. During inference, only ternary
values (2-bit weights) and scaling factors are needed, therefore our models are
nearly 16x smaller than full-precision models. Our ternary models can also be
viewed as sparse binary weight networks, which can potentially be accelerated
with custom circuit. Experiments on CIFAR-10 show that the ternary models
obtained by trained quantization method outperform full-precision models of
ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model
outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and
outperforms previous ternary models by 3%."
Song Han,Han_Song,arXiv:1612.00694,https://arxiv.org/abs/1612.00694,"Abstract:  Long Short-Term Memory (LSTM) is widely used in speech recognition. In order
to achieve higher prediction accuracy, machine learning scientists have built
larger and larger models. Such large model is both computation intensive and
memory intensive. Deploying such bulky model results in high power consumption
and leads to high total cost of ownership (TCO) of a data center. In order to
speedup the prediction and make it energy efficient, we first propose a
load-balance-aware pruning method that can compress the LSTM model size by 20x
(10x from pruning and 2x from quantization) with negligible loss of the
prediction accuracy. The pruned model is friendly for parallel processing.
Next, we propose scheduler that encodes and partitions the compressed model to
each PE for parallelism, and schedule the complicated LSTM data flow. Finally,
we design the hardware architecture, named Efficient Speech Recognition Engine
(ESE) that works directly on the compressed model. Implemented on Xilinx
XCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working
directly on the compressed LSTM network, corresponding to 2.52 TOPS on the
uncompressed one, and processes a full LSTM for speech recognition with a power
dissipation of 41 Watts. Evaluated on the LSTM for speech recognition
benchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X
GPU implementations. It achieves 40x and 11.5x higher energy efficiency
compared with the CPU and GPU respectively."
Song Han,Han_Song,arXiv:1611.07519,https://arxiv.org/abs/1611.07519,"Abstract:  In this paper, we theoretically and experimentally demonstrate a three
dimensional metamaterial that can motivate electromagnetic induced transparency
(EIT) by using circular polarized wave as stimulations. The unit cell consists
of a pair of metallic strips printed on both sides of the printed circuit board
(PCB), where a conductive cylinder junction is used to connect the metal strips
by drilling a hole inside the substrate. When a right circularly polarized wave
is incident, destructive interference is excited between meta-atoms of the 3D
structure, the transmission spectrum demonstrates a sharp transparency window.
A coupled oscillator model and an electrical equivalent circuit model are
applied to quantitatively and qualitatively analyze the coupling mechanism in
the EIT-like metamaterial. Analysis in detail shows the EIT window's amplitude
and frequency are modulated by changing the degree of symmetry breaking. The
proposed metamaterial may achieve potential applications in developing chiral
slow light devices."
Song Han,Han_Song,arXiv:1607.04381,https://arxiv.org/abs/1607.04381,"Abstract:  Modern deep neural networks have a large number of parameters, making them
very hard to train. We propose DSD, a dense-sparse-dense training flow, for
regularizing deep neural networks and achieving better optimization
performance. In the first D (Dense) step, we train a dense network to learn
connection weights and importance. In the S (Sparse) step, we regularize the
network by pruning the unimportant connections with small weights and
retraining the network given the sparsity constraint. In the final D (re-Dense)
step, we increase the model capacity by removing the sparsity constraint,
re-initialize the pruned parameters from zero and retrain the whole dense
network. Experiments show that DSD training can improve the performance for a
wide range of CNNs, RNNs and LSTMs on the tasks of image classification,
caption generation and speech recognition. On ImageNet, DSD improved the Top1
accuracy of GoogLeNet by 1.1%, VGG-16 by 4.3%, ResNet-18 by 1.2% and ResNet-50
by 1.1%, respectively. On the WSJ'93 dataset, DSD improved DeepSpeech and
DeepSpeech2 WER by 2.0% and 1.1%. On the Flickr-8K dataset, DSD improved the
NeuralTalk BLEU score by over 1.7. DSD is easy to use in practice: at training
time, DSD incurs only one extra hyper-parameter: the sparsity ratio in the S
step. At testing time, DSD doesn't change the network architecture or incur any
inference overhead. The consistent and significant performance gain of DSD
experiments shows the inadequacy of the current training methods for finding
the best local optimum, while DSD effectively achieves superior optimization
performance for finding a better solution. DSD models are available to download
at this https URL."
Song Han,Han_Song,arXiv:1607.00099,https://arxiv.org/abs/1607.00099,"Abstract:  For any semiring, the concept of k-congruences is introduced, criteria for
k-congruences are established, it is proved that there is an
inclusion-preserving bijection between k-congruences and k-ideals, and an
equivalent condition for the existence of a zero is presented with the help of
k-congruences. It is shown that a semiring is k-simple iff it is
k-congruence-simple, and that inclines are k-simple iff they have at most 2
elements. Lemma 2.12(i) in [Glas. Mat. 42(62) (2007) 301] is pointed out being
false."
Song Han,Han_Song,arXiv:1602.07360,https://arxiv.org/abs/1602.07360,"Abstract:  Recent research on deep neural networks has focused primarily on improving
accuracy. For a given accuracy level, it is typically possible to identify
multiple DNN architectures that achieve that accuracy level. With equivalent
accuracy, smaller DNN architectures offer at least three advantages: (1)
Smaller DNNs require less communication across servers during distributed
training. (2) Smaller DNNs require less bandwidth to export a new model from
the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on
FPGAs and other hardware with limited memory. To provide all of these
advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet
achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.
Additionally, with model compression techniques we are able to compress
SqueezeNet to less than 0.5MB (510x smaller than AlexNet).
The SqueezeNet architecture is available for download here:
this https URL"
Song Han,Han_Song,arXiv:1602.01895,https://arxiv.org/abs/1602.01895,"Abstract:  Generating natural language descriptions for images is a challenging task.
The traditional way is to use the convolutional neural network (CNN) to extract
image features, followed by recurrent neural network (RNN) to generate
sentences. In this paper, we present a new model that added memory cells to
gate the feeding of image features to the deep neural network. The intuition is
enabling our model to memorize how much information from images should be fed
at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed
that our model outperforms other state-of-the-art models with higher BLEU
scores."
Song Han,Han_Song,arXiv:1602.01528,https://arxiv.org/abs/1602.01528,"Abstract:  State-of-the-art deep neural networks (DNNs) have hundreds of millions of
connections and are both computationally and memory intensive, making them
difficult to deploy on embedded systems with limited hardware resources and
power budgets. While custom hardware helps the computation, fetching weights
from DRAM is two orders of magnitude more expensive than ALU operations, and
dominates the required power.
Previously proposed 'Deep Compression' makes it possible to fit large DNNs
(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by
pruning the redundant connections and having multiple connections share the
same weight. We propose an energy efficient inference engine (EIE) that
performs inference on this compressed network model and accelerates the
resulting sparse matrix-vector multiplication with weight sharing. Going from
DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;
Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.
Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to
CPU and GPU implementations of the same DNN without compression. EIE has a
processing power of 102GOPS/s working directly on a compressed network,
corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of
AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is
24,000x and 3,400x more energy efficient than a CPU and GPU respectively.
Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy
efficiency and area efficiency."
Song Han,Han_Song,arXiv:1510.07383,https://arxiv.org/abs/1510.07383,"Abstract:  An incline is an additively idempotent semiring in which the product of two
elements is always less than or equal to either factor. By making use of prime
numbers, this paper proves that A^{11} is less than or equal to A^5 for all 3x3
matrices A over an arbitrary commutative incline, thus giving an answer to an
open problem ""For 3x3 matrices over any incline (even noncommutative) is X^5
greater than or equal to X^{11}?"", proposed by Cao, Kim and Roush in a
monograph Incline Algebra and Applications, 1984."
Song Han,Han_Song,arXiv:1510.00149,https://arxiv.org/abs/1510.00149,"Abstract:  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems with limited hardware
resources. To address this limitation, we introduce ""deep compression"", a three
stage pipeline: pruning, trained quantization and Huffman coding, that work
together to reduce the storage requirement of neural networks by 35x to 49x
without affecting their accuracy. Our method first prunes the network by
learning only the important connections. Next, we quantize the weights to
enforce weight sharing, finally, we apply Huffman coding. After the first two
steps we retrain the network to fine tune the remaining connections and the
quantized centroids. Pruning, reduces the number of connections by 9x to 13x;
Quantization then reduces the number of bits that represent each connection
from 32 to 5. On the ImageNet dataset, our method reduced the storage required
by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method
reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of
accuracy. This allows fitting the model into on-chip SRAM cache rather than
off-chip DRAM memory. Our compression method also facilitates the use of
complex neural networks in mobile applications where application size and
download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,
compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy
efficiency."
Song Han,Han_Song,arXiv:1506.02626,https://arxiv.org/abs/1506.02626,"Abstract:  Neural networks are both computationally intensive and memory intensive,
making them difficult to deploy on embedded systems. Also, conventional
networks fix the architecture before training starts; as a result, training
cannot improve the architecture. To address these limitations, we describe a
method to reduce the storage and computation required by neural networks by an
order of magnitude without affecting their accuracy by learning only the
important connections. Our method prunes redundant connections using a
three-step method. First, we train the network to learn which connections are
important. Next, we prune the unimportant connections. Finally, we retrain the
network to fine tune the weights of the remaining connections. On the ImageNet
dataset, our method reduced the number of parameters of AlexNet by a factor of
9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar
experiments with VGG-16 found that the number of parameters can be reduced by
13x, from 138 million to 10.3 million, again with no loss of accuracy."
Song Han,Han_Song,arXiv:1308.6348,https://arxiv.org/abs/1308.6348,"Abstract:  An incline is an additively idempotent semiring in which the product of two
elements is always less than or equal to either factor. This paper proves that
the only regular inclines are distributive lattices, which also implies that
there is no noncommutative regular incline."
Song Han,Han_Song,arXiv:1304.6241,https://arxiv.org/abs/1304.6241,"Abstract:  In this paper, we proposed an identification and data encrypt key manage
protocol that can be used in some security system based on such secure devices
as secure USB memories or RFIDs, which are widely used for identifying persons
or other objects recently. In general, the default functions of the security
system using a mobile device are the authentication for the owner of the device
and secure storage of data stored on the device. We proposed a security model
that consists of the server and mobile devices in order to realize these
security features. In this model we defined the secure communication protocol
for the authentication and management of data encryption keys using a private
key encryption algorithm with the public key between the server and mobile
devices. In addition, we was performed the analysis for the attack to the
communication protocol between the mobile device and server. Using the
communication protocol, the system will attempt to authenticate the mobile
device. The data decrypt key is transmitted only if the authentication process
is successful. The data in the mobile device can be decrypted using the key.
Our analysis proved that this Protocol ensures anonymity, prevents replay
attacks and realizes the interactive identification between the security
devices and the authentication server."
Song Han,Han_Song,arXiv:1304.0374,https://arxiv.org/abs/1304.0374,"Abstract:  Cognitive complexity measures quantify human difficulty in understanding the
source code based on cognitive informatics foundation. The discipline derives
cognitive complexity on a basis of fundamental software factors i.e, inputs,
outputs, and internal processing architecture. An approach to integrating
Granular Computing into the new measure called Structured Cognitive Information
Measure or SCIM. The proposed measure unifies and re-organizes complexity
factors analogous to human cognitive process. However, according to the
methodology of software and the scope of the variables, Information Complexity
Number(ICN) of variables is depended on change of variable value and cognitive
complexity is measured in several ways. In this paper, we define the Scope
Information Complexity Number (SICN) and present the cognitive complexity based
on functional decomposition of software, including theoretical validation
through nine Weyuker's properties."
Song Han,Han_Song,arXiv:1212.3838,https://arxiv.org/abs/1212.3838,"Abstract:  DC has proved to be a promising tool for the specification and verification
of functional requirements on the design of hard real-time systems. Many works
were devoted to develop effective techniques for checking the models of hard
real-time systems against DC specifications. DC model checking theory is still
evolving and yet there is no available tools supporting practical verifications
due to the high undecidability of calculus and the great complexity of model
checking. Present situation of PDC model checking is much worse than the one of
DC model checking. In view of the results so far achieved, it is desirable to
develop approximate model checking techniques for DC and PDC specifications.
This work was motivated to develop approximate techniques checking automata
models of hard real-time systems for DC and PDC specifications. Unlike previous
works which only deal with decidable formulas, we want to develop approximate
techniques covering whole DC and PDC formulas. The first results of our work,
namely, approximate techniques checking real-time automata models of systems
for LDI and PLDI specifications, are described in this paper."
Song Han,Han_Song,arXiv:1212.2415,https://arxiv.org/abs/1212.2415,"Abstract:  Face recognition systems must be robust to the variation of various factors
such as facial expression, illumination, head pose and aging. Especially, the
robustness against illumination variation is one of the most important problems
to be solved for the practical use of face recognition systems. Gabor wavelet
is widely used in face detection and recognition because it gives the
possibility to simulate the function of human visual system. In this paper, we
propose a method for extracting Gabor wavelet features which is stable under
the variation of local illumination and show experiment results demonstrating
its effectiveness."
Song Han,Han_Song,arXiv:1009.3626,https://arxiv.org/abs/1009.3626,"Abstract:  This paper presents a survey on several RFID authentication protocols under
low cost restrictions. Low cost RFID are mainly addressed with limited security
and privacy protections. In this study, we explore several protocols with
various authentication mechanisms found in literature that satisfy low cost
restrictions. Assessments of these protocols are based on data protection,
tracking protection, forward security. Finally, it is concluded that no single
low cost RFID protocol fully meets the requirement of the given assessments.
While a protocol satisfies one or two assessments, it fails to fully meet the
requirement of the third assessment. This study provides a new insight in RFID
literature which can be used particularly by small and medium industries to
choose the appropriate RFID protocol for their needs."
Song Han,Han_Song,arXiv:1008.2452,https://arxiv.org/abs/1008.2452,"Abstract:  Security and privacy are the inherent problems in RFID communications. There
are several protocols have been proposed to overcome those problems. Hash chain
is commonly employed by the protocols to improve security and privacy for RFID
authentication. Although the protocols able to provide specific solution for
RFID security and privacy problems, they fail to provide integrated solution.
This article is a survey to closely observe those protocols in terms of its
focus and limitations."
Jongyoon Han,Han_Jongyoon,arXiv:1802.08565,https://arxiv.org/abs/1802.08565,"Abstract:  Series of short contributions that are part of Nobel Symposium 162 -
Microfluidics arXiv:1712.08369."
Jongyoon Han,Han_Jongyoon,arXiv:1801.01462,https://arxiv.org/abs/1801.01462,"Abstract:  We develop the first theoretical model for the analytical description of ion
concentration polarization (ICP)-based electrokinetic molecular concentration,
which had not been possible due to the extraordinary complexity of the system.
We define the two separate limits for the enrichment factor achievable in a
given system and derive the scaling laws for critical parameters, which are
validated by numerical simulations and experiments. This work provides clear
theoretical explanations on the diverse experimental behaviors previously
observed yet unexplainable, while setting solid foundation for the engineering
of ICP-based concentrators and other fluid-coupled electrokinetic systems."
Jongyoon Han,Han_Jongyoon,arXiv:1709.01859,https://arxiv.org/abs/1709.01859,"Abstract:  This paper studies mechanism of preconcentration of charged particles in a
straight micro-channel embedded with permselective membranes, by numerically
solving coupled transport equations of ions, charged particles and solvent
fluid without any simplifying assumptions. It is demonstrated that trapping and
preconcentration of charged particles are determined by the interplay between
drag force from the electroosmotic fluid flow and the electrophoretic force
applied trough the electric field. Several insightful characteristics are
revealed, including the diverse dynamics of co-ions and counter ions,
replacement of co-ions by focused particles, lowered ion concentrations in
particle enriched zone, and enhanced electroosmotic pumping effect etc.
Conditions for particles that may be concentrated are identified in terms of
charges, sizes and electrophoretic mobilities of particles and co-ions.
Dependences of enrichment factor on cross-membrane voltage, initial particle
concentration and buffer ion concentrations are analyzed and the underlying
reasons are elaborated. Finally, post priori a condition for validity of
decoupled simulation model is given based on charges carried by focused charge
particles and that by buffer co-ions. These results provide important guidance
in the design and optimization of nanofluidic preconcentration and other
related devices."
Ruonan Han,Han_Ruonan,arXiv:1810.01056,https://arxiv.org/abs/1810.01056,"Abstract:  The nitrogen vacancy (NV) center in diamond has emerged as a leading
solid-state quantum sensor for applications including magnetometry,
electrometry, thermometry, and chemical sensing. However, an outstanding
challenge for practical applications is that existing NV-based sensing
techniques require bulky and discrete instruments for spin control and
detection. Here, we address this challenge by integrating NV based quantum
sensing with complementary metal-oxide-semiconductor (CMOS) technology. Through
tailored CMOS-integrated microwave generation and photodetection, this work
dramatically reduces the instrumentation footprint for quantum magnetometry and
thermometry. This hybrid diamond-CMOS integration enables an ultra-compact and
scalable platform for quantum sensing and quantum information processing."
Thomas Heldt,Heldt_Thomas,arXiv:1806.09994,https://arxiv.org/abs/1806.09994,"Abstract:  A binary beat-by-beat classification algorithm for cerebral blood flow
velocity (CBFV) recordings based on amplitude, spectral and morphological
features is presented. The classification difference between 15 manually and
algorithmically annotated CBFV records is around 5%."
Berthold Horn,Horn_Berthold,arXiv:1810.06055,https://arxiv.org/abs/1810.06055,"Abstract:  For identification of change information in image sequences, most studies
focus on change detection in one image sequence, while few studies have
considered the change level comparison between two different image sequences.
Moreover, most studies require the detection of image information in details,
for example, object detection. Based on Uncertainty Coefficient(UC), this paper
proposes an innovative method CCUC for change comparison between two image
sequences. The proposed method is computationally efficient and simple to
implement. The change comparison stems from video monitoring system. The
limited number of provided screens and a large number of monitoring cameras
require the videos or image sequences ordered by change level. We demonstrate
this new method by applying it on two publicly available image sequences. The
results are able to show the method can distinguish the different change level
for sequences."
Berthold Horn,Horn_Berthold,arXiv:1612.08825,https://arxiv.org/abs/1612.08825,"Abstract:  Convolutions have long been regarded as fundamental to applied mathematics,
physics and engineering. Their mathematical elegance allows for common tasks
such as numerical differentiation to be computed efficiently on large data
sets. Efficient computation of convolutions is critical to artificial
intelligence in real-time applications, like machine vision, where convolutions
must be continuously and efficiently computed on tens to hundreds of kilobytes
per second. In this paper, we explore how convolutions are used in fundamental
machine vision applications. We present an accelerated n-dimensional
convolution package in the high performance computing language, Julia, and
demonstrate its efficacy in solving the time to contact problem for machine
vision. Results are measured against synthetically generated videos and
quantitatively assessed according to their mean squared error from the ground
truth. We achieve over an order of magnitude decrease in compute time and
allocated memory for comparable machine vision applications. All code is
packaged and integrated into the official Julia Package Manager to be used in
various other scenarios."
Qing Hu,Hu_Qing,arXiv:1812.03505,https://arxiv.org/abs/1812.03505,"Abstract:  Hyperspectral imaging is a technique that allows for the creation of
multi-color images. At terahertz wavelengths, it has emerged as a prominent
tool for a number of applications, ranging from non-ionizing cancer diagnosis
and pharmaceutical characterization to non-destructive artifact testing.
Contemporary terahertz imaging systems typically rely on non-linear optical
down-conversion of a fiber-based near-infrared femtosecond laser, requiring
complex optical systems. Here, we demonstrate hyperspectral imaging with
chip-scale frequency combs based on terahertz quantum cascade lasers. The dual
combs are free-running and emit coherent terahertz radiation that covers a
bandwidth of 220 GHz at 3.4 THz with ~10 {\mu}W per line. The combination of
the fast acquisition rate of dual-comb spectroscopy with the monolithic design,
scalability, and chip-scale size of the combs is highly appealing for future
imaging applications in biomedicine and in the pharmaceutical industry."
Qing Hu,Hu_Qing,arXiv:1807.10401,https://arxiv.org/abs/1807.10401,"Abstract:  For many applications Optical Frequency Combs (OFCs) require a high degree of
temporal coherence (narrow linewidth). Commonly OFCs are generated in nonlinear
media from a monochromatic narrow linewidth laser sources or from a mode-locked
laser pulses but in the all-important mid-infrared (MIR) and terahertz (THz)
regions of spectrum OFCs can be generated intrinsically by the free-running
quantum cascade lasers (QCLs) with high efficiency. These combs do not look
like conventional OFCs as the phases of each mode are different and in temporal
domain the OFC is a seemingly random combination of amplitude- and
phase-modulated signals rather than a short pulse. Despite this
pseudo-randomness, the experimental evidence suggests that the linewidth of the
QCL OFC is just as narrow as that of a QCL operating in the single mode. While
universally acknowledged, this seemingly observation is not fully understood.
In this work we rigorously prove this fact by deriving the expression for the
Schawlow-Townes linewidth of QCL OFC and offer a transparent physical
interpretation based on orthogonality of laser modes, indicating that despite
their very different temporal profiles MIR and THz QCL OFCs are just as good
for most applications as any other OFC."
Qing Hu,Hu_Qing,arXiv:1805.05759,https://arxiv.org/abs/1805.05759,"Abstract:  We present here a new type of cold atom interferometry gravimeter based on
Bragg diffraction, which is able to increase the gravity measurement
sensitivity and stability of common Raman atom gravimeters significantly. By
comparing with Raman transition, the principles and advantages of Bragg
diffraction-based atom gravimeters have been introduced. The theoretical model
for a time-domain Bragg atom gravimeter with atomic incident direction
parallels to the wave vector of Bragg lasers has been constructed. Some key
technical requirements for an nth-order Bragg diffraction-based atom gravimeter
have been deduced, including the temperature of atom cloud, the diameter,
curvature radius, frequency, intensity, and timing sequence of Bragg lasers,
etc. The analysis results were verified by the existing experimental data in
discussion. The present study provides a good reference for the understanding
and construction of a Bragg atom gravimeter."
Qing Hu,Hu_Qing,arXiv:1805.05707,https://arxiv.org/abs/1805.05707,"Abstract:  The loss of contrast due to atom expansion induced non-perfect Raman pulse
area in atom interferometers is investigated systematically. Based on the
theoretical simulation, we find that the expansion of the atomic cloud results
in a decrease of the {\pi} pulse fidelity and a change of the {\pi} pulse
duration, which lead to a significant reduction in fringe contrast. We propose
a mitigation strategy of increasing the intensities of the second and third
Raman pulses. Simulation results show that the fringe contrast can be improved
by 13.6% in a typical atom interferometer gravimeter using this intensity
compensation strategy. We also evaluate the effects of this mitigation strategy
in the case of a lower atomic cloud temperature and a larger Raman beam size
under different Raman pulse time interval conditions. This mitigation strategy
has potential applications in increasing the sensitivity of atom
interferometer-based precision measuring, including precision measuring of the
gravity, gravity gradient, rotation, and magnetic field gradient, as well as
testing of the Einstein equivalence principle."
Qing Hu,Hu_Qing,arXiv:1805.05704,https://arxiv.org/abs/1805.05704,"Abstract:  We present the derivation of the frequency dependent scalar, vector, and
tensor dynamical polarizabilities for the two hyperfine levels of the 87Rb atom
5s ground state. Based on the characterization of the dynamical
polarizabilities, we analyze and measure the differential vector and tensor
light shift between the 5s ground state sub-levels with near-resonant,
stimulated Raman transitions. These results clarify that the tensor
polarizabilities for the ground states of alkali atoms are absent when the
light field is far-detuned from the atomic resonance and the total electronic
angular momentum J is a good quantum number. In the near resonant case, the
light shifts are non-trivial and the determination of the frequency dependent
vector and tensor dynamic polarizabilities will help to achieve higher
fidelities for applications of neutral atoms in quantum information and
precision measurements."
Qing Hu,Hu_Qing,arXiv:1805.05159,https://arxiv.org/abs/1805.05159,"Abstract:  Precisely evaluating the systematic error induced by the quadratic Zeeman
effect is important for developing atom interferometer gravimeters aiming at an
accuracy in the regime ( ). This paper reports on the experimental
investigation of Raman spectroscopy-based magnetic field measurements and the
evaluation of the systematic error in the Gravimetric Atom Interferometer
(GAIN) due to quadratic Zeeman effect. We discuss Raman duration and frequency
step size dependent magnetic field measurement uncertainty, present vector
light shift (VLS) and tensor light shift (TLS) induced magnetic field
measurement offset, and map the absolute magnetic field inside the
interferometer chamber of GAIN with an uncertainty of 0.72 nT and a spatial
resolution of 12.8 mm. We evaluate the quadratic Zeeman effect induced gravity
measurement error in GAIN as . The methods shown in this paper are important
for precisely mapping the absolute magnetic field in vacuum and reducing the
quadratic Zeeman effect induced systematic error in Raman transition-based
precision measurements, such as atomic interferometer gravimeters."
Qing Hu,Hu_Qing,arXiv:1803.02913,https://arxiv.org/abs/1803.02913,"Abstract:  Topological materials bear gapped excitations in bulk yet protected gapless
excitations at boundaries. Magnetoplasmons (MPs), as high-frequency density
excitations of two-dimensional electron gas (2DEG) in a perpendicular magnetic
field, embody a prototype of band topology for bosons. The
time-reversal-breaking magnetic field opens a topological gap for bulk MPs up
to the cyclotron frequency; topologically-protected edge magnetoplasmons (EMPs)
bridge the bulk gap and propagate unidirectionally along system's boundaries.
However, all the EMPs known to date adhere to physical edges where the electron
density terminates abruptly. This restriction has made device application
extremely difficult. Here we demonstrate a new class of topological edge
plasmons -- domain-boundary magnetoplasmons (DBMPs), within a uniform edgeless
2DEG. Such DBMPs arise at the domain boundaries of an engineered sign-changing
magnetic field and are protected by the difference of gap Chern numbers (+/-1)
across the magnetic domains. They propagate unidirectionally along the domain
boundaries and are immune to domain defects. Moreover, they exhibit wide
tunability in the microwave frequency range under an applied magnetic field or
gate voltage. Our study opens a new direction to realize high-speed
reconfigurable topological devices."
Qing Hu,Hu_Qing,arXiv:1708.01445,https://arxiv.org/abs/1708.01445,"Abstract:  Diffusion of dopants in rutile is the fundamental process that determines the
performance of many devices in which rutile is used. The diffusion behavior is
known to be highly sample-dependent, but the reasons for this are less well
understood. Here, rutile is studied by using first-principles calculations, in
order to unravel the microscopic origins of the diverse diffusion behaviors for
different doping elements. Anomalous diffusion behavior in the open channel
along [001] direction is found: larger atoms include Sc and Zr have lower
energy barrier for diffusion via interstitial mechanism, apparently
contradicting their known slow diffusion rate. To resolve this, we present an
alternate model for the overall diffusion rate of the large-size dopants in
rutile, showing that parallel to the [001] channel, it is limited by the
formation of the interstitial states, whereas in the direction perpendicular to
[001], it proceeds via a kick-out mechanism. By contrast, Co and Ni, prefer to
stay in the interstitial site of rutile, and have conventional diffusion with a
very small migration barrier in the [001] channel. This leads to highly
anisotropic and fast diffusion. The diffusion mechanisms found in the present
study can explain the diffusion data measured by experiments, and these
findings provide novel understanding for the classic diffusion topic."
Qing Hu,Hu_Qing,arXiv:1605.09436,https://arxiv.org/abs/1605.09436,"Abstract:  Dual comb spectroscopy allows for high-resolution spectra to be measured over
broad bandwidths, but an essential requirement for coherent integration is the
availability of a phase reference. Usually, this means that the combs' phase
and timing errors must be measured and either minimized by stabilization or
removed by correction, limiting the technique's applicability. In this work, we
demonstrate that it is possible to extract the phase and timing signals of a
multiheterodyne spectrum completely computationally, without any extra
measurements or optical elements. These techniques are viable even when the
relative linewidth exceeds the repetition rate difference, and can tremendously
simplify any dual comb system. By reconceptualizing frequency combs in terms of
the temporal structure of their phase noise, not their frequency stability, we
are able to greatly expand the scope of multiheterodyne techniques."
Qing Hu,Hu_Qing,arXiv:1604.01048,https://arxiv.org/abs/1604.01048,"Abstract:  Frequency combs based on terahertz quantum cascade lasers feature broadband
coverage and high output powers in a compact package, making them an attractive
option for broadband spectroscopy. Here, we demonstrate the first
multi-heterodyne spectroscopy using two terahertz quantum cascade laser combs.
With just 100 $\mu$s of integration time, we achieve peak signal-to-noise
ratios exceeding 60 dB and a spectral coverage greater than 250 GHz centered at
2.8 THz. Even with room-temperature detectors we are able to achieve peak
signal-to-noise ratios of 50 dB, and as a proof-of-principle we use these combs
to measure the broadband transmission spectrum of etalon samples. Finally, we
show that with proper signal processing, it is possible to extend the
multi-heterodyne spectroscopy to quantum cascade laser combs operating in
pulsed mode, greatly expanding the range of quantum cascade lasers that could
be suitable for these techniques."
Qing Hu,Hu_Qing,arXiv:1603.08266,https://arxiv.org/abs/1603.08266,"Abstract:  Two-dimensional molecular aggregate (2DMA), a thin sheet of strongly
interacting dipole molecules self-assembled at close distance on an ordered
lattice, is a fascinating fluorescent material. It is distinctively different
from the single or colloidal dye molecules or quantum dots in most previous
research. In this paper, we verify for the first time that when a 2DMA is
placed at a nanometric distance from a metallic substrate, the strong and
coherent interaction between the dipoles inside the 2DMA dominates its
fluorescent decay at picosecond timescale. Our streak-camera lifetime
measurement and interacting lattice-dipole calculation reveal that the
metal-mediated dipole-dipole interaction shortens the fluorescent lifetime to
about one half and increases the energy dissipation rate by ten times than
expected from the noninteracting single-dipole picture. Our finding can enrich
our understanding of nanoscale energy transfer in molecular excitonic systems
and may designate a new direction for developing fast and efficient
optoelectronic devices."
Qing Hu,Hu_Qing,arXiv:1511.08620,https://arxiv.org/abs/1511.08620,"Abstract:  Interstitials (carbon and nitrogen) are crucial alloying elements for
optimizing the mechanical performance of the twinning-induced plasticity (TWIP)
steels in terms of the stacking fault energy (SFE). First-principles
calculations have been performed to study the effect of interstitial-induced
lattice expansion on the SFE. Comparing the predictions with the SFEs measured
for alloys containing C and N, our results suggest that the dominant effect of
these interstitials on the SFE is due to the lattice expansion effect."
Qing Hu,Hu_Qing,arXiv:1505.05443,https://arxiv.org/abs/1505.05443,"Abstract:  First-principles alloy theory, formulated within the exact muffin-tin
orbitals method in combination with the coherent-potential approximation, is
used to study the mechanical properties of ferromagnetic body-centered cubic
(bcc) Fe$_{1-x}$M$_x$ alloys (M=Mn or Ni, $0\le x \le 0.1$). We consider
several physical parameters accessible from \emph{ab initio} calculations and
their combinations in various phenomenological models to compare the effect of
Mn and Ni on the properties of Fe. Alloying is found to slightly alter the
lattice parameters and produce noticeable influence on elastic moduli. Both Mn
and Ni decrease the surface energy and the unstable stacking fault energy
associated with the $\{110\}$ surface facet and the $\{110\}\langle111\rangle$
slip system, respectively. Nickel is found to produce larger effect on the
planar fault energies than Mn. The semi-empirical ductility criteria by Rice
and Pugh consistently predict that Ni enhances the ductility of Fe but give
contradictory results in the case of Mn doping. The origin of the discrepancy
between the two criteria is discussed and an alternative measure of the
ductile-brittle behavior based on the theoretical cleavage strength and
single-crystal shear modulus $G\{110\}\langle111\rangle$ is proposed."
Qing Hu,Hu_Qing,arXiv:1504.07867,https://arxiv.org/abs/1504.07867,"Abstract:  We demonstrate an unexpectedly strong surface-plasmonic absorption at the
interface of silver and high-index dielectrics based on electron and photon
spectroscopy. The measured bandwidth and intensity of absorption deviate
significantly from the classical theory. Our density-functional calculation
well predicts the occurrence of this phenomenon. It reveals that due to the low
metal-to-dielectric work function at such interfaces, conduction electrons can
display a drastic quantum spillover, causing the interfacial electron-hole pair
production to dominate the decay of surface plasmons. This finding can be of
fundamental importance in understanding and designing quantum nano-plasmonic
devices that utilize noble metals and high-index dielectrics."
Qing Hu,Hu_Qing,arXiv:1311.0946,https://arxiv.org/abs/1311.0946,"Abstract:  On-chip nanophotonics serves as the foundation for the new generation of
information technology, but it is challenged by the diffraction limit of light.
With the capabilities of confining light into (deep) subwavelength volumes,
plasmonics makes it possible to dramatically miniaturize optical devices so as
to integrate them into silicon chips. Here we demonstrate that by cascading
nano-corrugation gratings with different periodicities on silver nanowires atop
silicon, different colors can be spatially separated and chronologically
released at different grating junctions. The released light frequency depends
on the grating arrangement and corrugation periodicities. Hence the nanowire
acts as a spectral splitter for sorting/demultiplexing photons at different
nano-scale positions with a ten-femtosecond-level interval. Such nanowires can
be constructed further into compact 2D networks or circuits. We believe that
this study provides a new and promising approach for realizing
spatiotemporal-sensitive spectral splitting and optical signal processing on
nanoscales, and for general integration of nanophotonics with microelectronics."
Qing Hu,Hu_Qing,arXiv:1307.6905,https://arxiv.org/abs/1307.6905,"Abstract:  We present new formulae for the matrix elements of one-body and two-body
physical operators in compact forms, which are applicable to arbitrary
Hartree-Fock-Bogoliubov wave functions, including those for multi-quasiparticle
excitations. The test calculations show that our formulae may substantially
accelerate the process of symmetry restoration when applied to the heavy
nuclear system."
Qing Hu,Hu_Qing,arXiv:1306.3051,https://arxiv.org/abs/1306.3051,"Abstract:  Overlap between Hartree-Fock-Bogoliubov(HFB) vacua is very important in the
beyond mean-field calculations. However, in the HFB transformation, the $U,V$
matrices are sometimes singular due to the exact emptiness ($v_i=0$) or full
occupation ($u_i=0$) of some single-particle orbits. This singularity may cause
some problem in evaluating the overlap between HFB vacua through Pfaffian. We
found that this problem can be well avoided by setting those zero occupation
numbers to some tiny values (e.g., $u_i,v_i=10^{-8}$). This treatment does not
change the HFB vacuum state because $u_i^2,v_i^2=10^{-16}$ are numerically zero
relative to 1. Therefore, for arbitrary HFB transformation, we say that the
$U,V$ matrices can always be nonsingular. From this standpoint, we present a
new convenient Pfaffian formula for the overlap between arbitrary HFB vacua,
which is especially suitable for symmetry restoration. Testing calculations
have been performed for this new formula. It turns out that our method is
reliable and accurate in evaluating the overlap between arbitrary HFB vacua."
Qing Hu,Hu_Qing,arXiv:1208.5776,https://arxiv.org/abs/1208.5776,"Abstract:  We report on a heterodyne receiver designed to observe the astrophysically
important neutral atomic oxygen [OI] line at 4.7448 THz. The local oscillator
is a third-order distributed feedback Quantum Cascade Laser operating in
continuous wave mode at 4.741 THz. A quasi-optical, superconducting NbN hot
electron bolometer is used as the mixer. We recorded a double sideband receiver
noise temperature (T^DSB_rec) of 815 K, which is ~7 times the quantum noise
limit (h{\nu}/2k_B) and an Allan variance time of 15 s at an effective noise
fluctuation bandwidth of 18 MHz. Heterodyne performance was confirmed by
measuring a methanol line spectrum."
Qing Hu,Hu_Qing,arXiv:1006.5915,https://arxiv.org/abs/1006.5915,"Abstract:  We study a ""strongly-coupled"" (SC) polariton system formed between the
atom-like intersubband transitions in a semiconductor nanostructure and the THz
optical modes that are localised at the edges of a gold aperture. The
polaritons can be excited optically, by incoherent excitation with bandgap
radiation, and we find that they also coherently scatter the same input laser,
to give strikingly sharp ""sideband"" (SB) spectral peaks in the backscattered
spectrum. The SB intensity is a sensitive track of the polariton density and
they can be detected down to a quantum noise floor that is more than 2500 times
lower than the excitation thresholds of comparable quantum cascade laser
diodes. Compared with other coherent scattering mechanisms, higher order SB
scattering events are readily observable, and we speculate that the effect may
find utility as a passive all-optical wavelength shifting mechanism in
telecommunications systems."
Qing Hu,Hu_Qing,arXiv:0910.2959,https://arxiv.org/abs/0910.2959,"Abstract:  We develop simple density-matrix models to describe the role of coherence in
resonant-tunneling (RT) transport of quantum-cascade lasers (QCLs).
Specifically, we investigate the effects of coherent coupling between the
lasing levels with other levels on the transport properties and gain spectra.
In the first part of the paper, we use a three-level density-matrix model to
obtain useful analytical expressions for current transport through the injector
barrier in a QCL. An expression for the slope discontinuity in the
current-voltage characteristics at the lasing threshold is derived. This value
is shown to be a direct measure of the population inversion at threshold, and
contradicts the previously held belief of it being indicative of ratio of the
laser level lifetimes. In the second part of the paper, we use density matrices
to compute the gain spectrum for a resonant-phonon terahertz QCL design. The
large anticrossing of the doublet of lower radiative levels is reflected in a
broad gain linewidth due to a coherent RT assisted depopulation process. At
certain bias conditions, the gain spectrum exhibits double peaks which is
supported by experimental observations."
Qing Hu,Hu_Qing,arXiv:cond-mat/0703354,https://arxiv.org/abs/cond-mat/0703354,"Abstract:  It is shown that the errors of present-day exchange-correlation (xc)
functionals are rather short ranged. For extended systems the correction can
therefore be evaluated by analyzing properly chosen clusters and employing
highest-quality quantum chemistry methods. The xc correction rapidly approaches
a universal dependence with cluster size. The method is applicable to bulk
systems as well as to defects in the bulk and at surfaces. It is demonstrated
here for CO adsorption at transition-metal surfaces, where present-day xc
functionals dramatically fail to predict the correct adsorption site, and for
the crystal bulk cohesive energy."
Piotr Indyk,Indyk_Piotr,arXiv:1901.08544,https://arxiv.org/abs/1901.08544,"Abstract:  Most of the efficient sublinear-time indexing algorithms for the
high-dimensional nearest neighbor search problem (NNS) are based on space
partitions of the ambient space $\mathbb{R}^d$. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing
such partitions that reduces the problem to balanced graph partitioning
followed by supervised classification. We instantiate this general approach
with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural
networks, respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions found by Neural LSH consistently
outperform partitions found by quantization- and tree-based methods."
Piotr Indyk,Indyk_Piotr,arXiv:1807.11648,https://arxiv.org/abs/1807.11648,"Abstract:  We study a spectral generalization of classical combinatorial graph spanners
to the spectral setting. Given a set of vectors $V\subseteq \Re^d$, we say a
set $U\subseteq V$ is an $\alpha$-spectral spanner if for all $v\in V$ there is
a probability distribution $\mu_v$ supported on $U$ such that $$vv^\intercal
\preceq \alpha\cdot\mathbb{E}_{u\sim\mu_v} uu^\intercal.$$ We show that any set
$V$ has an $\tilde{O}(d)$-spectral spanner of size $\tilde{O}(d)$ and this
bound is almost optimal in the worst case.
We use spectral spanners to study composable core-sets for spectral problems.
We show that for many objective functions one can use a spectral spanner,
independent of the underlying functions, as a core-set and obtain almost
optimal composable core-sets. For example, for the determinant maximization
problem we obtain an $\tilde{O}(k)^k$-composable core-set and we show that this
is almost optimal in the worst case.
Our algorithm is a spectral analogue of the classical greedy algorithm for
finding (combinatorial) spanners in graphs. We expect that our spanners find
many other applications in distributed or parallel models of computation. Our
proof is spectral. As a side result of our techniques, we show that the rank of
diagonally dominant lower-triangular matrices are robust under `small
perturbations' which could be of independent interests."
Piotr Indyk,Indyk_Piotr,arXiv:1807.00112,https://arxiv.org/abs/1807.00112,"Abstract:  We consider the $(1+\epsilon)$-approximate nearest neighbor search problem:
given a set $X$ of $n$ points in a $d$-dimensional space, build a data
structure that, given any query point $y$, finds a point $x \in X$ whose
distance to $y$ is at most $(1+\epsilon) \min_{x \in X} \|x-y\|$ for an
accuracy parameter $\epsilon \in (0,1)$. Our main result is a data structure
that occupies only $O(\epsilon^{-2} n \log(n) \log(1/\epsilon))$ bits of space,
assuming all point coordinates are integers in the range $\{-n^{O(1)} \ldots
n^{O(1)}\}$, i.e., the coordinates have $O(\log n)$ bits of precision. This
improves over the best previously known space bound of $O(\epsilon^{-2} n
\log(n)^2)$, obtained via the randomized dimensionality reduction method of
Johnson and Lindenstrauss (1984). We also consider the more general problem of
estimating all distances from a collection of query points to all data points
$X$, and provide almost tight upper and lower bounds for the space complexity
of this problem."
Piotr Indyk,Indyk_Piotr,arXiv:1806.09823,https://arxiv.org/abs/1806.09823,"Abstract:  The nearest neighbor problem is defined as follows: Given a set $P$ of $n$
points in some metric space $(X,D)$, build a data structure that, given any
point $q$, returns a point in $P$ that is closest to $q$ (its ""nearest
neighbor"" in $P$). The data structure stores additional information about the
set $P$, which is then used to find the nearest neighbor without computing all
distances between $q$ and $P$. The problem has a wide range of applications in
machine learning, computer vision, databases and other fields.
To reduce the time needed to find nearest neighbors and the amount of memory
used by the data structure, one can formulate the {\em approximate} nearest
neighbor problem, where the the goal is to return any point $p' \in P$ such
that the distance from $q$ to $p'$ is at most $c \cdot \min_{p \in P} D(q,p)$,
for some $c \geq 1$. Over the last two decades, many efficient solutions to
this problem were developed. In this article we survey these developments, as
well as their connections to questions in geometric functional analysis and
combinatorial geometry."
Piotr Indyk,Indyk_Piotr,arXiv:1711.01520,https://arxiv.org/abs/1711.01520,"Abstract:  We introduce a new distance-preserving compact representation of
multi-dimensional point-sets. Given $n$ points in a $d$-dimensional space where
each coordinate is represented using $B$ bits (i.e., $dB$ bits per point), it
produces a representation of size $O( d \log(d B/\epsilon) + \log n)$ bits per
point from which one can approximate the distances up to a factor of $1 \pm
\epsilon$. Our algorithm almost matches the recent bound
of~\cite{indyk2017near} while being much simpler. We compare our algorithm to
Product Quantization (PQ)~\cite{jegou2011product}, a state of the art heuristic
metric compression method. We evaluate both algorithms on several data sets:
SIFT (used in \cite{jegou2011product}), MNIST~\cite{lecun1998mnist}, New York
City taxi time series~\cite{guha2016robust} and a synthetic one-dimensional
data set embedded in a high-dimensional space. With appropriately tuned
parameters, our algorithm produces representations that are comparable to or
better than those produced by PQ, while having provable guarantees on its
performance."
Piotr Indyk,Indyk_Piotr,arXiv:1706.06935,https://arxiv.org/abs/1706.06935,"Abstract:  There is much interest in integrating millimeter wave radios (mmWave) into
wireless LANs and 5G cellular networks to benefit from their multiple GHz of
available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios
require highly directional antennas. Since the antennas have pencil-beams, the
transmitter and receiver need to align their antenna beams before they can
communicate. Existing solutions scan the entire space to find the best
alignment. Such a process has been shown to introduce up to seconds of delay,
and is unsuitable for wireless networks where an access point has to quickly
switch between users and accommodate mobile clients.
This paper presents Rapid-Link, a new protocol that can find the best mmWave
beam alignment without scanning the space. Given all possible directions for
setting the antenna beam, Rapid-Link provably finds the optimal direction in
logarithmic number of measurements. Further, Rapid-Link works within the
existing 802.11ad standard for mmWave LAN, and can support both clients and
access points. We have implemented Rapid-Link in a mmWave radio and evaluated
it empirically. Our results show that it reduces beam alignment delay by orders
of magnitude. In particular, for highly directional mmWave devices operating
under 802.11ad, the delay drops from over a second to 2.5 ms."
Piotr Indyk,Indyk_Piotr,arXiv:1704.02958,https://arxiv.org/abs/1704.02958,"Abstract:  Empirical risk minimization (ERM) is ubiquitous in machine learning and
underlies most supervised learning methods. While there has been a large body
of work on algorithms for various ERM problems, the exact computational
complexity of ERM is still not understood. We address this issue for multiple
popular ERM problems including kernel SVMs, kernel ridge regression, and
training the final layer of a neural network. In particular, we give
conditional hardness results for these problems based on complexity-theoretic
assumptions such as the Strong Exponential Time Hypothesis. Under these
assumptions, we show that there are no algorithms that solve the aforementioned
ERM problems to high accuracy in sub-quadratic time. We also give similar
hardness results for computing the gradient of the empirical loss, which is the
main computational burden in many non-convex learning tasks."
Piotr Indyk,Indyk_Piotr,arXiv:1609.08739,https://arxiv.org/abs/1609.08739,"Abstract:  In the Sparse Linear Regression (SLR) problem, given a $d \times n$ matrix
$M$ and a $d$-dimensional query $q$, the goal is to compute a $k$-sparse
$n$-dimensional vector $\tau$ such that the error $||M \tau-q||$ is minimized.
This problem is equivalent to the following geometric problem: given a set $P$
of $n$ points and a query point $q$ in $d$ dimensions, find the closest
$k$-dimensional subspace to $q$, that is spanned by a subset of $k$ points in
$P$. In this paper, we present data-structures/algorithms and conditional lower
bounds for several variants of this problem (such as finding the closest
induced $k$ dimensional flat/simplex instead of a subspace).
In particular, we present approximation algorithms for the online variants of
the above problems with query time $\tilde O(n^{k-1})$, which are of interest
in the ""low sparsity regime"" where $k$ is small, e.g., $2$ or $3$. For $k=d$,
this matches, up to polylogarithmic factors, the lower bound that relies on the
affinely degenerate conjecture (i.e., deciding if $n$ points in $\mathbb{R}^d$
contains $d+1$ points contained in a hyperplane takes $\Omega(n^d)$ time).
Moreover, our algorithms involve formulating and solving several geometric
subproblems, which we believe to be of independent interest."
Piotr Indyk,Indyk_Piotr,arXiv:1609.06295,https://arxiv.org/abs/1609.06295,"Abstract:  The metric sketching problem is defined as follows. Given a metric on $n$
points, and $\epsilon>0$, we wish to produce a small size data structure
(sketch) that, given any pair of point indices, recovers the distance between
the points up to a $1+\epsilon$ distortion. In this paper we consider metrics
induced by $\ell_2$ and $\ell_1$ norms whose spread (the ratio of the diameter
to the closest pair distance) is bounded by $\Phi>0$. A well-known
dimensionality reduction theorem due to Johnson and Lindenstrauss yields a
sketch of size $O(\epsilon^{-2} \log (\Phi n) n\log n)$, i.e., $O(\epsilon^{-2}
\log (\Phi n) \log n)$ bits per point. We show that this bound is not optimal,
and can be substantially improved to $O(\epsilon^{-2}\log(1/\epsilon) \cdot
\log n + \log\log \Phi)$ bits per point. Furthermore, we show that our bound is
tight up to a factor of $\log(1/\epsilon)$.
We also consider sketching of general metrics and provide a sketch of size
$O(n\log(1/\epsilon)+ \log\log \Phi)$ bits per point, which we show is optimal."
Piotr Indyk,Indyk_Piotr,arXiv:1604.02188,https://arxiv.org/abs/1604.02188,"Abstract:  Motivated by applications in computer vision and databases, we introduce and
study the Simultaneous Nearest Neighbor Search (SNN) problem. Given a set of
data points, the goal of SNN is to design a data structure that, given a
collection of queries, finds a collection of close points that are compatible
with each other. Formally, we are given $k$ query points $Q=q_1,\cdots,q_k$,
and a compatibility graph $G$ with vertices in $Q$, and the goal is to return
data points $p_1,\cdots,p_k$ that minimize (i) the weighted sum of the
distances from $q_i$ to $p_i$ and (ii) the weighted sum, over all edges $(i,j)$
in the compatibility graph $G$, of the distances between $p_i$ and $p_j$. The
problem has several applications, where one wants to return a set of consistent
answers to multiple related queries. This generalizes well-studied
computational problems, including NN, Aggregate NN and the 0-extension problem.
In this paper we propose and analyze the following general two-step method
for designing efficient data structures for SNN. In the first step, for each
query point $q_i$ we find its (approximate) nearest neighbor point $\hat{p}_i$;
this can be done efficiently using existing approximate nearest neighbor
structures. In the second step, we solve an off-line optimization problem over
sets $q_1,\cdots,q_k$ and $\hat{p}_1,\cdots,\hat{p}_k$; this can be done
efficiently given that $k$ is much smaller than $n$. Even though
$\hat{p}_1,\cdots,\hat{p}_k$ might not constitute the optimal answers to
queries $q_1,\cdots,q_k$, we show that, for the unweighted case, the resulting
algorithm is $O(\log k/\log \log k)$-approximation. Also, we show that the
approximation factor can be in fact reduced to a constant for compatibility
graphs frequently occurring in practice.
Finally, we show that the ""empirical approximation factor"" provided by the
above approach is very close to 1."
Piotr Indyk,Indyk_Piotr,arXiv:1511.07070,https://arxiv.org/abs/1511.07070,"Abstract:  Regular expressions constitute a fundamental notion in formal language theory
and are frequently used in computer science to define search patterns. A
classic algorithm for these problems constructs and simulates a
non-deterministic finite automaton corresponding to the expression, resulting
in an $O(mn)$ running time (where $m$ is the length of the pattern and $n$ is
the length of the text). This running time can be improved slightly (by a
polylogarithmic factor), but no significantly faster solutions are known. At
the same time, much faster algorithms exist for various special cases of
regular expressions, including dictionary matching, wildcard matching, subset
matching, word break problem etc.
In this paper, we show that the complexity of regular expression matching can
be characterized based on its {\em depth} (when interpreted as a formula). Our
results hold for expressions involving concatenation, OR, Kleene star and
Kleene plus. For regular expressions of depth two (involving any combination of
the above operators), we show the following dichotomy: matching and membership
testing can be solved in near-linear time, except for ""concatenations of
stars"", which cannot be solved in strongly sub-quadratic time assuming the
Strong Exponential Time Hypothesis (SETH). For regular expressions of depth
three the picture is more complex. Nevertheless, we show that all problems can
either be solved in strongly sub-quadratic time, or cannot be solved in
strongly sub-quadratic time assuming SETH.
An intriguing special case of membership testing involves regular expressions
of the form ""a star of an OR of concatenations"", e.g., $[a|ab|bc]^*$. This
corresponds to the so-called {\em word break} problem, for which a dynamic
programming algorithm with a runtime of (roughly) $O(n\sqrt{m})$ is known. We
show that the latter bound is not tight and improve the runtime to
$O(nm^{0.44\ldots})$."
Piotr Indyk,Indyk_Piotr,arXiv:1509.02897,https://arxiv.org/abs/1509.02897,"Abstract:  We show the existence of a Locality-Sensitive Hashing (LSH) family for the
angular distance that yields an approximate Near Neighbor Search algorithm with
the asymptotically optimal running time exponent. Unlike earlier algorithms
with this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn
2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving
upon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also
introduce a multiprobe version of this algorithm, and conduct experimental
evaluation on real and synthetic data sets.
We complement the above positive results with a fine-grained lower bound for
the quality of any LSH family for angular distance. Our lower bound implies
that the above LSH family exhibits a trade-off between evaluation time and
quality that is close to optimal for a natural class of LSH functions."
Piotr Indyk,Indyk_Piotr,arXiv:1509.00118,https://arxiv.org/abs/1509.00118,"Abstract:  We consider the classic Set Cover problem in the data stream model. For $n$
elements and $m$ sets ($m\geq n$) we give a $O(1/\delta)$-pass algorithm with a
strongly sub-linear $\tilde{O}(mn^{\delta})$ space and logarithmic
approximation factor. This yields a significant improvement over the earlier
algorithm of Demaine et al. [DIMV14] that uses exponentially larger number of
passes. We complement this result by showing that the tradeoff between the
number of passes and space exhibited by our algorithm is tight, at least when
the approximation factor is equal to $1$. Specifically, we show that any
algorithm that computes set cover exactly using $({1 \over 2\delta}-1)$ passes
must use $\tilde{\Omega}(mn^{\delta})$ space in the regime of $m=O(n)$.
Furthermore, we consider the problem in the geometric setting where the
elements are points in $\mathbb{R}^2$ and sets are either discs, axis-parallel
rectangles, or fat triangles in the plane, and show that our algorithm (with a
slight modification) uses the optimal $\tilde{O}(n)$ space to find a
logarithmic approximation in $O(1/\delta)$ passes.
Finally, we show that any randomized one-pass algorithm that distinguishes
between covers of size 2 and 3 must use a linear (i.e., $\Omega(mn)$) amount of
space. This is the first result showing that a randomized, approximate
algorithm cannot achieve a space bound that is sublinear in the input size.
This indicates that using multiple passes might be necessary in order to
achieve sub-linear space bounds for this problem while guaranteeing small
approximation factors."
Piotr Indyk,Indyk_Piotr,arXiv:1504.07648,https://arxiv.org/abs/1504.07648,"Abstract:  For every fixed constant $\alpha > 0$, we design an algorithm for computing
the $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \in
\mathbb{R}^N$ in time $k^{1+\alpha} (\log N)^{O(1)}$. Specifically, the
algorithm is given query access to $x$ and computes a $k$-sparse $\tilde{x} \in
\mathbb{R}^N$ satisfying $\|\tilde{x} - \hat{x}\|_1 \leq c \|\hat{x} -
H_k(\hat{x})\|_1$, for an absolute constant $c > 0$, where $\hat{x}$ is the
transform of $x$ and $H_k(\hat{x})$ is its best $k$-sparse approximation. Our
algorithm is fully deterministic and only uses non-adaptive queries to $x$
(i.e., all queries are determined and performed in parallel when the algorithm
starts).
An important technical tool that we use is a construction of nearly optimal
and linear lossless condensers which is a careful instantiation of the GUV
condenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a
deterministic and non-adaptive $\ell_1/\ell_1$ compressed sensing scheme based
on general lossless condensers that is equipped with a fast reconstruction
algorithm running in time $k^{1+\alpha} (\log N)^{O(1)}$ (for the GUV-based
condenser) and is of independent interest. Our scheme significantly simplifies
and improves an earlier expander-based construction due to Berinde, Gilbert,
Indyk, Karloff, Strauss (Allerton 2008).
Our methods use linear lossless condensers in a black box fashion; therefore,
any future improvement on explicit constructions of such condensers would
immediately translate to improved parameters in our framework (potentially
leading to $k (\log N)^{O(1)}$ reconstruction time with a reduced exponent in
the poly-logarithmic factor, and eliminating the extra parameter $\alpha$).
Finally, by allowing the algorithm to use randomness, while still using
non-adaptive queries, the running time of the algorithm can be improved to
$\tilde{O}(k \log^3 N)$."
Piotr Indyk,Indyk_Piotr,arXiv:1504.01076,https://arxiv.org/abs/1504.01076,"Abstract:  We initiate the study of trade-offs between sparsity and the number of
measurements in sparse recovery schemes for generic norms. Specifically, for a
norm $\|\cdot\|$, sparsity parameter $k$, approximation factor $K>0$, and
probability of failure $P>0$, we ask: what is the minimal value of $m$ so that
there is a distribution over $m \times n$ matrices $A$ with the property that
for any $x$, given $Ax$, we can recover a $k$-sparse approximation to $x$ in
the given norm with probability at least $1-P$? We give a partial answer to
this problem, by showing that for norms that admit efficient linear sketches,
the optimal number of measurements $m$ is closely related to the doubling
dimension of the metric induced by the norm $\|\cdot\|$ on the set of all
$k$-sparse vectors. By applying our result to specific norms, we cast known
measurement bounds in our general framework (for the $\ell_p$ norms, $p \in
[1,2]$) as well as provide new, measurement-efficient schemes (for the
Earth-Mover Distance norm). The latter result directly implies more succinct
linear sketches for the well-studied planar $k$-median clustering problem.
Finally, our lower bound for the doubling dimension of the EMD norm enables us
to address the open question of [Frahling-Sohler, STOC'05] about the space
complexity of clustering problems in the dynamic streaming model."
Piotr Indyk,Indyk_Piotr,arXiv:1412.3040,https://arxiv.org/abs/1412.3040,"Abstract:  Visualizations are frequently used as a means to understand trends and gather
insights from datasets, but often take a long time to generate. In this paper,
we focus on the problem of rapidly generating approximate visualizations while
preserving crucial visual proper- ties of interest to analysts. Our primary
focus will be on sampling algorithms that preserve the visual property of
ordering; our techniques will also apply to some other visual properties. For
instance, our algorithms can be used to generate an approximate visualization
of a bar chart very rapidly, where the comparisons between any two bars are
correct. We formally show that our sampling algorithms are generally applicable
and provably optimal in theory, in that they do not take more samples than
necessary to generate the visualizations with ordering guarantees. They also
work well in practice, correctly ordering output groups while taking orders of
magnitude fewer samples and much less time than conventional sampling schemes."
Piotr Indyk,Indyk_Piotr,arXiv:1412.0348,https://arxiv.org/abs/1412.0348,"Abstract:  The edit distance (a.k.a. the Levenshtein distance) between two strings is
defined as the minimum number of insertions, deletions or substitutions of
symbols needed to transform one string into another. The problem of computing
the edit distance between two strings is a classical computational task, with a
well-known algorithm based on dynamic programming. Unfortunately, all known
algorithms for this problem run in nearly quadratic time.
In this paper we provide evidence that the near-quadratic running time bounds
known for the problem of computing edit distance might be tight. Specifically,
we show that, if the edit distance can be computed in time $O(n^{2-\delta})$
for some constant $\delta>0$, then the satisfiability of conjunctive normal
form formulas with $N$ variables and $M$ clauses can be solved in time
$M^{O(1)} 2^{(1-\epsilon)N}$ for a constant $\epsilon>0$. The latter result
would violate the Strong Exponential Time Hypothesis, which postulates that
such algorithms do not exist."
Piotr Indyk,Indyk_Piotr,arXiv:1406.1579,https://arxiv.org/abs/1406.1579,"Abstract:  Compressive Sensing (CS) stipulates that a sparse signal can be recovered
from a small number of linear measurements, and that this recovery can be
performed efficiently in polynomial time. The framework of model-based
compressive sensing (model-CS) leverages additional structure in the signal and
prescribes new recovery schemes that can reduce the number of measurements even
further. However, model-CS requires an algorithm that solves the
model-projection problem: given a query signal, produce the signal in the model
that is also closest to the query signal. Often, this optimization can be
computationally very expensive. Moreover, an approximation algorithm is not
sufficient for this optimization task. As a result, the model-projection
problem poses a fundamental obstacle for extending model-CS to many interesting
models.
In this paper, we introduce a new framework that we call
approximation-tolerant model-based compressive sensing. This framework includes
a range of algorithms for sparse recovery that require only approximate
solutions for the model-projection problem. In essence, our work removes the
aforementioned obstacle to model-based compressive sensing, thereby extending
the applicability of model-CS to a much wider class of models. We instantiate
this new framework for the Constrained Earth Mover Distance (CEMD) model, which
is particularly useful for signal ensembles where the positions of the nonzero
coefficients do not change significantly as a function of spatial (or temporal)
location. We develop novel approximation algorithms for both the maximization
and the minimization versions of the model-projection problem via graph
optimization techniques. Leveraging these algorithms into our framework results
in a nearly sample-optimal sparse recovery scheme for the CEMD model."
Piotr Indyk,Indyk_Piotr,arXiv:1403.5804,https://arxiv.org/abs/1403.5804,"Abstract:  We give an algorithm for $\ell_2/\ell_2$ sparse recovery from Fourier
measurements using $O(k\log N)$ samples, matching the lower bound of
\cite{DIPW} for non-adaptive algorithms up to constant factors for any $k\leq
N^{1-\delta}$. The algorithm runs in $\tilde O(N)$ time. Our algorithm extends
to higher dimensions, leading to sample complexity of $O_d(k\log N)$, which is
optimal up to constant factors for any $d=O(1)$. These are the first sample
optimal algorithms for these problems.
A preliminary experimental evaluation indicates that our algorithm has
empirical sampling complexity comparable to that of other recovery methods
known in the literature, while providing strong provable guarantees on the
recovery quality."
Piotr Indyk,Indyk_Piotr,arXiv:1306.1547,https://arxiv.org/abs/1306.1547,"Abstract:  We present a new data structure for the c-approximate near neighbor problem
(ANN) in the Euclidean space. For n points in R^d, our algorithm achieves
O(n^{\rho} + d log n) query time and O(n^{1 + \rho} + d log n) space, where
\rho <= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over the
result by Andoni and Indyk (FOCS 2006) and the first data structure that
bypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu and
Zhou (ICS 2011). By a standard reduction we obtain a data structure for the
Hamming space and \ell_1 norm with \rho <= 7/(8c) + O(1/c^{3/2}) + o(1), which
is the first improvement over the result of Indyk and Motwani (STOC 1998)."
Piotr Indyk,Indyk_Piotr,arXiv:1304.3604,https://arxiv.org/abs/1304.3604,"Abstract:  The Restricted Isometry Property (RIP) is a fundamental property of a matrix
enabling sparse recovery. Informally, an m x n matrix satisfies RIP of order k
in the l_p norm if ||Ax||_p \approx ||x||_p for any vector x that is k-sparse,
i.e., that has at most k non-zeros. The minimal number of rows m necessary for
the property to hold has been extensively investigated, and tight bounds are
known. Motivated by signal processing models, a recent work of Baraniuk et al
has generalized this notion to the case where the support of x must belong to a
given model, i.e., a given family of supports. This more general notion is much
less understood, especially for norms other than l_2. In this paper we present
tight bounds for the model-based RIP property in the l_1 norm. Our bounds hold
for the two most frequently investigated models: tree-sparsity and
block-sparsity. We also show implications of our results to sparse recovery
problems."
Piotr Indyk,Indyk_Piotr,arXiv:1303.1209,https://arxiv.org/abs/1303.1209,"Abstract:  We present the first sample-optimal sublinear time algorithms for the sparse
Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our
algorithms are analyzed for /average case/ signals. For signals whose spectrum
is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,
where k is the expected sparsity of the signal. For signals whose spectrum is
approximately sparse, our algorithm uses O(k log n) samples and runs in O(k
log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of
samples used by our algorithms matches the known lower bounds for the
respective signal models.
By a known reduction, our algorithms give similar results for the
one-dimensional sparse Discrete Fourier Transform when n is a power of a small
composite number (e.g., n = 6^t)."
Piotr Indyk,Indyk_Piotr,arXiv:1208.2447,https://arxiv.org/abs/1208.2447,"Abstract:  We propose a framework for compressive sensing of images with local
distinguishable objects, such as stars, and apply it to solve a problem in
celestial navigation. Specifically, let x be an N-pixel real-valued image,
consisting of a small number of local distinguishable objects plus noise. Our
goal is to design an m-by-N measurement matrix A with m << N, such that we can
recover an approximation to x from the measurements Ax.
We construct a matrix A and recovery algorithm with the following properties:
(i) if there are k objects, the number of measurements m is O((k log N)/(log
k)), undercutting the best known bound of O(k log(N/k)) (ii) the matrix A is
very sparse, which is important for hardware implementations of compressive
sensing algorithms, and (iii) the recovery algorithm is empirically fast and
runs in time polynomial in k and log(N).
We also present a comprehensive study of the application of our algorithm to
attitude determination, or finding one's orientation in space. Spacecraft
typically use cameras to acquire an image of the sky, and then identify stars
in the image to compute their orientation. Taking pictures is very expensive
for small spacecraft, since camera sensors use a lot of power. Our algorithm
optically compresses the image before it reaches the camera's array of pixels,
reducing the number of sensors that are required."
Piotr Indyk,Indyk_Piotr,arXiv:1201.2501,https://arxiv.org/abs/1201.2501,"Abstract:  We consider the problem of computing the k-sparse approximation to the
discrete Fourier transform of an n-dimensional signal. We show:
* An O(k log n)-time randomized algorithm for the case where the input signal
has at most k non-zero Fourier coefficients, and
* An O(k log n log(n/k))-time randomized algorithm for general input signals.
Both algorithms achieve o(n log n) time, and thus improve over the Fast
Fourier Transform, for any k = o(n). They are the first known algorithms that
satisfy this property. Also, if one assumes that the Fast Fourier Transform is
optimal, the algorithm for the exactly k-sparse case is optimal for any k =
n^{\Omega(1)}.
We complement our algorithmic results by showing that any algorithm for
computing the sparse Fourier transform of a general signal must use at least
\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform
adaptive sampling."
Piotr Indyk,Indyk_Piotr,arXiv:1110.3850,https://arxiv.org/abs/1110.3850,"Abstract:  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation
$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal is
to recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for some
constant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or
$p=q=2$, this task can be accomplished using $m=O(k \log (n/k))$ non-adaptive
measurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].
In this paper we show that if one is allowed to perform measurements that are
adaptive, then the number of measurements can be considerably reduced.
Specifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)k
log log (n eps/k))$ measurements that uses $O(log* k \log \log (n eps/k))$
rounds. This is a significant improvement over the best possible non-adaptive
bound. - A scheme with $m=O((1/eps) k log (k/eps) + k \log (n/k))$ measurements
that uses /two/ rounds. This improves over the best possible non-adaptive
bound. To the best of our knowledge, these are the first results of this type.
As an independent application, we show how to solve the problem of finding a
duplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using
$O(log n)$ bits of space and $O(log log n)$ passes, improving over the best
possible space complexity achievable using a single pass."
Piotr Indyk,Indyk_Piotr,arXiv:1106.0365,https://arxiv.org/abs/1106.0365,"Abstract:  We consider the following k-sparse recovery problem: design an m x n matrix
A, such that for any signal x, given Ax we can efficiently recover x'
satisfying
||x-x'||_1 <= C min_{k-sparse} x""} ||x-x""||_1.
It is known that there exist matrices A with this property that have only O(k
log (n/k)) rows.
In this paper we show that this bound is tight. Our bound holds even for the
more general /randomized/ version of the problem, where A is a random variable
and the recovery algorithm is required to work for any fixed x with constant
probability (over A)."
Piotr Indyk,Indyk_Piotr,arXiv:1104.4674,https://arxiv.org/abs/1104.4674,"Abstract:  We initiate the study of sparse recovery problems under the Earth-Mover
Distance (EMD). Specifically, we design a distribution over m x n matrices A
such that for any x, given Ax, we can recover a k-sparse approximation to x
under the EMD distance. One construction yields m = O(k log(n/k)) and a 1 +
epsilon approximation factor, which matches the best achievable bound for other
error measures, such as the L_1 norm. Our algorithms are obtained by exploiting
novel connections to other problems and areas, such as streaming algorithms for
k-median clustering and model-based compressive sensing. We also provide novel
algorithms and results for the latter problems."
Piotr Indyk,Indyk_Piotr,arXiv:1001.0041,https://arxiv.org/abs/1001.0041,"Abstract:  It has been known since 1970's that the N-dimensional $\ell_1$-space contains
nearly Euclidean subspaces whose dimension is $\Omega(N)$. However, proofs of
existence of such subspaces were probabilistic, hence non-constructive, which
made the results not-quite-suitable for subsequently discovered applications to
high-dimensional nearest neighbor search, error-correcting codes over the
reals, compressive sensing and other computational problems. In this paper we
present a ""low-tech"" scheme which, for any $a > 0$, allows to exhibit nearly
Euclidean $\Omega(N)$-dimensional subspaces of $\ell_1^N$ while using only
$N^a$ random bits. Our results extend and complement (particularly) recent work
by Guruswami-Lee-Wigderson. Characteristic features of our approach include (1)
simplicity (we use only tensor products) and (2) yielding ""almost Euclidean""
subspaces with arbitrarily small distortions."
Piotr Indyk,Indyk_Piotr,arXiv:cs/0303001,https://arxiv.org/abs/cs/0303001,"Abstract:  In the first part of the paper, we present an (1+\mu)-approximation algorithm
to the minimum-spanning tree of points in a planar arrangement of lines, where
the metric is the number of crossings between the spanning tree and the lines.
The expected running time is O((n/\mu^5) alpha^3(n) log^5 n), where \mu > 0 is
a prescribed constant.
In the second part of our paper, we show how to embed such a crossing metric,
into high-dimensions, so that the distances are preserved. As a result, we can
deploy a large collection of subquadratic approximations algorithms \cite
im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric
as a distance function. Applications include matching, clustering,
nearest-neighbor, and furthest-neighbor."
Piotr Indyk,Indyk_Piotr,arXiv:cs/0009013,https://arxiv.org/abs/cs/0009013,"Abstract:  In this paper we present algorithms for a number of problems in geometric
pattern matching where the input consist of a collections of segments in the
plane. Our work consists of two main parts. In the first, we address problems
and measures that relate to collections of orthogonal line segments in the
plane. Such collections arise naturally from problems in mapping buildings and
robot exploration.
We propose a new measure of segment similarity called a \emph{coverage
measure}, and present efficient algorithms for maximising this measure between
sets of axis-parallel segments under translations. Our algorithms run in time
$O(n^3\polylog n)$ in the general case, and run in time $O(n^2\polylog n)$ for
the case when all segments are horizontal. In addition, we show that when
restricted to translations that are only vertical, the Hausdorff distance
between two sets of horizontal segments can be computed in time roughly
$O(n^{3/2}{\sl polylog}n)$. These algorithms form significant improvements over
the general algorithm of Chew et al. that takes time $O(n^4 \log^2 n)$. In the
second part of this paper we address the problem of matching polygonal chains.
We study the well known \Frd, and present the first algorithm for computing the
\Frd under general translations. Our methods also yield algorithms for
computing a generalization of the \Fr distance, and we also present a simple
approximation algorithm for the \Frd that runs in time $O(n^2\polylog n)$."
Erich Ippen,Ippen_Erich,arXiv:1410.8120,https://arxiv.org/abs/1410.8120,"Abstract:  A model for THz generation by optical rectification using tilted-pulse-fronts
is developed. It simultaneously accounts for (i) the spatio-temporal
distortions of the optical pump pulse, (ii) the nonlinear coupled interaction
of THz and optical radiation in two spatial dimensions (2-D), (iii) self-phase
modulation and (iv) stimulated Raman scattering. The model is validated by
quantitative agreement with experiments and analytic calculations. We show that
the optical pump beam is significantly broadened in the transverse-momentum
(kx) domain as a consequence of the spectral broadening caused by THz
generation. In the presence of this large frequency and transverse-momentum (or
angular) spread, group velocity dispersion causes a spatio-temporal break-up of
the optical pump pulse which inhibits further THz generation. The implications
of these effects on energy scaling and optimization of optical-to-THz
conversion efficiency are discussed. This suggests the use of optical pump
pulses with elliptical beam profiles for large optical pump energies. It is
seen that optimization of the setup is highly dependent on optical pump
conditions. Trade-offs of optimizing the optical-to-THz conversion efficiency
on the spatial and spectral properties of THz radiation is discussed to guide
the development of such sources."
Phillip Isola,Isola_Phillip,arXiv:1812.00231,https://arxiv.org/abs/1812.00231,"Abstract:  Good Visual Retargeting changes the global size and aspect ratio of a natural
image, while preserving the size and aspect ratio of all its local elements. We
propose formulating this principle by requiring that the distribution of
patches in the input matches the distribution of patches in the output. We
introduce a Deep-Learning approach for retargeting, based on an ""Internal GAN""
(InGAN). InGAN is an image-specific GAN. It incorporates the Internal
statistics of a single natural image in a GAN. It is trained on a single input
image and learns the distribution of its patches. It is then able to synthesize
natural looking target images composed from the input image patch-distribution.
InGAN is totally unsupervised, and requires no additional data other than the
input image itself. Moreover, once trained on the input image, it can generate
target images of any specified size or aspect ratio in real-time."
Phillip Isola,Isola_Phillip,arXiv:1802.04821,https://arxiv.org/abs/1802.04821,"Abstract:  We propose a metalearning approach for learning gradient-based reinforcement
learning (RL) algorithms. The idea is to evolve a differentiable loss function,
such that an agent, which optimizes its policy to minimize this loss, will
achieve high rewards. The loss is parametrized via temporal convolutions over
the agent's experience. Because this loss is highly flexible in its ability to
take into account the agent's history, it enables fast task learning. Empirical
results show that our evolved policy gradient algorithm (EPG) achieves faster
learning on several randomized environments compared to an off-the-shelf policy
gradient method. We also demonstrate that EPG's learned loss can generalize to
out-of-distribution test time tasks, and exhibits qualitatively different
behavior from other popular metalearning algorithms."
Phillip Isola,Isola_Phillip,arXiv:1801.03924,https://arxiv.org/abs/1801.03924,"Abstract:  While it is nearly effortless for humans to quickly assess the perceptual
similarity between two images, the underlying processes are thought to be quite
complex. Despite this, the most widely used perceptual metrics today, such as
PSNR and SSIM, are simple, shallow functions, and fail to account for many
nuances of human perception. Recently, the deep learning community has found
that features of the VGG network trained on ImageNet classification has been
remarkably useful as a training loss for image synthesis. But how perceptual
are these so-called ""perceptual losses""? What elements are critical for their
success? To answer these questions, we introduce a new dataset of human
perceptual similarity judgments. We systematically evaluate deep features
across different architectures and tasks and compare them with classic metrics.
We find that deep features outperform all previous metrics by large margins on
our dataset. More surprisingly, this result is not restricted to
ImageNet-trained VGG features, but holds across different deep architectures
and levels of supervision (supervised, self-supervised, or even unsupervised).
Our results suggest that perceptual similarity is an emergent property shared
across deep visual representations."
Phillip Isola,Isola_Phillip,arXiv:1711.03213,https://arxiv.org/abs/1711.03213,"Abstract:  Domain adaptation is critical for success in new, unseen environments.
Adversarial adaptation models applied in feature spaces discover domain
invariant representations, but are difficult to visualize and sometimes fail to
capture pixel-level and low-level domain shifts. Recent work has shown that
generative adversarial networks combined with cycle-consistency constraints are
surprisingly effective at mapping images between domains, even without the use
of aligned image pairs. We propose a novel discriminatively-trained
Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts
representations at both the pixel-level and feature-level, enforces
cycle-consistency while leveraging a task loss, and does not require aligned
pairs. Our model can be applied in a variety of visual recognition and
prediction settings. We show new state-of-the-art results across multiple
adaptation tasks, including digit classification and semantic segmentation of
road scenes demonstrating transfer from synthetic to real world domains."
Phillip Isola,Isola_Phillip,arXiv:1707.08390,https://arxiv.org/abs/1707.08390,"Abstract:  Sketch-based modeling strives to bring the ease and immediacy of drawing to
the 3D world. However, while drawings are easy for humans to create, they are
very challenging for computers to interpret due to their sparsity and
ambiguity. We propose a data-driven approach that tackles this challenge by
learning to reconstruct 3D shapes from one or more drawings. At the core of our
approach is a deep convolutional neural network (CNN) that predicts occupancy
of a voxel grid from a line drawing. This CNN provides us with an initial 3D
reconstruction as soon as the user completes a single drawing of the desired
shape. We complement this single-view network with an updater CNN that refines
an existing prediction given a new drawing of the shape created from a novel
viewpoint. A key advantage of our approach is that we can apply the updater
iteratively to fuse information from an arbitrary number of viewpoints, without
requiring explicit stroke correspondences between the drawings. We train both
CNNs by rendering synthetic contour drawings from hand-modeled shape
collections as well as from procedurally-generated abstract shapes. Finally, we
integrate our CNNs in a minimal modeling interface that allows users to
seamlessly draw an object, rotate it to see its 3D reconstruction, and refine
it by re-drawing from another vantage point using the 3D reconstruction as
guidance. The main strengths of our approach are its robustness to freehand
bitmap drawings, its ability to adapt to different object categories, and the
continuum it offers between single-view and multi-view sketch-based modeling."
Phillip Isola,Isola_Phillip,arXiv:1705.02999,https://arxiv.org/abs/1705.02999,"Abstract:  We propose a deep learning approach for user-guided image colorization. The
system directly maps a grayscale image, along with sparse, local user ""hints""
to an output colorization with a Convolutional Neural Network (CNN). Rather
than using hand-defined rules, the network propagates user edits by fusing
low-level cues along with high-level semantic information, learned from
large-scale data. We train on a million images, with simulated user inputs. To
guide the user towards efficient input selection, the system recommends likely
colors based on the input image and current user inputs. The colorization is
performed in a single feed-forward pass, enabling real-time use. Even with
randomly simulated user inputs, we show that the proposed system helps novice
users quickly create realistic colorizations, and offers large improvements in
colorization quality with just a minute of use. In addition, we demonstrate
that the framework can incorporate other user ""hints"" to the desired
colorization, showing an application to color histogram transfer. Our code and
models are available at this https URL."
Phillip Isola,Isola_Phillip,arXiv:1703.10593,https://arxiv.org/abs/1703.10593,"Abstract:  Image-to-image translation is a class of vision and graphics problems where
the goal is to learn the mapping between an input image and an output image
using a training set of aligned image pairs. However, for many tasks, paired
training data will not be available. We present an approach for learning to
translate an image from a source domain $X$ to a target domain $Y$ in the
absence of paired examples. Our goal is to learn a mapping $G: X \rightarrow Y$
such that the distribution of images from $G(X)$ is indistinguishable from the
distribution $Y$ using an adversarial loss. Because this mapping is highly
under-constrained, we couple it with an inverse mapping $F: Y \rightarrow X$
and introduce a cycle consistency loss to push $F(G(X)) \approx X$ (and vice
versa). Qualitative results are presented on several tasks where paired
training data does not exist, including collection style transfer, object
transfiguration, season transfer, photo enhancement, etc. Quantitative
comparisons against several prior methods demonstrate the superiority of our
approach."
Phillip Isola,Isola_Phillip,arXiv:1703.02018,https://arxiv.org/abs/1703.02018,"Abstract:  Manipulation of deformable objects, such as ropes and cloth, is an important
but challenging problem in robotics. We present a learning-based system where a
robot takes as input a sequence of images of a human manipulating a rope from
an initial to goal configuration, and outputs a sequence of actions that can
reproduce the human demonstration, using only monocular images as input. To
perform this task, the robot learns a pixel-level inverse dynamics model of
rope manipulation directly from images in a self-supervised manner, using about
60K interactions with the rope collected autonomously by the robot. The human
demonstration provides a high-level plan of what to do and the low-level
inverse model is used to execute the plan. We show that by combining the high
and low-level plans, the robot can successfully manipulate a rope into a
variety of target shapes using only a sequence of human-provided images for
direction."
Phillip Isola,Isola_Phillip,arXiv:1611.09842,https://arxiv.org/abs/1611.09842,"Abstract:  We propose split-brain autoencoders, a straightforward modification of the
traditional autoencoder architecture, for unsupervised representation learning.
The method adds a split to the network, resulting in two disjoint sub-networks.
Each sub-network is trained to perform a difficult task -- predicting one
subset of the data channels from another. Together, the sub-networks extract
features from the entire input signal. By forcing the network to solve
cross-channel prediction tasks, we induce a representation within the network
which transfers well to other, unseen tasks. This method achieves
state-of-the-art performance on several large-scale transfer learning
benchmarks."
Phillip Isola,Isola_Phillip,arXiv:1611.07004,https://arxiv.org/abs/1611.07004,"Abstract:  We investigate conditional adversarial networks as a general-purpose solution
to image-to-image translation problems. These networks not only learn the
mapping from input image to output image, but also learn a loss function to
train this mapping. This makes it possible to apply the same generic approach
to problems that traditionally would require very different loss formulations.
We demonstrate that this approach is effective at synthesizing photos from
label maps, reconstructing objects from edge maps, and colorizing images, among
other tasks. Indeed, since the release of the pix2pix software associated with
this paper, a large number of internet users (many of them artists) have posted
their own experiments with our system, further demonstrating its wide
applicability and ease of adoption without the need for parameter tweaking. As
a community, we no longer hand-engineer our mapping functions, and this work
suggests we can achieve reasonable results without hand-engineering our loss
functions either."
Phillip Isola,Isola_Phillip,arXiv:1603.08511,https://arxiv.org/abs/1603.08511,"Abstract:  Given a grayscale photograph as input, this paper attacks the problem of
hallucinating a plausible color version of the photograph. This problem is
clearly underconstrained, so previous approaches have either relied on
significant user interaction or resulted in desaturated colorizations. We
propose a fully automatic approach that produces vibrant and realistic
colorizations. We embrace the underlying uncertainty of the problem by posing
it as a classification task and use class-rebalancing at training time to
increase the diversity of colors in the result. The system is implemented as a
feed-forward pass in a CNN at test time and is trained on over a million color
images. We evaluate our algorithm using a ""colorization Turing test,"" asking
human participants to choose between a generated and ground truth color image.
Our method successfully fools humans on 32% of the trials, significantly higher
than previous methods. Moreover, we show that colorization can be a powerful
pretext task for self-supervised feature learning, acting as a cross-channel
encoder. This approach results in state-of-the-art performance on several
feature learning benchmarks."
Phillip Isola,Isola_Phillip,arXiv:1512.08512,https://arxiv.org/abs/1512.08512,"Abstract:  Objects make distinctive sounds when they are hit or scratched. These sounds
reveal aspects of an object's material properties, as well as the actions that
produced them. In this paper, we propose the task of predicting what sound an
object makes when struck as a way of studying physical interactions within a
visual scene. We present an algorithm that synthesizes sound from silent videos
of people hitting and scratching objects with a drumstick. This algorithm uses
a recurrent neural network to predict sound features from videos and then
produces a waveform from these features with an example-based synthesis
procedure. We show that the sounds predicted by our model are realistic enough
to fool participants in a ""real or fake"" psychophysical experiment, and that
they convey significant information about material properties and physical
interactions."
Phillip Isola,Isola_Phillip,arXiv:1511.06811,https://arxiv.org/abs/1511.06811,"Abstract:  We propose a self-supervised framework that learns to group visual entities
based on their rate of co-occurrence in space and time. To model statistical
dependencies between the entities, we set up a simple binary classification
problem in which the goal is to predict if two visual primitives occur in the
same spatial or temporal context. We apply this framework to three domains:
learning patch affinities from spatial adjacency in images, learning frame
affinities from temporal adjacency in videos, and learning photo affinities
from geospatial proximity in image collections. We demonstrate that in each
case the learned affinities uncover meaningful semantic groupings. From patch
affinities we generate object proposals that are competitive with
state-of-the-art supervised methods. From frame affinities we generate movie
scene segmentations that correlate well with DVD chapter structure. Finally,
from geospatial affinities we learn groups that relate well to semantic place
categories."
Phillip Isola,Isola_Phillip,arXiv:1412.7884,https://arxiv.org/abs/1412.7884,"Abstract:  In this paper, we study the problem of reproducing the world lighting from a
single image of an object covered with random specular microfacets on the
surface. We show that such reflectors can be interpreted as a randomized
mapping from the lighting to the image. Such specular objects have very
different optical properties from both diffuse surfaces and smooth specular
objects like metals, so we design special imaging system to robustly and
effectively photograph them. We present simple yet reliable algorithms to
calibrate the proposed system and do the inference. We conduct experiments to
verify the correctness of our model assumptions and prove the effectiveness of
our pipeline."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1902.02037,https://arxiv.org/abs/1902.02037,"Abstract:  We consider the problem of inferring the values of an arbitrary set of
variables (e.g., risk of diseases) given other observed variables (e.g.,
symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images
or EEG). This is a common problem in healthcare since variables of interest
often differ for different patients. Existing methods including Bayesian
networks and structured prediction either do not incorporate high-dimensional
signals or fail to model conditional dependencies among variables. To address
these issues, we propose bidirectional inference networks (BIN), which stich
together multiple probabilistic neural networks, each modeling a conditional
dependency. Predictions are then made via iteratively updating variables using
backpropagation (BP) to maximize corresponding posterior probability.
Furthermore, we extend BIN to composite BIN (CBIN), which involves the
iterative prediction process in the training stage and improves both accuracy
and computational efficiency by adaptively smoothing the optimization
landscape. Experiments on synthetic and real-world datasets (a sleep study and
a dermatology dataset) show that CBIN is a single model that can achieve
state-of-the-art performance and obtain better accuracy in most inference tasks
than multiple models each specifically trained for a different task."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1812.01070,https://arxiv.org/abs/1812.01070,"Abstract:  We view molecular optimization as a graph-to-graph translation problem. The
goal is to learn to map from one molecular graph to another with better
properties based on an available corpus of paired molecules. Since molecules
can be optimized in different ways, there are multiple viable translations for
each input graph. A key challenge is therefore to model diverse translation
outputs. Our primary contributions include a junction tree encoder-decoder for
learning diverse graph translations along with a novel adversarial training
method for aligning distributions of molecules. Diverse output distributions in
our model are explicitly realized by low-dimensional latent vectors that
modulate the translation process. We evaluate our model on multiple molecular
optimization tasks and show that our model outperforms previous
state-of-the-art baselines."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1809.00013,https://arxiv.org/abs/1809.00013,"Abstract:  Cross-lingual or cross-domain correspondences play key roles in tasks ranging
from machine translation to transfer learning. Recently, purely unsupervised
methods operating on monolingual embeddings have become effective alignment
tools. Current state-of-the-art methods, however, involve multiple steps,
including heuristic post-hoc refinement strategies. In this paper, we cast the
correspondence problem directly as an optimal transport (OT) problem, building
on the idea that word embeddings arise from metric recovery algorithms. Indeed,
we exploit the Gromov-Wasserstein distance that measures how similarities
between pairs of words relate across languages. We show that our OT objective
can be estimated efficiently, requires little or no tuning, and results in
performance comparable with the state-of-the-art in various unsupervised word
translation tasks."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1807.08919,https://arxiv.org/abs/1807.08919,"Abstract:  Hierarchical Bayesian methods can unify many related tasks (e.g. k-shot
classification, conditional and unconditional generation) as inference within a
single generative model. However, when this generative model is expressed as a
powerful neural network such as a PixelCNN, we show that existing learning
techniques typically fail to effectively use latent variables. To address this,
we develop a modification of the Variational Autoencoder in which encoded
observations are decoded to new elements from the same class. This technique,
which we call a Variational Homoencoder (VHE), produces a hierarchical latent
variable model which better utilises latent variables. We use the VHE framework
to learn a hierarchical PixelCNN on the Omniglot dataset, which outperforms all
existing models on test set likelihood and achieves strong performance on
one-shot generation and classification tasks. We additionally validate the VHE
on natural images from the YouTube Faces database. Finally, we develop
extensions of the model that apply to richer dataset structures such as
factorial and hierarchical categories."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1807.00130,https://arxiv.org/abs/1807.00130,"Abstract:  Interpretability has arisen as a key desideratum of machine learning models
alongside performance. Approaches so far have been primarily concerned with
fixed dimensional inputs emphasizing feature relevance or selection. In
contrast, we focus on temporal modeling and the problem of tailoring the
predictor, functionally, towards an interpretable family. To this end, we
propose a co-operative game between the predictor and an explainer without any
a priori restrictions on the functional class of the predictor. The goal of the
explainer is to highlight, locally, how well the predictor conforms to the
chosen interpretable family of temporal models. Our co-operative game is setup
asymmetrically in terms of information sets for efficiency reasons. We develop
and illustrate the framework in the context of temporal sequence models with
examples."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.09277,https://arxiv.org/abs/1806.09277,"Abstract:  Many problems in machine learning involve calculating correspondences between
sets of objects, such as point clouds or images. Discrete optimal transport
(OT) provides a natural and successful approach to such tasks whenever the two
sets of objects can be represented in the same space or when we can evaluate
distances between the objects. Unfortunately neither requirement is likely to
hold when object representations are learned from data. Indeed, automatically
derived representations such as word embeddings are typically fixed only up to
some global transformations, for example, reflection or rotation. As a result,
pairwise distances across the two types of objects are ill-defined without
specifying their relative transformation. In this work, we propose a general
framework for optimal transport in the presence of latent global
transformations. We discuss algorithms for the specific case of orthonormal
transformations, and show promising results in unsupervised word alignment."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.08049,https://arxiv.org/abs/1806.08049,"Abstract:  We argue that robustness of explanations---i.e., that similar inputs should
give rise to similar explanations---is a key desideratum for interpretability.
We introduce metrics to quantify robustness and demonstrate that current
methods do not perform well according to these metrics. Finally, we propose
ways that robustness can be enforced on existing interpretability approaches."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.07538,https://arxiv.org/abs/1806.07538,"Abstract:  Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1806.02867,https://arxiv.org/abs/1806.02867,"Abstract:  Reparameterization of variational auto-encoders with continuous latent spaces
is an effective method for reducing the variance of their gradient estimates.
However, using the same approach when latent variables are discrete is
problematic, due to the resulting non-differentiable objective. In this work,
we present a direct optimization method that propagates gradients through a
non-differentiable $\arg \max$ prediction operation. We apply this method to
discrete variational auto-encoders, by modeling a discrete random variable by
the $\arg \max$ function of the Gumbel-Max perturbation model."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1802.04364,https://arxiv.org/abs/1802.04364,"Abstract:  We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1712.06199,https://arxiv.org/abs/1712.06199,"Abstract:  Optimal Transport has recently gained interest in machine learning for
applications ranging from domain adaptation, sentence similarities to deep
learning. Yet, its ability to capture frequently occurring structure beyond the
""ground metric"" is limited. In this work, we develop a nonlinear generalization
of (discrete) optimal transport that is able to reflect much additional
structure. We demonstrate how to leverage the geometry of this new model for
fast algorithms, and explore connections and properties. Illustrative
experiments highlight the benefit of the induced structured couplings for tasks
in domain adaptation and natural language processing."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1709.04555,https://arxiv.org/abs/1709.04555,"Abstract:  The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1708.00133,https://arxiv.org/abs/1708.00133,"Abstract:  In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. Specifically, by learning to ground the
meaning of text to the dynamics of the environment such as transitions and
rewards, an autonomous agent can effectively bootstrap policy learning on a new
domain given its description. We employ a model-based RL approach consisting of
a differentiable planning module, a model-free component and a factorized state
representation to effectively use entity descriptions. Our model outperforms
prior work on both transfer and multi-task scenarios in a variety of different
environments. For instance, we achieve up to 14% and 11.5% absolute improvement
over previously existing models in terms of average and initial rewards,
respectively."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1707.01943,https://arxiv.org/abs/1707.01943,"Abstract:  We interpret the predictions of any black-box structured input-structured
output model around a specific input-output pair. Our method returns an
""explanation"" consisting of groups of input-output tokens that are causally
related. These dependencies are inferred by querying the black-box model with
perturbed inputs, generating a graph over tokens from the responses, and
solving a partitioning problem to select the most relevant components. We focus
the general approach on sequence-to-sequence problems, adopting a variational
autoencoder to yield meaningful input perturbations. We test our method across
several NLP sequence generation tasks."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1705.09655,https://arxiv.org/abs/1705.09655,"Abstract:  This paper focuses on style transfer on the basis of non-parallel text. This
is an instance of a broad family of problems including machine translation,
decipherment, and sentiment modification. The key challenge is to separate the
content from other aspects such as style. We assume a shared latent content
distribution across different text corpora, and propose a method that leverages
refined alignment of latent representations to perform style transfer. The
transferred sentences from one style should match example sentences from the
other style as a population. We demonstrate the effectiveness of this
cross-alignment method on three tasks: sentiment modification, decipherment of
word substitution ciphers, and recovery of word order."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1705.09037,https://arxiv.org/abs/1705.09037,"Abstract:  The design of neural architectures for structured objects is typically guided
by experimental insights rather than a formal process. In this work, we appeal
to kernels over combinatorial structures, such as sequences and graphs, to
derive appropriate neural operations. We introduce a class of deep recurrent
neural operations and formally characterize their associated kernel spaces. Our
recurrent modules compare the input to virtual reference objects (cf. filters
in CNN) via the kernels. Similar to traditional neural operations, these
reference objects are parameterized and directly optimized in end-to-end
training. We empirically evaluate the proposed class of neural architectures on
standard applications such as language modeling and molecular graph regression,
achieving state-of-the-art results across these applications."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1701.00188,https://arxiv.org/abs/1701.00188,"Abstract:  We introduce a neural method for transfer learning between two (source and
target) classification tasks or aspects over the same domain. Rather than
training on target labels, we use a few keywords pertaining to source and
target aspects indicating sentence relevance instead of document class labels.
Documents are encoded by learning to embed and softly select relevant sentences
in an aspect-dependent manner. A shared classifier is trained on the source
encoded documents and labels, and applied to target encoded documents. We
ensure transfer through aspect-adversarial training so that encoded documents
are, as sets, aspect-invariant. Experimental results demonstrate that our
approach outperforms different baselines and model variants on two datasets,
yielding an improvement of 27% on a pathology dataset and 5% on a review
dataset."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1606.05027,https://arxiv.org/abs/1606.05027,"Abstract:  Our goal is to identify beneficial interventions from observational data. We
consider interventions that are narrowly focused (impacting few covariates) and
may be tailored to each individual or globally enacted over a population. For
applications where harmful intervention is drastically worse than proposing no
change, we propose a conservative definition of the optimal intervention.
Assuming the underlying relationship remains invariant under intervention, we
develop efficient algorithms to identify the optimal intervention policy from
limited data and provide theoretical guarantees for our approach in a Gaussian
Process setting. Although our methods assume covariates can be precisely
adjusted, they remain capable of improving outcomes in misspecified settings
where interventions incur unintentional downstream effects. Empirically, our
approach identifies good interventions in two practical applications: gene
perturbation and writing improvement."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1606.04155,https://arxiv.org/abs/1606.04155,"Abstract:  Prediction without justification has limited applicability. As a remedy, we
learn to extract pieces of input text as justifications -- rationales -- that
are tailored to be short and coherent, yet sufficient for making the same
prediction. Our approach combines two modular components, generator and
encoder, which are trained to operate well together. The generator specifies a
distribution over text fragments as candidate rationales and these are passed
through the encoder for prediction. Rationales are never given during training.
Instead, the model is regularized by desiderata for rationales. We evaluate the
approach on multi-aspect sentiment analysis against manually annotated test
cases. Our approach outperforms attention-based baseline by a significant
margin. We also successfully illustrate the method on the question retrieval
task."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1602.03571,https://arxiv.org/abs/1602.03571,"Abstract:  This paper presents a new approach, called perturb-max, for high-dimensional
statistical inference that is based on applying random perturbations followed
by optimization. This framework injects randomness to maximum a-posteriori
(MAP) predictors by randomly perturbing the potential function for the input. A
classic result from extreme value statistics asserts that perturb-max
operations generate unbiased samples from the Gibbs distribution using
high-dimensional perturbations. Unfortunately, the computational cost of
generating so many high-dimensional random variables can be prohibitive.
However, when the perturbations are of low dimension, sampling the perturb-max
prediction is as efficient as MAP optimization. This paper shows that the
expected value of perturb-max inference with low dimensional perturbations can
be used sequentially to generate unbiased samples from the Gibbs distribution.
Furthermore the expected value of the maximal perturbations is a natural bound
on the entropy of such perturb-max models. A measure concentration result for
perturb-max values shows that the deviation of their sampled average from its
expectation decays exponentially in the number of samples, allowing effective
approximation of the expectation."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1512.05726,https://arxiv.org/abs/1512.05726,"Abstract:  Question answering forums are rapidly growing in size with no effective
automated ability to refer to and reuse answers already available for previous
posted questions. In this paper, we develop a methodology for finding
semantically related questions. The task is difficult since 1) key pieces of
information are often buried in extraneous details in the question body and 2)
available annotations on similar questions are scarce and fragmented. We design
a recurrent and convolutional model (gated convolution) to effectively map
questions to their semantic representations. The models are pre-trained within
an encoder-decoder framework (from body to title) on the basis of the entire
raw corpus, and fine-tuned discriminatively from limited annotations. Our
evaluation demonstrates that our model yields substantial gains over a standard
IR baseline and various neural network architectures (including CNNs, LSTMs and
GRUs)."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1511.04486,https://arxiv.org/abs/1511.04486,"Abstract:  We present a nonparametric framework to model a short sequence of probability
distributions that vary both due to underlying effects of sequential
progression and confounding noise. To distinguish between these two types of
variation and estimate the sequential-progression effects, our approach
leverages an assumption that these effects follow a persistent trend. This work
is motivated by the recent rise of single-cell RNA-sequencing experiments over
a brief time course, which aim to identify genes relevant to the progression of
a particular biological process across diverse cell populations. While
classical statistical tools focus on scalar-response regression or
order-agnostic differences between distributions, it is desirable in this
setting to consider both the full distributions as well as the structure
imposed by their ordering. We introduce a new regression model for ordinal
covariates where responses are univariate distributions and the underlying
relationship reflects consistent changes in the distributions over increasing
levels of the covariate. This concept is formalized as a ""trend"" in
distributions, which we define as an evolution that is linear under the
Wasserstein metric. Implemented via a fast alternating projections algorithm,
our method exhibits numerous strengths in simulations and analyses of
single-cell gene expression data."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1511.00573,https://arxiv.org/abs/1511.00573,"Abstract:  Large unweighted directed graphs are commonly used to capture relations
between entities. A fundamental problem in the analysis of such networks is to
properly define the similarity or dissimilarity between any two vertices.
Despite the significance of this problem, statistical characterization of the
proposed metrics has been limited. We introduce and develop a class of
techniques for analyzing random walks on graphs using stochastic calculus.
Using these techniques we generalize results on the degeneracy of hitting times
and analyze a metric based on the Laplace transformed hitting time (LTHT). The
metric serves as a natural, provably well-behaved alternative to the expected
hitting time. We establish a general correspondence between hitting times of
the Brownian motion and analogous hitting times on the graph. We show that the
LTHT is consistent with respect to the underlying metric of a geometric graph,
preserves clustering tendency, and remains robust against random addition of
non-geometric edges. Tests on simulated and real-world data show that the LTHT
matches theoretical predictions and outperforms alternatives."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1510.08956,https://arxiv.org/abs/1510.08956,"Abstract:  We introduce principal differences analysis (PDA) for analyzing differences
between high-dimensional distributions. The method operates by finding the
projection that maximizes the Wasserstein divergence between the resulting
univariate populations. Relying on the Cramer-Wold device, it requires no
assumptions about the form of the underlying distributions, nor the nature of
their inter-class differences. A sparse variant of the method is introduced to
identify features responsible for the differences. We provide algorithms for
both the original minimax formulation as well as its semidefinite relaxation.
In addition to deriving some convergence results, we illustrate how the
approach may be applied to identify differences between cell populations in the
somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our
broader framework extends beyond the specific choice of Wasserstein divergence."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1509.05808,https://arxiv.org/abs/1509.05808,"Abstract:  Continuous vector representations of words and objects appear to carry
surprisingly rich semantic content. In this paper, we advance both the
conceptual and theoretical understanding of word embeddings in three ways.
First, we ground embeddings in semantic spaces studied in
cognitive-psychometric literature and introduce new evaluation tasks. Second,
in contrast to prior work, we take metric recovery as the key object of study,
unify existing algorithms as consistent metric recovery methods based on
co-occurrence counts from simple Markov random walks, and propose a new
recovery algorithm. Third, we generalize metric recovery to graphs and
manifolds, relating co-occurence counts on random walks in graphs and random
processes on manifolds to the underlying metric to be recovered, thereby
reconciling manifold estimation and embedding algorithms. We compare embedding
algorithms across a range of tasks, from nonlinear dimensionality reduction to
three semantic language tasks, including analogies, sequence completion, and
classification."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.05133,https://arxiv.org/abs/1508.05133,"Abstract:  Contemporary deep neural networks exhibit impressive results on practical
problems. These networks generalize well although their inherent capacity may
extend significantly beyond the number of training examples. We analyze this
behavior in the context of deep, infinite neural networks. We show that deep
infinite layers are naturally aligned with Gaussian processes and kernel
methods, and devise stochastic kernels that encode the information of these
networks. We show that stability results apply despite the size, offering an
explanation for their empirical success."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.04112,https://arxiv.org/abs/1508.04112,"Abstract:  The success of deep learning often derives from well-chosen operational
building blocks. In this work, we revise the temporal convolution operation in
CNNs to better adapt it to text processing. Instead of concatenating word
representations, we appeal to tensor algebra and use low-rank n-gram tensors to
directly exploit interactions between words already at the convolution stage.
Moreover, we extend the n-gram convolution to non-consecutive words to
recognize patterns with intervening words. Through a combination of low-rank
tensors, and pattern weighting, we can efficiently evaluate the resulting
convolution operation via dynamic programming. We test the resulting
architecture on standard sentiment classification and news categorization
tasks. Our model achieves state-of-the-art performance both in terms of
accuracy and training speed. For instance, we obtain 51.2% accuracy on the
fine-grained sentiment classification task."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1508.00945,https://arxiv.org/abs/1508.00945,"Abstract:  Margin-based structured prediction commonly uses a maximum loss over all
possible structured outputs \cite{Altun03,Collins04b,Taskar03}. In natural
language processing, recent work \cite{Zhang14,Zhang15} has proposed the use of
the maximum loss over random structured outputs sampled independently from some
proposal distribution. This method is linear-time in the number of random
structured outputs and trivially parallelizable. We study this family of loss
functions in the PAC-Bayes framework under Gaussian perturbations
\cite{McAllester07}. Under some technical conditions and up to statistical
accuracy, we show that this family of loss functions produces a tighter upper
bound of the Gibbs decoder distortion than commonly used methods. Thus, using
the maximum loss over random structured outputs is a principled way of learning
the parameter of structured prediction models. Besides explaining the
experimental success of \cite{Zhang14,Zhang15}, our theoretical results show
that more general techniques are possible."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1506.07609,https://arxiv.org/abs/1506.07609,"Abstract:  We present a framework for clustering with cluster-specific feature
selection. The framework, CRAFT, is derived from asymptotic log posterior
formulations of nonparametric MAP-based clustering models. CRAFT handles
assorted data, i.e., both numeric and categorical data, and the underlying
objective functions are intuitively appealing. The resulting algorithm is
simple to implement and scales nicely, requires minimal parameter tuning,
obviates the need to specify the number of clusters a priori, and compares
favorably with other methods on real datasets."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1503.02335,https://arxiv.org/abs/1503.02335,"Abstract:  Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1411.5720,https://arxiv.org/abs/1411.5720,"Abstract:  We analyze directed, unweighted graphs obtained from $x_i\in \mathbb{R}^d$ by
connecting vertex $i$ to $j$ iff $|x_i - x_j| < \epsilon(x_i)$. Examples of
such graphs include $k$-nearest neighbor graphs, where $\epsilon(x_i)$ varies
from point to point, and, arguably, many real world graphs such as
co-purchasing graphs. We ask whether we can recover the underlying Euclidean
metric $\epsilon(x_i)$ and the associated density $p(x_i)$ given only the
directed graph and $d$.
We show that consistent recovery is possible up to isometric scaling when the
vertex degree is at least $\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})$. Our estimator
is based on a careful characterization of a random walk over the directed graph
and the associated continuum limit. As an algorithm, it resembles the PageRank
centrality metric. We demonstrate empirically that the estimator performs well
on simulated examples as well as on real-world co-purchasing graphs even with a
small number of points and degree scaling as low as $\log(n)$."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1310.4227,https://arxiv.org/abs/1310.4227,"Abstract:  The maximum a-posteriori (MAP) perturbation framework has emerged as a useful
approach for inference and learning in high dimensional complex models. By
maximizing a randomly perturbed potential function, MAP perturbations generate
unbiased samples from the Gibbs distribution. Unfortunately, the computational
cost of generating so many high-dimensional random variables can be
prohibitive. More efficient algorithms use sequential sampling strategies based
on the expected value of low dimensional MAP perturbations. This paper develops
new measure concentration inequalities that bound the number of samples needed
to estimate such expected values. Applying the general result to MAP
perturbations can yield a more efficient algorithm to approximate sampling from
the Gibbs distribution. The measure concentration result is of general interest
and may be applicable to other areas involving expected estimations."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1309.7598,https://arxiv.org/abs/1309.7598,"Abstract:  In this paper we describe how MAP inference can be used to sample efficiently
from Gibbs distributions. Specifically, we provide means for drawing either
approximate or unbiased samples from Gibbs' distributions by introducing low
dimensional perturbations and solving the corresponding MAP assignments. Our
approach also leads to new ways to derive lower bounds on partition functions.
We demonstrate empirically that our method excels in the typical ""high signal -
high coupling"" regime. The setting results in ragged energy landscapes that are
challenging for alternative approaches to sampling and/or lower bounds."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1309.6838,https://arxiv.org/abs/1309.6838,"Abstract:  We propose maximum likelihood estimation for learning Gaussian graphical
models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast
to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show
that our optimization problem leads to a Riccati matrix equation, which has a
closed form solution. We propose an efficient algorithm that performs a
singular value decomposition of the training data. Our algorithm is
O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is
tailored to high-dimensional problems (N gg T), in which sparseness promoting
methods become intractable. Furthermore, instead of obtaining a single solution
for a specific regularization parameter, our algorithm finds the whole solution
path. We show that the method has logarithmic sample complexity under the
spiked covariance model. We also propose sparsification of the dense solution
with provable performance guarantees. We provide techniques for using our
learnt models, such as removing unimportant variables, computing likelihoods
and conditional distributions. Finally, we show promising results in several
gene expressions datasets."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1302.3586,https://arxiv.org/abs/1302.3586,"Abstract:  We present deterministic techniques for computing upper and lower bounds on
marginal probabilities in sigmoid and noisy-OR networks. These techniques
become useful when the size of the network (or clique size) precludes exact
computations. We illustrate the tightness of the bounds by numerical
experiments."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.3875,https://arxiv.org/abs/1301.3875,"Abstract:  In this paper we present decomposable priors, a family of priors over
structure and parameters of tree belief nets for which Bayesian learning with
complete observations is tractable, in the sense that the posterior is also
decomposable and can be completely determined analytically in polynomial time.
This follows from two main results: First, we show that factored distributions
over spanning trees in a graph can be integrated in closed form. Second, we
examine priors over tree parameters and show that a set of assumptions similar
to (Heckerman and al. 1995) constrain the tree parameter priors to be a
compactly parameterized product of Dirichlet distributions. Beside allowing for
exact Bayesian learning, these results permit us to formulate a new class of
tractable latent variable models in which the likelihood of a data point is
computed through an ensemble average over tree structures."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.3865,https://arxiv.org/abs/1301.3865,"Abstract:  Incorporating feature selection into a classification or regression method
often carries a number of advantages. In this paper we formalize feature
selection specifically from a discriminative perspective of improving
classification/regression accuracy. The feature selection method is developed
as an extension to the recently proposed maximum entropy discrimination (MED)
framework. We describe MED as a flexible (Bayesian) regularization approach
that subsumes, e.g., support vector classification, regression and exponential
family models. For brevity, we restrict ourselves primarily to feature
selection in the context of linear classification/regression methods and
demonstrate that the proposed approach indeed carries substantial improvements
in practice. Moreover, we discuss and develop various extensions of feature
selection, including the problem of dealing with example specific but
unobserved degrees of freedom -- alignments or invariants."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0610,https://arxiv.org/abs/1301.0610,"Abstract:  Bounds on the log partition function are important in a variety of contexts,
including approximate inference, model fitting, decision theory, and large
deviations analysis. We introduce a new class of upper bounds on the log
partition function, based on convex combinations of distributions in the
exponential domain, that is applicable to an arbitrary undirected graphical
model. In the special case of convex combinations of tree-structured
distributions, we obtain a family of variational problems, similar to the Bethe
free energy, but distinguished by the following desirable properties: i. they
are cnvex, and have a unique global minimum; and ii. the global minimum gives
an upper bound on the log partition function. The global minimum is defined by
stationary conditions very similar to those defining fixed points of belief
propagation or tree-based reparameterization Wainwright et al., 2001. As with
BP fixed points, the elements of the minimizing argument can be used as
approximations to the marginals of the original model. The analysis described
here can be extended to structures of higher treewidth e.g., hypertrees,
thereby making connections with more advanced approximations e.g., Kikuchi and
variants Yedidia et al., 2001; Minka, 2001."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0602,https://arxiv.org/abs/1301.0602,"Abstract:  Active learning is a powerful approach to analyzing data effectively. We show
that the feasibility of active learning depends crucially on the choice of
measure with respect to which the query is being optimized. The standard
information gain, for example, does not permit an accurate evaluation with a
small committee, a representative subset of the model space. We propose a
surrogate measure requiring only a small committee and discuss the properties
of this new measure. We devise, in addition, a bootstrap approach for committee
selection. The advantages of this approach are illustrated in the context of
recovering (regulatory) network models."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1301.0562,https://arxiv.org/abs/1301.0562,"Abstract:  A number of modern learning tasks involve estimation from heterogeneous
information sources. This includes classification with labeled and unlabeled
data as well as other problems with analogous structure such as competitive
(game theoretic) problems. The associated estimation problems can be typically
reduced to solving a set of fixed point equations (consistency conditions). We
introduce a general method for combining a preferred information source with
another in this setting by evolving continuous paths of fixed points at
intermediate allocations. We explicitly identify critical points along the
unique paths to either increase the stability of estimation or to ensure a
significant departure from the initial source. The homotopy continuation
approach is guaranteed to terminate at the second source, and involves no
combinatorial effort. We illustrate the power of these ideas both in
classification tasks with labeled and unlabeled data, as well as in the context
of a competitive (min-max) formulation of DNA sequence motif discovery."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1212.2466,https://arxiv.org/abs/1212.2466,"Abstract:  We formulate a principle for classification with the knowledge of the
marginal distribution over the data points (unlabeled data). The principle is
cast in terms of Tikhonov style regularization where the regularization penalty
articulates the way in which the marginal density should constrain otherwise
unrestricted conditional distributions. Specifically, the regularization
penalty penalizes any information introduced between the examples and labels
beyond what is provided by the available labeled examples. The work extends
Szummer and Jaakkola's information regularization (NIPS 2002) to multiple
dimensions, providing a regularizer independent of the covering of the space
used in the derivation. We show in addition how the information regularizer can
be used as a measure of complexity of the classification task with unlabeled
data and prove a relevant sample-complexity bound. We illustrate the
regularization principle in practice by restricting the class of conditional
distributions to be logistic regression models and constructing the
regularization penalty from a finite set of unlabeled examples."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1208.5159,https://arxiv.org/abs/1208.5159,"Abstract:  This is the Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29
2005."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1207.4255,https://arxiv.org/abs/1207.4255,"Abstract:  In this paper, we present $\ell_{1,p}$ multi-task structure learning for
Gaussian graphical models. We analyze the sufficient number of samples for the
correct recovery of the support union and edge signs. We also analyze the
necessary number of samples for any conceivable method by providing
information-theoretic lower bounds. We compare the statistical efficiency of
multi-task learning versus that of single-task learning. For experiments, we
use a block coordinate descent method that is provably convergent and generates
a sequence of positive definite solutions. We provide experimental validation
on synthetic data as well as on two publicly available real-world data sets,
including functional magnetic resonance imaging and gene expression data."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.6410,https://arxiv.org/abs/1206.6410,"Abstract:  In this paper we relate the partition function to the max-statistics of
random variables. In particular, we provide a novel framework for approximating
and bounding the partition function using MAP inference on randomly perturbed
models. As a result, we can use efficient MAP solvers such as graph-cuts to
evaluate the corresponding partition function. We show that our method excels
in the typical ""high signal - high coupling"" regime that results in ragged
energy landscapes difficult for alternative approaches."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.5243,https://arxiv.org/abs/1206.5243,"Abstract:  Inference problems in graphical models are often approximated by casting them
as constrained optimization problems. Message passing algorithms, such as
belief propagation, have previously been suggested as methods for solving these
optimization problems. However, there are few convergence guarantees for such
algorithms, and the algorithms are therefore not guaranteed to solve the
corresponding optimization problem. Here we present an oriented tree
decomposition algorithm that is guaranteed to converge to the global optimum of
the Tree-Reweighted (TRW) variational problem. Our algorithm performs local
updates in the convex dual of the TRW problem - an unconstrained generalized
geometric program. Primal updates, also local, correspond to oriented
reparametrization operations that leave the distribution intact."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:1206.3288,https://arxiv.org/abs/1206.3288,"Abstract:  Linear Programming (LP) relaxations have become powerful tools for finding
the most probable (MAP) configuration in graphical models. These relaxations
can be solved efficiently using message-passing algorithms such as belief
propagation and, when the relaxation is tight, provably find the MAP
configuration. The standard LP relaxation is not tight enough in many
real-world problems, however, and this has lead to the use of higher order
cluster-based LP relaxations. The computational cost increases exponentially
with the size of the clusters and limits the number and type of clusters we can
use. We propose to solve the cluster selection problem monotonically in the
dual LP, iteratively selecting clusters with guaranteed improvement, and
quickly re-solving with the added clusters by reusing the existing solution.
Our dual message-passing algorithm finds the MAP configuration in protein
sidechain placement, protein design, and stereo problems, in cases where the
standard LP relaxation fails."
Tommi Jaakkola,Jaakkola_Tommi,arXiv:cs/0508070,https://arxiv.org/abs/cs/0508070,"Abstract:  We develop and analyze methods for computing provably optimal {\em maximum a
posteriori} (MAP) configurations for a subclass of Markov random fields defined
on graphs with cycles. By decomposing the original distribution into a convex
combination of tree-structured distributions, we obtain an upper bound on the
optimal value of the original problem (i.e., the log probability of the MAP
assignment) in terms of the combined optimal values of the tree problems. We
prove that this upper bound is tight if and only if all the tree distributions
share an optimal configuration in common. An important implication is that any
such shared configuration must also be a MAP configuration for the original
distribution. Next we develop two approaches to attempting to obtain tight
upper bounds: (a) a {\em tree-relaxed linear program} (LP), which is derived
from the Lagrangian dual of the upper bounds; and (b) a {\em tree-reweighted
max-product message-passing algorithm} that is related to but distinct from the
max-product algorithm. In this way, we establish a connection between a certain
LP relaxation of the mode-finding problem, and a reweighted form of the
max-product (min-sum) message-passing algorithm."
Daniel Jackson,Jackson_Daniel,arXiv:1812.07540,https://arxiv.org/abs/1812.07540,"Abstract:  Coherent excitation of an ensemble of quantum objects underpins quantum
many-body phenomena, and offers the opportunity to realize a quantum memory to
store information from a qubit. Thus far, a deterministic and coherent
interface between a single quantum system, e.g. a qubit, and such an ensemble
has remained elusive. We first use an electron to cool the mesoscopic
nuclear-spin ensemble of a semiconductor quantum dot to the nuclear
sideband-resolved regime. We then implement an all-optical approach to access
these individual quantized electronic-nuclear spin transitions. Finally, we
perform coherent optical rotations of a single collective nuclear spin
excitation corresponding to a spin wave called a nuclear magnon. These results
constitute the building blocks of a dedicated local memory per quantum-dot spin
qubit and promise a solid-state platform for quantum-state engineering of
isolated many-body systems."
Daniel Jackson,Jackson_Daniel,arXiv:1808.08026,https://arxiv.org/abs/1808.08026,"Abstract:  The Einstein-Yang-Mills equations are the source of many interesting
solutions within general relativity, including families of particle-like and
black hole solutions, and critical phenomena of more than one type. These
solutions, discovered in the last thirty years, all assume a restricted form
for the Yang-Mills gauge potential known as the ""magnetic"" ansatz. In this
thesis we relax that assumption and investigate the most general solutions of
the Einstein-Yang-Mills system assuming spherically symmetry, a Yang-Mills
gauge group of SU(2), and zero cosmological constant. We proceed primarily by
numerically integrating the equations and find new static solutions, for both
regular and black hole boundary conditions, which are not asymptotically flat,
and attempt to classify the possible static behaviours. We develop a code to
solve the dynamic equations that uses a novel adaptive mesh refinement
algorithm making full use of double-null coordinates. We find that the ""type
II"" critical behaviour in the general case exhibits non-universal critical
solutions, in contrast to the magnetic case and to all previously observed type
II critical behaviour."
Daniel Jackson,Jackson_Daniel,arXiv:1805.09288,https://arxiv.org/abs/1805.09288,"Abstract:  The recently discovered (Rb,Cs)EuFe4As4 compounds exhibit an unusual
combination of superconductivity (Tc = 35 K) and ferromagnetism (Tm = 15 K). We
have performed a series of x-ray diffraction, ac magnetic susceptibility, dc
magnetization, and electrical resistivity measurements on both RbEuFe4As4 and
CsEuFe4As4 to pressures as high as 30 GPa. We find that the superconductivity
onset is suppressed monotonically by pressure while the magnetic transition is
enhanced at initial rates of dTm/dP = 1.7 K/GPa and 1.5 K/GPa for RbEuFe4As4
and CsEuFe4As4, respectively. Near 7 GPa, Tc onset and Tm become comparable. At
higher pressures, signatures of bulk superconductivity gradually disappear.
Room temperature x-ray diffraction measurements suggest the onset of a
transition from tetragonal (T) to a half collapsed-tetragonal (hcT) phase at 10
GPa (RbEuFe4As4) and 12 GPa (CsEuFe4As4). The ability to tune Tc and Tm into
coincidence with relatively modest pressures highlights (Rb,Cs)EuFe4As4
compounds as ideal systems to study the interplay of superconductivity and
ferromagnetism."
Daniel Jackson,Jackson_Daniel,arXiv:1009.3478,https://arxiv.org/abs/1009.3478,"Abstract:  We describe an algorithm for distinguishing hyperbolic components in the
parameter space of quadratic rational maps with a periodic critical point. We
then illustrate computer images of the hyperbolic components of the parameter
spaces V1 - V4, which were produced using our algorithm. We also resolve the
singularities of the projective closure of V5 by blowups, giving an alternative
proof that as an algebraic curve, the geometric genus of V5 is 1. This explains
why we are unable to produce an image for V5."
Daniel Jackson,Jackson_Daniel,arXiv:cs/0605109,https://arxiv.org/abs/cs/0605109,"Abstract:  Knowledge flow analysis offers a simple and flexible way to find flaws in
security protocols. A protocol is described by a collection of rules
constraining the propagation of knowledge amongst principals. Because this
characterization corresponds closely to informal descriptions of protocols, it
allows a succinct and natural formalization; because it abstracts away message
ordering, and handles communications between principals and applications of
cryptographic primitives uniformly, it is readily represented in a standard
logic. A generic framework in the Alloy modelling language is presented, and
instantiated for two standard protocols, and a new key management scheme."
Daniel Jackson,Jackson_Daniel,arXiv:math/0505014,https://arxiv.org/abs/math/0505014,"Abstract:  We classify invariant curves for birational surface maps that are expanding
on cohomology. When the expansion is exponential, the arithmetic genus of an
invariant curve is at most one. This implies severe constraints on both the
type and number of irreducible components of the curve. In the case of an
invariant curve with genus equal to one, we show that there is an associated
invariant meromorphic two form."
Patrick Jaillet,Jaillet_Patrick,arXiv:1811.08516,https://arxiv.org/abs/1811.08516,"Abstract:  We consider the following problem: Does there exist a probability
distribution over subsets of a finite partially ordered set (poset), such that
a set of constraints involving marginal probabilities of the poset's elements
and maximal chains is satisfied? In this article, we present a combinatorial
algorithm to positively resolve this question. We show that this result plays a
crucial role in the equilibrium analysis of a generic security game on a
capacitated flow network. The game involves a routing entity that sends its
flow through the network while facing path transportation costs, and an
interdictor who simultaneously interdicts one or more edges while facing edge
interdiction costs. The first (resp. second) player seeks to maximize the value
of effective (resp. interdicted) flow net the total transportation (resp.
interdiction) cost. Using our existence result on posets and strict
complementary slackness in linear programming, we show that the equilibrium
properties of this game can be described using primal and dual solutions of a
minimum cost circulation problem. Our analysis provides a new characterization
of the critical network components."
Patrick Jaillet,Jaillet_Patrick,arXiv:1810.07732,https://arxiv.org/abs/1810.07732,"Abstract:  In this brief paper we find computable exponential convergence rates for a
large class of stochastically ordered Markov processes. We extend the result of
Lund, Meyn, and Tweedie (1996), who found exponential convergence rates for
stochastically ordered Markov processes starting from a fixed initial state, by
allowing for a random initial condition that is also stochastically ordered.
Our bounds are formulated in terms of moment-generating functions of hitting
times. To illustrate our result, we find an explicit exponential convergence
rate for an M/M/1 queue beginning in equilibrium and then experiencing a change
in its arrival or departure rates, a setting which has not been studied to our
knowledge."
Patrick Jaillet,Jaillet_Patrick,arXiv:1810.00447,https://arxiv.org/abs/1810.00447,"Abstract:  For online resource allocation problems, we propose a new demand arrival
model where the sequence of arrivals contains both an adversarial component and
a stochastic one. Our model requires no demand forecasting; however, due to the
presence of the stochastic component, we can partially predict future demand as
the sequence of arrivals unfolds. Under the proposed model, we study the
problem of the online allocation of a single resource to two types of
customers, and design online algorithms that outperform existing ones. Our
algorithms are adjustable to the relative size of the stochastic component, and
our analysis reveals that as the portion of the stochastic component grows, the
loss due to making online decisions decreases. This highlights the value of
(even partial) predictability in online resource allocation. We impose no
conditions on how the resource capacity scales with the maximum number of
customers. However, we show that using an adaptive algorithm---which makes
online decisions based on observed data---is particularly beneficial when
capacity scales linearly with the number of customers. Our work serves as a
first step in bridging the long-standing gap between the two well-studied
approaches to the design and analysis of online algorithms based on (1)
adversarial models and (2) stochastic ones. Using novel algorithm design, we
demonstrate that even if the arrival sequence contains an adversarial
component, we can take advantage of the limited information that the data
reveals to improve allocation decisions. We also study the classical secretary
problem under our proposed arrival model, and we show that randomizing over
multiple stopping rules may increase the probability of success."
Patrick Jaillet,Jaillet_Patrick,arXiv:1808.03526,https://arxiv.org/abs/1808.03526,"Abstract:  We study the problem of matching agents who arrive at a marketplace over time
and leave after d time periods. Agents can only be matched while they are
present in the marketplace. Each pair of agents can yield a different match
value, and the planner's goal is to maximize the total value over a finite time
horizon. First we study the case in which vertices arrive in an adversarial
order. We provide a randomized 0.25-competitive algorithm building on a result
by Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the
case in which departure times are drawn independently from a distribution with
non-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.
When the arrival order is chosen uniformly at random, we show that a batching
algorithm, which computes a maximum-weighted matching every (d+1) periods, is
0.279-competitive."
Patrick Jaillet,Jaillet_Patrick,arXiv:1803.08415,https://arxiv.org/abs/1803.08415,"Abstract:  Vehicle-to-Infrastructure (V2I) communications are increasingly supporting
highway operations such as electronic toll collection, carpooling, and vehicle
platooning. In this paper we study the incentives of strategic misbehavior by
individual vehicles who can exploit the security vulnerabilities in V2I
communications and negatively impact the highway operations. We consider a
V2I-enabled highway segment facing two classes of vehicles (agent populations),
each with an authorized access to one server (subset of lanes). Vehicles are
strategic in that they can misreport their class (type) to the system operator
and get an unauthorized access to the server dedicated to the other class. This
misbehavior causes additional congestion externality on the compliant vehicles,
and thus, needs to be deterred. We focus on an environment where the operator
is able to inspect the vehicles for misbehavior. The inspection is costly and
successful detection incurs a fine on the misbehaving vehicle. We formulate a
signaling game to study the strategic interaction between the vehicle classes
and the operator. Our equilibrium analysis provides conditions on the cost
parameters that govern the vehicles' incentive to misbehave or not. We also
determine the operator's equilibrium inspection strategy."
Patrick Jaillet,Jaillet_Patrick,arXiv:1803.01285,https://arxiv.org/abs/1803.01285,"Abstract:  We study the problem of matching agents who arrive at a marketplace over time
and leave after d time periods. Agents can only be matched while they are
present in the marketplace. Each pair of agents can yield a different match
value, and the planner's goal is to maximize the total value over a finite time
horizon. We study matching algorithms that perform well over any sequence of
arrivals when there is no a priori information about the match values or
arrival times.
Our main contribution is a 1/4-competitive algorithm. The algorithm randomly
selects a subset of agents who will wait until right before their departure to
get matched, and maintains a maximum-weight matching with respect to the other
agents. The primal-dual analysis of the algorithm hinges on a careful
comparison between the initial dual value associated with an agent when it
first arrives, and the final value after d time steps.
It is also shown that no algorithm is 1/2-competitive. We extend the model to
the case in which departure times are drawn i.i.d from a distribution with
non-decreasing hazard rate, and establish a 1/8-competitive algorithm in this
setting. Finally we show on real-world data that a modified version of our
algorithm performs well in practice."
Patrick Jaillet,Jaillet_Patrick,arXiv:1711.00221,https://arxiv.org/abs/1711.00221,"Abstract:  This paper presents a novel variational inference framework for deriving a
family of Bayesian sparse Gaussian process regression (SGPR) models whose
approximations are variationally optimal with respect to the full-rank GPR
model enriched with various corresponding correlation structures of the
observation noises.
Our variational Bayesian SGPR (VBSGPR) models jointly treat both the
distributions of the inducing variables and hyperparameters as variational
parameters, which enables the decomposability of the variational lower bound
that in turn can be exploited for stochastic optimization.
Such a stochastic optimization involves iteratively following the stochastic
gradient of the variational lower bound to improve its estimates of the optimal
variational distributions of the inducing variables and hyperparameters (and
hence the predictive distribution) of our VBSGPR models and is guaranteed to
achieve asymptotic convergence to them.
We show that the stochastic gradient is an unbiased estimator of the exact
gradient and can be computed in constant time per iteration, hence achieving
scalability to big data.
We empirically evaluate the performance of our proposed framework on two
real-world, massive datasets."
Patrick Jaillet,Jaillet_Patrick,arXiv:1703.04769,https://arxiv.org/abs/1703.04769,"Abstract:  The Container Relocation Problem (CRP) is concerned with finding a sequence
of moves of containers that minimizes the number of relocations needed to
retrieve all containers, while respecting a given order of retrieval. However,
the assumption of knowing the full retrieval order of containers is
particularly unrealistic in real operations. This paper studies the stochastic
CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model,
called the batch model, is introduced, motivated, and compared with an existing
model (the online model). The two main contributions are an optimal algorithm
called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm
called PBFS-Approximate with a bounded average error. Both algorithms,
applicable in the batch and online models, are based on a new family of lower
bounds for which we show some theoretical properties. Moreover, we introduce
two new heuristics outperforming the best existing heuristics. Algorithms,
bounds and heuristics are tested in an extensive computational section.
Finally, based on strong computational evidence, we conjecture the optimality
of the ""Leveling"" heuristic in a special ""no information"" case, where at any
retrieval stage, any of the remaining containers is equally likely to be
retrieved next."
Patrick Jaillet,Jaillet_Patrick,arXiv:1703.01484,https://arxiv.org/abs/1703.01484,"Abstract:  We study a convex resource allocation problem in which lower and upper bounds
are imposed on partial sums of allocations. This model is linked to a large
range of applications, including production planning, speed optimization,
stratified sampling, support vector machines, portfolio management, and
telecommunications. We propose an efficient gradient-free divide-and-conquer
algorithm, which uses monotonicity arguments to generate valid bounds from the
recursive calls, and eliminate linking constraints based on the information
from sub-problems. This algorithm does not need strict convexity or
differentiability. It produces an $\epsilon$-approximate solution for the
continuous problem in $\mathcal{O}(n \log m \log \frac{n B}{\epsilon})$ time
and an integer solution in $\mathcal{O}(n \log m \log B)$ time, where $n$ is
the number of decision variables, $m$ is the number of constraints, and $B$ is
the resource bound. A complexity of $\mathcal{O}(n \log m)$ is also achieved
for the linear and quadratic cases. These are the best complexities known to
date for this important problem class. Our experimental analyses confirm the
good performance of the method, which produces optimal solutions for problems
with up to 1,000,000 variables in a few seconds. Promising applications to the
support vector ordinal regression problem are also investigated."
Patrick Jaillet,Jaillet_Patrick,arXiv:1611.07096,https://arxiv.org/abs/1611.07096,"Abstract:  We propose a general approach for supervised learning with structured output
spaces, such as combinatorial and polyhedral sets, that is based on minimizing
estimated conditional risk functions. Given a loss function defined over pairs
of output labels, we first estimate the conditional risk function by solving a
(possibly infinite) collection of regularized least squares problems. A
prediction is made by solving an inference problem that minimizes the estimated
conditional risk function over the output space. We show that this approach
enables, in some cases, efficient training and inference without explicitly
introducing a convex surrogate for the original loss function, even when it is
discontinuous. Empirical evaluations on real-world and synthetic data sets
demonstrate the effectiveness of our method in adapting to a variety of loss
functions."
Patrick Jaillet,Jaillet_Patrick,arXiv:1606.03626,https://arxiv.org/abs/1606.03626,"Abstract:  We study dynamic matching in an infinite-horizon stochastic market. While all
agents are potentially compatible with each other, some are hard-to-match and
others are easy-to-match. Agents prefer to be matched as soon as possible and
matches are formed either bilaterally or indirectly through chains. We adopt an
asymptotic approach and compute tight bounds on the limit of waiting time of
agents under myopic policies that differ in matching technology and
prioritization.
We find that the market composition is a key factor in the desired matching
technology and prioritization level. When hard-to-match agents arrive less
frequently than easy-to-match ones (i) bilateral matching is almost as
efficient as chains (waiting times scale similarly under both, though chains
always outperform bilateral matching by a constant factor), and (ii) assigning
priorities to hard-to-match agents improves their waiting times. When
hard-to-match agents arrive more frequently, chains are much more efficient
than bilateral matching and prioritization has no impact.
We further conduct comparative statics on arrival rates. Somewhat
surprisingly, we find that in a heterogeneous market and under bilateral
matching, increasing arrival rate has a non-monotone effect on waiting times,
due to the fact that, under some market compositions, there is an adverse
effect of competition. Our comparative statics shed light on the impact of
merging markets and attracting altruistic agents (that initiate chains) or
easy-to-match agents.
This work uncovers fundamental differences between heterogeneous and
homogeneous dynamic markets, and potentially helps policy makers to generate
insights on the operations of matching markets such as kidney exchange
programs."
Patrick Jaillet,Jaillet_Patrick,arXiv:1603.00522,https://arxiv.org/abs/1603.00522,"Abstract:  In order to find Nash-equilibria for two-player zero-sum games where each
player plays combinatorial objects like spanning trees, matchings etc, we
consider two online learning algorithms: the online mirror descent (OMD)
algorithm and the multiplicative weights update (MWU) algorithm. The OMD
algorithm requires the computation of a certain Bregman projection, that has
closed form solutions for simple convex sets like the Euclidean ball or the
simplex. However, for general polyhedra one often needs to exploit the general
machinery of convex optimization. We give a novel primal-style algorithm for
computing Bregman projections on the base polytopes of polymatroids. Next, in
the case of the MWU algorithm, although it scales logarithmically in the number
of pure strategies or experts $N$ in terms of regret, the algorithm takes time
polynomial in $N$; this especially becomes a problem when learning
combinatorial objects. We give a general recipe to simulate the multiplicative
weights update algorithm in time polynomial in their natural dimension. This is
useful whenever there exists a polynomial time generalized counting oracle
(even if approximate) over these objects. Finally, using the combinatorial
structure of symmetric Nash-equilibria (SNE) when both players play bases of
matroids, we show that these can be found with a single projection or convex
minimization (without using online learning)."
Patrick Jaillet,Jaillet_Patrick,arXiv:1511.06890,https://arxiv.org/abs/1511.06890,"Abstract:  This paper presents a novel nonmyopic adaptive Gaussian process planning
(GPP) framework endowed with a general class of Lipschitz continuous reward
functions that can unify some active learning/sensing and Bayesian optimization
criteria and offer practitioners some flexibility to specify their desired
choices for defining new tasks/problems. In particular, it utilizes a
principled Bayesian sequential decision problem framework for jointly and
naturally optimizing the exploration-exploitation trade-off. In general, the
resulting induced GPP policy cannot be derived exactly due to an uncountable
set of candidate observations. A key contribution of our work here thus lies in
exploiting the Lipschitz continuity of the reward functions to solve for a
nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real
time, we further propose an asymptotically optimal, branch-and-bound anytime
variant of epsilon-GPP with performance guarantee. We empirically demonstrate
the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian
optimization and an energy harvesting task."
Patrick Jaillet,Jaillet_Patrick,arXiv:1510.01800,https://arxiv.org/abs/1510.01800,"Abstract:  Optimal regret bounds for Multi-Armed Bandit problems are now well
documented. They can be classified into two categories based on the growth rate
with respect to the time horizon $T$: (i) small, distribution-dependent, bounds
of order of magnitude $\ln(T)$ and (ii) robust, distribution-free, bounds of
order of magnitude $\sqrt{T}$. The Bandits with Knapsacks model, an extension
to the framework allowing to model resource consumption, lacks this clear-cut
distinction. While several algorithms have been shown to achieve asymptotically
optimal distribution-free bounds on regret, there has been little progress
toward the development of small distribution-dependent regret bounds. We
partially bridge the gap by designing a general-purpose algorithm with
distribution-dependent regret bounds that are logarithmic in the initial
endowments of resources in several important cases that cover many practical
applications, including dynamic pricing with limited supply, bid optimization
in online advertisement auctions, and dynamic procurement."
Patrick Jaillet,Jaillet_Patrick,arXiv:1505.04229,https://arxiv.org/abs/1505.04229,"Abstract:  The Container Relocation Problem (CRP) is concerned with finding a sequence
of moves of containers that minimizes the number of relocations needed to
retrieve all containers respecting a given order of retrieval. While the
problem is known to be NP-hard, certain algorithms such as the A* search and
heuristics perform reasonably well on many instances of the problem. In this
paper, we first focus on the A* search algorithm, and analyze lower and upper
bounds that are easy to compute and can be used to prune nodes. Our analysis
sheds light on which bounds result in fast computation within a given
approximation gap. We present extensive simulation results that improve upon
our theoretical analysis, and further show that our method finds the optimum
solution on most instances of medium-size bays. On ""hard"" instances, our method
finds an approximate solution with a small gap and within a time frame that is
fast for practical applications. We also study the average-case asymptotic
behavior of the CRP where the number of columns grows. We calculate the
expected number of relocations in the limit, and show that the optimum number
of relocations converges to a simple and intuitive lower-bound. We further
study the CRP with incomplete information by relaxing the assumption that the
order of retrieval of all containers are initially known. This assumption is
particularly unrealistic in ports without an appointment system. We assume that
the retrieval order of a subset of containers is known initially and the
retrieval order of the remaining containers is observed later at a given
specific time. Before this time, we assume a probabilistic distribution on the
retrieval order of unknown containers. We combine the A* algorithm with
sampling technique to solve this two-stage stochastic optimization problem. We
show that our algorithm is fast and the error due to sampling and pruning is
reasonably small."
Patrick Jaillet,Jaillet_Patrick,arXiv:1503.01535,https://arxiv.org/abs/1503.01535,"Abstract:  We introduce a new model and mathematical formulation for planning crane
moves in the storage yard of container terminals. Our objective is to develop a
tool that captures customer centric elements, especially service time, and
helps operators to manage costly relocation moves. Our model incorporates
several practical details and provides port operators with expanded
capabilities including planning repositioning moves in off-peak hours,
controlling wait times of each customer as well as total service time,
optimizing the number of relocations and wait time jointly, and optimizing
simultaneously the container stacking and retrieval process. We also study a
class of flexible service policies which allow for out-of-order retrieval. We
show that under such flexible policies, we can decrease the number of
relocations and retrieval delays without creating inequities."
Patrick Jaillet,Jaillet_Patrick,arXiv:1412.2123,https://arxiv.org/abs/1412.2123,"Abstract:  We consider and formulate a class of distributed multi-depot routing
problems, where servers are to visit a set of requests, with the aim of
minimizing the total distance travelled by all servers. These problems fall
into two categories: distributed offline routing problems where all the
requests that need to be visited are known from the start; distributed online
routing problems where the requests come to be known incrementally. A critical
and novel feature of our formulations is that communications are not allowed
among the servers, hence posing an interesting and challenging question: what
performance can be achieved in comparison to the best possible solution
obtained from an omniscience planner with perfect communication capabilities?
The worst-case (over all possible request-set instances) performance metrics
are given by the approximation ratio (offline case) and the competitive ratio
(online case).
Our first result indicates that the online and offline problems are
effectively equivalent: for the same request-set instance, the approximation
ratio and the competitive ratio differ by at most an additive factor of 2,
irrespective of the release dates in the online case. Therefore, we can
restrict our attention to the offline problem. For the offline problem, we show
that the approximation ratio given by the Voronoi partition is m (the number of
servers). For two classes of depot configurations, when the depots form a line
and when the ratios between the distances of pairs of depots are upper bounded
by a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes
with sublinear approximation ratios O(log m) and {\Theta}(f(m)) respectively.
We also discuss several interesting open problems in our formulations: in
particular, how our initial results (on the two deliberately chosen classes of
depots) shape our conjecture on the open problems."
Patrick Jaillet,Jaillet_Patrick,arXiv:1411.5649,https://arxiv.org/abs/1411.5649,"Abstract:  In the convex optimization approach to online regret minimization, many
methods have been developed to guarantee a $O(\sqrt{T})$ bound on regret for
subdifferentiable convex loss functions with bounded subgradients, by using a
reduction to linear loss functions. This suggests that linear loss functions
tend to be the hardest ones to learn against, regardless of the underlying
decision spaces. We investigate this question in a systematic fashion looking
at the interplay between the set of possible moves for both the decision maker
and the adversarial environment. This allows us to highlight sharp distinctive
behaviors about the learnability of piecewise linear loss functions. On the one
hand, when the decision set of the decision maker is a polyhedron, we establish
$\Omega(\sqrt{T})$ lower bounds on regret for a large class of piecewise linear
loss functions with important applications in online linear optimization,
repeated zero-sum Stackelberg games, online prediction with side information,
and online two-stage optimization. On the other hand, we exhibit $o(\sqrt{T})$
learning rates, achieved by the Follow-The-Leader algorithm, in online linear
optimization when the boundary of the decision maker's decision set is curved
and when $0$ does not lie in the convex hull of the environment's decision set.
Hence, the curvature of the decision maker's decision set is a determining
factor for the optimal learning rate. These results hold in a completely
adversarial setting."
Patrick Jaillet,Jaillet_Patrick,arXiv:1411.4510,https://arxiv.org/abs/1411.4510,"Abstract:  The expressive power of a Gaussian process (GP) model comes at a cost of poor
scalability in the data size. To improve its scalability, this paper presents a
low-rank-cum-Markov approximation (LMA) of the GP model that is novel in
leveraging the dual computational advantages stemming from complementing a
low-rank approximate representation of the full-rank GP based on a support set
of inputs with a Markov approximation of the resulting residual process; the
latter approximation is guaranteed to be closest in the Kullback-Leibler
distance criterion subject to some constraint and is considerably more refined
than that of existing sparse GP models utilizing low-rank representations due
to its more relaxed conditional independence assumption (especially with larger
data). As a result, our LMA method can trade off between the size of the
support set and the order of the Markov property to (a) incur lower
computational cost than such sparse GP models while achieving predictive
performance comparable to them and (b) accurately represent features/patterns
of any scale. Interestingly, varying the Markov order produces a spectrum of
LMAs with PIC approximation and full-rank GP at the two extremes. An advantage
of our LMA method is that it is amenable to parallelization on multiple
machines/cores, thereby gaining greater scalability. Empirical evaluation on
three real-world datasets in clusters of up to 32 computing nodes shows that
our centralized and parallel LMA methods are significantly more time-efficient
and scalable than state-of-the-art sparse and full-rank GP regression methods
while achieving comparable predictive performances."
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.3374,https://arxiv.org/abs/1408.3374,"Abstract:  We consider the problem of finding an optimal history-dependent routing
strategy on a directed graph weighted by stochastic arc costs when the
objective is to minimize the risk of spending more than a prescribed budget. To
help mitigate the impact of the lack of information on the arc cost probability
distributions, we introduce a robust counterpart where the distributions are
only known through confidence intervals on some statistics such as the mean,
the mean absolute deviation, and any quantile. Leveraging recent results in
distributionally robust optimization, we develop a general-purpose algorithm to
compute an approximate optimal strategy. To illustrate the benefits of the
robust approach, we run numerical experiments with field data from the
Singapore road network."
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.2060,https://arxiv.org/abs/1408.2060,"Abstract:  Gaussian processes (GP) are Bayesian non-parametric models that are widely
used for probabilistic regression. Unfortunately, it cannot scale well with
large data nor perform real-time predictions due to its cubic time cost in the
data size. This paper presents two parallel GP regression methods that exploit
low-rank covariance matrix approximations for distributing the computational
load among parallel machines to achieve time efficiency and scalability. We
theoretically guarantee the predictive performances of our proposed parallel
GPs to be equivalent to that of some centralized approximate GP regression
methods: The computation of their centralized counterparts can be distributed
among parallel machines, hence achieving greater time efficiency and
scalability. We analytically compare the properties of our parallel GPs such as
time, space, and communication complexity. Empirical evaluation on two
real-world datasets in a cluster of 20 computing nodes shows that our parallel
GPs are significantly more time-efficient and scalable than their centralized
counterparts and exact/full GP while achieving predictive performances
comparable to full GP."
Patrick Jaillet,Jaillet_Patrick,arXiv:1408.2046,https://arxiv.org/abs/1408.2046,"Abstract:  The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-oftheart centralized algorithms while achieving comparable
predictive performance."
Patrick Jaillet,Jaillet_Patrick,arXiv:1404.6694,https://arxiv.org/abs/1404.6694,"Abstract:  We propose an exact polynomial algorithm for a resource allocation problem
with convex costs and constraints on partial sums of resource consumptions, in
the presence of either continuous or integer variables. No assumption of strict
convexity or differentiability is needed. The method solves a hierarchy of
resource allocation subproblems, whose solutions are used to convert
constraints on sums of resources into bounds for separate variables at higher
levels. The resulting time complexity for the integer problem is $O(n \log m
\log (B/n))$, and the complexity of obtaining an $\epsilon$-approximate
solution for the continuous case is $O(n \log m \log (B/\epsilon))$, $n$ being
the number of variables, $m$ the number of ascending constraints (such that $m
< n$), $\epsilon$ a desired precision, and $B$ the total resource. This
algorithm attains the best-known complexity when $m = n$, and improves it when
$\log m = o(\log n)$. Extensive experimental analyses are conducted with four
recent algorithms on various continuous problems issued from theory and
practice. The proposed method achieves a higher performance than previous
algorithms, addressing all problems with up to one million variables in less
than one minute on a modern computer."
Patrick Jaillet,Jaillet_Patrick,arXiv:1401.7043,https://arxiv.org/abs/1401.7043,"Abstract:  The minmax regret problem for combinatorial optimization under uncertainty
can be viewed as a zero-sum game played between an optimizing player and an
adversary, where the optimizing player selects a solution and the adversary
selects costs with the intention of maximizing the regret of the player. The
existing minmax regret model considers only deterministic solutions/strategies,
and minmax regret versions of most polynomial solvable problems are NP-hard. In
this paper, we consider a randomized model where the optimizing player selects
a probability distribution (corresponding to a mixed strategy) over solutions
and the adversary selects costs with knowledge of the player's distribution,
but not its realization. We show that under this randomized model, the minmax
regret version of any polynomial solvable combinatorial problem becomes
polynomial solvable. This holds true for both the interval and discrete
scenario representations of uncertainty. Using the randomized model, we show
new proofs of existing approximation algorithms for the deterministic model
based on primal-dual approaches. Finally, we prove that minmax regret problems
are NP-hard under general convex uncertainty."
Patrick Jaillet,Jaillet_Patrick,arXiv:1308.6705,https://arxiv.org/abs/1308.6705,"Abstract:  Many modern and growing cities are facing declines in public transport usage,
with few efficient methods to explain why. In this article, we show that urban
mobility patterns and transport mode choices can be derived from cellphone call
detail records coupled with public transport data recorded from smart cards.
Specifically, we present new data mining approaches to determine the spatial
and temporal variability of public and private transportation usage and
transport mode preferences across Singapore. Our results, which were validated
by Singapore's quadriennial Household Interview Travel Survey (HITS), revealed
that there are 3.5 (HITS: 3.5 million) million and 4.3 (HITS: 4.4 million)
million inter-district passengers by public and private transport,
respectively. Along with classifying which transportation connections are weak
or underserved, the analysis shows that the mode share of public transport use
increases from 38 percent in the morning to 44 percent around mid-day and 52
percent in the evening."
Patrick Jaillet,Jaillet_Patrick,arXiv:1307.2536,https://arxiv.org/abs/1307.2536,"Abstract:  We study the average performance of online greedy matching algorithms on
$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges
occurring independently with probability $p=p(n)$. In the online model,
vertices on one side of the graph are given up front while vertices on the
other side arrive sequentially; when a vertex arrives its edges are revealed
and it must be immediately matched or dropped. We begin by analyzing the
\textsc{oblivious} algorithm, which tries to match each arriving vertex to a
random neighbor, even if the neighbor has already been matched. The algorithm
is shown to have a performance ratio of at least $1-1/e$ for all monotonic
functions $p(n)$, where the performance ratio is defined asymptotically as the
ratio of the expected matching size given by the algorithm to the expected
maximum matching size. Next we show that the conventional \textsc{greedy}
algorithm, which assigns each vertex to a random unmatched neighbor, has a
performance ratio of at least 0.837 for all monotonic functions $p(n)$. Under
the $G(n,n,p)$ model, the performance of \textsc{greedy} is equivalent to the
performance of the well known \textsc{ranking} algorithm, so our results show
that \textsc{ranking} has a performance ratio of at least 0.837. We finally
consider vertex-weighted bipartite matching. Our proofs are based on simple
differential equations that describe the evolution of the matching process."
Patrick Jaillet,Jaillet_Patrick,arXiv:1305.5826,https://arxiv.org/abs/1305.5826,"Abstract:  Gaussian processes (GP) are Bayesian non-parametric models that are widely
used for probabilistic regression. Unfortunately, it cannot scale well with
large data nor perform real-time predictions due to its cubic time cost in the
data size. This paper presents two parallel GP regression methods that exploit
low-rank covariance matrix approximations for distributing the computational
load among parallel machines to achieve time efficiency and scalability. We
theoretically guarantee the predictive performances of our proposed parallel
GPs to be equivalent to that of some centralized approximate GP regression
methods: The computation of their centralized counterparts can be distributed
among parallel machines, hence achieving greater time efficiency and
scalability. We analytically compare the properties of our parallel GPs such as
time, space, and communication complexity. Empirical evaluation on two
real-world datasets in a cluster of 20 computing nodes shows that our parallel
GPs are significantly more time-efficient and scalable than their centralized
counterparts and exact/full GP while achieving predictive performances
comparable to full GP."
Patrick Jaillet,Jaillet_Patrick,arXiv:1304.2488,https://arxiv.org/abs/1304.2488,"Abstract:  We present bounds of quadratic form for the logarithm of the Gaussian
Q-function. We also show an analytical method for deriving log-quadratic
approximations of the Q-function and give an approximation with absolute error
less than $10^{-3}$."
Patrick Jaillet,Jaillet_Patrick,arXiv:1301.4529,https://arxiv.org/abs/1301.4529,"Abstract:  Rollout algorithms have demonstrated excellent performance on a variety of
dynamic and discrete optimization problems. Interpreted as an approximate
dynamic programming algorithm, a rollout algorithm estimates the value-to-go at
each decision stage by simulating future events while following a greedy
policy, referred to as the base policy. While in many cases rollout algorithms
are guaranteed to perform as well as their base policies, there have been few
theoretical results showing additional improvement in performance. In this
paper we perform a probabilistic analysis of the subset sum problem and
knapsack problem, giving theoretical evidence that rollout algorithms perform
strictly better than their base policies. Using a stochastic model from the
existing literature, we analyze two rollout methods that we refer to as the
consecutive rollout and exhaustive rollout, both of which employ a simple
greedy base policy. For the subset sum problem, we prove that after only a
single iteration of the rollout algorithm, both methods yield at least a 30%
reduction in the expected gap between the solution value and capacity, relative
to the base policy. Analogous results are shown for the knapsack problem."
Patrick Jaillet,Jaillet_Patrick,arXiv:1301.3509,https://arxiv.org/abs/1301.3509,"Abstract:  Current kidney exchange pools are of moderate size and thin, as they consist
of many highly sensitized patients. Creating a thicker pool can be done by
waiting for many pairs to arrive. We analyze a simple class of matching
algorithms that search periodically for allocations. We find that if only 2-way
cycles are conducted, in order to gain a significant amount of matches over the
online scenario (matching each time a new incompatible pair joins the pool) the
waiting period should be ""very long"". If 3-way cycles are also allowed we find
regimes in which waiting for a short period also increases the number of
matches considerably. Finally, a significant increase of matches can be
obtained by using even one non-simultaneous chain while still matching in an
online fashion. Our theoretical findings and data-driven computational
experiments lead to policy recommendations."
Patrick Jaillet,Jaillet_Patrick,arXiv:1208.2596,https://arxiv.org/abs/1208.2596,"Abstract:  In this paper, we study a general online linear programming problem whose
formulation encompasses many practical dynamic resource allocation problems,
including internet advertising display applications, revenue management,
various routing, packing, and auction problems. We propose a model, which under
mild assumptions, allows us to design near-optimal learning-based online
algorithms that do not require the a priori knowledge about the total number of
online requests to come, a first of its kind. We then consider two variants of
the problem that relax the initial assumptions imposed on the proposed model."
Patrick Jaillet,Jaillet_Patrick,arXiv:1207.1333,https://arxiv.org/abs/1207.1333,"Abstract:  The most well-known conjecture in the context of matroid secretary problems
claims the existence of a constant-factor approximation applicable to any
matroid. Whereas this conjecture remains open, modified forms of it were shown
to be true, when assuming that the assignment of weights to the secretaries is
not adversarial but uniformly random (Soto [SODA 2011], Oveis Gharan and
Vondrák [ESA 2011]). However, so far, there was no variant of the matroid
secretary problem with adversarial weight assignment for which a
constant-factor approximation was found. We address this point by presenting a
9-approximation for the \emph{free order model}, a model suggested shortly
after the introduction of the matroid secretary problem, and for which no
constant-factor approximation was known so far. The free order model is a
relaxed version of the original matroid secretary problem, with the only
difference that one can choose the order in which secretaries are interviewed.
Furthermore, we consider the classical matroid secretary problem for the
special case of laminar matroids. Only recently, a constant-factor
approximation has been found for this case, using a clever but rather involved
method and analysis (Im and Wang, [SODA 2011]) that leads to a
16000/3-approximation. This is arguably the most involved special case of the
matroid secretary problem for which a constant-factor approximation is known.
We present a considerably simpler and stronger $3\sqrt{3}e\approx
14.12$-approximation, based on reducing the problem to a matroid secretary
problem on a partition matroid."
Patrick Jaillet,Jaillet_Patrick,arXiv:1206.6230,https://arxiv.org/abs/1206.6230,"Abstract:  The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-of-the-art centralized algorithms while achieving
comparable predictive performance."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1901.00032,https://arxiv.org/abs/1901.00032,"Abstract:  Leveraging new data sources is a key step in accelerating the pace of
materials design and discovery. To complement the strides in synthesis planning
driven by historical, experimental, and computed data, we present an automated
method for connecting scientific literature to synthesis insights. Starting
from natural language text, we apply word embeddings from language models,
which are fed into a named entity recognition model, upon which a conditional
variational autoencoder is trained to generate syntheses for arbitrary
materials. We show the potential of this technique by predicting precursors for
two perovskite materials, using only training data published over a decade
prior to their first reported syntheses. We demonstrate that the model learns
representations of materials corresponding to synthesis-related properties, and
that the model's behavior complements existing thermodynamic knowledge.
Finally, we apply the model to perform synthesizability screening for proposed
novel perovskite compounds."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1810.10775,https://arxiv.org/abs/1810.10775,"Abstract:  In this paper, we consider the problem of Gaussian process (GP) optimization
with an added robustness requirement: The returned point may be perturbed by an
adversary, and we require the function value to remain as high as possible even
after this perturbation. This problem is motivated by settings in which the
underlying functions during optimization and implementation stages are
different, or when one is interested in finding an entire region of good inputs
rather than only a single point. We show that standard GP optimization
algorithms do not exhibit the desired robustness properties, and provide a
novel confidence-bound based algorithm StableOpt for this purpose. We
rigorously establish the required number of samples for StableOpt to find a
near-optimal point, and we complement this guarantee with an
algorithm-independent lower bound. We experimentally demonstrate several
potential applications of interest using real-world data sets, and we show that
StableOpt consistently succeeds in finding a stable maximizer where several
baseline methods fail."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1810.00826,https://arxiv.org/abs/1810.00826,"Abstract:  Graph Neural Networks (GNNs) for representation learning of graphs broadly
follow a neighborhood aggregation framework, where the representation vector of
a node is computed by recursively aggregating and transforming feature vectors
of its neighboring nodes. Many GNN variants have been proposed and have
achieved state-of-the-art results on both node and graph classification tasks.
However, despite GNNs revolutionizing graph representation learning, there is
limited understanding of their representational properties and limitations.
Here, we present a theoretical framework for analyzing the expressive power of
GNNs in capturing different graph structures. Our results characterize the
discriminative power of popular GNN variants, such as Graph Convolutional
Networks and GraphSAGE, and show that they cannot learn to distinguish certain
simple graph structures. We then develop a simple architecture that is provably
the most expressive among the class of GNNs and is as powerful as the
Weisfeiler-Lehman graph isomorphism test. We empirically validate our
theoretical findings on a number of graph classification benchmarks, and
demonstrate that our model achieves state-of-the-art performance."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1807.01808,https://arxiv.org/abs/1807.01808,"Abstract:  We consider the problem of inference in discrete probabilistic models, that
is, distributions over subsets of a finite ground set. These encompass a range
of well-known models in machine learning, such as determinantal point processes
and Ising models. Locally-moving Markov chain Monte Carlo algorithms, such as
the Gibbs sampler, are commonly used for inference in such models, but their
convergence is, at times, prohibitively slow. This is often caused by
state-space bottlenecks that greatly hinder the movement of such samplers. We
propose a novel sampling strategy that uses a specific mixture of product
distributions to propose global moves and, thus, accelerate convergence.
Furthermore, we show how to construct such a mixture using semigradient
information. We illustrate the effectiveness of combining our sampler with
existing ones, both theoretically on an example model, as well as practically
on three models learned from real-world data sets."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.10909,https://arxiv.org/abs/1806.10909,"Abstract:  We demonstrate that a very deep ResNet with stacked modules with one neuron
per hidden layer and ReLU activation functions can uniformly approximate any
Lebesgue integrable function in $d$ dimensions, i.e. $\ell_1(\mathbb{R}^d)$.
Because of the identity mapping inherent to ResNets, our network has
alternating layers of dimension one and $d$. This stands in sharp contrast to
fully connected networks, which are not universal approximators if their width
is the input dimension $d$ [Lu et al, 2017; Hanin and Sellke, 2017]. Hence, our
result implies an increase in representational power for narrow deep networks
by the ResNet architecture."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.09277,https://arxiv.org/abs/1806.09277,"Abstract:  Many problems in machine learning involve calculating correspondences between
sets of objects, such as point clouds or images. Discrete optimal transport
(OT) provides a natural and successful approach to such tasks whenever the two
sets of objects can be represented in the same space or when we can evaluate
distances between the objects. Unfortunately neither requirement is likely to
hold when object representations are learned from data. Indeed, automatically
derived representations such as word embeddings are typically fixed only up to
some global transformations, for example, reflection or rotation. As a result,
pairwise distances across the two types of objects are ill-defined without
specifying their relative transformation. In this work, we propose a general
framework for optimal transport in the presence of latent global
transformations. We discuss algorithms for the specific case of orthonormal
transformations, and show promising results in unsupervised word alignment."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1806.03536,https://arxiv.org/abs/1806.03536,"Abstract:  Recent deep learning approaches for representation learning on graphs follow
a neighborhood aggregation procedure. We analyze some important properties of
these models, and propose a strategy to overcome those. In particular, the
range of ""neighboring"" nodes that a node's representation draws from strongly
depends on the graph structure, analogous to the spread of a random walk. To
adapt to local neighborhood properties and tasks, we explore an architecture --
jumping knowledge (JK) networks -- that flexibly leverages, for each node,
different neighborhood ranges to enable better structure-aware representation.
In a number of experiments on social, bioinformatics and citation networks, we
demonstrate that our model achieves state-of-the-art performance. Furthermore,
combining the JK framework with models like Graph Convolutional Networks,
GraphSAGE and Graph Attention Networks consistently improves those models'
performance."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1802.09700,https://arxiv.org/abs/1802.09700,"Abstract:  Robustness of deep learning models is a property that has recently gained
increasing attention. We formally define a notion of robustness for generative
adversarial models, and show that, perhaps surprisingly, the GAN in its
original form is not robust. Indeed, the discriminator in GANs may be viewed as
merely offering ""teaching feedback"". Our notion of robustness relies on a
dishonest discriminator, or noisy, adversarial interference with its feedback.
We explore, theoretically and empirically, the effect of model and training
properties on this robustness. In particular, we show theoretical conditions
for robustness that are supported by empirical evidence. We also test the
effect of regularization. Our results suggest variations of GANs that are
indeed more robust to noisy attacks, and have overall more stable training
behavior."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1802.05249,https://arxiv.org/abs/1802.05249,"Abstract:  Submodular functions have applications throughout machine learning, but in
many settings, we do not have direct access to the underlying function $f$. We
focus on stochastic functions that are given as an expectation of functions
over a distribution $P$. In practice, we often have only a limited set of
samples $f_i$ from $P$. The standard approach indirectly optimizes $f$ by
maximizing the sum of $f_i$. However, this ignores generalization to the true
(unknown) distribution. In this paper, we achieve better performance on the
actual underlying function $f$ by directly optimizing a combination of bias and
variance. Algorithmically, we accomplish this by showing how to carry out
distributionally robust optimization (DRO) for submodular functions, providing
efficient algorithms backed by theoretical guarantees which leverage several
novel contributions to the general theory of DRO. We also show compelling
empirical evidence that DRO improves generalization to the unknown stochastic
submodular function."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1712.06199,https://arxiv.org/abs/1712.06199,"Abstract:  Optimal Transport has recently gained interest in machine learning for
applications ranging from domain adaptation, sentence similarities to deep
learning. Yet, its ability to capture frequently occurring structure beyond the
""ground metric"" is limited. In this work, we develop a nonlinear generalization
of (discrete) optimal transport that is able to reflect much additional
structure. We demonstrate how to leverage the geometry of this new model for
fast algorithms, and explore connections and properties. Illustrative
experiments highlight the benefit of the induced structured couplings for tasks
in domain adaptation and natural language processing."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1712.05510,https://arxiv.org/abs/1712.05510,"Abstract:  We introduce Graph-Sparse Logistic Regression, a new algorithm for
classification for the case in which the support should be sparse but connected
on a graph. We val- idate this algorithm against synthetic data and benchmark
it against L1-regularized Logistic Regression. We then explore our technique in
the bioinformatics context of proteomics data on the interactome graph. We make
all our experimental code public and provide GSLR as an open source package."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.09549,https://arxiv.org/abs/1706.09549,"Abstract:  We propose a framework for adversarial training that relies on a sample
rather than a single sample point as the fundamental unit of discrimination.
Inspired by discrepancy measures and two-sample tests between probability
distributions, we propose two such distributional adversaries that operate and
predict on samples, and show how they can be easily implemented on top of
existing models. Various experimental results show that generators trained with
our distributional adversaries are much more stable and are remarkably less
prone to mode collapse than traditional models trained with pointwise
prediction discriminators. The application of our framework to domain
adaptation also results in considerable improvement over recent
state-of-the-art."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.03583,https://arxiv.org/abs/1706.03583,"Abstract:  The need for real time analysis of rapidly producing data streams (e.g.,
video and image streams) motivated the design of streaming algorithms that can
efficiently extract and summarize useful information from massive data ""on the
fly"". Such problems can often be reduced to maximizing a submodular set
function subject to various constraints. While efficient streaming methods have
been recently developed for monotone submodular maximization, in a wide range
of applications, such as video summarization, the underlying utility function
is non-monotone, and there are often various constraints imposed on the
optimization problem to consider privacy or personalization. We develop the
first efficient single pass streaming algorithm, Streaming Local Search, that
for any streaming monotone submodular maximization algorithm with approximation
guarantee $\alpha$ under a collection of independence systems ${\cal I}$,
provides a constant $1/\big(1+2/\sqrt{\alpha}+1/\alpha
+2d(1+\sqrt{\alpha})\big)$ approximation guarantee for maximizing a
non-monotone submodular function under the intersection of ${\cal I}$ and $d$
knapsack constraints. Our experiments show that for video summarization, our
method runs more than 1700 times faster than previous work, while maintaining
practically the same performance."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1706.01445,https://arxiv.org/abs/1706.01445,"Abstract:  Bayesian optimization (BO) has become an effective approach for black-box
function optimization problems when function evaluations are expensive and the
optimum can be achieved within a relatively small number of queries. However,
many cases, such as the ones with high-dimensional inputs, may require a much
larger number of observations for optimization. Despite an abundance of
observations thanks to parallel experiments, current BO techniques have been
limited to merely a few thousand observations. In this paper, we propose
ensemble Bayesian optimization (EBO) to address three current challenges in BO
simultaneously: (1) large-scale observations; (2) high dimensional input
spaces; and (3) selections of batch queries that balance quality and diversity.
The key idea of EBO is to operate on an ensemble of additive Gaussian process
models, each of which possesses a randomized strategy to divide and conquer. We
show unprecedented, previously impossible results of scaling up BO to tens of
thousands of observations within minutes of computation."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1705.07443,https://arxiv.org/abs/1705.07443,"Abstract:  Efficiently aggregating data from different sources is a challenging problem,
particularly when samples from each source are distributed differently. These
differences can be inherent to the inference task or present for other reasons:
sensors in a sensor network may be placed far apart, affecting their individual
measurements. Conversely, it is computationally advantageous to split Bayesian
inference tasks across subsets of data, but data need not be identically
distributed across subsets. One principled way to fuse probability
distributions is via the lens of optimal transport: the Wasserstein barycenter
is a single distribution that summarizes a collection of input measures while
respecting their geometry. However, computing the barycenter scales poorly and
requires discretization of all input distributions and the barycenter itself.
Improving on this situation, we present a scalable, communication-efficient,
parallel algorithm for computing the Wasserstein barycenter of arbitrary
distributions. Our algorithm can operate directly on continuous input
distributions and is optimized for streaming data. Our method is even robust to
nonstationary input distributions and produces a barycenter estimate that
tracks the input measures over time. The algorithm is semi-discrete, needing to
discretize only the barycenter estimate. To the best of our knowledge, we also
provide the first bounds on the quality of the approximate barycenter as the
discretization becomes finer. Finally, we demonstrate the practical
effectiveness of our method, both in tracking moving distributions on a sphere,
as well as in a large-scale Bayesian inference task."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1705.06616,https://arxiv.org/abs/1705.06616,"Abstract:  We consider the problem of far-field sensing by means of a sensor array.
Traditional array geometry design techniques are agnostic to prior information
about the far-field scene. However, in many applications such priors are
available and may be utilized to design more efficient array topologies. We
formulate the problem of array geometry design with scene prior as one of
finding a sampling configuration that enables efficient inference, which turns
out to be a combinatorial optimization problem. While generic combinatorial
optimization problems are NP-hard and resist efficient solvers, we show how for
array design problems the theory of submodular optimization may be utilized to
obtain efficient algorithms that are guaranteed to achieve solutions within a
constant approximation factor from the optimum. We leverage the connection
between array design problems and submodular optimization and port several
results of interest. We demonstrate efficient methods for designing arrays with
constraints on the sensing aperture, as well as arrays respecting combinatorial
placement constraints. This novel connection between array design and
submodularity suggests the possibility for utilizing other insights and
techniques from the growing body of literature on submodular optimization in
the field of array design."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.02674,https://arxiv.org/abs/1703.02674,"Abstract:  We study dual volume sampling, a method for selecting k columns from an n x m
short and wide matrix (n <= k <= m) such that the probability of selection is
proportional to the volume spanned by the rows of the induced submatrix. This
method was proposed by Avron and Boutsidis (2013), who showed it to be a
promising method for column subset selection and its multiple applications.
However, its wider adoption has been hampered by the lack of polynomial time
sampling algorithms. We remove this hindrance by developing an exact
(randomized) polynomial time sampling algorithm as well as its derandomization.
Thereafter, we study dual volume sampling via the theory of real stable
polynomials and prove that its distribution satisfies the ""Strong Rayleigh""
property. This result has numerous consequences, including a provably
fast-mixing Markov chain sampler that makes dual volume sampling much more
attractive to practitioners. This sampler is closely related to classical
algorithms for popular experimental design methods that are to date lacking
theoretical analysis but are known to empirically work well."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.01973,https://arxiv.org/abs/1703.01973,"Abstract:  Optimization of high-dimensional black-box functions is an extremely
challenging problem. While Bayesian optimization has emerged as a popular
approach for optimizing black-box functions, its applicability has been limited
to low-dimensional problems due to its computational and statistical challenges
arising from high-dimensional settings. In this paper, we propose to tackle
these challenges by (1) assuming a latent additive structure in the function
and inferring it properly for more efficient and effective BO, and (2)
performing multiple evaluations in parallel to reduce the number of iterations
required by the method. Our novel approach learns the latent structure with
Gibbs sampling and constructs batched queries using determinantal point
processes. Experimental validations on both synthetic and real-world functions
demonstrate that the proposed method outperforms the existing state-of-the-art
approaches."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1703.01968,https://arxiv.org/abs/1703.01968,"Abstract:  Entropy Search (ES) and Predictive Entropy Search (PES) are popular and
empirically successful Bayesian Optimization techniques. Both rely on a
compelling information-theoretic motivation, and maximize the information
gained about the $\arg\max$ of the unknown function; yet, both are plagued by
the expensive computation for estimating entropies. We propose a new criterion,
Max-value Entropy Search (MES), that instead uses the information about the
maximum function value. We show relations of MES to other Bayesian optimization
methods, and establish a regret bound. We observe that MES maintains or
improves the good empirical performance of ES/PES, while tremendously
lightening the computational burden. In particular, MES is much more robust to
the number of samples used for computing the entropy, and hence more efficient
for higher dimensional problems."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1702.08791,https://arxiv.org/abs/1702.08791,"Abstract:  The optimal allocation of resources for maximizing influence, spread of
information or coverage, has gained attention in the past years, in particular
in machine learning and data mining. But in applications, the parameters of the
problem are rarely known exactly, and using wrong parameters can lead to
undesirable outcomes. We hence revisit a continuous version of the Budget
Allocation or Bipartite Influence Maximization problem introduced by Alon et
al. (2012) from a robust optimization perspective, where an adversary may
choose the least favorable parameters within a confidence set. The resulting
problem is a nonconvex-concave saddle point problem (or game). We show that
this nonconvex problem can be solved exactly by leveraging connections to
continuous submodular functions, and by solving a constrained submodular
minimization problem. Although constrained submodular minimization is hard in
general, here, we establish conditions under which such a problem can be solved
to arbitrary precision $\epsilon$."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1612.01213,https://arxiv.org/abs/1612.01213,"Abstract:  Learning the representation and the similarity metric in an end-to-end
fashion with deep networks have demonstrated outstanding results for clustering
and retrieval. However, these recent approaches still suffer from the
performance degradation stemming from the local metric training procedure which
is unaware of the global structure of the embedding space.
We propose a global metric learning scheme for optimizing the deep metric
embedding with the learnable clustering function and the clustering metric
(NMI) in a novel structured prediction framework.
Our experiments on CUB200-2011, Cars196, and Stanford online products
datasets show state of the art performance both on the clustering and retrieval
tasks measured in the NMI and Recall@K evaluation metrics."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1608.01008,https://arxiv.org/abs/1608.01008,"Abstract:  We study probability measures induced by set functions with constraints. Such
measures arise in a variety of real-world settings, where prior knowledge,
resource limitations, or other pragmatic considerations impose constraints. We
consider the task of rapidly sampling from such constrained measures, and
develop fast Markov chain samplers for them. Our first main result is for MCMC
sampling from Strongly Rayleigh (SR) measures, for which we present sharp
polynomial bounds on the mixing time. As a corollary, this result yields a fast
mixing sampler for Determinantal Point Processes (DPPs), yielding (to our
knowledge) the first provably fast MCMC sampler for DPPs since their inception
over four decades ago. Beyond SR measures, we develop MCMC samplers for
probabilistic models with hard constraints and identify sufficient conditions
under which their chains mix rapidly. We illustrate our claims by empirically
verifying the dependence of mixing times on the key factors governing our
theoretical bounds."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1607.07762,https://arxiv.org/abs/1607.07762,"Abstract:  We introduce a framework for model learning and planning in stochastic
domains with continuous state and action spaces and non-Gaussian transition
models. It is efficient because (1) local models are estimated only when the
planner requires them; (2) the planner focuses on the most relevant states to
the current planning problem; and (3) the planner focuses on the most
informative and/or high-value actions. Our theoretical analysis shows the
validity and asymptotic optimality of the proposed approach. Empirically, we
demonstrate the effectiveness of our algorithm on a simulated multi-modal
pushing problem."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1607.03559,https://arxiv.org/abs/1607.03559,"Abstract:  In this note we consider sampling from (non-homogeneous) strongly Rayleigh
probability measures. As an important corollary, we obtain a fast mixing Markov
Chain sampler for Determinantal Point Processes."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1603.06052,https://arxiv.org/abs/1603.06052,"Abstract:  The Nyström method has long been popular for scaling up kernel methods. Its
theoretical guarantees and empirical performance rely critically on the quality
of the landmarks selected. We study landmark selection for Nyström using
Determinantal Point Processes (DPPs), discrete probability models that allow
tractable generation of diverse samples. We prove that landmarks selected via
DPPs guarantee bounds on approximation errors; subsequently, we analyze
implications for kernel ridge regression. Contrary to prior reservations due to
cubic complexity of DPPsampling, we show that (under certain conditions) Markov
chain DPP sampling requires only linear time in the size of the data. We
present several empirical results that support our theoretical analysis, and
demonstrate the superior performance of DPP-based landmark selection compared
with existing approaches."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1512.01904,https://arxiv.org/abs/1512.01904,"Abstract:  We present a framework for accelerating a spectrum of machine learning
algorithms that require computation of bilinear inverse forms $u^\top A^{-1}u$,
where $A$ is a positive definite matrix and $u$ a given vector. Our framework
is built on Gauss-type quadrature and easily scales to large, sparse matrices.
Further, it allows retrospective computation of lower and upper bounds on
$u^\top A^{-1}u$, which in turn accelerates several algorithms. We prove that
these bounds tighten iteratively and converge at a linear (geometric) rate. To
our knowledge, ours is the first work to demonstrate these key properties of
Gauss-type quadrature, which is a classical and deeply studied topic. We
illustrate empirical consequences of our results by using quadrature to
accelerate machine learning tasks involving determinantal point processes and
submodular optimization, and observe tremendous speedups in several instances."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1511.07069,https://arxiv.org/abs/1511.07069,"Abstract:  Precisely-labeled data sets with sufficient amount of samples are very
important for training deep convolutional neural networks (CNNs). However, many
of the available real-world data sets contain erroneously labeled samples and
those errors substantially hinder the learning of very accurate CNN models. In
this work, we consider the problem of training a deep CNN model for image
classification with mislabeled training samples - an issue that is common in
real image data sets with tags supplied by amateur users. To solve this
problem, we propose an auxiliary image regularization technique, optimized by
the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm,
that automatically exploits the mutual context information among training
images and encourages the model to select reliable images to robustify the
learning process. Comprehensive experiments on benchmark data sets clearly
demonstrate our proposed regularized CNN model is resistant to label noise in
training data."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1511.06452,https://arxiv.org/abs/1511.06452,"Abstract:  Learning the distance metric between pairs of examples is of great importance
for learning and visual recognition. With the remarkable success from the state
of the art convolutional neural networks, recent works have shown promising
results on discriminatively training the networks to learn semantic feature
embeddings where similar examples are mapped close to each other and dissimilar
examples are mapped farther apart. In this paper, we describe an algorithm for
taking full advantage of the training batches in the neural network training by
lifting the vector of pairwise distances within the batch to the matrix of
pairwise distances. This step enables the algorithm to learn the state of the
art feature embedding by optimizing a novel structured prediction objective on
the lifted problem. Additionally, we collected Online Products dataset: 120k
images of 23k classes of online products for metric learning. Our experiments
on the CUB-200-2011, CARS196, and Online Products datasets demonstrate
significant improvement over existing deep feature embedding methods on all
experimented embedding sizes with the GoogLeNet network."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1510.06423,https://arxiv.org/abs/1510.06423,"Abstract:  Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses an estimate of the argmax of the function. This strategy offers both
practical and theoretical advantages: no tradeoff parameter needs to be
selected, and, moreover, we establish close connections to the popular GP-UCB
and GP-PI strategies. Our approach can be understood as automatically and
adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We
illustrate the effects of this adaptive tuning via bounds on the regret as well
as an extensive empirical evaluation on robotics and vision tasks,
demonstrating the robustness of this strategy for a range of performance
criteria."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1509.01618,https://arxiv.org/abs/1509.01618,"Abstract:  Determinantal Point Processes (DPPs) are elegant probabilistic models of
repulsion and diversity over discrete sets of items. But their applicability to
large sets is hindered by expensive cubic-complexity matrix operations for
basic tasks such as sampling. In light of this, we propose a new method for
approximate sampling from discrete $k$-DPPs. Our method takes advantage of the
diversity property of subsets sampled from a DPP, and proceeds in two stages:
first it constructs coresets for the ground set of items; thereafter, it
efficiently samples subsets based on the constructed coresets. As opposed to
previous approaches, our algorithm aims to minimize the total variation
distance to the original distribution. Experiments on both synthetic and real
datasets indicate that our sampling algorithm works efficiently on large data
sets, and yields more accurate samples than previous approaches."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1503.01563,https://arxiv.org/abs/1503.01563,"Abstract:  Energy minimization has been an intensely studied core problem in computer
vision. With growing image sizes (2D and 3D), it is now highly desirable to run
energy minimization algorithms in parallel. But many existing algorithms, in
particular, some efficient combinatorial algorithms, are difficult to
par-allelize. By exploiting results from convex and submodular theory, we
reformulate the quadratic energy minimization problem as a total variation
denoising problem, which, when viewed geometrically, enables the use of
projection and reflection based convex methods. The resulting min-cut algorithm
(and code) is conceptually very simple, and solves a sequence of TV denoising
problems. We perform an extensive empirical evaluation comparing
state-of-the-art combinatorial algorithms and convex optimization techniques.
On small problems the iterative convex methods match the combinatorial max-flow
algorithms, while on larger problems they offer other flexibility and important
gains: (a) their memory footprint is small; (b) their straightforward
parallelizability fits multi-core platforms; (c) they can easily be
warm-started; and (d) they quickly reach approximately good solutions, thereby
enabling faster ""inexact"" solutions. A key consequence of our approach based on
submodularity and convexity is that it is allows to combine any arbitrary
combinatorial or convex methods as subroutines, which allows one to obtain
hybrid combinatorial and convex optimization algorithms that benefit from the
strengths of both."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1501.05973,https://arxiv.org/abs/1501.05973,"Abstract:  We introduce and study methods for inferring and learning from
correspondences among neurons. The approach enables alignment of data from
distinct multiunit studies of nervous systems. We show that the methods for
inferring correspondences combine data effectively from cross-animal studies to
make joint inferences about behavioral decision making that are not possible
with the data from a single animal. We focus on data collection, machine
learning, and prediction in the representative and long-studied invertebrate
nervous system of the European medicinal leech. Acknowledging the computational
intractability of the general problem of identifying correspondences among
neurons, we introduce efficient computational procedures for matching neurons
across animals. The methods include techniques that adjust for missing cells or
additional cells in the different data sets that may reflect biological or
experimental variation. The methods highlight the value harnessing inference
and learning in new kinds of computational microscopes for multiunit
neurobiological studies."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1411.1752,https://arxiv.org/abs/1411.1752,"Abstract:  To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing, robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In
structured prediction problems, this becomes a daunting task, as the solution
space (image labelings, sentence parses, etc.) is exponentially large. We study
greedy algorithms for finding a diverse subset of solutions in
structured-output spaces by drawing new connections between submodular
functions over combinatorial item sets and High-Order Potentials (HOPs) studied
for graphical models. Specifically, we show via examples that when marginal
gains of submodular diversity functions allow structured representations, this
enables efficient (sub-linear time) approximate maximization by reducing the
greedy augmentation step to inference in a factor graph with appropriately
constructed HOPs. We discuss benefits, tradeoffs, and show that our
constructions lead to significantly better proposals."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1406.6507,https://arxiv.org/abs/1406.6507,"Abstract:  The increasing prominence of weakly labeled data nurtures a growing demand
for object detection methods that can cope with minimal supervision. We propose
an approach that automatically identifies discriminative configurations of
visual patterns that are characteristic of a given object class. We formulate
the problem as a constrained submodular optimization problem and demonstrate
the benefits of the discovered configurations in remedying mislocalizations and
finding informative positive and negative training examples. Together, these
lead to state-of-the-art weakly-supervised detection results on the challenging
PASCAL VOC dataset."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1406.6474,https://arxiv.org/abs/1406.6474,"Abstract:  Submodular functions describe a variety of discrete problems in machine
learning, signal processing, and computer vision. However, minimizing
submodular functions poses a number of algorithmic challenges. Recent work
introduced an easy-to-use, parallelizable algorithm for minimizing submodular
functions that decompose as the sum of ""simple"" submodular functions.
Empirically, this algorithm performs extremely well, but no theoretical
analysis was given. In this paper, we show that the algorithm converges
linearly, and we provide upper and lower bounds on the rate of convergence. Our
proof relies on the geometry of submodular polyhedra and draws on results from
spectral graph theory."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1403.1024,https://arxiv.org/abs/1403.1024,"Abstract:  Learning to localize objects with minimal supervision is an important problem
in computer vision, since large fully annotated datasets are extremely costly
to obtain. In this paper, we propose a new method that achieves this goal with
only image-level labels of whether the objects are present or not. Our approach
combines a discriminative submodular cover problem for automatically
discovering a set of positive object windows with a smoothed latent SVM
formulation. The latter allows us to leverage efficient quasi-Newton
optimization techniques. Our experiments demonstrate that the proposed approach
provides a 50% relative improvement in mean average precision over the current
state-of-the-art on PASCAL VOC 2007 detection."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1402.0240,https://arxiv.org/abs/1402.0240,"Abstract:  We study an extension of the classical graph cut problem, wherein we replace
the modular (sum of edge weights) cost function by a submodular set function
defined over graph edges. Special cases of this problem have appeared in
different applications in signal processing, machine learning, and computer
vision. In this paper, we connect these applications via the generic
formulation of ""cooperative graph cuts"", for which we study complexity,
algorithms, and connections to polymatroidal network flows. Finally, we compare
the proposed algorithms empirically."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1311.4296,https://arxiv.org/abs/1311.4296,"Abstract:  Recently, it has become evident that submodularity naturally captures widely
occurring concepts in machine learning, signal processing and computer vision.
Consequently, there is need for efficient optimization procedures for
submodular functions, especially for minimization problems. While general
submodular minimization is challenging, we propose a new method that exploits
existing decomposability of submodular functions. In contrast to previous
approaches, our method is neither approximate, nor impractical, nor does it
need any cumbersome parameter tuning. Moreover, it is easy to implement and
parallelize. A key component of our method is a formulation of the discrete
submodular minimization problem as a continuous best approximation problem that
is solved through a sequence of reflections, and its solution can be easily
thresholded to obtain an optimal discrete solution. This method solves both the
continuous and discrete formulations of the problem, and therefore has
applications in learning, inference, and reconstruction. In our experiments, we
illustrate the benefits of our method on two image segmentation tasks."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1311.2110,https://arxiv.org/abs/1311.2110,"Abstract:  We investigate three related and important problems connected to machine
learning: approximating a submodular function everywhere, learning a submodular
function (in a PAC-like setting [53]), and constrained minimization of
submodular functions. We show that the complexity of all three problems depends
on the 'curvature' of the submodular function, and provide lower and upper
bounds that refine and improve previous results [3, 16, 18, 52]. Our proof
techniques are fairly generic. We either use a black-box transformation of the
function (for approximation and learning), or a transformation of algorithms to
use an appropriate surrogate function (for minimization). Curiously, curvature
has been known to influence approximations for submodular maximization [7, 55],
but its effect on minimization, approximation and learning has hitherto been
open. We complete this picture, and also support our theoretical claims by
empirical results."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1308.1006,https://arxiv.org/abs/1308.1006,"Abstract:  We present a practical and powerful new framework for both unconstrained and
constrained submodular function optimization based on discrete
semidifferentials (sub- and super-differentials). The resulting algorithms,
which repeatedly compute and then efficiently optimize submodular
semigradients, offer new and generalize many old methods for submodular
optimization. Our approach, moreover, takes steps towards providing a unifying
paradigm applicable to both submodular min- imization and maximization,
problems that historically have been treated quite distinctly. The practicality
of our algorithms is important since interest in submodularity, owing to its
natural and wide applicability, has recently been in ascendance within machine
learning. We analyze theoretical properties of our algorithms for minimization
and maximization, and show that many state-of-the-art maximization algorithms
are special cases. Lastly, we complement our theoretical analyses with
supporting empirical experiments."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:1307.8049,https://arxiv.org/abs/1307.8049,"Abstract:  Research on distributed machine learning algorithms has focused primarily on
one of two extremes - algorithms that obey strict concurrency constraints or
algorithms that obey few or no such constraints. We consider an intermediate
alternative in which algorithms optimistically assume that conflicts are
unlikely and if conflicts do arise a conflict-resolution protocol is invoked.
We view this ""optimistic concurrency control"" paradigm as particularly
appropriate for large-scale machine learning algorithms, particularly in the
unsupervised setting. We demonstrate our approach in three problem areas:
clustering, feature learning and online facility location. We evaluate our
methods via large-scale experiments in a cluster computing environment."
Stefanie Jegelka,Jegelka_Stefanie,arXiv:0812.0389,https://arxiv.org/abs/0812.0389,"Abstract:  In the past few years powerful generalizations to the Euclidean k-means
problem have been made, such as Bregman clustering [7], co-clustering (i.e.,
simultaneous clustering of rows and columns of an input matrix) [9,18], and
tensor clustering [8,34]. Like k-means, these more general problems also suffer
from the NP-hardness of the associated optimization. Researchers have developed
approximation algorithms of varying degrees of sophistication for k-means,
k-medians, and more recently also for Bregman clustering [2]. However, there
seem to be no approximation algorithms for Bregman co- and tensor clustering.
In this paper we derive the first (to our knowledge) guaranteed methods for
these increasingly important clustering settings. Going beyond Bregman
divergences, we also prove an approximation factor for tensor clustering with
arbitrary separable metrics. Through extensive experiments we evaluate the
characteristics of our method, and show that it also has practical impact."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1901.06109,https://arxiv.org/abs/1901.06109,"Abstract:  This paper addresses the problem of planning for a robot with a directional
obstacle-detection sensor that must move through a cluttered environment. The
planning objective is to remain safe by finding a path for the complete robot,
including sensor, that guarantees that the robot will not move into any part of
the workspace before it has been seen by the sensor. Although a great deal of
work has addressed a version of this problem in which the ""field of view"" of
the sensor is a sphere around the robot, there is very little work addressing
robots with a narrow or occluded field of view. We give a formal definition of
the problem, several solution methods with different computational trade-offs,
and experimental results in illustrative domains."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1901.00279,https://arxiv.org/abs/1901.00279,"Abstract:  In this paper, we theoretically prove that we can eliminate all suboptimal
local minima by adding one neuron per output unit to any deep neural network,
for multi-class classification, binary classification, and regression with an
arbitrary loss function. At every local minimum of any deep neural network with
added neurons, the set of parameters of the original neural network (without
added neurons) is guaranteed to be a global minimum of the original neural
network. The effects of the added neurons are proven to automatically vanish at
every local minimum. Unlike many related results in the literature, our
theoretical results are directly applicable to common deep learning tasks
because the results only rely on the assumptions that automatically hold in the
common tasks. Moreover, we discuss several limitations in eliminating the
suboptimal local minima in this manner by providing additional theoretical
results and several examples."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1812.07768,https://arxiv.org/abs/1812.07768,"Abstract:  Modular meta-learning is a new framework that generalizes to unseen datasets
by combining a small set of neural modules in different ways. In this work we
propose abstract graph networks: using graphs as abstractions of a system's
subparts without a fixed assignment of nodes to system subparts, for which we
would need supervision. We combine this idea with modular meta-learning to get
a flexible framework with combinatorial generalization to new tasks built in.
We then use it to model the pushing of arbitrarily shaped objects from little
or no training data."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1812.06298,https://arxiv.org/abs/1812.06298,"Abstract:  We present Residual Policy Learning (RPL): a simple method for improving
nondifferentiable policies using model-free deep reinforcement learning. RPL
thrives in complex robotic manipulation tasks where good but imperfect
controllers are available. In these tasks, reinforcement learning from scratch
remains data-inefficient or intractable, but learning a residual on top of the
initial controller can yield substantial improvements. We study RPL in six
challenging MuJoCo tasks involving partial observability, sensor noise, model
misspecification, and controller miscalibration. For initial controllers, we
consider both hand-designed policies and model-predictive controllers with
known or learned transition models. By combining learning with control
algorithms, RPL can perform long-horizon, sparse-reward tasks for which
reinforcement learning alone fails. Moreover, we find that RPL consistently and
substantially improves on the initial controllers. We argue that RPL is a
promising approach for combining the complementary strengths of deep
reinforcement learning and robotic control, pushing the boundaries of what
either can achieve independently. Video and code at
this https URL."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1811.09558,https://arxiv.org/abs/1811.09558,"Abstract:  Bayesian optimization usually assumes that a Bayesian prior is given.
However, the strong theoretical guarantees in Bayesian optimization are often
regrettably compromised in practice because of unknown parameters in the prior.
In this paper, we adopt a variant of empirical Bayes and show that, by
estimating the Gaussian process prior from offline data sampled from the same
prior and constructing unbiased estimators of the posterior, variants of both
GP-UCB and probability of improvement achieve a near-zero regret bound, which
decreases to a constant proportional to the observational noise as the number
of offline data and the number of online evaluations increase. Empirically, we
have verified our approach on challenging simulated robotic problems featuring
task and motion planning."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1811.08150,https://arxiv.org/abs/1811.08150,"Abstract:  In this paper, we analyze the effects of depth and width on the quality of
local minima, without strong over-parameterization and simplification
assumptions in the literature. Without any simplification assumption, for deep
nonlinear neural networks with the squared loss, we theoretically show that the
quality of local minima tends to improve towards the global minimum value as
depth and width increase. Furthermore, with a locally-induced structure on deep
nonlinear neural networks, the values of local minima of neural networks are
theoretically proven to be no worse than the globally optimal values of
corresponding classical machine learning models. We empirically support our
theoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 and
SVHN datasets. When compared to previous studies with strong
over-parameterization assumptions, the results in this paper do not require
over-parameterization, and instead show the gradual effects of
over-parameterization as consequences of general results."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1810.11177,https://arxiv.org/abs/1810.11177,"Abstract:  We present a representation for describing transition models in complex
uncertain domains using relational rules. For any action, a rule selects a set
of relevant objects and computes a distribution over properties of just those
objects in the resulting state given their properties in the previous state. An
iterative greedy algorithm is used to construct a set of deictic references
that determine which objects are relevant in any given state. Feed-forward
neural networks are used to learn the transition distribution on the relevant
objects' properties. This strategy is demonstrated to be both more versatile
and more sample efficient than learning a monolithic transition model in a
simulated domain in which a robot pushes stacks of objects on a cluttered
table."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1809.07878,https://arxiv.org/abs/1809.07878,"Abstract:  Multi-object manipulation problems in continuous state and action spaces can
be solved by planners that search over sampled values for the continuous
parameters of operators. The efficiency of these planners depends critically on
the effectiveness of the samplers used, but effective sampling in turn depends
on details of the robot, environment, and task. Our strategy is to learn
functions called specializers that generate values for continuous operator
parameters, given a state description and values for the discrete parameters.
Rather than trying to learn a single specializer for each operator from large
amounts of data on a single task, we take a modular meta-learning approach. We
train on multiple tasks and learn a variety of specializers that, on a new
task, can be quickly adapted using relatively little data -- thus, our system
""learns quickly to plan quickly"" using these specializers. We validate our
approach experimentally in simulated 3D pick-and-place tasks with continuous
state and action spaces."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1808.03246,https://arxiv.org/abs/1808.03246,"Abstract:  An efficient, generalizable physical simulator with universal uncertainty
estimates has wide applications in robot state estimation, planning, and
control. In this paper, we build such a simulator for two scenarios, planar
pushing and ball bouncing, by augmenting an analytical rigid-body simulator
with a neural network that learns to model uncertainty as residuals. Combining
symbolic, deterministic simulators with learnable, stochastic neural nets
provides us with expressiveness, efficiency, and generalizability
simultaneously. Our model outperforms both purely analytical and purely learned
simulators consistently on real, standard benchmarks. Compared with methods
that model uncertainty using Gaussian processes, our model runs much faster,
generalizes better to new object shapes, and is able to characterize the
complex distribution of object trajectories."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1807.09962,https://arxiv.org/abs/1807.09962,"Abstract:  In this paper, we propose a learning algorithm that speeds up the search in
task and motion planning problems. Our algorithm proposes solutions to three
different challenges that arise in learning to improve planning efficiency:
what to predict, how to represent a planning problem instance, and how to
transfer knowledge from one problem instance to another. We propose a method
that predicts constraints on the search space based on a generic representation
of a planning problem instance, called score-space, where we represent a
problem instance in terms of the performance of a set of solutions attempted so
far. Using this representation, we transfer knowledge, in the form of
constraints, from previous problems based on the similarity in score space. We
design a sequential algorithm that efficiently predicts these constraints, and
evaluate it in three different challenging task and motion planning problems.
Results indicate that our approach performs orders of magnitudes faster than an
unguided planner"
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1806.10166,https://arxiv.org/abs/1806.10166,"Abstract:  Many prediction problems, such as those that arise in the context of
robotics, have a simplifying underlying structure that could accelerate
learning. In this paper, we present a strategy for learning a set of neural
network modules that can be combined in different ways. We train different
modular structures on a set of related tasks and generalize to new tasks by
composing the learned modules in new ways. We show this improves performance in
two robotics-related problems."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1805.08263,https://arxiv.org/abs/1805.08263,"Abstract:  In many robotic applications, an autonomous agent must act within and explore
a partially observed environment that is unobserved by its human teammate. We
consider such a setting in which the agent can, while acting, transmit
declarative information to the human that helps them understand aspects of this
unseen environment. In this work, we address the algorithmic question of how
the agent should plan out what actions to take and what information to
transmit. Naturally, one would expect the human to have preferences, which we
model information-theoretically by scoring transmitted information based on the
change it induces in weighted entropy of the human's belief state. We formulate
this setting as a belief MDP and give a tractable algorithm for solving it
approximately. Then, we give an algorithm that allows the agent to learn the
human's preferences online, through exploration. We validate our approach
experimentally in simulated discrete and continuous partially observed
search-and-recover domains. Visit this http URL for a
supplementary video."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1805.02874,https://arxiv.org/abs/1805.02874,"Abstract:  In many applications that involve processing high-dimensional data, it is
important to identify a small set of entities that account for a significant
fraction of detections. Rather than formalize this as a clustering problem, in
which all detections must be grouped into hard or soft categories, we formalize
it as an instance of the frequent items or heavy hitters problem, which finds
groups of tightly clustered objects that have a high density in the feature
space. We show that the heavy hitters formulation generates solutions that are
more accurate and effective than the clustering formulation. In addition, we
present a novel online algorithm for heavy hitters, called HAC, which addresses
problems in continuous space, and demonstrate its effectiveness on real video
and household domains."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1803.00967,https://arxiv.org/abs/1803.00967,"Abstract:  The objective of this work is to augment the basic abilities of a robot by
learning to use new sensorimotor primitives to enable the solution of complex
long-horizon problems. Solving long-horizon problems in complex domains
requires flexible generative planning that can combine primitive abilities in
novel combinations to solve problems as they arise in the world. In order to
plan to combine primitive actions, we must have models of the preconditions and
effects of those actions: under what circumstances will executing this
primitive achieve some particular effect in the world?
We use, and develop novel improvements on, state-of-the-art methods for
active learning and sampling. We use Gaussian process methods for learning the
conditions of operator effectiveness from small numbers of expensive training
examples collected by experimentation on a robot. We develop adaptive sampling
methods for generating diverse elements of continuous sets (such as robot
configurations and object poses) during planning for solving a new task, so
that planning is as efficient as possible. We demonstrate these methods in an
integrated system, combining newly learned models with an efficient
continuous-space robot task and motion planner to learn to solve long horizon
problems more efficiently than was previously possible."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1803.00119,https://arxiv.org/abs/1803.00119,"Abstract:  In partially observed environments, it can be useful for a human to provide
the robot with declarative information that represents probabilistic relational
constraints on properties of objects in the world, augmenting the robot's
sensory observations. For instance, a robot tasked with a search-and-rescue
mission may be informed by the human that two victims are probably in the same
room. An important question arises: how should we represent the robot's
internal knowledge so that this information is correctly processed and combined
with raw sensory information? In this paper, we provide an efficient belief
state representation that dynamically selects an appropriate factoring,
combining aspects of the belief when they are correlated through information
and separating them when they are not. This strategy works in open domains, in
which the set of possible objects is not known in advance, and provides
significant improvements in inference time over a static factoring, leading to
more efficient planning for complex partially observed tasks. We validate our
approach experimentally in two open-domain planning problems: a 2D discrete
gridworld task and a 3D continuous cooking task. A supplementary video can be
found at this http URL."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1802.08705,https://arxiv.org/abs/1802.08705,"Abstract:  Many planning applications involve complex relationships defined on
high-dimensional, continuous variables. For example, robotic manipulation
requires planning with kinematic, collision, and motion constraints involving
robot configurations, object transforms, and robot trajectories. These
constraints typically require specialized procedures to sample satisfying
values. We extend the STRIPS planning language to support a generic,
declarative specification for these procedures while treating their
implementation as blackboxes. We provide several domain-independent algorithms
that reduce STRIPStream problems to a sequence of finite-domain STRIPS planning
problems. Additionally, we describe cost-sensitive planning within this
framework. Finally, we evaluate our algorithms on three robotic task and motion
planning domains."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1802.07426,https://arxiv.org/abs/1802.07426,"Abstract:  This paper introduces a novel measure-theoretic theory for machine learning
that does not require statistical assumptions. Based on this theory, a new
regularization method in deep learning is derived and shown to outperform
previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed
theory provides a theoretical basis for a family of practically successful
regularization methods in deep learning. We discuss several consequences of our
results on one-shot learning, representation learning, deep learning, and
curriculum learning. Unlike statistical learning theory, the proposed learning
theory analyzes each problem instance individually via measure theory, rather
than a set of problem instances via statistics. As a result, it provides
different types of results and insights when compared to statistical learning
theory."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1801.00680,https://arxiv.org/abs/1801.00680,"Abstract:  This paper presents a general-purpose formulation of a large class of
discrete-time planning problems, with hybrid state and control-spaces, as
factored transition systems. Factoring allows state transitions to be described
as the intersection of several constraints each affecting a subset of the state
and control variables. Robotic manipulation problems with many movable objects
involve constraints that only affect several variables at a time and therefore
exhibit large amounts of factoring. We develop a theoretical framework for
solving factored transition systems with sampling-based algorithms. The
framework characterizes conditions on the submanifold in which solutions lie,
leading to a characterization of robust feasibility that incorporates
dimensionality-reducing constraints. It then connects those conditions to
corresponding conditional samplers that can be composed to produce values on
this submanifold. We present two domain-independent, probabilistically complete
planning algorithms that take, as input, a set of conditional samplers. We
demonstrate the empirical efficiency of these algorithms on a set of
challenging task and motion planning problems involving picking, placing, and
pushing."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1711.03243,https://arxiv.org/abs/1711.03243,"Abstract:  Program synthesis is a class of regression problems where one seeks a
solution, in the form of a source-code program, mapping the inputs to their
corresponding outputs exactly. Due to its precise and combinatorial nature,
program synthesis is commonly formulated as a constraint satisfaction problem,
where input-output examples are encoded as constraints and solved with a
constraint solver. A key challenge of this formulation is scalability: while
constraint solvers work well with a few well-chosen examples, a large set of
examples can incur significant overhead in both time and memory. We describe a
method to discover a subset of examples that is both small and representative:
the subset is constructed iteratively, using a neural network to predict the
probability of unchosen examples conditioned on the chosen examples in the
subset, and greedily adding the least probable example. We empirically evaluate
the representativeness of the subsets constructed by our method, and
demonstrate such subsets can significantly improve synthesis time and
stability."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1711.01391,https://arxiv.org/abs/1711.01391,"Abstract:  In robotics, it is essential to be able to plan efficiently in
high-dimensional continuous state-action spaces for long horizons. For such
complex planning problems, unguided uniform sampling of actions until a path to
a goal is found is hopelessly inefficient, and gradient-based approaches often
fall short when the optimization manifold of a given problem is not smooth. In
this paper we present an approach that guides the search of a state-space
planner, such as A*, by learning an action-sampling distribution that can
generalize across different instances of a planning problem. The motivation is
that, unlike typical learning approaches for planning for continuous action
space that estimate a policy, an estimated action sampler is more robust to
error since it has a planner to fall back on. We use a Generative Adversarial
Network (GAN), and address an important issue: search experience consists of a
relatively large number of actions that are not on a solution path and a
relatively small number of actions that actually are on a solution path. We
introduce a new technique, based on an importance-ratio estimation method, for
using samples from a non-target distribution to make GAN learning more
data-efficient. We provide theoretical guarantees and empirical evaluation in
three challenging continuous robot planning problems to illustrate the
effectiveness of our algorithm."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1710.05468,https://arxiv.org/abs/1710.05468,"Abstract:  Throughout this chapter, we provide theoretical insights into why and how
deep learning can generalize well, despite its large capacity, complexity,
possible algorithmic instability, nonrobustness, and sharp minima, responding
to an open question in the literature. We also propose new open problems and
discuss the limitations of our results."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1705.10907,https://arxiv.org/abs/1705.10907,"Abstract:  As drones and autonomous cars become more widespread it is becoming
increasingly important that robots can operate safely under realistic
conditions. The noisy information fed into real systems means that robots must
use estimates of the environment to plan navigation. Efficiently guaranteeing
that the resulting motion plans are safe under these circumstances has proved
difficult. We examine how to guarantee that a trajectory or policy is safe with
only imperfect observations of the environment. We examine the implications of
various mathematical formalisms of safety and arrive at a mathematical notion
of safety of a long-term execution, even when conditioned on observational
information. We present efficient algorithms that can prove that trajectories
or policies are safe with much tighter bounds than in previous work. Notably,
the complexity of the environment does not affect our methods ability to
evaluate if a trajectory or policy is safe. We then use these safety checking
methods to design a safe variant of the RRT planning algorithm."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1704.06131,https://arxiv.org/abs/1704.06131,"Abstract:  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1701.00287,https://arxiv.org/abs/1701.00287,"Abstract:  Many robotic planning applications involve continuous actions with highly
non-linear constraints, which cannot be modeled using modern planners that
construct a propositional representation. We introduce STRIPStream: an
extension of the STRIPS language which can model these domains by supporting
the specification of blackbox generators to handle complex constraints. The
outputs of these generators interact with actions through possibly infinite
streams of objects and static predicates. We provide two algorithms which both
reduce STRIPStream problems to a sequence of finite-domain planning problems.
The representation and algorithms are entirely domain independent. We
demonstrate our framework on simple illustrative domains, and then on a
high-dimensional, continuous robotic task and motion planning domain."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1608.01335,https://arxiv.org/abs/1608.01335,"Abstract:  Mobile manipulation problems involving many objects are challenging to solve
due to the high dimensionality and multi-modality of their hybrid configuration
spaces. Planners that perform a purely geometric search are prohibitively slow
for solving these problems because they are unable to factor the configuration
space. Symbolic task planners can efficiently construct plans involving many
variables but cannot represent the geometric and kinematic constraints required
in manipulation. We present the FFRob algorithm for solving task and motion
planning problems. First, we introduce Extended Action Specification (EAS) as a
general purpose planning representation that supports arbitrary predicates as
conditions. We adapt existing heuristic search ideas for solving \proc{strips}
planning problems, particularly delete-relaxations, to solve EAS problem
instances. We then apply the EAS representation and planners to manipulation
problems resulting in FFRob. FFRob iteratively discretizes task and motion
planning problems using batch sampling of manipulation primitives and a
multi-query roadmap structure that can be conditionalized to evaluate
reachability under different placements of movable objects. This structure
enables the EAS planner to efficiently compute heuristics that incorporate
geometric and kinematic planning constraints to give a tight estimate of the
distance to the goal. Additionally, we show FFRob is probabilistically complete
and has finite expected runtime. Finally, we empirically demonstrate FFRob's
effectiveness on complex and diverse task and motion planning tasks including
rearrangement planning and navigation among movable objects."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1608.01302,https://arxiv.org/abs/1608.01302,"Abstract:  We investigate learning heuristics for domain-specific planning. Prior work
framed learning a heuristic as an ordinary regression problem. However, in a
greedy best-first search, the ordering of states induced by a heuristic is more
indicative of the resulting planner's performance than mean squared error.
Thus, we instead frame learning a heuristic as a learning to rank problem which
we solve using a RankSVM formulation. Additionally, we introduce new methods
for computing features that capture temporal interactions in an approximate
plan. Our experiments on recent International Planning Competition problems
show that the RankSVM learned heuristics outperform both the original
heuristics and heuristics learned through ordinary regression."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1607.07762,https://arxiv.org/abs/1607.07762,"Abstract:  We introduce a framework for model learning and planning in stochastic
domains with continuous state and action spaces and non-Gaussian transition
models. It is efficient because (1) local models are estimated only when the
planner requires them; (2) the planner focuses on the most relevant states to
the current planning problem; and (3) the planner focuses on the most
informative and/or high-value actions. Our theoretical analysis shows the
validity and asymptotic optimality of the proposed approach. Empirically, we
demonstrate the effectiveness of our algorithm on a simulated multi-modal
pushing problem."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1604.03468,https://arxiv.org/abs/1604.03468,"Abstract:  In this paper we address planning problems in high-dimensional hybrid
configuration spaces, with a particular focus on manipulation planning problems
involving many objects. We present the hybrid backward-forward (HBF) planning
algorithm that uses a backward identification of constraints to direct the
sampling of the infinite action space in a forward search from the initial
state towards a goal configuration. The resulting planner is probabilistically
complete and can effectively construct long manipulation plans requiring both
prehensile and nonprehensile actions in cluttered environments."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1604.01348,https://arxiv.org/abs/1604.01348,"Abstract:  This paper presents a Bayesian optimization method with exponential
convergence without the need of auxiliary optimization and without the
delta-cover sampling. Most Bayesian optimization methods require auxiliary
optimization: an additional non-convex global optimization problem, which can
be time-consuming and hard to implement in practice. Also, the existing
Bayesian optimization method with exponential convergence requires access to
the delta-cover sampling, which was considered to be impractical. Our approach
eliminates both requirements and achieves an exponential convergence rate."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1512.00573,https://arxiv.org/abs/1512.00573,"Abstract:  To accomplish tasks in human-centric indoor environments, robots need to
represent and understand the world in terms of objects and their attributes. We
refer to this attribute-based representation as a world model, and consider how
to acquire it via noisy perception and maintain it over time, as objects are
added, changed, and removed in the world. Previous work has framed this as
multiple-target tracking problem, where objects are potentially in motion at
all times. Although this approach is general, it is computationally expensive.
We argue that such generality is not needed in typical world modeling tasks,
where objects only change state occasionally. More efficient approaches are
enabled by restricting ourselves to such semi-static environments.
We consider a previously-proposed clustering-based world modeling approach
that assumed static environments, and extend it to semi-static domains by
applying a dependent Dirichlet-process (DDP) mixture model. We derive a novel
MAP inference algorithm under this model, subject to data association
constraints. We demonstrate our approach improves computational performance in
semi-static environments."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1408.1484,https://arxiv.org/abs/1408.1484,"Abstract:  Cooperative games are those in which both agents share the same payoff
structure. Value-based reinforcement-learning algorithms, such as variants of
Q-learning, have been applied to learning cooperative games, but they only
apply when the game state is completely observable to both agents. Policy
search methods are a reasonable alternative to value-based methods for
partially observable environments. In this paper, we provide a gradient-based
distributed policy-search method for cooperative games and compare the notion
of local optimum to that of Nash equilibrium. We demonstrate the effectiveness
of this method experimentally in a small, partially observable simulated soccer
domain."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1402.2871,https://arxiv.org/abs/1402.2871,"Abstract:  We describe a probabilistic framework for synthesizing control policies for
general multi-robot systems, given environment and sensor models and a cost
function. Decentralized, partially observable Markov decision processes
(Dec-POMDPs) are a general model of decision processes where a team of agents
must cooperate to optimize some objective (specified by a shared reward or cost
function) in the presence of uncertainty, but where communication limitations
mean that the agents cannot share their state, so execution must proceed in a
decentralized fashion. While Dec-POMDPs are typically intractable to solve for
real-world problems, recent research on the use of macro-actions in Dec-POMDPs
has significantly increased the size of problem that can be practically solved
as a Dec-POMDP. We describe this general model, and show how, in contrast to
most existing methods that are specialized to a particular problem class, it
can synthesize control policies that use whatever opportunities for
coordination are present in the problem, while balancing off uncertainty in
outcomes, sensor information, and information about other agents. We use three
variations on a warehouse task to show that a single planner of this type can
generate cooperative behavior using task allocation, direct communication, and
signaling, as appropriate."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1303.1491,https://arxiv.org/abs/1303.1491,"Abstract:  We describe a method for time-critical decision making involving sequential
tasks and stochastic processes. The method employs several iterative refinement
routines for solving different aspects of the decision making problem. This
paper concentrates on the meta-level control problem of deliberation
scheduling, allocating computational resources to these routines. We provide
different models corresponding to optimization problems that capture the
different circumstances and computational strategies for decision making under
time constraints. We consider precursor models in which all decision making is
performed prior to execution and recurrent models in which decision making is
performed in parallel with execution, accounting for the states observed during
execution and anticipating future states. We describe algorithms for precursor
and recurrent models and provide the results of our empirical investigations to
date."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1302.4971,https://arxiv.org/abs/1302.4971,"Abstract:  Markov decision problems (MDPs) provide the foundations for a number of
problems of interest to AI researchers studying automated planning and
reinforcement learning. In this paper, we summarize results regarding the
complexity of solving MDPs and the running time of MDP solution algorithms. We
argue that, although MDPs can be solved efficiently in theory, more study is
needed to reveal practical algorithms for solving large problems quickly. To
encourage future research, we sketch some alternative methods of analysis that
rely on the structure of MDPs."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.7381,https://arxiv.org/abs/1301.7381,"Abstract:  We investigate the use of temporally abstract actions, or macro-actions, in
the solution of Markov decision processes. Unlike current models that combine
both primitive actions and macro-actions and leave the state space unchanged,
we propose a hierarchical model (using an abstract MDP) that works with
macro-actions only, and that significantly reduces the size of the state space.
This is achieved by treating macroactions as local policies that act in certain
regions of state space, and by restricting states in the abstract MDP to those
at the boundaries of regions. The abstract MDP approximates the original and
can be solved more efficiently. We discuss several ways in which macro-actions
can be generated to ensure good solution quality. Finally, we consider ways in
which macro-actions can be reused to solve multiple, related MDPs; and we show
that this can justify the computational overhead of macro-action generation."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6730,https://arxiv.org/abs/1301.6730,"Abstract:  Many applications require that we learn the parameters of a model from data.
EM is a method used to learn the parameters of probabilistic models for which
the data for some of the variables in the models is either missing or hidden.
There are instances in which this method is slow to converge. Therefore,
several accelerations have been proposed to improve the method. None of the
proposed acceleration methods are theoretically dominant and experimental
comparisons are lacking. In this paper, we present the different proposed
accelerations and try to compare them experimentally. From the results of the
experiments, we argue that some acceleration of EM is always possible, but that
which acceleration is superior depends on properties of the problem."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6721,https://arxiv.org/abs/1301.6721,"Abstract:  Reactive (memoryless) policies are sufficient in completely observable Markov
decision processes (MDPs), but some kind of memory is usually necessary for
optimal control of a partially observable MDP. Policies with finite memory can
be represented as finite-state automata. In this paper, we extend Baird and
Moore's VAPS algorithm to the problem of learning general finite-state
automata. Because it performs stochastic gradient descent, this algorithm can
be shown to converge to a locally optimal finite-state controller. We provide
the details of the algorithm and then consider the question of under what
conditions stochastic gradient descent will outperform exact gradient descent.
We conclude with empirical results comparing the performance of stochastic and
exact gradient descent, and showing the ability of our algorithm to extract the
useful information contained in the sequence of past observations to compensate
for the lack of observability at each time-step."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.6720,https://arxiv.org/abs/1301.6720,"Abstract:  Solving partially observable Markov decision processes (POMDPs) is highly
intractable in general, at least in part because the optimal policy may be
infinitely large. In this paper, we explore the problem of finding the optimal
policy from a restricted set of policies, represented as finite state automata
of a given size. This problem is also intractable, but we show that the
complexity can be greatly reduced when the POMDP and/or policy are further
constrained. We demonstrate good empirical results with a branch-and-bound
method for finding globally optimal deterministic policies, and a
gradient-ascent method for finding locally optimal stochastic policies."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.3882,https://arxiv.org/abs/1301.3882,"Abstract:  Sampling is an important tool for estimating large, complex sums and
integrals over high dimensional spaces. For instance, important sampling has
been used as an alternative to exact methods for inference in belief networks.
Ideally, we want to have a sampling distribution that provides optimal-variance
estimators. In this paper, we present methods that improve the sampling
distribution by systematically adapting it as we obtain information from the
samples. We present a stochastic-gradient-descent method for sequentially
updating the sampling distribution based on the direct minization of the
variance. We also present other stochastic-gradient-descent methods based on
the minimizationof typical notions of distance between the current sampling
distribution and approximations of the target, optimal distribution. We finally
validate and compare the different methods empirically by applying them to the
problem of action evaluation in influence diagrams."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1301.0567,https://arxiv.org/abs/1301.0567,"Abstract:  Most reinforcement learning methods operate on propositional representations
of the world state. Such representations are often intractably large and
generalize poorly. Using a deictic representation is believed to be a viable
alternative: they promise generalization while allowing the use of existing
reinforcement-learning methods. Yet, there are few experiments on learning with
deictic representations reported in the literature. In this paper we explore
the effectiveness of two forms of deictic representation and a naïve
propositional representation in a simple blocks-world domain. We find,
empirically, that the deictic representations actually worsen learning
performance. We conclude with a discussion of possible causes of these results
and strategies for more effective learning in domains with objects."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1206.5928,https://arxiv.org/abs/1206.5928,"Abstract:  We apply decision theoretic techniques to construct non-player characters
that are able to assist a human player in collaborative games. The method is
based on solving Markov decision processes, which can be difficult when the
game state is described by many variables. To scale to more complex games, the
method allows decomposition of a game task into subtasks, each of which can be
modelled by a Markov decision process. Intention recognition is used to infer
the subtask that the human is currently performing, allowing the helper to
assist the human in performing the correct task. Experiments show that the
method can be effective, giving near-human level performance in helping a human
in a collaborative game."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:1206.5249,https://arxiv.org/abs/1206.5249,"Abstract:  The ways in which an agent's actions affect the world can often be modeled
compactly using a set of relational probabilistic planning rules. This paper
addresses the problem of learning such rule sets for multiple related tasks. We
take a hierarchical Bayesian approach, in which the system learns a prior
distribution over rule sets. We present a class of prior distributions
parameterized by a rule set prototype that is stochastically modified to
produce a task-specific rule set. We also describe a coordinate ascent
algorithm that iteratively optimizes the task-specific rule sets and the prior
distribution. Experiments using this algorithm show that transferring
information from related tasks significantly reduces the amount of training
data required to predict action effects in blocks-world domains."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:cs/0105032,https://arxiv.org/abs/cs/0105032,"Abstract:  Cooperative games are those in which both agents share the same payoff
structure. Value-based reinforcement-learning algorithms, such as variants of
Q-learning, have been applied to learning cooperative games, but they only
apply when the game state is completely observable to both agents. Policy
search methods are a reasonable alternative to value-based methods for
partially observable environments. In this paper, we provide a gradient-based
distributed policy-search method for cooperative games and compare the notion
of local optimum to that of Nash equilibrium. We demonstrate the effectiveness
of this method experimentally in a small, partially observable simulated soccer
domain."
Leslie Kaelbling,Kaelbling_Leslie,arXiv:cs/0103003,https://arxiv.org/abs/cs/0103003,"Abstract:  In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems."
David Karger,Karger_David,arXiv:1705.08992,https://arxiv.org/abs/1705.08992,"Abstract:  This paper formulates a novel problem on graphs: find the minimal subset of
edges in a fully connected graph, such that the resulting graph contains all
spanning trees for a set of specifed sub-graphs. This formulation is motivated
by an un-supervised grammar induction problem from computational linguistics.
We present a reduction to some known problems and algorithms from graph theory,
provide computational complexity results, and describe an approximation
algorithm."
David Karger,Karger_David,arXiv:1409.6680,https://arxiv.org/abs/1409.6680,"Abstract:  Constructing a good conference schedule for a large multi-track conference
needs to take into account the preferences and constraints of organizers,
authors, and attendees. Creating a schedule which has fewer conflicts for
authors and attendees, and thematically coherent sessions is a challenging
task.
Cobi introduced an alternative approach to conference scheduling by engaging
the community to play an active role in the planning process. The current Cobi
pipeline consists of committee-sourcing and author-sourcing to plan a
conference schedule. We further explore the design space of community-sourcing
by introducing attendee-sourcing -- a process that collects input from
conference attendees and encodes them as preferences and constraints for
creating sessions and schedule. For CHI 2014, a large multi-track conference in
human-computer interaction with more than 3,000 attendees and 1,000 authors, we
collected attendees' preferences by making available all the accepted papers at
the conference on a paper recommendation tool we built called Confer, for a
period of 45 days before announcing the conference program (sessions and
schedule). We compare the preferences marked on Confer with the preferences
collected from Cobi's author-sourcing approach. We show that attendee-sourcing
can provide insights beyond what can be discovered by author-sourcing. For CHI
2014, the results show value in the method and attendees' participation. It
produces data that provides more alternatives in scheduling and complements
data collected from other methods for creating coherent sessions and reducing
conflicts."
David Karger,Karger_David,arXiv:1401.3488,https://arxiv.org/abs/1401.3488,"Abstract:  We present a novel Bayesian topic model for learning discourse-level document
structure. Our model leverages insights from discourse theory to constrain
latent topic assignments in a way that reflects the underlying organization of
document topics. We propose a global model in which both topic selection and
ordering are biased to be similar across a collection of related documents. We
show that this space of orderings can be effectively represented using a
distribution over permutations called the Generalized Mallows Model. We apply
our method to three complementary discourse-level tasks: cross-document
alignment, document segmentation, and information ordering. Our experiments
show that incorporating our permutation-based model in these applications
yields substantial improvements in performance over previously proposed
methods."
David Karger,Karger_David,arXiv:1204.2995,https://arxiv.org/abs/1204.2995,"Abstract:  Realtime crowdsourcing research has demonstrated that it is possible to
recruit paid crowds within seconds by managing a small, fast-reacting worker
pool. Realtime crowds enable crowd-powered systems that respond at interactive
speeds: for example, cameras, robots and instant opinion polls. So far, these
techniques have mainly been proof-of-concept prototypes: research has not yet
attempted to understand how they might work at large scale or optimize their
cost/performance trade-offs. In this paper, we use queueing theory to analyze
the retainer model for realtime crowdsourcing, in particular its expected wait
time and cost to requesters. We provide an algorithm that allows requesters to
minimize their cost subject to performance requirements. We then propose and
analyze three techniques to improve performance: push notifications, shared
retainer pools, and precruitment, which involves recalling retainer workers
before a task actually arrives. An experimental validation finds that
precruited workers begin a task 500 milliseconds after it is posted, delivering
results below the one-second cognitive threshold for an end-user to stay in
flow."
David Karger,Karger_David,arXiv:1110.3564,https://arxiv.org/abs/1110.3564,"Abstract:  Crowdsourcing systems, in which numerous tasks are electronically distributed
to numerous ""information piece-workers"", have emerged as an effective paradigm
for human-powered solving of large scale problems in domains such as image
classification, data entry, optical character recognition, recommendation, and
proofreading. Because these low-paid workers can be unreliable, nearly all such
systems must devise schemes to increase confidence in their answers, typically
by assigning each task multiple times and combining the answers in an
appropriate manner, e.g. majority voting.
In this paper, we consider a general model of such crowdsourcing tasks and
pose the problem of minimizing the total price (i.e., number of task
assignments) that must be paid to achieve a target overall reliability. We give
a new algorithm for deciding which tasks to assign to which workers and for
inferring correct answers from the workers' answers. We show that our
algorithm, inspired by belief propagation and low-rank matrix approximation,
significantly outperforms majority voting and, in fact, is optimal through
comparison to an oracle that knows the reliability of every worker. Further, we
compare our approach with a more general class of algorithms which can
dynamically assign tasks. By adaptively deciding which questions to ask to the
next arriving worker, one might hope to reduce uncertainty more efficiently. We
show that, perhaps surprisingly, the minimum price necessary to achieve a
target reliability scales in the same manner under both adaptive and
non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under
both scenarios. This strongly relies on the fact that workers are fleeting and
can not be exploited. Therefore, architecturally, our results suggest that
building a reliable worker-reputation system is essential to fully harnessing
the potential of adaptive designs."
David Karger,Karger_David,arXiv:1109.6881,https://arxiv.org/abs/1109.6881,"Abstract:  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible
to task people with small jobs, such as labeling images or looking up phone
numbers, via a programmatic interface. MTurk tasks for processing datasets with
humans are currently designed with significant reimplementation of common
workflows and ad-hoc selection of parameters such as price to pay per task. We
describe how we have integrated crowds into a declarative workflow engine
called Qurk to reduce the burden on workflow designers. In this paper, we focus
on how to use humans to compare items for sorting and joining data, two of the
most common operations in DBMSs. We describe our basic query interface and the
user interface of the tasks we post to MTurk. We also propose a number of
optimizations, including task batching, replacing pairwise comparisons with
numerical ratings, and pre-filtering tables before joining them, which
dramatically reduce the overall cost of running sorts and joins on the crowd.
In an experiment joining two sets of images, we reduce the overall cost from
$67 in a naive implementation to about $3, without substantially affecting
accuracy or latency. In an end-to-end experiment, we reduced cost by a factor
of 14.5."
David Karger,Karger_David,arXiv:1107.3013,https://arxiv.org/abs/1107.3013,"Abstract:  We present an algorithm for generating Poisson-disc patterns taking O(N) time
to generate $N$ points. The method is based on a grid of regions which can
contain no more than one point in the final pattern, and uses an explicit model
of point arrival times under a uniform Poisson process."
David Karger,Karger_David,arXiv:1104.2527,https://arxiv.org/abs/1104.2527,"Abstract:  We use network coding to improve the speed of distributed computation in the
dynamic network model of Kuhn, Lynch and Oshman [STOC '10]. In this model an
adversary adaptively chooses a new network topology in every round, making even
basic distributed computations challenging.
Kuhn et al. show that n nodes, each starting with a d-bit token, can
broadcast them to all nodes in time O(n^2) using b-bit messages, where b > d +
log n. Their algorithms take the natural approach of {token forwarding}: in
every round each node broadcasts some particular token it knows. They prove
matching Omega(n^2) lower bounds for a natural class of token forwarding
algorithms and an Omega(n log n) lower bound that applies to all
token-forwarding algorithms.
We use network coding, transmitting random linear combinations of tokens, to
break both lower bounds. Our algorithm's performance is quadratic in the
message size b, broadcasting the n tokens in roughly d/b^2 * n^2 rounds. For b
= d = O(log n) our algorithms use O(n^2/log n) rounds, breaking the first lower
bound, while for larger message sizes we obtain linear-time algorithms. We also
consider networks that change only every T rounds, and achieve an additional
factor T^2 speedup. This contrasts with related lower and upper bounds of Kuhn
et al. implying that for natural token-forwarding algorithms a speedup of T,
but not more, can be obtained. Lastly, we give a general way to derandomize
random linear network coding, that also leads to new deterministic information
dissemination algorithms."
David Karger,Karger_David,arXiv:0802.2418,https://arxiv.org/abs/0802.2418,"Abstract:  This paper presents improved approximation algorithms for the problem of
multiprocessor scheduling under uncertainty, or SUU, in which the execution of
each job may fail probabilistically. This problem is motivated by the
increasing use of distributed computing to handle large, computationally
intensive tasks. In the SUU problem we are given n unit-length jobs and m
machines, a directed acyclic graph G of precedence constraints among jobs, and
unrelated failure probabilities q_{ij} for each job j when executed on machine
i for a single timestep. Our goal is to find a schedule that minimizes the
expected makespan, which is the expected time at which all jobs complete.
Lin and Rajaraman gave the first approximations for this NP-hard problem for
the special cases of independent jobs, precedence constraints forming disjoint
chains, and precedence constraints forming trees. In this paper, we present
asymptotically better approximation algorithms. In particular, we give an
O(loglog min(m,n))-approximation for independent jobs (improving on the
previously best O(log n)-approximation). We also give an O(log(n+m) loglog
min(m,n))-approximation algorithm for precedence constraints that form disjoint
chains (improving on the previously best
O(log(n)log(m)log(n+m)/loglog(n+m))-approximation by a (log n/loglog n)^2
factor when n = poly(m). Our algorithm for precedence constraints forming
chains can also be used as a component for precedence constraints forming
trees, yielding a similar improvement over the previously best algorithms for
trees."
David Karger,Karger_David,arXiv:cs/0603022,https://arxiv.org/abs/cs/0603022,"Abstract:  We examine the issue of separation and code design for networks that operate
over finite fields. We demonstrate that source-channel (or source-network)
separation holds for several canonical network examples like the noisy multiple
access channel and the erasure degraded broadcast channel, when the whole
network operates over a common finite field. This robustness of separation is
predicated on the fact that noise and inputs are independent, and we examine
the failure of separation when noise is dependent on inputs in multiple access
channels.
Our approach is based on the sufficiency of linear codes. Using a simple and
unifying framework, we not only re-establish with economy the optimality of
linear codes for single-transmitter, single-receiver channels and for
Slepian-Wolf source coding, but also establish the optimality of linear codes
for multiple access and for erasure degraded broadcast channels. The linearity
allows us to obtain simple optimal code constructions and to study capacity
regions of the noisy multiple access and the degraded broadcast channel. The
linearity of both source and network coding blurs the delineation between
source and network codes. While our results point to the fact that separation
of source coding and channel coding is optimal in some canonical networks, we
show that decomposing networks into canonical subnetworks may not be effective.
Thus, we argue that it may be the lack of decomposability of a network into
canonical network modules, rather than the lack of separation between source
and channel coding, that presents major challenges for coding over networks."
David Karger,Karger_David,arXiv:cs/0503064,https://arxiv.org/abs/cs/0503064,"Abstract:  We consider the problem of establishing minimum-cost multicast connections
over coded packet networks, i.e. packet networks where the contents of outgoing
packets are arbitrary, causal functions of the contents of received packets. We
consider both wireline and wireless packet networks as well as both static
multicast (where membership of the multicast group remains constant for the
duration of the connection) and dynamic multicast (where membership of the
multicast group changes in time, with nodes joining and leaving the group).
For static multicast, we reduce the problem to a polynomial-time solvable
optimization problem, and we present decentralized algorithms for solving it.
These algorithms, when coupled with existing decentralized schemes for
constructing network codes, yield a fully decentralized approach for achieving
minimum-cost multicast. By contrast, establishing minimum-cost static multicast
connections over routed packet networks is a very difficult problem even using
centralized computation, except in the special cases of unicast and broadcast
connections.
For dynamic multicast, we reduce the problem to a dynamic programming problem
and apply the theory of dynamic programming to suggest how it may be solved."
David Karger,Karger_David,arXiv:cs/0207078,https://arxiv.org/abs/cs/0207078,"Abstract:  We improve on random sampling techniques for approximately solving problems
that involve cuts and flows in graphs. We give a near-linear-time construction
that transforms any graph on n vertices into an O(n\log n)-edge graph on the
same vertices whose cuts have approximately the same value as the original
graph's. In this new graph, for example, we can run the O(m^{3/2})-time maximum
flow algorithm of Goldberg and Rao to find an s--t minimum cut in O(n^{3/2})
time. This corresponds to a (1+epsilon)-times minimum s--t cut in the original
graph. In a similar way, we can approximate a sparsest cut to within O(log n)
in O(n^2) time using a previous O(mn)-time algorithm. A related approach leads
to a randomized divide and conquer algorithm producing an approximately maximum
flow in O(m sqrt{n}) time."
David Karger,Karger_David,arXiv:cs/0205051,https://arxiv.org/abs/cs/0205051,"Abstract:  The multiway-cut problem is, given a weighted graph and k >= 2 terminal
nodes, to find a minimum-weight set of edges whose removal separates all the
terminals. The problem is NP-hard, and even NP-hard to approximate within
1+delta for some small delta > 0.
Calinescu, Karloff, and Rabani (1998) gave an algorithm with performance
guarantee 3/2-1/k, based on a geometric relaxation of the problem. In this
paper, we give improved randomized rounding schemes for their relaxation,
yielding a 12/11-approximation algorithm for k=3 and a 1.3438-approximation
algorithm in general.
Our approach hinges on the observation that the problem of designing a
randomized rounding scheme for a geometric relaxation is itself a linear
programming problem. The paper explores computational solutions to this
problem, and gives a proof that for a general class of geometric relaxations,
there are always randomized rounding schemes that match the integrality gap."
David Karger,Karger_David,arXiv:cs/9812008,https://arxiv.org/abs/cs/9812008,"Abstract:  We consider the problem of coloring k-colorable graphs with the fewest
possible colors. We present a randomized polynomial time algorithm that colors
a 3-colorable graph on $n$ vertices with min O(Delta^{1/3} log^{1/2} Delta log
n), O(n^{1/4} log^{1/2} n) colors where Delta is the maximum degree of any
vertex. Besides giving the best known approximation ratio in terms of n, this
marks the first non-trivial approximation result as a function of the maximum
degree Delta. This result can be generalized to k-colorable graphs to obtain a
coloring using min O(Delta^{1-2/k} log^{1/2} Delta log n), O(n^{1-3/(k+1)}
log^{1/2} n) colors. Our results are inspired by the recent work of Goemans and
Williamson who used an algorithm for semidefinite optimization problems, which
generalize linear programs, to obtain improved approximations for the MAX CUT
and MAX 2-SAT problems. An intriguing outcome of our work is a duality
relationship established between the value of the optimum solution to our
semidefinite program and the Lovasz theta-function. We show lower bounds on the
gap between the optimum solution of our semidefinite program and the actual
chromatic number; by duality this also demonstrates interesting new facts about
the theta-function."
David Karger,Karger_David,arXiv:cs/9812007,https://arxiv.org/abs/cs/9812007,"Abstract:  We significantly improve known time bounds for solving the minimum cut
problem on undirected graphs. We use a ``semi-duality'' between minimum cuts
and maximum spanning tree packings combined with our previously developed
random sampling techniques. We give a randomized algorithm that finds a minimum
cut in an m-edge, n-vertex graph with high probability in O(m log^3 n) time. We
also give a simpler randomized algorithm that finds all minimum cuts with high
probability in O(n^2 log n) time. This variant has an optimal RNC
parallelization. Both variants improve on the previous best time bound of O(n^2
log^3 n). Other applications of the tree-packing approach are new, nearly tight
bounds on the number of near minimum cuts a graph may have and a new data
structure for representing them in a space-efficient manner."
David Karger,Karger_David,arXiv:cs/9809012,https://arxiv.org/abs/cs/9809012,"Abstract:  The classic all-terminal network reliability problem posits a graph, each of
whose edges fails independently with some given probability."
Dina Katabi,Katabi_Dina,arXiv:1902.02037,https://arxiv.org/abs/1902.02037,"Abstract:  We consider the problem of inferring the values of an arbitrary set of
variables (e.g., risk of diseases) given other observed variables (e.g.,
symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images
or EEG). This is a common problem in healthcare since variables of interest
often differ for different patients. Existing methods including Bayesian
networks and structured prediction either do not incorporate high-dimensional
signals or fail to model conditional dependencies among variables. To address
these issues, we propose bidirectional inference networks (BIN), which stich
together multiple probabilistic neural networks, each modeling a conditional
dependency. Predictions are then made via iteratively updating variables using
backpropagation (BP) to maximize corresponding posterior probability.
Furthermore, we extend BIN to composite BIN (CBIN), which involves the
iterative prediction process in the training stage and improves both accuracy
and computational efficiency by adaptively smoothing the optimization
landscape. Experiments on synthetic and real-world datasets (a sleep study and
a dermatology dataset) show that CBIN is a single model that can achieve
state-of-the-art performance and obtain better accuracy in most inference tasks
than multiple models each specifically trained for a different task."
Dina Katabi,Katabi_Dina,arXiv:1706.06935,https://arxiv.org/abs/1706.06935,"Abstract:  There is much interest in integrating millimeter wave radios (mmWave) into
wireless LANs and 5G cellular networks to benefit from their multiple GHz of
available spectrum. Yet unlike existing technologies, e.g., WiFi, mmWave radios
require highly directional antennas. Since the antennas have pencil-beams, the
transmitter and receiver need to align their antenna beams before they can
communicate. Existing solutions scan the entire space to find the best
alignment. Such a process has been shown to introduce up to seconds of delay,
and is unsuitable for wireless networks where an access point has to quickly
switch between users and accommodate mobile clients.
This paper presents Rapid-Link, a new protocol that can find the best mmWave
beam alignment without scanning the space. Given all possible directions for
setting the antenna beam, Rapid-Link provably finds the optimal direction in
logarithmic number of measurements. Further, Rapid-Link works within the
existing 802.11ad standard for mmWave LAN, and can support both clients and
access points. We have implemented Rapid-Link in a mmWave radio and evaluated
it empirically. Our results show that it reduces beam alignment delay by orders
of magnitude. In particular, for highly directional mmWave devices operating
under 802.11ad, the delay drops from over a second to 2.5 ms."
Dina Katabi,Katabi_Dina,arXiv:1612.02307,https://arxiv.org/abs/1612.02307,"Abstract:  Many sensor applications are interested in computing a function over
measurements (e.g., sum, average, max) as opposed to collecting all sensor
data. Today, such data aggregation is done in a cluster-head. Sensor nodes
transmit their values sequentially to a cluster-head node, which calculates the
aggregation function and forwards it to the base station. In contrast, this
paper explores the possibility of computing a desired function over the air. We
devise a solution that enables sensors to transmit coherently over the wireless
medium so that the cluster-head directly receives the value of the desired
function. We present analysis and preliminary results that demonstrate that
such a design yield a large improvement in network throughput."
Dina Katabi,Katabi_Dina,arXiv:1505.03446,https://arxiv.org/abs/1505.03446,"Abstract:  Time-of-flight, i.e., the time incurred by a signal to travel from
transmitter to receiver, is perhaps the most intuitive way to measure distances
using wireless signals. It is used in major positioning systems such as GPS,
RADAR, and SONAR. However, attempts at using time-of-flight for indoor
localization have failed to deliver acceptable accuracy due to fundamental
limitations in measuring time on Wi-Fi and other RF consumer technologies.
While the research community has developed alternatives for RF-based indoor
localization that do not require time-of-flight, those approaches have their
own limitations that hamper their use in practice. In particular, many existing
approaches need receivers with large antenna arrays while commercial Wi-Fi
nodes have two or three antennas. Other systems require fingerprinting the
environment to create signal maps. More fundamentally, none of these methods
support indoor positioning between a pair of Wi-Fi devices
without~third~party~support.
In this paper, we present a set of algorithms that measure the time-of-flight
to sub-nanosecond accuracy on commercial Wi-Fi cards. We implement these
algorithms and demonstrate a system that achieves accurate device-to-device
localization, i.e. enables a pair of Wi-Fi devices to locate each other without
any support from the infrastructure, not even the location of the access
points."
Dina Katabi,Katabi_Dina,arXiv:1303.1209,https://arxiv.org/abs/1303.1209,"Abstract:  We present the first sample-optimal sublinear time algorithms for the sparse
Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our
algorithms are analyzed for /average case/ signals. For signals whose spectrum
is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,
where k is the expected sparsity of the signal. For signals whose spectrum is
approximately sparse, our algorithm uses O(k log n) samples and runs in O(k
log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of
samples used by our algorithms matches the known lower bounds for the
respective signal models.
By a known reduction, our algorithms give similar results for the
one-dimensional sparse Discrete Fourier Transform when n is a power of a small
composite number (e.g., n = 6^t)."
Dina Katabi,Katabi_Dina,arXiv:1201.2501,https://arxiv.org/abs/1201.2501,"Abstract:  We consider the problem of computing the k-sparse approximation to the
discrete Fourier transform of an n-dimensional signal. We show:
* An O(k log n)-time randomized algorithm for the case where the input signal
has at most k non-zero Fourier coefficients, and
* An O(k log n log(n/k))-time randomized algorithm for general input signals.
Both algorithms achieve o(n log n) time, and thus improve over the Fast
Fourier Transform, for any k = o(n). They are the first known algorithms that
satisfy this property. Also, if one assumes that the Fast Fourier Transform is
optimal, the algorithm for the exactly k-sparse case is optimal for any k =
n^{\Omega(1)}.
We complement our algorithmic results by showing that any algorithm for
computing the sparse Fourier transform of a general signal must use at least
\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to perform
adaptive sampling."
Manolis Kellis,Kellis_Manolis,arXiv:1901.08540,https://arxiv.org/abs/1901.08540,"Abstract:  Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails."
Manolis Kellis,Kellis_Manolis,arXiv:1811.01431,https://arxiv.org/abs/1811.01431,"Abstract:  Artificial Intelligence (AI) incorporating genetic and medical information
have been applied in disease risk prediction, unveiling disease mechanism, and
advancing therapeutics. However, AI training relies on highly sensitive and
private data which significantly limit their applications and robustness
evaluation. Moreover, the data access management after sharing across
organization heavily relies on legal restriction, and there is no guarantee in
preventing data leaking after sharing. Here, we present Genie, a secure AI
platform which allows AI models to be trained on medical data securely. The
platform combines the security of Intel Software Guarded eXtensions (SGX),
transparency of blockchain technology, and verifiability of open algorithms and
source codes. Genie shares insights of genetic and medical data without
exposing anyone's raw data. All data is instantly encrypted upon upload and
contributed to the models that the user chooses. The usage of the model and the
value generated from the genetic and health data will be tracked via a
blockchain, giving the data transparent and immutable ownership."
Manolis Kellis,Kellis_Manolis,arXiv:1811.00464,https://arxiv.org/abs/1811.00464,"Abstract:  Electronic health records (EHR) are rich heterogeneous collection of patient
health information, whose broad adoption provides great opportunities for
systematic health data mining. However, heterogeneous EHR data types and biased
ascertainment impose computational challenges. Here, we present mixEHR, an
unsupervised generative model integrating collaborative filtering and latent
topic models, which jointly models the discrete distributions of data
observation bias and actual data using latent disease-topic distributions. We
apply mixEHR on 12.8 million phenotypic observations from the MIMIC dataset,
and use it to reveal latent disease topics, interpret EHR results, impute
missing data, and predict mortality in intensive care units. Using both
simulation and real data, we show that mixEHR outperforms previous methods and
reveals meaningful multi-disease insights."
Manolis Kellis,Kellis_Manolis,arXiv:1606.07383,https://arxiv.org/abs/1606.07383,"Abstract:  Several significant models have been developed that enable the study of
diffusion of signals across biological, social and engineered networks. Within
these established frameworks, the inverse problem of identifying the source of
the propagated signal is challenging, owing to the numerous alternative
possibilities for signal progression through the network. In real world
networks, the challenge of determining sources is compounded as the true
propagation dynamics are typically unknown, and when they have been directly
measured, they rarely conform to the assumptions of any of the well-studied
models. In this paper we introduce a method called Network Infusion (NI) that
has been designed to circumvent these issues, making source inference practical
for large, complex real world networks. The key idea is that to infer the
source node in the network, full characterization of diffusion dynamics, in
many cases, may not be necessary. This objective is achieved by creating a
diffusion kernel that well-approximates standard diffusion models, but lends
itself to inversion, by design, via likelihood maximization or error
minimization. We apply NI for both single-source and multi-source diffusion,
for both single-snapshot and multi-snapshot observations, and for both
homogeneous and heterogeneous diffusion setups. We prove the mean-field
optimality of NI for different scenarios, and demonstrate its effectiveness
over several synthetic networks. Moreover, we apply NI to a real-data
application, identifying news sources in the Digg social network, and
demonstrate the effectiveness of NI compared to existing methods. Finally, we
propose an integrative source inference framework that combines NI with a
distance centrality-based method, which leads to a robust performance in cases
where the underlying dynamics are unknown."
Manolis Kellis,Kellis_Manolis,arXiv:1606.04789,https://arxiv.org/abs/1606.04789,"Abstract:  We introduce Network Maximal Correlation (NMC) as a multivariate measure of
nonlinear association among random variables. NMC is defined via an
optimization that infers transformations of variables by maximizing aggregate
inner products between transformed variables. For finite discrete and jointly
Gaussian random variables, we characterize a solution of the NMC optimization
using basis expansion of functions over appropriate basis functions. For finite
discrete variables, we propose an algorithm based on alternating conditional
expectation to determine NMC. Moreover we propose a distributed algorithm to
compute an approximation of NMC for large and dense graphs using graph
partitioning. For finite discrete variables, we show that the probability of
discrepancy greater than any given level between NMC and NMC computed using
empirical distributions decays exponentially fast as the sample size grows. For
jointly Gaussian variables, we show that under some conditions the NMC
optimization is an instance of the Max-Cut problem. We then illustrate an
application of NMC in inference of graphical model for bijective functions of
jointly Gaussian variables. Finally, we show NMC's utility in a data
application of learning nonlinear dependencies among genes in a cancer dataset."
Manolis Kellis,Kellis_Manolis,arXiv:1602.04181,https://arxiv.org/abs/1602.04181,"Abstract:  Graph alignment refers to the problem of finding a bijective mapping across
vertices of two graphs such that, if two nodes are connected in the first
graph, their images are connected in the second graph. This problem arises in
many fields such as computational biology, social sciences, and computer vision
and is often cast as a quadratic assignment problem (QAP). Most standard graph
alignment methods consider an optimization that maximizes the number of matches
between the two graphs, ignoring the effect of mismatches. We propose a
generalized graph alignment formulation that considers both matches and
mismatches in a standard QAP formulation. This modification can have a major
impact in aligning graphs with different sizes and heterogenous edge densities.
Moreover, we propose two methods for solving the generalized graph alignment
problem based on spectral decomposition of matrices. We compare the performance
of proposed methods with some existing graph alignment algorithms including
Natalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite
programming-based method over various synthetic and real graph models. Our
proposed method based on simultaneous alignment of multiple eigenvectors leads
to consistently good performance in different graph models. In particular, in
the alignment of regular graph structures which is one of the most difficult
graph alignment cases, our proposed method significantly outperforms other
methods."
Manolis Kellis,Kellis_Manolis,arXiv:1507.05941,https://arxiv.org/abs/1507.05941,"Abstract:  Information theory is rapidly approaching its 70th birthday. What are
promising future directions for research in information theory? Where will
information theory be having the most impact in 10-20 years? What new and
emerging areas are ripe for the most impact, of the sort that information
theory has had on the telecommunications industry over the last 60 years? How
should the IEEE Information Theory Society promote high-risk new research
directions and broaden the reach of information theory, while continuing to be
true to its ideals and insisting on the intellectual rigor that makes its
breakthroughs so powerful? These are some of the questions that an ad hoc
committee (composed of the present authors) explored over the past two years.
We have discussed and debated these questions, and solicited detailed inputs
from experts in fields including genomics, biology, economics, and
neuroscience. This report is the result of these discussions."
Manolis Kellis,Kellis_Manolis,arXiv:1411.6307,https://arxiv.org/abs/1411.6307,"Abstract:  We propose a novel diverse feature selection method based on determinantal
point processes (DPPs). Our model enables one to flexibly define diversity
based on the covariance of features (similar to orthogonal matching pursuit) or
alternatively based on side information. We introduce our approach in the
context of Bayesian sparse regression, employing a DPP as a variational
approximation to the true spike and slab posterior distribution. We
subsequently show how this variational DPP approximation generalizes and
extends mean-field approximation, and can be learned efficiently by exploiting
the fast sampling properties of DPPs. Our motivating application comes from
bioinformatics, where we aim to identify a diverse set of genes whose
expression profiles predict a tumor type where the diversity is defined with
respect to a gene-gene interaction network. We also explore an application in
spatial statistics. In both cases, we demonstrate that the proposed method
yields significantly more diverse feature sets than classic sparse methods,
without compromising accuracy."
Jing Kong,Kong_Jing,arXiv:1812.07111,https://arxiv.org/abs/1812.07111,"Abstract:  When the Fermi level matches the Dirac point in graphene, the reduced charge
screening can dramatically enhance electron-electron (e-e) scattering to
produce a strongly interacting Dirac liquid. While the dominance of e-e
scattering already leads to novel behaviors, such as electron hydrodynamic
flow, further exotic phenomena have been predicted to arise specifically from
the unique kinematics of e-e scattering in massless Dirac systems. Here, we use
optoelectronic probes, which are highly sensitive to the kinematics of electron
scattering, to uncover a giant intrinsic photocurrent response in pristine
graphene. This photocurrent emerges exclusively at the charge neutrality point
and vanishes abruptly at non-zero charge densities. Moreover, it is observed at
places with broken reflection symmetry, and it is selectively enhanced at free
graphene edges with sharp bends. Our findings reveal that the photocurrent
relaxation is strongly suppressed by a drastic change of fast photocarrier
kinematics in graphene when its Fermi level matches the Dirac point. The
emergence of robust photocurrents in neutral Dirac materials promises new
energy-harvesting functionalities and highlights intriguing electron dynamics
in the optoelectronic response of Dirac fluids."
Jing Kong,Kong_Jing,arXiv:1811.12777,https://arxiv.org/abs/1811.12777,"Abstract:  Hexagonal boron nitride (h-BN) is a two dimensional (2D) layered insulator
with superior dielectric performance that offers excellent interaction with
other 2D materials (e.g. graphene, MoS2). Large-area h-BN can be readily grown
on metallic substrates via chemical vapor deposition (CVD), but the impact of
local inhomogeneities on the electrical properties of the h-BN and their effect
in electronic devices is unknown. Here it is shown that the electrical
properties of h-BN stacks grown on polycrystalline Pt vary a lot depending on
the crystalline orientation of the Pt grain, but within the same grain the
electrical properties of the h-BN are very homogeneous. The reason is that the
thickness of the CVD-grown h-BN stack is different on each Pt grain. Conductive
atomic force microscopy (CAFM) maps show that the tunneling current across the
h-BN stack fluctuates up to 3 orders of magnitude from one Pt grain to another.
However, probe station experiments reveal that the variability of electronic
devices fabricated within the same Pt grain is surprisingly small. As
cutting-edge electronic devices are ultra-scaled, and as the size of the
metallic substrate grains can easily exceed 100 {\mu}m (in diameter), CVD-grown
h-BN stacks may be useful to fabricate electronic devices with low variability."
Jing Kong,Kong_Jing,arXiv:1807.06154,https://arxiv.org/abs/1807.06154,"Abstract:  Property by design is one appealing idea in material synthesis but hard to
achieve in practice. A recent successful example is the demonstration of van
der Waals (vdW) heterostructures,1-3 in which atomic layers are stacked on each
other and different ingredients can be combined beyond symmetry and lattice
matching. This concept, usually described as a nanoscale Lego blocks, allows to
build sophisticated structures layer by layer. However, this concept has been
so far limited in two dimensional (2D) materials. Here we show a class of new
material where different layers are coaxially (instead of planarly) stacked. As
the structure is in one dimensional (1D) form, we name it ""1D vdW
heterostructures"". We demonstrate a 5 nm diameter nanotube consisting of three
different materials: an inner conductive carbon nanotube (CNT), a middle
insulating hexagonal boron nitride nanotube (BNNT) and an outside
semiconducting MoS2 nanotube. As the technique is highly applicable to other
materials in the current 2D libraries,4-6 we anticipate our strategy to be a
starting point for discovering a class of new semiconducting nanotube
materials. A plethora of function-designable 1D heterostructures will appear
after the combination of CNTs, BNNTs and semiconducting nanotubes."
Jing Kong,Kong_Jing,arXiv:1806.03493,https://arxiv.org/abs/1806.03493,"Abstract:  Second-order nonlinear optical interactions, including second harmonic
generation (SHG) and sum-frequency generation (SFG), can reveal a wealth of
information about chemical, electronic, and vibrational dynamics at the
nanoscale. Here, we demonstrate a powerful and flexible new approach, called
phase-modulated degenerate parametric amplification (DPA). The technique, which
allows for facile retrieval of both the amplitude and phase of the second-order
nonlinear optical response, has many advantages over conventional or
heterodyne-detected SHG, including the flexibility to detect the signal at
either the second harmonic or fundamental field wavelength. We demonstrate the
capabilities of this approach by imaging multi-grain flakes of single-layer
MoS2. We identify the absolute crystal orientation of each MoS2 domain and
resolve grain boundaries with high signal contrast and sub-diffraction-limited
spatial resolution. This robust all-optical method can be used to characterize
structure and dynamics in organic and inorganic systems, including biological
tissue, soft materials, and metal and semiconductor nanostructures, and is
particularly well-suited for imaging in media that are absorptive or highly
scattering to visible and ultraviolet light."
Jing Kong,Kong_Jing,arXiv:1804.10347,https://arxiv.org/abs/1804.10347,"Abstract:  We report a rare atom-like interaction between excitons in monolayer WS2,
measured using ultrafast absorption spectroscopy. At increasing excitation
density, the exciton resonance energy exhibits a pronounced redshift followed
by an anomalous blueshift. Using both material-realistic computation and
phenomenological modeling, we attribute this observation to plasma effects and
an attraction-repulsion crossover of the exciton-exciton interaction that
mimics the Lennard-Jones potential between atoms. Our experiment demonstrates a
strong analogy between excitons and atoms with respect to inter-particle
interaction, which holds promise to pursue the predicted liquid and crystalline
phases of excitons in two-dimensional materials."
Jing Kong,Kong_Jing,arXiv:1804.01061,https://arxiv.org/abs/1804.01061,"Abstract:  The ability to confine light into tiny spatial dimensions is important for
applications such as microscopy, sensing and nanoscale lasers. While plasmons
offer an appealing avenue to confine light, Landau damping in metals imposes a
trade-off between optical field confinement and losses. We show that a
graphene-insulator-metal heterostructure can overcome that trade-off, and
demonstrate plasmon confinement down to the ultimate limit of the lengthscale
of one atom. This is achieved by far-field excitation of plasmon modes squeezed
into an atomically thin hexagonal boron nitride dielectric h-BN spacer between
graphene and metal rods. A theoretical model which takes into account the
non-local optical response of both graphene and metal is used to describe the
results. These ultra-confined plasmonic modes, addressed with far-field light
excitation, enables a route to new regimes of ultra-strong light-matter
interactions."
Jing Kong,Kong_Jing,arXiv:1803.01369,https://arxiv.org/abs/1803.01369,"Abstract:  Atomic-level structural changes in materials are important but challenging to
study. Here, we demonstrate the dynamics and the possibility of manipulating a
phosphorus dopant atom in graphene using scanning transmission electron
microscopy (STEM). The mechanisms of various processes are explored and
compared with those of other dopant species by first-principles calculations.
This work paves the way for designing a more precise and optimized protocol for
atomic engineering."
Jing Kong,Kong_Jing,arXiv:1712.07925,https://arxiv.org/abs/1712.07925,"Abstract:  The valley pseudospin in monolayer transition metal dichalcogenides (TMDs)
has been proposed as a new way to manipulate information in various
optoelectronic devices. This relies on a large valley polarization that remains
stable over long timescales (hundreds of ns). However, time resolved
measurements report valley lifetimes of only a few ps. This has been attributed
to mechanisms such as phonon-mediated inter-valley scattering and a precession
of the valley psedospin through electron-hole exchange. Here we use transient
spin grating to directly measure the valley depolarization lifetime in
monolayer MoSe$_{2}$. We find a fast valley decay rate that scales linearly
with the excitation density at different temperatures. This establishes the
presence of strong exciton-exciton Coulomb exchange interactions enhancing the
valley depolarization. Our work highlights the microscopic processes inhibiting
the efficient use of the exciton valley pseudospin in monolayer TMDs."
Jing Kong,Kong_Jing,arXiv:1708.05369,https://arxiv.org/abs/1708.05369,"Abstract:  High-performance materials rely on small reorganization energies to
facilitate both charge separation and charge transport. Here, we performed DFT
calculations to predict small reorganization energies of rectangular silicene
nanoclusters with hydrogen-passivated edges denoted by H-SiNC. We observe that
across all geometries, H-SiNCs feature large electron affinities and highly
stabilized anionic states, indicating their potential as n-type materials. Our
findings suggest that fine-tuning the size of H-SiNCs along the zigzag and
armchair directions may permit the design of novel n-type electronic materials
and spinctronics devices that incorporate both high electron affinities and
very low internal reorganization energies."
Jing Kong,Kong_Jing,arXiv:1703.07346,https://arxiv.org/abs/1703.07346,"Abstract:  Coherent light-matter interaction can be used to manipulate the energy levels
of atoms, molecules and solids. When light with frequency {\omega} is detuned
away from a resonance {\omega}o, repulsion between the photon-dressed (Floquet)
states can lead to a shift of energy resonance. The dominant effect is the
optical Stark shift (1/({\omega}0-{\omega})), but there is an additional
contribution from the so-called Bloch-Siegert shift (1/({\omega}o+{\omega})).
Although it is common in atoms and molecules, the observation of Bloch-Siegert
shift in solids has so far been limited only to artificial atoms since the
shifts were small (<1 {\mu}eV) and inseparable from the optical Stark shift.
Here we observe an exceptionally large Bloch-Siegert shift (~10 meV) in
monolayer WS2 under infrared optical driving by virtue of the strong
light-matter interaction in this system. Moreover, we can disentangle the
Bloch-Siegert shift entirely from the optical Stark shift, because the two
effects are found to obey opposite selection rules at different valleys. By
controlling the light helicity, we can confine the Bloch-Siegert shift to occur
only at one valley, and the optical Stark shift at the other valley. Such a
valley-exclusive Bloch-Siegert shift allows for enhanced control over the
valleytronic properties in two-dimensional materials, and offers a new avenue
to explore quantum optics in solids."
Jing Kong,Kong_Jing,arXiv:1703.04531,https://arxiv.org/abs/1703.04531,"Abstract:  Coherent optical dressing of quantum materials offers technological
advantages to control their electronic properties, such as the electronic
valley degree of freedom in monolayer transition metal dichalcogenides (TMDs).
Here, we observe a new type of optical Stark effect in monolayer WS2, one that
is mediated by intervalley biexcitons under the blue-detuned driving with
circularly polarized light. We found that such helical optical driving not only
induces an exciton energy downshift at the excitation valley, but also causes
an anomalous energy upshift at the opposite valley, which is normally forbidden
by the exciton selection rules but now made accessible through the intervalley
biexcitons. These findings reveal the critical, but hitherto neglected, role of
biexcitons to couple the two seemingly independent valleys, and to enhance the
optical control in valleytronics."
Jing Kong,Kong_Jing,arXiv:1703.01666,https://arxiv.org/abs/1703.01666,"Abstract:  Two-dimensional (2-D) materials are of tremendous interest to integrated
photonics given their singular optical characteristics spanning light emission,
modulation, saturable absorption, and nonlinear optics. To harness their
optical properties, these atomically thin materials are usually attached onto
prefabricated devices via a transfer process. In this paper, we present a new
route for 2-D material integration with planar photonics. Central to this
approach is the use of chalcogenide glass, a multifunctional material which can
be directly deposited and patterned on a wide variety of 2-D materials and can
simultaneously function as the light guiding medium, a gate dielectric, and a
passivation layer for 2-D materials. Besides claiming improved fabrication
yield and throughput compared to the traditional transfer process, our
technique also enables unconventional multilayer device geometries optimally
designed for enhancing light-matter interactions in the 2-D layers.
Capitalizing on this facile integration method, we demonstrate a series of
high-performance glass-on-graphene devices including ultra-broadband on-chip
polarizers, energy-efficient thermo-optic switches, as well as graphene-based
mid-infrared (mid-IR) waveguide-integrated photodetectors and modulators."
Jing Kong,Kong_Jing,arXiv:1612.05119,https://arxiv.org/abs/1612.05119,"Abstract:  Plasmons in graphene nanostructures show great promise for mid-infrared
applications ranging from a few to tens of microns. However, mid-infrared
plasmonic resonances in graphene nanostructures are usually weak and
narrow-banded, limiting their potential in light manipulation and detection.
Here we investigate the coupling among graphene plasmonic nanostructures and
further show that by engineering the coupling, enhancement of light-graphene
interaction strength and broadening of spectral width can be achieved
simultaneously. Leveraging the concept of coupling, we demonstrate a hybrid
2-layer graphene nanoribbon array which shows 5 to 7% extinction within the
entire 8 to 14 {\mu}m (~700 to 1250 cm-1) wavelength range, covering one of the
important atmosphere ""infrared transmission windows"". Such coupled hybrid
graphene plasmonic nanostructures may find applications in infrared sensing and
free-space communications."
Jing Kong,Kong_Jing,arXiv:1610.07646,https://arxiv.org/abs/1610.07646,"Abstract:  In the effort to make 2D materials-based devices smaller, faster, and more
efficient, it is important to control charge carrier at lengths approaching the
nanometer scale. Traditional gating techniques based on capacitive coupling
through a gate dielectric cannot generate strong and uniform electric fields at
this scale due to divergence of the fields in dielectrics. This field
divergence limits the gating strength, boundary sharpness, and pitch size of
periodic structures, and restricts possible geometries of local gates (due to
wire packaging), precluding certain device concepts, such as plasmonics and
transformation optics based on metamaterials. Here we present a new gating
concept based on a dielectric-free self-aligned electrolyte technique that
allows spatially modulating charges with nanometer resolution. We employ a
combination of a solid-polymer electrolyte gate and an ion-impenetrable
e-beam-defined resist mask to locally create excess charges on top of the gated
surface. Electrostatic simulations indicate high carrier density variations of
$\Delta n =10^{14}\text{cm}^{-2}$ across a length of 10 nm at the mask
boundaries on the surface of a 2D conductor, resulting in a sharp depletion
region and a strong in-plane electric field of $6\times10^8 \text{Vm}^{-1}$
across the so-created junction. We apply this technique to the 2D material
graphene to demonstrate the creation of tunable p-n junctions for
optoelectronic applications. We also demonstrate the spatial versatility and
self-aligned properties of this technique by introducing a novel graphene
thermopile photodetector."
Jing Kong,Kong_Jing,arXiv:1601.02265,https://arxiv.org/abs/1601.02265,"Abstract:  Ultrafast electron thermalization - the process leading to Auger
recombination, carrier multiplication via impact ionization and hot carrier
luminescence - occurs when optically excited electrons in a material undergo
rapid electron-electron scattering to redistribute excess energy and reach
electronic thermal equilibrium. Due to extremely short time and length scales,
the measurement and manipulation of electron thermalization in nanoscale
devices remains challenging even with the most advanced ultrafast laser
techniques. Here, we overcome this challenge by leveraging the atomic thinness
of two-dimensional van der Waals (vdW) materials in order to introduce a highly
tunable electron transfer pathway that directly competes with electron
thermalization. We realize this scheme in a graphene-boron nitride-graphene
(G-BN-G) vdW heterostructure, through which optically excited carriers are
transported from one graphene layer to the other. By applying an interlayer
bias voltage or varying the excitation photon energy, interlayer carrier
transport can be controlled to occur faster or slower than the intralayer
scattering events, thus effectively tuning the electron thermalization pathways
in graphene. Our findings, which demonstrate a novel means to probe and
directly modulate electron energy transport in nanoscale materials, represent
an important step toward designing and implementing novel optoelectronic and
energy-harvesting devices with tailored microscopic properties."
Jing Kong,Kong_Jing,arXiv:1512.04492,https://arxiv.org/abs/1512.04492,"Abstract:  Diverse parallel stitched two-dimensional heterostructures are synthesized,
including metal-semiconductor (graphene-MoS2), semiconductor-semiconductor
(WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective
sowing of aromatic molecules as the seeds in chemical vapor deposition (CVD)
method. Our methodology enables the large-scale fabrication of lateral
heterostructures with arbitrary patterns, and clean and precisely aligned
interfaces, which offers tremendous potential for its application in integrated
circuits."
Jing Kong,Kong_Jing,arXiv:1502.07804,https://arxiv.org/abs/1502.07804,"Abstract:  As a new two-dimensional layered material, black phosphorus (BP) is a
promising material for nanoelectronics and nano-optoelectronics. We use Raman
spectroscopy and first-principles theory to report our findings related to
low-frequency (LF) interlayer breathing modes (<100 cm-1) in few-layer BP for
the first time. The breathing modes are assigned to Ag symmetry by the laser
polarization dependence study and group theory analysis. Compared to the
high-frequency (HF) Raman modes, the LF breathing modes are much more sensitive
to interlayer coupling and thus their frequencies show much stronger dependence
on the number of layers. Hence, they could be used as effective means to probe
both the crystalline orientation and thickness for few-layer BP. Furthermore,
the temperature dependence study shows that the breathing modes have a harmonic
behavior, in contrast to HF Raman modes which are known to exhibit
anharmonicity."
Jing Kong,Kong_Jing,arXiv:1410.6750,https://arxiv.org/abs/1410.6750,"Abstract:  A single-term density functional model for nondynamic and strong correlation
is presented, based on single-determinant Kohn-Sham density functional theory.
It is derived from modeling the adiabatic connection and contains only two
nonlinear empirical parameters. Preliminary tests show that the model recovers
majority of nondynamic correlation during a molecular dissociation and at the
same time performs reasonably for atomization energies. It demonstrates the
feasibility of developing DFT functionals for nondynamic and strong correlation
within the single-determinant KS scheme."
Jing Kong,Kong_Jing,arXiv:1407.7297,https://arxiv.org/abs/1407.7297,"Abstract:  Variable selection is of increasing importance to address the difficulties of
high dimensionality in many scientific areas. In this paper, we demonstrate a
property for distance covariance, which is incorporated in a novel feature
screening procedure together with the use of distance correlation. The approach
makes no distributional assumptions for the variables and does not require the
specification of a regression model, and hence is especially attractive in
variable selection given an enormous number of candidate attributes without
much information about the true model with the response. The method is applied
to two genetic risk problems, where issues including uncertainty of variable
selection via cross validation, subgroup of hard-to-classify cases and the
application of a reject option are discussed."
Jing Kong,Kong_Jing,arXiv:1407.6997,https://arxiv.org/abs/1407.6997,"Abstract:  Recently emerging large-area single-layer MoS2 grown by chemical vapor
deposition has triggered great interest due to its exciting potential for
applications in advanced electronic and optoelectronic devices. Unlike gapless
graphene, MoS2 has an intrinsic band gap in the visible which crosses over from
an indirect to a direct gap when reduced to a single atomic layer. In this
article, we report a comprehensive study of fundamental optical properties of
MoS2 revealed by optical spectroscopy of Raman, photoluminescence, and vacuum
ultraviolet spectroscopic ellipsometry. A band gap of 1.42 eV is determined by
the absorption threshold of bulk MoS2 that shifts to 1.83 eV in monolayer MoS2.
We extracted the high precision dielectric function up to 9.0 eV which leads to
the identification of many unique interband transitions at high symmetry points
in the MoS2 momentum space. The positions of the A and B excitons in single
layers are found to shift upwards in energy compared with those of the bulk
form and have smaller separation. A very strong optical critical point
predicted to correspond to a quasi-particle gap is observed at 2.86 eV, which
is attributed to optical transitions along the parallel bands between the M and
gama points in the reduced Brillouin zone. The absence of the bulk MoS2
spin-orbit interaction peak at ~ 3.0 eV in monolayer MoS2 is, as predicted, the
consequence of the coalescence of nearby excitons. A higher energy optical
transition at 3.98 eV, commonly occurred in bulk semiconductors, is associated
with a combination of several critical points.These optical transitions herein
reported enhance our understanding of monolayer MoS2 as well as of
two-dimensional systems in general, and thus provide informative guidelines for
MoS2 optical device designs and theoretical considerations."
Jing Kong,Kong_Jing,arXiv:1407.1825,https://arxiv.org/abs/1407.1825,"Abstract:  Breaking space-time symmetries in two-dimensional crystals (2D) can
dramatically influence their macroscopic electronic properties. Monolayer
transition-metal dichalcogenides (TMDs) are prime examples where the
intrinsically broken crystal inversion symmetry permits the generation of
valley-selective electron populations, even though the two valleys are
energetically degenerate, locked by time-reversal symmetry. Lifting the valley
degeneracy in these materials is of great interest because it would allow for
valley-specific band engineering and offer additional control in valleytronic
applications. While applying a magnetic field should in principle accomplish
this task, experiments to date have observed no valley-selective energy level
shifts in fields accessible in the laboratory. Here we show the first direct
evidence of lifted valley degeneracy in the monolayer TMD WS2. By applying
intense circularly polarized light, which breaks time-reversal symmetry, we
demonstrate that the exciton level in each valley can be selectively tuned by
as much as 18 meV via the optical Stark effect. These results offer a novel way
to control valley degree of freedom, and may provide a means to realize new
valley-selective Floquet topological phases in 2D TMDs."
Jing Kong,Kong_Jing,arXiv:1401.4951,https://arxiv.org/abs/1401.4951,"Abstract:  Layered transition metal dichalcogenides display a wide range of attractive
physical and chemical properties and are potentially important for various
device applications. Here we report the electronic transport and device
properties of monolayer molybdenum disulphide (MoS2) grown by chemical vapour
deposition (CVD). We show that these devices have the potential to suppress
short channel effects and have high critical breakdown electric field. However,
our study reveals that the electronic properties of these devices are at
present, severely limited by the presence of a significant amount of band tail
trapping states. Through capacitance and ac conductance measurements, we
systematically quantify the density-of-states and response time of these
states. Due to the large amount of trapped charges, the measured effective
mobility also leads to a large underestimation of the true band mobility and
the potential of the material. Continual engineering efforts on improving the
sample quality are needed for its potential applications."
Jing Kong,Kong_Jing,arXiv:1312.2918,https://arxiv.org/abs/1312.2918,"Abstract:  Interactions between two excitons can result in the formation of bound
quasiparticles, known as biexcitons. Their properties are determined by the
constituent excitons, with orbital and spin states resembling those of atoms.
Monolayer transition metal dichalcogenides (TMDs) present a unique system where
excitons acquire a new degree of freedom, the valley pseudospin, from which a
novel intervalley biexciton can be created. These biexcitons comprise two
excitons from different valleys, which are distinct from biexcitons in
conventional semiconductors and have no direct analogue in atomic and molecular
systems. However, their valley properties are not accessible to traditional
transport and optical measurements. Here, we report the observation of
intervalley biexcitons in the monolayer TMD MoS2 using ultrafast pump-probe
spectroscopy. By applying broadband probe pulses with different helicities, we
identify two species of intervalley biexcitons with large binding energies of
60 meV and 40 meV. In addition, we also reveal effects beyond biexcitonic
pairwise interactions in which the exciton energy redshifts at increasing
exciton densities, indicating the presence of many-body interactions among
them."
Jing Kong,Kong_Jing,arXiv:1305.5890,https://arxiv.org/abs/1305.5890,"Abstract:  Due to their exceptional mechanical and optical properties, dielectric
silicon nitride (SiN) micromembrane resonators have become the centerpiece of
many optomechanical experiments. Efficient capacitive coupling of the membrane
to an electrical system would facilitate exciting hybrid optoelectromechanical
devices. However, capacitive coupling of such dielectric membranes is rather
weak. Here we add a single layer of graphene on SiN micromembranes and compare
electromechanical coupling and mechanical properties to bare dielectric
membranes and to membranes metallized with an aluminium layer. The
electrostatic coupling of graphene coated membranes is found to be equal to a
perfectly conductive membrane. Our results show that a single layer of graphene
substantially enhances the electromechanical capacitive coupling without
significantly adding mass, decreasing the superior mechanical quality factor or
affecting the optical properties of SiN micromembrane resonators."
Jing Kong,Kong_Jing,arXiv:1302.4027,https://arxiv.org/abs/1302.4027,"Abstract:  2D nanoelectronics based on single-layer MoS2 offers great advantages for
both conventional and ubiquitous applications. This paper discusses the
large-scale CVD growth of single-layer MoS2 and fabrication of devices and
circuits for the first time. Both digital and analog circuits are fabricated to
demonstrate its capability for mixed-signal applications."
Jing Kong,Kong_Jing,arXiv:1208.1078,https://arxiv.org/abs/1208.1078,"Abstract:  Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have
been shown to exhibit excellent electrical and optical properties. The
semiconducting nature of MoS2 allows it to overcome the shortcomings of
zero-bandgap graphene, while still sharing many of graphene's advantages for
electronic and optoelectronic applications. Discrete electronic and
optoelectronic components, such as field-effect transistors, sensors and
photodetectors made from few-layer MoS2 show promising performance as potential
substitute of Si in conventional electronics and of organic and amorphous Si
semiconductors in ubiquitous systems and display applications. An important
next step is the fabrication of fully integrated multi-stage circuits and logic
building blocks on MoS2 to demonstrate its capability for complex digital logic
and high-frequency ac applications. This paper demonstrates an inverter, a NAND
gate, a static random access memory, and a five-stage ring oscillator based on
a direct-coupled transistor logic technology. The circuits comprise between two
to twelve transistors seamlessly integrated side-by-side on a single sheet of
bilayer MoS2. Both enhancement-mode and depletion-mode transistors were
fabricated thanks to the use of gate metals with different work functions."
Jing Kong,Kong_Jing,arXiv:1207.3369,https://arxiv.org/abs/1207.3369,"Abstract:  The chemical functionalization of graphene enables control over electronic
properties and sensor recognition sites. However, its study is confounded by an
unusually strong influence of the underlying substrate. In this paper, we show
a stark difference in the rate of electron transfer chemistry with aryl
diazonium salts on monolayer graphene supported on a broad range of substrates.
Reactions proceed rapidly when graphene is on SiO_2 and Al_2O_3 (sapphire), but
negligibly on alkyl-terminated and hexagonal boron nitride (hBN) surfaces. The
effect is contrary to expectations based on doping levels and can instead be
described using a reactivity model accounting for substrate-induced
electron-hole puddles in graphene. Raman spectroscopic mapping is used to
characterize the effect of the substrates on graphene. Reactivity imprint
lithography (RIL) is demonstrated as a technique for spatially patterning
chemical groups on graphene by patterning the underlying substrate, and is
applied to the covalent tethering of proteins on graphene."
Jing Kong,Kong_Jing,arXiv:1112.4831,https://arxiv.org/abs/1112.4831,"Abstract:  In this letter, we analyze the carrier transit delay in graphene field-effect
transistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire
substrate. For a device with a gate length of 210 nm, a current gain cut-off
frequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding.
The extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex
and Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd
allows the intrinsic ({\tau}_int), extrinsic ({\tau}_ext) and parasitic delays
({\tau}_par) to be obtained. In addition, the extraction of the intrinsic delay
provides a new way to directly estimate carrier velocity from the experimental
data while the breakdown of the total delay into intrinsic, extrinsic, and
parasitic components can offer valuable information for optimizing RF GFETs
structures."
Jing Kong,Kong_Jing,arXiv:1110.4356,https://arxiv.org/abs/1110.4356,"Abstract:  We present experimental measurements of the electronic contribution to the
Raman spectra of individual metallic single-walled carbon nanotubes (MSWNTs).
Photoexcited carriers are inelastically scattered by a continuum of low-energy
electron-hole pairs created across the graphenelike linear electronic subbands
of the MSWNTs. The optical resonances in MSWNTs give rise to well-defined
electronic Raman peaks. This resonant electronic Raman scattering is a unique
feature of the electronic structure of these one-dimensional quasimetals."
Jing Kong,Kong_Jing,arXiv:1101.4985,https://arxiv.org/abs/1101.4985,"Abstract:  We report strong THz-induced transparency in CVD-grown graphene where 92%-96%
of the peak-field is transmitted compared to 74% at lower field strength.
Time-resolved THz-pump/THz-probe studies reveal that the absorption recovers in
2-3 ps. The induced transparency is believed to arise from nonlinear pumping of
carriers in graphene which suppresses the mobility and consequently the
conductivity in a spectral region where the light-matter interaction is
particularly strong."
Jing Kong,Kong_Jing,arXiv:0906.4037,https://arxiv.org/abs/0906.4037,"Abstract:  We demonstrate anisotropic etching of single-layer graphene by
thermally-activated nickel nanoparticles. Using this technique, we obtain
sub-10nm nanoribbons and other graphene nanostructures with edges aligned along
a single crystallographic direction. We observe a new catalytic channeling
behavior, whereby etched cuts do not intersect, resulting in continuously
connected geometries. Raman spectroscopy and electronic measurements show that
the quality of the graphene is resilient under the etching conditions,
indicating that this method may serve as a powerful technique to produce
graphene nanocircuits with well-defined crystallographic edges."
Jing Kong,Kong_Jing,arXiv:0906.2236,https://arxiv.org/abs/0906.2236,"Abstract:  We report graphene films composed mostly of one or two layers of graphene
grown by controlled carbon precipitation on the surface of polycrystalline Ni
thin films during atmospheric chemical vapor deposition(CVD). Controlling both
the methane concentration during CVD and the substrate cooling rate during
graphene growth can significantly improve the thickness uniformity. As a
result, one- or two- layer graphene regions occupy up to 87% of the film area.
Single layer coverage accounts for 5-11% of the overall film. These regions
expand across multiple grain boundaries of the underlying polycrystalline Ni
film. The number density of sites with multilayer graphene/graphite (>2 layers)
is reduced as the cooling rate decreases. These films can also be transferred
to other substrates and their sizes are only limited by the sizes of the Ni
film and the CVD chamber. Here, we demonstrate the formation of films as large
as 1 in2. These findings represent an important step towards the fabrication of
large-scale high-quality graphene samples."
Jing Kong,Kong_Jing,arXiv:cond-mat/0610196,https://arxiv.org/abs/cond-mat/0610196,"Abstract:  We study the electrical transport properties of well-contacted ballistic
single-walled carbon nanotubes in a three-terminal configuration at low
temperatures. We observe signatures of strong electron-electron interactions:
the conductance exhibits bias-voltage-dependent amplitudes of quantum
interference oscillation, and both the current noise and Fano factor manifest
bias-voltage-dependent power-law scalings. We analyze our data within the
Tomonaga-Luttinger liquid model using the non-equilibrium Keldysh formalism and
find qualitative and quantitative agreement between experiment and theory."
Jing Kong,Kong_Jing,arXiv:cond-mat/0504059,https://arxiv.org/abs/cond-mat/0504059,"Abstract:  Progress in the fabrication of nanometer-scale electronic devices is opening
new opportunities to uncover the deepest aspects of the Kondo effect, one of
the paradigmatic phenomena in the physics of strongly correlated electrons.
Artificial single-impurity Kondo systems have been realized in various
nanostructures, including semiconductor quantum dots, carbon nanotubes and
individual molecules. The Kondo effect is usually regarded as a spin-related
phenomenon, namely the coherent exchange of the spin between a localized state
and a Fermi sea of electrons. In principle, however, the role of the spin could
be replaced by other degrees of freedom, such as an orbital quantum number.
Here we demonstrate that the unique electronic structure of carbon nanotubes
enables the observation of a purely orbital Kondo effect. We use a magnetic
field to tune spin-polarized states into orbital degeneracy and conclude that
the orbital quantum number is conserved during tunneling. When orbital and spin
degeneracies are simultaneously present, we observe a strongly enhanced Kondo
effect, with a multiple splitting of the Kondo resonance at finite field and
predicted to obey a so-called SU(4) symmetry."
Jing Kong,Kong_Jing,arXiv:cond-mat/0309044,https://arxiv.org/abs/cond-mat/0309044,"Abstract:  Contacting metallic single-walled carbon nanotubes by palladium (Pd) affords
highly reproducible ohmic contacts and allows for detailed elucidation of
ballistic transport in metallic nanotubes. The Pd ohmic contacts are more
reliable than titanium (Ti) previously used for ballistic nanotube devices. In
contrast, Pt contacts appear to give non-ohmic contacts to metallic nanotubes.
For both ohmic and non-ohmic contacts, the length of the nanotube under the
metal contact area is electrically turned off. Transport occurs from metal to
nanotube at the edges of the contacts. Measurements with large numbers of Pd
contacted nanotube samples reveal that the mean free path for defect scattering
in SWNTs grown by chemical vapor deposition can be up to 4 microns. The mean
free paths for acoustic phonon scattering are on the order of 500 nm at room
temperature and >> 4 microns at low temperatures."
Butler Lampson,Lampson_Butler,arXiv:1209.3811,https://arxiv.org/abs/1209.3811,"Abstract:  In Programming by Example, a system attempts to infer a program from input
and output examples, generally by searching for a composition of certain base
functions. Performing a naive brute force search is infeasible for even mildly
involved tasks. We note that the examples themselves often present clues as to
which functions to compose, and how to rank the resulting programs. In text
processing, which is our domain of interest, clues arise from simple textual
features: for example, if parts of the input and output strings are
permutations of one another, this suggests that sorting may be useful. We
describe a system that learns the reliability of such clues, allowing for
faster search and a principled ranking over programs. Experiments on a
prototype of this system show that this learning scheme facilitates efficient
inference on a range of text processing tasks."
Jeffrey Lang,Lang_Jeffrey,arXiv:1603.02157,https://arxiv.org/abs/1603.02157,"Abstract:  Spatially-dense pressure measurements are needed on curved surfaces in marine
environments to provide marine vehicles with the detailed, real-time
measurements of the near-field flow necessary to improve performance through
flow control. To address this challenge, a waterproof and conformal pressure
sensor array comprising carbon black-doped-silicone closed-cell foam (CBPDMS
foam) was developed for use in marine applications. The response of the CBPDMS
foam sensor arrays was characterized using periodic hydrodynamic pressure
stimuli from vertical plunging, from which a piecewise polynomial calibration
was developed to describe the sensor response. Inspired by the distributed
pressure and velocity sensing capabilities of the fish lateral line, the CBPDMS
foam sensor arrays have significant advantages over existing commercial sensors
for distributed flow reconstruction and control. Experimental results have
shown the sensor arrays to have sensitivity on the order of 5 Pascal, dynamic
range of 50-500 Pascal; are contained in a waterproof and completely flexible
package, and have material cost less than $10 per sensor."
Charles Leiserson,Leiserson_Charles,arXiv:1812.00076,https://arxiv.org/abs/1812.00076,"Abstract:  Organized crime inflicts human suffering on a genocidal scale: the Mexican
drug cartels have murdered 150,000 people since 2006, upwards of 700,000 people
per year are ""exported"" in a human trafficking industry enslaving an estimated
40 million people. These nefarious industries rely on sophisticated money
laundering schemes to operate. Despite tremendous resources dedicated to
anti-money laundering (AML) only a tiny fraction of illicit activity is
prevented. The research community can help. In this brief paper, we map the
structural and behavioral dynamics driving the technical challenge. We review
AML methods, current and emergent. We provide a first look at scalable graph
convolutional neural networks for forensic analysis of financial data, which is
massive, dense, and dynamic. We report preliminary experimental results using a
large synthetic graph (1M nodes, 9M edges) generated by a data simulator we
created called AMLSim. We consider opportunities for high performance
efficiency, in terms of computation and memory, and we share results from a
simple graph compression experiment. Our results support our working hypothesis
that graph deep learning for AML bears great promise in the fight against
criminal financial activity."
Charles Leiserson,Leiserson_Charles,arXiv:1804.04773,https://arxiv.org/abs/1804.04773,"Abstract:  This paper investigates a variant of the work-stealing algorithm that we call
the localized work-stealing algorithm. The intuition behind this variant is
that because of locality, processors can benefit from working on their own
work. Consequently, when a processor is free, it makes a steal attempt to get
back its own work. We call this type of steal a steal-back. We show that the
expected running time of the algorithm is $T_1/P+O(T_\infty P)$, and that under
the ""even distribution of free agents assumption"", the expected running time of
the algorithm is $T_1/P+O(T_\infty\lg P)$. In addition, we obtain another
running-time bound based on ratios between the sizes of serial tasks in the
computation. If $M$ denotes the maximum ratio between the largest and the
smallest serial tasks of a processor after removing a total of $O(P)$ serial
tasks across all processors from consideration, then the expected running time
of the algorithm is $T_1/P+O(T_\infty M)$."
Charles Leiserson,Leiserson_Charles,arXiv:1706.03184,https://arxiv.org/abs/1706.03184,"Abstract:  Inspired by applications in parallel computing, we analyze the setting of
work stealing in multithreaded computations. We obtain tight upper bounds on
the number of steals when the computation can be modeled by rooted trees. In
particular, we show that if the computation with $n$ processors starts with one
processor having a complete $k$-ary tree of height $h$ (and the remaining $n-1$
processors having nothing), the maximum possible number of steals is
$\sum_{i=1}^n(k-1)^i\binom{h}{i}$."
Charles Leiserson,Leiserson_Charles,arXiv:1408.0393,https://arxiv.org/abs/1408.0393,"Abstract:  It is our view that the state of the art in constructing a large collection
of graph algorithms in terms of linear algebraic operations is mature enough to
support the emergence of a standard set of primitive building blocks. This
paper is a position paper defining the problem and announcing our intention to
launch an open effort to define this standard."
Jae Lim,Lim_Jae,arXiv:1804.07204,https://arxiv.org/abs/1804.07204,"Abstract:  Public safety networks avail to disseminate information during emergency
situations through its dedicated servers. Public safety networks accommodate
public safety communication (PSC) applications to track the location of its
utilizers and enable to sustain transmissions even in the crucial scenarios.
Despite that, if the traditional setups responsible for PSCs are unavailable,
it becomes prodigiously arduous to handle any of the safety applications, which
may cause havoc in the society. Dependence on a secondary network may assist to
solve such an issue. But, the secondary networks should be facilely deployable
and must not cause exorbitant overheads in terms of cost and operation. For
this, LoRaWAN can be considered as an ideal solution as it provides low power
and long-range communication. However, an excessive utilization of the
secondary network may result in high depletion of its own resources and can
lead to a complete shutdown of services, which is a quandary at hand. As a
solution, this paper proposes a novel network model via a combination of
LoRaWAN and traditional public safety networks, and uses a self-enforcing
agreement based game theory for allocating resources efficiently amongst the
available servers. The proposed approach adopts memory and energy constraints
as agreements, which are satisfied through Nash equilibrium. The numerical
results show that the proposed approach is capable of efficiently allocating
the resources with sufficiently high gains for resource conservation, network
sustainability, resource restorations and probability to continue at the
present conditions even in the complete absence of traditional Access Points
(APs) compared with a baseline scenario with no failure of nodes."
Jae Lim,Lim_Jae,arXiv:1705.02894,https://arxiv.org/abs/1705.02894,"Abstract:  Generative Adversarial Nets (GANs) represent an important milestone for
effective generative models, which has inspired numerous variants seemingly
different from each other. One of the main contributions of this paper is to
reveal a unified geometric structure in GAN and its variants. Specifically, we
show that the adversarial generative model training can be decomposed into
three geometric steps: separating hyperplane search, discriminator parameter
update away from the separating hyperplane, and the generator update along the
normal vector direction of the separating hyperplane. This geometric intuition
reveals the limitations of the existing approaches and leads us to propose a
new formulation called geometric GAN using SVM separating hyperplane that
maximizes the margin. Our theoretical analysis shows that the geometric GAN
converges to a Nash equilibrium between the discriminator and generator. In
addition, extensive numerical results show that the superior performance of
geometric GAN."
Jae Lim,Lim_Jae,arXiv:1108.4315,https://arxiv.org/abs/1108.4315,"Abstract:  Detecting the edges of objects within images is critical for quality image
processing. We present an edge-detecting technique that uses morphological
amoebas that adjust their shape based on variation in image contours. We
evaluate the method both quantitatively and qualitatively for edge detection of
images, and compare it to classic morphological methods. Our amoeba-based
edge-detection system performed better than the classic edge detectors."
Barbara Liskov,Liskov_Barbara,arXiv:1811.04967,https://arxiv.org/abs/1811.04967,"Abstract:  Optimistic concurrency control (OCC) can exploit the strengths of parallel
hardware to provide excellent performance for uncontended transactions, and is
popular in high-performance in-memory databases and transactional systems. But
at high contention levels, OCC is susceptible to frequent aborts, leading to
wasted work and degraded performance. Contention managers, mixed
optimistic/pessimistic concurrency control algorithms, and novel
optimistic-inspired concurrency control algorithms, such as TicToc, aim to
address this problem, but these mechanisms introduce sometimes-high overheads
of their own. We show that in real-world benchmarks, traditional OCC can
outperform these alternative mechanisms by simply adding fine-grained version
timestamps (using different timestamps for disjoint components of each record).
With fine-grained timestamps, OCC gets 1.14x TicToc's throughput in TPC-C at
128 cores (previous work reported TicToc having 1.8x higher throughput than OCC
at 80 hyperthreads). Our study shows that timestamp granularity has a greater
impact than previously thought on the performance of transaction processing
systems, and should not be overlooked in the push for faster concurrency
control schemes."
Luqiao Liu,Liu_Luqiao,arXiv:1806.01167,https://arxiv.org/abs/1806.01167,"Abstract:  Due to the difficulty in detecting and manipulating magnetic states of
antiferromagnetic materials, studying their switching dynamics using electrical
methods remains a challenging task. In this work, by employing heavy metal/rare
earth-transition metal alloy bilayers, we experimentally studied
current-induced domain wall dynamics in an antiferromagnetically coupled
system. We show that the current-induced domain wall mobility reaches a maximum
close to the angular momentum compensation. With experiment and modelling, we
further reveal the internal structures of domain walls and the underlying
mechanisms for their fast motion. We show that the chirality of the
ferrimagnetic domain walls remains the same across the compensation points,
suggesting that spin orientations of specific sublattices rather than net
magnetization determine Dzyaloshinskii-Moriya interaction in heavy
metal/ferrimagnet bilayers. The high current-induced domain wall mobility and
the robust domain wall chirality in compensated ferrimagnetic material opens
new opportunities for high-speed spintronic devices."
Luqiao Liu,Liu_Luqiao,arXiv:1703.07470,https://arxiv.org/abs/1703.07470,"Abstract:  Recent studies on the magneto-transport properties of topological insulators
(TI) have attracted great attention due to the rich spin-orbit physics and
promising applications in spintronic devices. Particularly the strongly
spin-moment coupled electronic states have been extensively pursued to realize
efficient spin-orbit torque (SOT) switching. However, so far current-induced
magnetic switching with TI has only been observed at cryogenic temperatures. It
remains a controversial issue whether the topologically protected electronic
states in TI could benefit spintronic applications at room temperature. In this
work, we report full SOT switching in a TI/ferromagnet bilayer heterostructure
with perpendicular magnetic anisotropy at room temperature. The low switching
current density provides a definitive proof on the high SOT efficiency from TI.
The effective spin Hall angle of TI is determined to be several times larger
than commonly used heavy metals. Our results demonstrate the robustness of TI
as an SOT switching material and provide a direct avenue towards applicable
TI-based spintronic devices."
Luqiao Liu,Liu_Luqiao,arXiv:1610.09200,https://arxiv.org/abs/1610.09200,"Abstract:  Despite the potential advantages of information storage in
antiferromagnetically coupled materials, it remains unclear whether one can
control the magnetic moment orientation efficiently because of the cancelled
magnetic moment. Here, we report spin-orbit torque induced magnetization
switching of ferrimagnetic Co1-xTbx films with perpendicular magnetic
anisotropy. Current induced switching is demonstrated in all of the studied
film compositions, including those near the magnetization compensation point.
The spin-orbit torque induced effective field is further quantified in the
domain wall motion regime. A divergent behavior that scales with the inverse of
magnetic moment is confirmed close to the compensation point, which is
consistent with angular momentum conservation. Moreover, we also quantify the
Dzyaloshinskii-Moriya interaction energy in the Ta/Co1-xTbx system and we find
that the energy density increases as a function of the Tb concentration. The
demonstrated spin-orbit torque switching, in combination with the fast magnetic
dynamics and minimal net magnetization of ferrimagnetic alloys, promises
spintronic devices that are faster and with higher density than traditional
ferromagnetic systems."
Luqiao Liu,Liu_Luqiao,arXiv:1410.7494,https://arxiv.org/abs/1410.7494,"Abstract:  We demonstrate that the charge-spin conversion efficiency of topological
insulators (TI) can be experimentally determined by injecting spin-polarized
tunneling electrons into a TI. Through a comparative study between bismuth
selenide and bismuth antimony telluride, we verified the
topological-surface-state origin of the observed giant spin signals. By
injecting energetic electrons into bismuth selenide, we further studied the
energy dependence of the effective spin polarization at the TI surface. The
experimentally verified large spin polarization, as well as our calculations,
provides new insights into optimizing TI materials for near room-temperature
spintronic applications."
Luqiao Liu,Liu_Luqiao,arXiv:1209.0962,https://arxiv.org/abs/1209.0962,"Abstract:  Two promising strategies for achieving efficient control of magnetization in
future magnetic memory and non-volatile spin logic devices are spin transfer
torque from spin polarized currents and voltage-controlled magnetic anisotropy
(VCMA). Spin transfer torque is in widespread development as the write
mechanism for next-generation magnetic memory, while VCMA offers the potential
of even better energy performance due to smaller Ohmic losses. Here we
introduce a 3-terminal magnetic tunnel junction (MTJ) device that combines both
of these mechanisms to achieve new functionality: gate-voltage-modulated spin
torque switching. This gating makes possible both more energy-efficient
switching and also improved architectures for memory and logic applications,
including a simple approach for making magnetic memories with a maximum-density
cross-point geometry that does not require a control transistor for every MTJ."
Luqiao Liu,Liu_Luqiao,arXiv:1209.0655,https://arxiv.org/abs/1209.0655,"Abstract:  We show that direct current in a tantalum microstrip can induce steady-state
magnetic oscillations in an adjacent nanomagnet through spin torque from the
spin Hall effect (SHE). The oscillations are detected electrically via a
magnetic tunnel junction (MTJ) contacting the nanomagnet. The oscillation
frequency can be controlled using the MTJ bias to tune the magnetic anisotropy.
In this 3-terminal device the SHE torque and the MTJ bias therefore provide
independent controls of the oscillation amplitude and frequency, enabling new
approaches for developing tunable spin torque nano-oscillators."
Luqiao Liu,Liu_Luqiao,arXiv:1208.1711,https://arxiv.org/abs/1208.1711,"Abstract:  We report a giant spin Hall effect (SHE) in {\beta}-W thin films. Using spin
torque induced ferromagnetic resonance with a {\beta}-W/CoFeB bilayer
microstrip we determine the spin Hall angle to be |\theta|=0.30\pm0.02, large
enough for an in-plane current to efficiently reverse the orientation of an
in-plane magnetized CoFeB free layer of a nanoscale magnetic tunnel junction
adjacent to a thin {\beta}-W layer. From switching data obtained with such
3-terminal devices we independently determine |\theta|=0.33\pm0.06. We also
report variation of the spin Hall switching efficiency with W layers of
different resistivities and hence of variable ({\alpha} and {\beta}) phase
composition."
Luqiao Liu,Liu_Luqiao,arXiv:1203.3266,https://arxiv.org/abs/1203.3266,"Abstract:  A pure spin current generated within a nonlocal spin valve can exert a spin
transfer torque on a nanomagnet. This nonlocal torque enables new design
schemes for magnetic memory devices that do not require the application of
large voltages across tunnel barriers that can suffer electrical breakdown.
Here we report a quantitative measurement of this nonlocal spin torque using
spin-torque-driven ferromagnetic resonance. Our measurement agrees well with
the prediction of an effective circuit model for spin transport. Based on this
model, we suggest strategies for optimizing the strength of nonlocal torque."
Luqiao Liu,Liu_Luqiao,arXiv:1203.2875,https://arxiv.org/abs/1203.2875,"Abstract:  We report a giant spin Hall effect (SHE) in {\beta}-Ta that generates spin
currents intense enough to induce efficient spin-transfer-torque switching of
ferromagnets, thereby providing a new approach for controlling magnetic devices
that can be superior to existing technologies. We quantify this SHE by three
independent methods and demonstrate spin-torque (ST) switching of both
out-of-plane and in-plane magnetized layers. We implement a three-terminal
device that utilizes current passing through a low impedance Ta-ferromagnet
bilayer to effect switching of a nanomagnet, with a higher-impedance magnetic
tunnel junction for read-out. The efficiency and reliability of this device,
together with its simplicity of fabrication, suggest that this three-terminal
SHE-ST design can eliminate the main obstacles currently impeding the
development of magnetic memory and non-volatile spin logic technologies."
Luqiao Liu,Liu_Luqiao,arXiv:1111.3702,https://arxiv.org/abs/1111.3702,"Abstract:  Several different experimental techniques have been used in efforts to
measure the spin Hall conductivity and the spin Hall angle in Pt samples at
room temperature, with results that disagree by more than a factor of 20, with
spin Hall conductivities from 2.4 x 10^4 to 5.1 x 10^5 [hbar/(2e)] (Ohm-m)^-1
and spin Hall angles from 0.0037 to 0.08. We review this work, and analyze
possible reasons for the discrepancies. We explain that the smallest values for
the spin Hall angle that have been reported, based on measurements of lateral
permalloy/copper/platinum devices, are incorrect because the original analyses
did not properly take into account that copper layers in these devices will
shunt charge current flowing through adjacent platinum wires, thereby greatly
reducing the size of the spin-Hall-related signals. We suggest that differences
between the results for the spin Hall angle found by other experimental
techniques are primarily a consequence of different assumptions about the value
of the spin diffusion length in Pt. We present a new measurement of the spin
diffusion length in Pt within sputtered Pt/permalloy bilayer thin films at room
temperature, finding 1.4 \pm 0.3 nm, a much smaller value than has generally
been assumed previously. With this value for the spin diffusion length, the
previously-discordant results can be brought into much better agreement, with
the result that the spin Hall conductivities are (1.4 - 3.4) x 10^5 [hbar/(2e)]
(Ohm-m)^-1 and the spin Hall angles are greater than 0.05. These values are
sufficiently large that the spin Hall effect in Pt can be used to generate spin
currents and spin transfer torques strong enough for efficient manipulation of
magnetic moments in adjacent ferromagnetic layers."
Luqiao Liu,Liu_Luqiao,arXiv:1110.6846,https://arxiv.org/abs/1110.6846,"Abstract:  The spin Hall effect (SHE) generates spin currents within nonmagnetic
materials. Previously, studies of the SHE have been motivated primarily to
understand its fundamental origin and magnitude. Here we demonstrate, using
measurement and modeling, that in a Pt/Co bilayer with perpendicular magnetic
anisotropy the SHE can produce a spin transfer torque that is strong enough to
efficiently rotate and reversibly switch the Co magnetization, thereby
providing a new strategy both to understand the SHE and to manipulate magnets.
We suggest that the SHE torque can have a similarly strong influence on
current-driven magnetic domain wall motion in Pt/ferromagnet multilayers. We
estimate that in optimized devices the SHE torque can switch magnetic moments
using currents comparable to those in magnetic tunnel junctions operated by
conventional spin-torque switching, meaning that the SHE can enable magnetic
memory and logic devices with similar performance but simpler architecture than
the current state of the art."
Luqiao Liu,Liu_Luqiao,arXiv:1011.2788,https://arxiv.org/abs/1011.2788,"Abstract:  We demonstrate that the spin Hall effect in a thin film with strong
spin-orbit scattering can excite magnetic precession in an adjacent
ferromagnetic film. The flow of alternating current through a Pt/NiFe bilayer
generates an oscillating transverse spin current in the Pt, and the resultant
transfer of spin angular momentum to the NiFe induces ferromagnetic resonance
(FMR) dynamics. The Oersted field from the current also generates an FMR signal
but with a different symmetry. The ratio of these two signals allows a
quantitative determination of the spin current and the spin Hall angle."
Nancy Lynch,Lynch_Nancy,arXiv:1811.10577,https://arxiv.org/abs/1811.10577,"Abstract:  In the paper titled ""The SNOW Theorem"" the authors proposed four desirable
properties in transaction processing systems for achieving low-latency of READ
transactions, with asynchronous and reliable communications, and referred to
them collectively as the SNOW properties: The underlying properties, in the
context of an execution, are (i) strict serializability (S) property where READ
and WRITE transactions seem to occur atomically, (ii) non-blocking (N) property
implies for read operations, (iii) one version and one round (O) property,
where reads operations completes in one-round of client-server communication
and only one version of the object value is sent, and (iv) concurrent WRITE
transactions (W) property, which means WRITE transactions can occur. Then they
argued that it is impossible to implement all the four properties, in the same
system, even with at least three clients. They referred to their result as the
SNOW theorem, and they posed the two-client setting as an open question. Here
we revisit the results of the work and present several new results. In our
first result, we resolve the two-client scenario: We prove that even with two
clients, without client-to-client messaging, it is impossible to design an
transaction processing system which satisfies the SNOW properties. Second, we
provide a rigorous proof of the SNOW theorem for systems with at least three
clients, i.e., we show that it is impossible to implement a transaction
processing system, consisting of at least three clients, even with
client-to-client messaging, that satisfies the SNOW properties. Next we derive
a useful property for executions of algorithms that implement objects of data
types considered in our work that helps us show the S property of algorithms
presented in the paper. Then we present two new algorithms that satisfies S, N,
W and wreaked versions O property."
Nancy Lynch,Lynch_Nancy,arXiv:1811.03968,https://arxiv.org/abs/1811.03968,"Abstract:  We consider multi-armed bandit problems in social groups wherein each
individual has bounded memory and shares the common goal of learning the best
arm/option. We say an individual learns the best option if eventually (as $t\to
\infty$) it pulls only the arm with the highest expected reward. While this
goal is provably impossible for an isolated individual due to bounded memory,
we show that, in social groups, this goal can be achieved easily with the aid
of social persuasion (i.e., communication) as long as the communication
networks/graphs satisfy some mild conditions. To deal with the interplay
between the randomness in the rewards and in the social interaction, we employ
the {\em mean-field approximation} method. Considering the possibility that the
individuals in the networks may not be exchangeable when the communication
networks are not cliques, we go beyond the classic mean-field techniques and
apply a refined version of mean-field approximation:
(1) Using coupling we show that, if the communication graph is connected and
is either regular or has doubly-stochastic degree-weighted adjacency matrix,
with probability $\to 1$ as the social group size $N \to \infty$, every
individual in the social group learns the best option.
(2) If the minimum degree of the graph diverges as $N \to \infty$, over an
arbitrary but given finite time horizon, the sample paths describing the
opinion evolutions of the individuals are asymptotically independent. In
addition, the proportions of the population with different opinions converge to
the unique solution of a system of ODEs. In the solution of the obtained ODEs,
the proportion of the population holding the correct opinion converges to $1$
exponentially fast in time.
Notably, our results hold even if the communication graphs are highly sparse."
Nancy Lynch,Lynch_Nancy,arXiv:1808.03884,https://arxiv.org/abs/1808.03884,"Abstract:  This paper is part of a project on developing an algorithmic theory of brain
networks, based on stochastic Spiking Neural Network (SNN) models. Inspired by
tasks that seem to be solved in actual brains, we are defining abstract
problems to be solved by these networks. In our work so far, we have developed
models and algorithms for the Winner-Take-All problem from computational
neuroscience [LMP17a,Mus18], and problems of similarity detection and neural
coding [LMP17b]. We plan to consider many other problems and networks,
including both static networks and networks that learn.
This paper is about basic theory for the stochastic SNN model. In particular,
we define a simple version of the model. This version assumes that the neurons'
only state is a Boolean, indicating whether the neuron is firing or not. In
later work, we plan to develop variants of the model with more elaborate state.
We also define an external behavior notion for SNNs, which can be used for
stating requirements to be satisfied by the networks.
We then define a composition operator for SNNs. We prove that our external
behavior notion is ""compositional"", in the sense that the external behavior of
a composed network depends only on the external behaviors of the component
networks. We also define a hiding operator that reclassifies some output
behavior of an SNN as internal. We give basic results for hiding.
Finally, we give a formal definition of a problem to be solved by an SNN, and
give basic results showing how composition and hiding of networks affect the
problems that they solve. We illustrate our definitions with three examples:
building a circuit out of gates, building an ""Attention"" network out of a
""Winner-Take-All"" network and a ""Filter"" network, and a toy example involving
combining two networks in a cyclic fashion."
Nancy Lynch,Lynch_Nancy,arXiv:1805.03727,https://arxiv.org/abs/1805.03727,"Abstract:  Atomicity or strong consistency is one of the fundamental, most intuitive,
and hardest to provide primitives in distributed shared memory emulations. To
ensure survivability, scalability, and availability of a storage service in the
presence of failures, traditional approaches for atomic memory emulation, in
message passing environments, replicate the objects across multiple servers.
Compared to replication based algorithms, erasure code-based atomic memory
algorithms has much lower storage and communication costs, but usually, they
are harder to design. The difficulty of designing atomic memory algorithms
further grows, when the set of servers may be changed to ensure survivability
of the service over software and hardware upgrades, while avoiding service
interruptions. Atomic memory algorithms for performing server reconfiguration,
in the replicated systems, are very few, complex, and are still part of an
active area of research; reconfigurations of erasure-code based algorithms are
non-existent.
In this work, we present ARES, an algorithmic framework that allows
reconfiguration of the underlying servers, and is particularly suitable for
erasure-code based algorithms emulating atomic objects. ARES introduces new
configurations while keeping the service available. To use with ARES we also
propose a new, and to our knowledge, the first two-round erasure code based
algorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects
in asynchronous, message-passing environments, with near-optimal communication
and storage costs. Our algorithms can tolerate crash failures of any client and
some fraction of servers, and yet, guarantee safety and liveness property.
Moreover, by bringing together the advantages of ARES and TREAS, we propose an
optimized algorithm where new configurations can be installed without the
objects values passing through the reconfiguration clients."
Nancy Lynch,Lynch_Nancy,arXiv:1805.03691,https://arxiv.org/abs/1805.03691,"Abstract:  We study the problem of distributed task allocation inspired by the behavior
of social insects, which perform task allocation in a setting of limited
capabilities and noisy environment feedback. We assume that each task has a
demand that should be satisfied but not exceeded, i.e., there is an optimal
number of ants that should be working on this task at a given time. The goal is
to assign a near-optimal number of workers to each task in a distributed manner
and without explicit access to the values of the demands nor the number of ants
working on the task.
We seek to answer the question of how the quality of task allocation depends
on the accuracy of assessing whether too many (overload) or not enough (lack)
ants are currently working on a given task. Concretely, we address the open
question of solving task allocation in the model where each ant receives
feedback that depends on the deficit defined as the (possibly negative)
difference between the optimal demand and the current number of workers in the
task. The feedback is modeled as a random variable that takes value lack or
overload with probability given by a sigmoid of the deficit. Each ants receives
the feedback independently, but the higher the overload or lack of workers for
a task, the more likely it is that all the ants will receive the same, correct
feedback from this task; the closer the deficit is to zero, the less reliable
the feedback becomes. We measure the performance of task allocation algorithms
using the notion of regret, defined as the absolute value of the deficit summed
over all tasks and summed over time.
We propose a simple, constant-memory, self-stabilizing, distributed algorithm
that quickly converges from any initial distribution to a near-optimal
assignment. We also show that our algorithm works not only under stochastic
noise but also in an adversarial noise setting."
Nancy Lynch,Lynch_Nancy,arXiv:1803.02216,https://arxiv.org/abs/1803.02216,"Abstract:  In this paper, we study local and global broadcast in the dual graph model,
which describes communication in a radio network with both reliable and
unreliable links. Existing work proved that efficient solutions to these
problems are impossible in the dual graph model under standard assumptions. In
real networks, however, simple back-off strategies tend to perform well for
solving these basic communication tasks. We address this apparent paradox by
introducing a new set of constraints to the dual graph model that better
generalize the slow/fast fading behavior common in real networks. We prove that
in the context of these new constraints, simple back-off strategies now provide
efficient solutions to local and global broadcast in the dual graph model. We
also precisely characterize how this efficiency degrades as the new constraints
are reduced down to non-existent, and prove new lower bounds that establish
this degradation as near optimal for a large class of natural algorithms. We
conclude with a preliminary investigation of the performance of these
strategies when we include additional generality to the model. These results
provide theoretical foundations for the practical observation that simple
back-off algorithms tend to work well even amid the complicated link dynamics
of real radio networks."
Nancy Lynch,Lynch_Nancy,arXiv:1802.08159,https://arxiv.org/abs/1802.08159,"Abstract:  We consider multi-armed bandit problems in social groups wherein each
individual has bounded memory and shares the common goal of learning the best
arm/option. We say an individual learns the best option if eventually (as $t
\to \infty$) it pulls only the arm with the highest average reward. While this
goal is provably impossible for an isolated individual, we show that, in social
groups, this goal can be achieved easily with the aid of social persuasion,
i.e., communication. Specifically, we study the learning dynamics wherein an
individual sequentially decides on which arm to pull next based on not only its
private reward feedback but also the suggestions provided by randomly chosen
peers. Our learning dynamics are hard to analyze via explicit probabilistic
calculations due to the stochastic dependency induced by social interaction.
Instead, we employ the mean-field approximation method from statistical physics
and we show:
(1) With probability $\to 1$ as the social group size $N \to \infty $, every
individual in the social group learns the best option.
(2) Over an arbitrary finite time horizon $[0, T]$, with high probability (in
$N$), the fraction of individuals that prefer the best option grows to 1
exponentially fast as $t$ increases ($t\in [0, T]$).
A major innovation of our mean-filed analysis is a simple yet powerful
technique to deal with absorbing states in the interchange of limits $N \to
\infty$ and $t \to \infty $. The mean-field approximation method allows us to
approximate the probabilistic sample paths of our learning dynamics by a
deterministic and smooth trajectory that corresponds to the unique solution of
a well-behaved system of ordinary differential equations (ODEs). Such an
approximation is desired because the analysis of a system of ODEs is relatively
easier than that of the original stochastic system."
Nancy Lynch,Lynch_Nancy,arXiv:1706.01382,https://arxiv.org/abs/1706.01382,"Abstract:  We study distributed algorithms implemented in a simplified biologically
inspired model for stochastic spiking neural networks. We focus on tradeoffs
between computation time and network complexity, along with the role of
randomness in efficient neural computation.
It is widely accepted that neural computation is inherently stochastic. In
recent work, we explored how this stochasticity could be leveraged to solve the
`winner-take-all' leader election task. Here, we focus on using randomness in
neural algorithms for similarity testing and compression. In the most basic
setting, given two $n$-length patterns of firing neurons, we wish to
distinguish if the patterns are equal or $\epsilon$-far from equal.
Randomization allows us to solve this task with a very compact network, using
$O \left (\frac{\sqrt{n}\log n}{\epsilon}\right)$ auxiliary neurons, which is
sublinear in the input size. At the heart of our solution is the design of a
$t$-round neural random access memory, or indexing network, which we call a
neuro-RAM. This module can be implemented with $O(n/t)$ auxiliary neurons and
is useful in many applications beyond similarity testing.
Using a VC dimension-based argument, we show that the tradeoff between
runtime and network size in our neuro-RAM is nearly optimal. Our result has
several implications -- since our neuro-RAM can be implemented with
deterministic threshold gates, it shows that, in contrast to similarity
testing, randomness does not provide significant computational advantages for
this problem. It also establishes a separation between feedforward networks
whose gates spike with sigmoidal probability functions, and well-studied
deterministic sigmoidal networks, whose gates output real number sigmoidal
values, and which can implement a neuro-RAM much more efficiently."
Nancy Lynch,Lynch_Nancy,arXiv:1704.07133,https://arxiv.org/abs/1704.07133,"Abstract:  We adapt a recent algorithm by Ghaffari [SODA'16] for computing a Maximal
Independent Set in the LOCAL model, so that it works in the significantly
weaker BEEP model. For networks with maximum degree $\Delta$, our algorithm
terminates locally within time $O((\log \Delta + \log (1/\epsilon)) \cdot
\log(1/\epsilon))$, with probability at least $1 - \epsilon$. The key idea of
the modification is to replace explicit messages about transmission
probabilities with estimates based on the number of received messages.
After the successful introduction (and implicit use) of local analysis, e.g.,
by Barenboim et al. [JACM'16], Chung et al. [PODC'14], Ghaffari [SODA'16], and
Halldorsson et al. [PODC'15], we study this concept in the BEEP model for the
first time.
By doing so, we improve over local bounds that are implicitly derived from
previous work (that uses traditional global analysis) on computing a Maximal
Independent Set in the \beep model for a large range of values of the parameter
$\Delta$. At the same time, we show that our algorithm in the \beep model only
needs to pay a $\log(1/\epsilon)$ factor in the runtime compared to the best
known MIS algorithm in the much more powerful \local model. We demonstrate that
this overhead is negligible, as communication via beeps can be implemented
using significantly less resources than communication in the LOCAL model. In
particular, when looking at implementing these models, one round of the \local
model needs at least $O(\Delta)$ time units, while one round in the BEEP model
needs $O(\log\Delta)$ time units, an improvement that diminishes the loss of a
$\log(1/\epsilon)$ factor in most settings."
Nancy Lynch,Lynch_Nancy,arXiv:1703.01286,https://arxiv.org/abs/1703.01286,"Abstract:  Motivated by emerging applications to the edge computing paradigm, we
introduce a two-layer erasure-coded fault-tolerant distributed storage system
offering atomic access for read and write operations. In edge computing,
clients interact with an edge-layer of servers that is geographically near; the
edge-layer in turn interacts with a back-end layer of servers. The edge-layer
provides low latency access and temporary storage for client operations, and
uses the back-end layer for persistent storage. Our algorithm, termed Layered
Data Storage (LDS) algorithm, offers several features suitable for
edge-computing systems, works under asynchronous message-passing environments,
supports multiple readers and writers, and can tolerate $f_1 < n_1/2$ and $f_2
< n_2/3$ crash failures in the two layers having $n_1$ and $n_2$ servers,
respectively. We use a class of erasure codes known as regenerating codes for
storage of data in the back-end layer. The choice of regenerating codes,
instead of popular choices like Reed-Solomon codes, not only optimizes the cost
of back-end storage, but also helps in optimizing communication cost of read
operations, when the value needs to be recreated all the way from the back-end.
The two-layer architecture permits a modular implementation of atomicity and
erasure-code protocols; the implementation of erasure-codes is mostly limited
to interaction between the two layers. We prove liveness and atomicity of LDS,
and also compute performance costs associated with read and write operations.
Further, in a multi-object system running $N$ independent instances of LDS,
where only a small fraction of the objects undergo concurrent accesses at any
point during the execution, the overall storage cost is dominated by that of
persistent storage in the back-end layer, and is given by $\Theta(N)$."
Nancy Lynch,Lynch_Nancy,arXiv:1610.02084,https://arxiv.org/abs/1610.02084,"Abstract:  We initiate a line of investigation into biological neural networks from an
algorithmic perspective. We develop a simplified but biologically plausible
model for distributed computation in stochastic spiking neural networks and
study tradeoffs between computation time and network complexity in this model.
Our aim is to abstract real neural networks in a way that, while not capturing
all interesting features, preserves high-level behavior and allows us to make
biologically relevant conclusions.
In this paper, we focus on the important `winner-take-all' (WTA) problem,
which is analogous to a neural leader election unit: a network consisting of
$n$ input neurons and $n$ corresponding output neurons must converge to a state
in which a single output corresponding to a firing input (the `winner') fires,
while all other outputs remain silent. Neural circuits for WTA rely on
inhibitory neurons, which suppress the activity of competing outputs and drive
the network towards a converged state with a single firing winner. We attempt
to understand how the number of inhibitors used affects network convergence
time.
We show that it is possible to significantly outperform naive WTA
constructions through a more refined use of inhibition, solving the problem in
$O(\theta)$ rounds in expectation with just $O(\log^{1/\theta} n)$ inhibitors
for any $\theta$. An alternative construction gives convergence in
$O(\log^{1/\theta} n)$ rounds with $O(\theta)$ inhibitors. We compliment these
upper bounds with our main technical contribution, a nearly matching lower
bound for networks using $\ge \log\log n$ inhibitors. Our lower bound uses
familiar indistinguishability and locality arguments from distributed computing
theory. It lets us derive a number of interesting conclusions about the
structure of any network solving WTA with good probability, and the use of
randomness and inhibition within such a network."
Nancy Lynch,Lynch_Nancy,arXiv:1605.06844,https://arxiv.org/abs/1605.06844,"Abstract:  The focus of this paper is to understand storage costs of emulating an atomic
shared memory over an asynchronous, distributed message passing system.
Previous literature has developed several shared memory emulation algorithms
based on replication and erasure coding techniques. In this paper, we present
information-theoretic lower bounds on the storage costs incurred by shared
memory emulation algorithms. Our storage cost lower bounds are universally
applicable, that is, we make no assumption on the structure of the algorithm or
the method of encoding the data.
We consider an arbitrary algorithm $A$ that implements an atomic multi-writer
single-reader (MWSR) shared memory variable whose values come from a finite set
$\mathcal{V}$ over a system of $N$ servers connected by point-to-point
asynchronous links. We require that in every fair execution of algorithm $A$
where the number of server failures is smaller than a parameter $f$, every
operation invoked at a non-failing client terminates. We define the storage
cost of a server in algorithm $A$ as the logarithm (to base 2) of number of
states it can take on; the total-storage cost of algorithm $A$ is the sum of
the storage cost of all servers.
Our results are as follows. (i) We show that if algorithm $A$ does not use
server gossip, then the total storage cost is lower bounded by $2
\frac{N}{N-f+1}\log_2|\mathcal{V}|-o(\log_2|\mathcal{V}|)$. (ii) The total
storage cost is at least $2 \frac{N}{N-f+2}
\log_{2}|\mathcal{V}|-o(\log_{2}|\mathcal{V}|)$ even if the algorithm uses
server gossip. (iii) We consider algorithms where the write protocol sends
information about the value in at most one phase. We show that the total
storage cost is at least $\nu^* \frac{N}{N-f+\nu^*-1} \log_2( |\mathcal{V}|)-
o(\log_2(|\mathcal{V}|),$ where $\nu^*$ is the minimum of $f+1$ and the number
of active write operations of an execution."
Nancy Lynch,Lynch_Nancy,arXiv:1605.05717,https://arxiv.org/abs/1605.05717,"Abstract:  Erasure codes offer an efficient way to decrease storage and communication
costs while implementing atomic memory service in asynchronous distributed
storage systems. In this paper, we provide erasure-code-based algorithms having
the additional ability to perform background repair of crashed nodes. A repair
operation of a node in the crashed state is triggered externally, and is
carried out by the concerned node via message exchanges with other active nodes
in the system. Upon completion of repair, the node re-enters active state, and
resumes participation in ongoing and future read, write, and repair operations.
To guarantee liveness and atomicity simultaneously, existing works assume
either the presence of nodes with stable storage, or presence of nodes that
never crash during the execution. We demand neither of these; instead we
consider a natural, yet practical network stability condition $N1$ that only
restricts the number of nodes in the crashed/repair state during broadcast of
any message.
We present an erasure-code based algorithm $RADON_C$ that is always live, and
guarantees atomicity as long as condition $N1$ holds. In situations when the
number of concurrent writes is limited, $RADON_C$ has significantly improved
storage and communication cost over a replication-based algorithm $RADON_R$,
which also works under $N1$. We further show how a slightly stronger network
stability condition $N2$ can be used to construct algorithms that never violate
atomicity. The guarantee of atomicity comes at the expense of having an
additional phase during the read and write operations."
Nancy Lynch,Lynch_Nancy,arXiv:1605.01748,https://arxiv.org/abs/1605.01748,"Abstract:  Erasure codes are increasingly being studied in the context of implementing
atomic memory objects in large scale asynchronous distributed storage systems.
When compared with the traditional replication based schemes, erasure codes
have the potential of significantly lowering storage and communication costs
while simultaneously guaranteeing the desired resiliency levels. In this work,
we propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing
atomic memory objects in the multi-writer multi-reader setting. SODA uses
Maximum Distance Separable (MDS) codes, and is specifically designed to
optimize the total storage cost for a given fault-tolerance requirement. For
tolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$
MDS code with $k=n-f$, and incurs a total storage cost of $\frac{n}{n-f}$. SODA
is designed under the assumption of reliable point-to-point communication
channels. The communication cost of a write and a read operation are
respectively given by $O(f^2)$ and $\frac{n}{n-f}(\delta_w+1)$, where
$\delta_w$ denotes the number of writes that are concurrent with the particular
read. In comparison with the recent CASGC algorithm, which also uses MDS codes,
SODA offers lower storage cost while pays more on the communication cost.
We also present a modification of SODA, called SODA$_{\text{err}}$, to handle
the case where some of the servers can return erroneous coded elements during a
read operation. Specifically, in order to tolerate $f$ server failures and $e$
error-prone coded elements, the SODA$_{\text{err}}$ algorithm uses an $[n, k]$
MDS code such that $k=n-2e-f$. SODA$_{\text{err}}$ also guarantees liveness and
atomicity, while maintaining an optimized total storage cost of
$\frac{n}{n-f-2e}$."
Nancy Lynch,Lynch_Nancy,arXiv:1604.06030,https://arxiv.org/abs/1604.06030,"Abstract:  We present dynamic I/O automata (DIOA), a compositional model of dynamic
systems. In DIOA, automata can be created and destroyed dynamically, as
computation proceeds, and an automaton can dynamically change its signature,
i.e., the set of actions in which it can participate.
DIOA features operators for parallel composition, action hiding, action
renaming, a notion of automaton creation, and a notion of behavioral subtyping
by means of trace inclusion. DIOA can model mobility, using signature
modification, and is hierarchical: a dynamically changing system of interacting
automata is itself modeled as a single automaton.
We also show that parallel composition, action hiding, action renaming, and
(subject to some technical conditions) automaton creation are all monotonic
with respect to trace inclusion: if one component is replaced by another whose
traces are a subset of the former, then the set of traces of the system as a
whole can only be reduced."
Nancy Lynch,Lynch_Nancy,arXiv:1603.02981,https://arxiv.org/abs/1603.02981,"Abstract:  Many ant species employ distributed population density estimation in
applications ranging from quorum sensing [Pra05], to task allocation [Gor99],
to appraisal of enemy colony strength [Ada90]. It has been shown that ants
estimate density by tracking encounter rates -- the higher the population
density, the more often the ants bump into each other [Pra05,GPT93].
We study distributed density estimation from a theoretical perspective. We
prove that a group of anonymous agents randomly walking on a grid are able to
estimate their density within a small multiplicative error in few steps by
measuring their rates of encounter with other agents. Despite dependencies
inherent in the fact that nearby agents may collide repeatedly (and, worse,
cannot recognize when this happens), our bound nearly matches what would be
required to estimate density by independently sampling grid locations.
From a biological perspective, our work helps shed light on how ants and
other social insects can obtain relatively accurate density estimates via
encounter rates. From a technical perspective, our analysis provides new tools
for understanding complex dependencies in the collision probabilities of
multiple random walks. We bound the strength of these dependencies using
$local\ mixing\ properties$ of the underlying graph. Our results extend beyond
the grid to more general graphs and we discuss applications to size estimation
for social networks and density estimation for robot swarms."
Nancy Lynch,Lynch_Nancy,arXiv:1508.03660,https://arxiv.org/abs/1508.03660,"Abstract:  This paper studies the theory of the additive wireless network model, in
which the received signal is abstracted as an addition of the transmitted
signals. Our central observation is that the crucial challenge for computing in
this model is not high contention, as assumed previously, but rather
guaranteeing a bounded amount of \emph{information} in each neighborhood per
round, a property that we show is achievable using a new random coding
technique.
Technically, we provide efficient algorithms for fundamental distributed
tasks in additive networks, such as solving various symmetry breaking problems,
approximating network parameters, and solving an \emph{asymmetry revealing}
problem such as computing a maximal input.
The key method used is a novel random coding technique that allows a node to
successfully decode the received information, as long as it does not contain
too many distinct values. We then design our algorithms to produce a limited
amount of information in each neighborhood in order to leverage our enriched
toolbox for computing in additive networks."
Nancy Lynch,Lynch_Nancy,arXiv:1505.04514,https://arxiv.org/abs/1505.04514,"Abstract:  We present the first algorithm that implements an abstract MAC (absMAC) layer
in the Signal-to-Interference-plus-Noise-Ratio (SINR) wireless network model.
We first prove that efficient SINR implementations are not possible for the
standard absMAC specification. We modify that specification to an ""approximate""
version that better suits the SINR model. We give an efficient algorithm to
implement the modified specification, and use it to derive efficient algorithms
for higher-level problems of global broadcast and consensus.
In particular, we show that the absMAC progress property has no efficient
implementation in terms of the SINR strong connectivity graph $G_{1-\epsilon}$,
which contains edges between nodes of distance at most $(1-\epsilon)$ times the
transmission range, where $\epsilon>0$ is a small constant that can be chosen
by the user. This progress property bounds the time until a node is guaranteed
to receive some message when at least one of its neighbors is transmitting.
To overcome this limitation, we introduce the slightly weaker notion of
approximate progress into the absMAC specification. We provide a fast
implementation of the modified specification, based on decomposing a known
algorithm into local and global parts. We analyze our algorithm in terms of
local parameters such as node degrees, rather than global parameters such as
the overall number of nodes. A key contribution is our demonstration that such
a local analysis is possible even in the presence of global interference.
Our absMAC algorithm leads to several new, efficient algorithms for solving
higher-level problems in the SINR model. Namely, by combining our algorithm
with known high-level algorithms, we obtain an improved algorithm for global
single-message broadcast in the SINR model, and the first efficient algorithm
for multi-message broadcast in that model."
Nancy Lynch,Lynch_Nancy,arXiv:1505.03799,https://arxiv.org/abs/1505.03799,"Abstract:  We introduce the study of the ant colony house-hunting problem from a
distributed computing perspective. When an ant colony's nest becomes unsuitable
due to size constraints or damage, the colony must relocate to a new nest. The
task of identifying and evaluating the quality of potential new nests is
distributed among all ants. The ants must additionally reach consensus on a
final nest choice and the full colony must be transported to this single new
nest. Our goal is to use tools and techniques from distributed computing theory
in order to gain insight into the house-hunting process.
We develop a formal model for the house-hunting problem inspired by the
behavior of the Temnothorax genus of ants. We then show a \Omega(log n) lower
bound on the time for all n ants to agree on one of k candidate nests. We also
present two algorithms that solve the house-hunting problem in our model. The
first algorithm solves the problem in optimal O(log n) time but exhibits some
features not characteristic of natural ant behavior. The second algorithm runs
in O(k log n) time and uses an extremely simple and natural rule for each ant
to decide on the new nest."
Nancy Lynch,Lynch_Nancy,arXiv:1502.02538,https://arxiv.org/abs/1502.02538,"Abstract:  The FLP result shows that crash-tolerant consensus is impossible to solve in
asynchronous systems, and several solutions have been proposed for
crash-tolerant consensus under alternative (stronger) models. One popular
approach is to augment the asynchronous system with appropriate failure
detectors, which provide (potentially unreliable) information about process
crashes in the system, to circumvent the FLP impossibility.
In this paper, we demonstrate the exact mechanism by which (sufficiently
powerful) asynchronous failure detectors enable solving crash-tolerant
consensus. Our approach, which borrows arguments from the FLP impossibility
proof and the famous result from CHT, which shows that $\Omega$ is a weakest
failure detector to solve consensus, also yields a natural proof to $\Omega$ as
a weakest asynchronous failure detector to solve consensus. The use of I/O
automata theory in our approach enables us to model execution in a more
detailed fashion than CHT and also addresses the latent assumptions and
assertions in the original result in CHT."
Nancy Lynch,Lynch_Nancy,arXiv:1407.4167,https://arxiv.org/abs/1407.4167,"Abstract:  This paper considers the communication and storage costs of emulating atomic
(linearizable) multi-writer multi-reader shared memory in distributed
message-passing systems. The paper contains three main contributions: (1) We
present a atomic shared-memory emulation algorithm that we call Coded Atomic
Storage (CAS). This algorithm uses erasure coding methods. In a storage system
with $N$ servers that is resilient to $f$ server failures, we show that the
communication cost of CAS is $\frac{N}{N-2f}$. The storage cost of CAS is
unbounded. (2) We present a modification of the CAS algorithm known as CAS with
Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer
$\delta$ and has a bounded storage cost. We show that in every execution where
the number of write operations that are concurrent with a read operation is no
bigger than $\delta$, the CASGC algorithm with parameter $\delta$ satisfies
atomicity and liveness. We explicitly characterize the storage cost of CASGC,
and show that it has the same communication cost as CAS. (3) We describe an
algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS)
algorithm that achieves a smaller communication cost than CAS and CASGC. In
particular, CCOAS incurs read and write communication costs of $\frac{N}{N-f}$
measured in terms of number of object values. We also discuss drawbacks of
CCOAS as compared with CAS and CASGC."
Nancy Lynch,Lynch_Nancy,arXiv:1405.1688,https://arxiv.org/abs/1405.1688,"Abstract:  We consider the ANTS problem [Feinerman et al.] in which a group of agents
collaboratively search for a target in a two-dimensional plane. Because this
problem is inspired by the behavior of biological species, we argue that in
addition to studying the {\em time complexity} of solutions it is also
important to study the {\em selection complexity}, a measure of how likely a
given algorithmic strategy is to arise in nature due to selective pressures. In
more detail, we propose a new selection complexity metric $\chi$, defined for
algorithm ${\cal A}$ such that $\chi({\cal A}) = b + \log \ell$, where $b$ is
the number of memory bits used by each agent and $\ell$ bounds the fineness of
available probabilities (agents use probabilities of at least $1/2^\ell$). In
this paper, we study the trade-off between the standard performance metric of
speed-up, which measures how the expected time to find the target improves with
$n$, and our new selection metric.
In particular, consider $n$ agents searching for a treasure located at
(unknown) distance $D$ from the origin (where $n$ is sub-exponential in $D$).
For this problem, we identify $\log \log D$ as a crucial threshold for our
selection complexity metric. We first prove a new upper bound that achieves a
near-optimal speed-up of $(D^2/n +D) \cdot 2^{O(\ell)}$ for $\chi({\cal A})
\leq 3 \log \log D + O(1)$. In particular, for $\ell \in O(1)$, the speed-up is
asymptotically optimal. By comparison, the existing results for this problem
[Feinerman et al.] that achieve similar speed-up require $\chi({\cal A}) =
\Omega(\log D)$. We then show that this threshold is tight by describing a
lower bound showing that if $\chi({\cal A}) < \log \log D - \omega(1)$, then
with high probability the target is not found within $D^{2-o(1)}$ moves per
agent. Hence, there is a sizable gap to the straightforward $\Omega(D^2/n + D)$
lower bound in this setting."
Nancy Lynch,Lynch_Nancy,arXiv:1405.1671,https://arxiv.org/abs/1405.1671,"Abstract:  We study the multi-message broadcast problem using abstract MAC layer models
of wireless networks. These models capture the key guarantees of existing MAC
layers while abstracting away low-level details such as signal propagation and
contention. We begin by studying upper and lower bounds for this problem in a
{\em standard abstract MAC layer model}---identifying an interesting dependence
between the structure of unreliable links and achievable time complexity. In
more detail, given a restriction that devices connected directly by an
unreliable link are not too far from each other in the reliable link topology,
we can (almost) match the efficiency of the reliable case. For the related
restriction, however, that two devices connected by an unreliable link are not
too far from each other in geographic distance, we prove a new lower bound that
shows that this efficiency is impossible. We then investigate how much extra
power must be added to the model to enable a new order of magnitude of
efficiency. In more detail, we consider an {\em enhanced abstract MAC layer
model} and present a new multi-message broadcast algorithm that (under certain
natural assumptions) solves the problem in this model faster than any known
solutions in an abstract MAC layer setting."
Nancy Lynch,Lynch_Nancy,arXiv:1208.6125,https://arxiv.org/abs/1208.6125,"Abstract:  Efficient communication in wireless networks is typically challenged by the
possibility of interference among several transmitting nodes. Much important
research has been invested in decreasing the number of collisions in order to
obtain faster algorithms for communication in such networks.
This paper proposes a novel approach for wireless communication, which
embraces collisions rather than avoiding them, over an additive channel. It
introduces a coding technique called Bounded-Contention Coding (BCC) that
allows collisions to be successfully decoded by the receiving nodes into the
original transmissions and whose complexity depends on a bound on the
contention among the transmitters.
BCC enables deterministic local broadcast in a network with n nodes and at
most a transmitters with information of l bits each within O(a log n + al) bits
of communication with full-duplex radios, and O((a log n + al)(log n)) bits,
with high probability, with half-duplex radios. When combined with random
linear network coding, BCC gives global broadcast within O((D + a + log n)(a
log n + l)) bits, with high probability. This also holds in dynamic networks
that can change arbitrarily over time by a worst-case adversary. When no bound
on the contention is given, it is shown how to probabilistically estimate it
and obtain global broadcast that is adaptive to the true contention in the
network."
Nancy Lynch,Lynch_Nancy,arXiv:1206.0154,https://arxiv.org/abs/1206.0154,"Abstract:  The local broadcast problem assumes that processes in a wireless network are
provided messages, one by one, that must be delivered to their neighbors. In
this paper, we prove tight bounds for this problem in two well-studied wireless
network models: the classical model, in which links are reliable and collisions
consistent, and the more recent dual graph model, which introduces unreliable
edges. Our results prove that the Decay strategy, commonly used for local
broadcast in the classical setting, is optimal. They also establish a
separation between the two models, proving that the dual graph setting is
strictly harder than the classical setting, with respect to this primitive."
Nancy Lynch,Lynch_Nancy,arXiv:math/9409221,https://arxiv.org/abs/math/9409221,"Abstract:  A method of analyzing time bounds for randomized distributed algorithms is
presented, in the context of a new and general framework for describing and
reasoning about randomized algorithms. The method consists of proving auxiliary
statements of the form U (t)->(p) U', which means that whenever the algorithm
begins in a state in set U, with probability p, it will reach a state in set U'
within time t. The power of the method is illustrated by its use in proving a
constant upper bound on the expected time for some process to reach its
critical region, in Lehmann and Rabin's Dining Philosophers algorithm."
Nancy Lynch,Lynch_Nancy,arXiv:math/9409220,https://arxiv.org/abs/math/9409220,"Abstract:  We consider the following scheduling problem. A system is composed of $n$
processors drawn from a pool of $N$. The processors can become faulty while in
operation and faulty processors never recover. A report is issued whenever a
fault occurs. This report states only the existence of a fault, but does not
indicate its location. Based on this report, the scheduler can reconfigure the
system and choose another set of $n$ processors. The system operates
satisfactorily as long as at most $f$ of the $n$ selected processors are
faulty. We exhibit a scheduling strategy allowing the system to operate
satisfactorily until approximately $(N/n)f$ faults are reported in the worst
case. Our precise bound is tight."
Samuel Madden,Madden_Samuel,arXiv:1811.02059,https://arxiv.org/abs/1811.02059,"Abstract:  In this paper, we present STAR, a new distributed in-memory database with
asymmetric replication. By employing a single-node non-partitioned architecture
for some replicas and a partitioned architecture for other replicas, STAR is
able to efficiently run both highly partitionable workloads and workloads that
involve cross-partition transactions. The key idea is a new phase-switching
algorithm where the execution of single-partition and cross-partition
transactions are separated. In the partitioned phase, single-partition
transactions are run on multiple machines in parallel to exploit more
concurrency. In the single-master phase, mastership for the entire database is
switched to a designated coordinator node, which can execute these transactions
without the use of expensive coordination protocols like two-phase commit.
Because the coordinator node has a full copy of the database, this
phase-switching can be done at negligible cost. Our experiments on two popular
benchmarks (YCSB and TPC-C) show that high availability via replication can
coexist with fast serializable transaction execution in distributed in-memory
databases, with STAR outperforming systems that employ conventional concurrency
control and replication algorithms by up to one order of magnitude."
Samuel Madden,Madden_Samuel,arXiv:1808.02515,https://arxiv.org/abs/1808.02515,"Abstract:  Thanks to the rapid proliferation of connected devices, sensor-generated time
series constitute a large and growing portion of the world's data. Often, this
data is collected from distributed, resource-constrained devices and
centralized at one or more servers. A key challenge in this setup is reducing
the size of the transmitted data without sacrificing its quality. Lower quality
reduces the data's utility, but smaller size enables both reduced network and
storage costs at the servers and reduced power consumption in sensing devices.
A natural solution is to compress the data at the sensing devices.
Unfortunately, existing compression algorithms either violate the memory and
latency constraints common for these devices or, as we show experimentally,
perform poorly on sensor-generated time series.
We introduce a time series compression algorithm that achieves
state-of-the-art compression ratios while requiring less than 1KB of memory and
adding virtually no latency. This method is suitable not only for low-power
devices collecting data, but also for servers storing and querying data; in the
latter context, it can decompress at over 3GB/s in a single thread, even faster
than many algorithms with much lower compression ratios. A key component of our
method is a high-speed forecasting algorithm that can be trained online and
significantly outperforms alternatives such as delta coding.
Extensive experiments on datasets from many domains show that these results
hold not only for sensor data but also across a wide array of other time
series."
Samuel Madden,Madden_Samuel,arXiv:1806.03723,https://arxiv.org/abs/1806.03723,"Abstract:  As neural networks become widely deployed in different applications and on
different hardware, it has become increasingly important to optimize inference
time and model size along with model accuracy. Most current techniques optimize
model size, model accuracy and inference time in different stages, resulting in
suboptimal results and computational inefficiency. In this work, we propose a
new technique called Smallify that optimizes all three of these metrics at the
same time. Specifically we present a new method to simultaneously optimize
network size and model performance by neuron-level pruning during training.
Neuron-level pruning not only produces much smaller networks but also produces
dense weight matrices that are amenable to efficient inference. By applying our
technique to convolutional as well as fully connected models, we show that
Smallify can reduce network size by 35X with a 6X improvement in inference time
with similar accuracy as models found by traditional training techniques."
Samuel Madden,Madden_Samuel,arXiv:1710.11528,https://arxiv.org/abs/1710.11528,"Abstract:  Many database columns contain string or numerical data that conforms to a
pattern, such as phone numbers, dates, addresses, product identifiers, and
employee ids. These patterns are useful in a number of data processing
applications, including understanding what a specific field represents when
field names are ambiguous, identifying outlier values, and finding similar
fields across data sets. One way to express such patterns would be to learn
regular expressions for each field in the database. Unfortunately, exist- ing
techniques on regular expression learning are slow, taking hundreds of seconds
for columns of just a few thousand values. In contrast, we develop XSystem, an
efficient method to learn patterns over database columns in significantly less
time. We show that these patterns can not only be built quickly, but are
expressive enough to capture a number of key applications, including detecting
outliers, measuring column similarity, and assigning semantic labels to columns
(based on a library of regular expressions). We evaluate these applications
with datasets that range from chemical databases (based on a collaboration with
a pharmaceutical company), our university data warehouse, and open data from
MassData.gov."
Samuel Madden,Madden_Samuel,arXiv:1709.10436,https://arxiv.org/abs/1709.10436,"Abstract:  Data integration has been a long-standing challenge in data management with
many applications. A key step in data integration is entity consolidation. It
takes a collection of clusters of duplicate records as input and produces a
single ""golden record"" for each cluster, which contains the canonical value for
each attribute. Truth discovery and data fusion methods, as well as Master Data
Management (MDM) systems, can be used for entity consolidation. However, to
achieve better results, the variant values (i.e., values that are logically the
same with different formats) in the clusters need to be consolidated before
applying these methods.
For this purpose, we propose a data-driven method to standardize the variant
values based on two observations: (1) the variant values usually can be
transformed to the same representation (e.g., ""Mary Lee"" and ""Lee, Mary"") and
(2) the same transformation often appears repeatedly across different clusters
(e.g., transpose the first and last name). Our approach first uses an
unsupervised method to generate groups of value pairs that can be transformed
in the same way (i.e., they share a transformation). Then the groups are
presented to a human for verification and the approved ones are used to
standardize the data. In a real-world dataset with 17,497 records, our method
achieved 75% recall and 99.5% precision in standardizing variant values by
asking a human 100 yes/no questions, which completely outperformed a state of
the art data wrangling tool."
Samuel Madden,Madden_Samuel,arXiv:1709.06416,https://arxiv.org/abs/1709.06416,"Abstract:  Data analytics applications combine multiple functions from different
libraries and frameworks. Even when each function is optimized in isolation,
the performance of the combined application can be an order of magnitude below
hardware limits due to extensive data movement across these functions. To
address this problem, we propose Weld, a new interface between data-intensive
libraries that can optimize across disjoint libraries and functions. Weld
exposes a lazily-evaluated API where diverse functions can submit their
computations in a simple but general intermediate representation that captures
their data-parallel structure. It then optimizes data movement across these
functions and emits efficient code for diverse hardware. Weld can be integrated
into existing frameworks such as Spark, TensorFlow, Pandas and NumPy without
changing their user-facing APIs. We demonstrate that Weld can speed up
applications using these frameworks by up to 29x."
Samuel Madden,Madden_Samuel,arXiv:1707.00721,https://arxiv.org/abs/1707.00721,"Abstract:  A polystore system is a database management system (DBMS) composed of
integrated heterogeneous database engines and multiple programming languages.
By matching data to the storage engine best suited to its needs, complex
analytics run faster and flexible storage choices helps improve data
organization. BigDAWG (Big Data Working Group) is our reference implementation
of a polystore system. In this paper, we describe the current BigDAWG software
release which supports PostgreSQL, Accumulo and SciDB. We describe the overall
architecture, API and initial results of applying BigDAWG to the MIMIC II
medical dataset."
Samuel Madden,Madden_Samuel,arXiv:1704.04738,https://arxiv.org/abs/1704.04738,"Abstract:  Determining if two sets are related - that is, if they have similar values or
if one set contains the other - is an important problem with many applications
in data cleaning, data integration, and information retrieval. A particularly
popular metric that has been proposed is to measure the relatedness of two sets
by treating the elements as vertices of a bipartite graph and calculating the
score of the maximum matching pairing between elements. Compared to other
metrics which require exact matchings between elements, this metric uses a
similarity function to compare elements between the two sets, making it robust
to small dissimilarities in elements and more useful for real-world, dirty
data. Unfortunately, the metric suffers from expensive computational cost,
taking O(n^3) time, where n is the number of elements in sets, for each
set-to-set comparison. Thus for applications which try to search for all
pairings of related sets in a brute-force manner, the runtime becomes
unacceptably large.
To address this challenge, we developed SilkMoth, a system capable of rapidly
discovering related set pairs in collections of sets. Internally, SilkMoth
creates a signature for each set, with the property that any other set which is
related must match the signature. SilkMoth then uses these signatures to prune
the search space, so only sets which match the signatures are left as
candidates. Finally, SilkMoth applies the maximum matching metric on remaining
candidates to verify which of these candidates are truly related sets. Thus, a
contribution of this paper is the characterization of the space of signatures
which enable this property. We show that selecting the optimal signature in
this space is NP-complete, and based on insights from the characterization of
the space, we propose two novel filters which help to prune the candidates
further before verification."
Samuel Madden,Madden_Samuel,arXiv:1701.06093,https://arxiv.org/abs/1701.06093,"Abstract:  Big data applications have fast arriving data that must be quickly ingested.
At the same time, they have specific needs to preprocess and transform the data
before it could be put to use. The current practice is to do these preparatory
transformations once the data is already ingested, however, this is expensive
to run and cumbersome to manage. As a result, there is a need to push data
preprocessing down to the ingestion itself. In this paper, we present a
declarative data ingestion system, called INGESTBASE, to allow application
developers to plan and specify their data ingestion logic in a more systematic
manner. We introduce the notion of ingestions plans, analogous to query plans,
and present a declarative ingestion language to help developers easily build
sophisticated ingestion plans. INGESTBASE provides an extensible ingestion
optimizer to rewrite and optimize ingestion plans by applying rules such as
operator reordering and pipelining. Finally, the INGESTBASE runtime engine runs
the optimized ingestion plan in a distributed and fault-tolerant manner. Later,
at query processing time, INGESTBASE supports ingestion-aware data access and
interfaces with upstream query processors, such as Hadoop MapReduce and Spark,
to post- process the ingested data. We demonstrate through a number of
experiments that INGESTBASE: (i) is flexible enough to express a variety of
ingestion techniques, (ii) incurs a low ingestion overhead, (iii) provides
efficient access to the ingested data, and (iv) has much better performance, up
to 6 times, than preparing data as an afterthought, via a query processor."
Samuel Madden,Madden_Samuel,arXiv:1701.05799,https://arxiv.org/abs/1701.05799,"Abstract:  The Intel Science and Technology Center for Big Data is developing a
reference implementation of a Polystore database. The BigDAWG (Big Data Working
Group) system supports ""many sizes"" of database engines, multiple programming
languages and complex analytics for a variety of workloads. Our recent efforts
include application of BigDAWG to an ocean metagenomics problem and
containerization of BigDAWG. We intend to release an open source BigDAWG v1.0
in the Spring of 2017. In this article, we will demonstrate a number of
polystore applications developed with oceanographic researchers at MIT and
describe our forthcoming open source release of the BigDAWG system."
Samuel Madden,Madden_Samuel,arXiv:1611.04705,https://arxiv.org/abs/1611.04705,"Abstract:  Existing database systems are not optimized for queries with a LIMIT
clause---operating instead in an all-or-nothing manner. In this paper, we
propose a fast LIMIT query evaluation engine, called NeedleTail, aimed at
letting analysts browse a small sample of the query results on large datasets
as quickly as possible, independent of the overall size of the result set.
NeedleTail introduces density maps, a lightweight in-memory indexing structure,
and a set of efficient algorithms (with desirable theoretical guarantees) to
quickly locate promising blocks, trading off locality and density. In settings
where the samples are used to compute aggregates, we extend techniques from
survey sampling to mitigate the bias in our samples. Our experimental results
demonstrate that NeedleTail returns results 4x faster on HDDs and 9x faster on
SSDs on average, while occupying up to 23x less memory than existing
techniques."
Samuel Madden,Madden_Samuel,arXiv:1609.07548,https://arxiv.org/abs/1609.07548,"Abstract:  Organizations are often faced with the challenge of providing data management
solutions for large, heterogenous datasets that may have different underlying
data and programming models. For example, a medical dataset may have
unstructured text, relational data, time series waveforms and imagery. Trying
to fit such datasets in a single data management system can have adverse
performance and efficiency effects. As a part of the Intel Science and
Technology Center on Big Data, we are developing a polystore system designed
for such problems. BigDAWG (short for the Big Data Analytics Working Group) is
a polystore system designed to work on complex problems that naturally span
across different processing or storage engines. BigDAWG provides an
architecture that supports diverse database systems working with different data
models, support for the competing notions of location transparency and semantic
completeness via islands and a middleware that provides a uniform multi--island
interface. Initial results from a prototype of the BigDAWG system applied to a
medical dataset validate polystore concepts. In this article, we will describe
polystore databases, the current BigDAWG architecture and its application on
the MIMIC II medical dataset, initial performance results and our future
development plans."
Samuel Madden,Madden_Samuel,arXiv:1603.00567,https://arxiv.org/abs/1603.00567,"Abstract:  As data volumes continue to rise, manual inspection is becoming increasingly
untenable. In response, we present MacroBase, a data analytics engine that
prioritizes end-user attention in high-volume fast data streams. MacroBase
enables efficient, accurate, and modular analyses that highlight and aggregate
important and unusual behavior, acting as a search engine for fast data.
MacroBase is able to deliver order-of-magnitude speedups over alternatives by
optimizing the combination of explanation and classification tasks and by
leveraging a new reservoir sampler and heavy-hitters sketch specialized for
fast data streams. As a result, MacroBase delivers accurate results at speeds
of up to 2M events per second per query on a single core. The system has
delivered meaningful results in production, including at a telematics company
monitoring hundreds of thousands of vehicles."
Samuel Madden,Madden_Samuel,arXiv:1602.08791,https://arxiv.org/abs/1602.08791,"Abstract:  BigDAWG is a polystore system designed to work on complex problems that
naturally span across different processing or storage engines. BigDAWG provides
an architecture that supports diverse database systems working with different
data models, support for the competing notions of location transparency and
semantic completeness via islands of information and a middleware that provides
a uniform multi-island interface. In this article, we describe the current
architecture of BigDAWG, its application on the MIMIC II medical dataset, and
our plans for the mechanics of cross-system queries. During the presentation,
we will also deliver a brief demonstration of the current version of BigDAWG."
Samuel Madden,Madden_Samuel,arXiv:1506.04815,https://arxiv.org/abs/1506.04815,"Abstract:  Organizations and teams collect and acquire data from various sources, such
as social interactions, financial transactions, sensor data, and genome
sequencers. Different teams in an organization as well as different data
scientists within a team are interested in extracting a variety of insights
which require combining and collaboratively analyzing datasets in diverse ways.
DataHub is a system that aims to provide robust version control and provenance
management for such a scenario. To be truly useful for collaborative data
science, one also needs the ability to specify queries and analysis tasks over
the versioning and the provenance information in a unified manner. In this
paper, we present an initial design of our query language, called VQuel, that
aims to support such unified querying over both types of information, as well
as the intermediate and final results of analyses. We also discuss some of the
key language design and implementation challenges moving forward."
Samuel Madden,Madden_Samuel,arXiv:1503.01143,https://arxiv.org/abs/1503.01143,"Abstract:  Stream processing addresses the needs of real-time applications. Transaction
processing addresses the coordination and safety of short atomic computations.
Heretofore, these two modes of operation existed in separate, stove-piped
systems. In this work, we attempt to fuse the two computational paradigms in a
single system called S-Store. In this way, S-Store can simultaneously
accommodate OLTP and streaming applications. We present a simple transaction
model for streams that integrates seamlessly with a traditional OLTP system. We
chose to build S-Store as an extension of H-Store, an open-source, in-memory,
distributed OLTP database system. By implementing S-Store in this way, we can
make use of the transaction processing facilities that H-Store already
supports, and we can concentrate on the additional implementation features that
are needed to support streaming. Similar implementations could be done using
other main-memory OLTP platforms. We show that we can actually achieve higher
throughput for streaming workloads in S-Store than an equivalent deployment in
H-Store alone. We also show how this can be achieved within H-Store with the
addition of a modest amount of new functionality. Furthermore, we compare
S-Store to two state-of-the-art streaming systems, Spark Streaming and Storm,
and show how S-Store matches and sometimes exceeds their performance while
providing stronger transactional guarantees."
Samuel Madden,Madden_Samuel,arXiv:1412.5263,https://arxiv.org/abs/1412.5263,"Abstract:  Graph analytics is becoming increasingly popular, with a deluge of new
systems for graph analytics having been proposed in the past few years. These
systems often start from the assumption that a new storage or query processing
system is needed, in spite of graph data being often collected and stored in a
relational database in the first place. In this paper, we study Vertica
relational database as a platform for graph analytics. We show that
vertex-centric graph analysis can be translated to SQL queries, typically
involving table scans and joins, and that modern column-oriented databases are
very well suited to running such queries. Specifically, we present an
experimental evaluation of the Vertica relational database system on a variety
of graph analytics, including iterative analysis, a combination of graph and
relational analyses, and more complex 1- hop neighborhood graph analytics,
showing that it is competitive to two popular vertex-centric graph analytics
systems, namely Giraph and GraphLab."
Samuel Madden,Madden_Samuel,arXiv:1409.0798,https://arxiv.org/abs/1409.0798,"Abstract:  Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software version
control systems like git, we propose (a) a dataset version control system,
giving users the ability to create, branch, merge, difference and search large,
divergent collections of datasets, and (b) a platform, DataHub, that gives
users the ability to perform collaborative data analysis building on this
version control system. We outline the challenges in providing dataset version
control at scale."
Samuel Madden,Madden_Samuel,arXiv:1209.3686,https://arxiv.org/abs/1209.3686,"Abstract:  Crowd-sourcing has become a popular means of acquiring labeled data for a
wide variety of tasks where humans are more accurate than computers, e.g.,
labeling images, matching objects, or analyzing sentiment. However, relying
solely on the crowd is often impractical even for data sets with thousands of
items, due to time and cost constraints of acquiring human input (which cost
pennies and minutes per label). In this paper, we propose algorithms for
integrating machine learning into crowd-sourced databases, with the goal of
allowing crowd-sourcing applications to scale, i.e., to handle larger datasets
at lower costs. The key observation is that, in many of the above tasks, humans
and machine learning algorithms can be complementary, as humans are often more
accurate but slow and expensive, while algorithms are usually less accurate,
but faster and cheaper.
Based on this observation, we present two new active learning algorithms to
combine humans and algorithms together in a crowd-sourced database. Our
algorithms are based on the theory of non-parametric bootstrap, which makes our
results applicable to a broad class of machine learning models. Our results, on
three real-life datasets collected with Amazon's Mechanical Turk, and on 15
well-known UCI data sets, show that our methods on average ask humans to label
one to two orders of magnitude fewer items to achieve the same accuracy as a
baseline that labels random images, and two to eight times fewer questions than
previous active learning schemes."
Samuel Madden,Madden_Samuel,arXiv:1208.2925,https://arxiv.org/abs/1208.2925,"Abstract:  This paper presents a new approach to select events of interest to a user in
a social media setting where events are generated by the activities of the
user's friends through their mobile devices. We argue that given the unique
requirements of the social media setting, the problem is best viewed as an
inductive learning problem, where the goal is to first generalize from the
users' expressed ""likes"" and ""dislikes"" of specific events, then to produce a
program that can be manipulated by the system and distributed to the collection
devices to collect only data of interest. The key contribution of this paper is
a new algorithm that combines existing machine learning techniques with new
program synthesis technology to learn users' preferences. We show that when
compared with the more standard approaches, our new algorithm provides up to
order-of-magnitude reductions in model training time, and significantly higher
prediction accuracies for our target application. The approach also improves on
standard machine learning techniques in that it produces clear programs that
can be manipulated to optimize data collection and filtering."
Samuel Madden,Madden_Samuel,arXiv:1208.2013,https://arxiv.org/abs/1208.2013,"Abstract:  Developing high-performance applications that interact with databases is a
difficult task, as developers need to understand both the details of the
language in which their applications are written in, and also the intricacies
of the relational model. One popular solution to this problem is the use of
object-relational mapping (ORM) libraries that provide transparent access to
the database using the same language that the application is written in.
Unfortunately, using such frameworks can easily lead to applications with poor
performance because developers often end up implementing relational operations
in application code, and doing so usually does not take advantage of the
optimized implementations of relational operations, efficient query plans, or
push down of predicates that database systems provide. In this paper we present
QBS, an algorithm that automatically identifies fragments of application logic
that can be pushed into SQL queries. The QBS algorithm works by automatically
synthesizing invariants and postconditions for the original code fragment. The
postconditions and invariants are expressed using a theory of ordered relations
that allows us to reason precisely about the contents and order of the records
produced even by complex code fragments that compute joins and aggregates. The
theory is close in expressiveness to SQL, so the synthesized postconditions can
be readily translated to SQL queries. Using 40 code fragments extracted from
over 120k lines of open-source code written using the Java Hibernate ORM, we
demonstrate that our approach can convert a variety of imperative constructs
into relational specifications."
Samuel Madden,Madden_Samuel,arXiv:1208.0271,https://arxiv.org/abs/1208.0271,"Abstract:  Database-backed applications are nearly ubiquitous in our daily lives.
Applications that make many small accesses to the database create two
challenges for developers: increased latency and wasted resources from numerous
network round trips. A well-known technique to improve transactional database
application performance is to convert part of the application into stored
procedures that are executed on the database server. Unfortunately, this
conversion is often difficult. In this paper we describe Pyxis, a system that
takes database-backed applications and automatically partitions their code into
two pieces, one of which is executed on the application server and the other on
the database server. Pyxis profiles the application and server loads,
statically analyzes the code's dependencies, and produces a partitioning that
minimizes the number of control transfers as well as the amount of data sent
during each transfer. Our experiments using TPC-C and TPC-W show that Pyxis is
able to generate partitions with up to 3x reduction in latency and 1.7x
improvement in throughput when compared to a traditional non-partitioned
implementation and has comparable performance to that of a custom stored
procedure implementation."
Samuel Madden,Madden_Samuel,arXiv:1203.6049,https://arxiv.org/abs/1203.6049,"Abstract:  Replicating data across multiple data centers not only allows moving the data
closer to the user and, thus, reduces latency for applications, but also
increases the availability in the event of a data center failure. Therefore, it
is not surprising that companies like Google, Yahoo, and Netflix already
replicate user data across geographically different regions.
However, replication across data centers is expensive. Inter-data center
network delays are in the hundreds of milliseconds and vary significantly.
Synchronous wide-area replication is therefore considered to be unfeasible with
strong consistency and current solutions either settle for asynchronous
replication which implies the risk of losing data in the event of failures,
restrict consistency to small partitions, or give up consistency entirely. With
MDCC (Multi-Data Center Consistency), we describe the first optimistic commit
protocol, that does not require a master or partitioning, and is strongly
consistent at a cost similar to eventually consistent protocols. MDCC can
commit transactions in a single round-trip across data centers in the normal
operational case. We further propose a new programming model which empowers the
application developer to handle longer and unpredictable latencies caused by
inter-data center communication. Our evaluation using the TPC-W benchmark with
MDCC deployed across 5 geographically diverse data centers shows that MDCC is
able to achieve throughput and latency similar to eventually consistent quorum
protocols and that MDCC is able to sustain a data center outage without a
significant impact on response times while guaranteeing strong consistency."
Samuel Madden,Madden_Samuel,arXiv:1203.5485,https://arxiv.org/abs/1203.5485,"Abstract:  In this paper, we present BlinkDB, a massively parallel, sampling-based
approximate query engine for running ad-hoc, interactive SQL queries on large
volumes of data. The key insight that BlinkDB builds on is that one can often
make reasonable decisions in the absence of perfect answers. For example,
reliably detecting a malfunctioning server using a distributed collection of
system logs does not require analyzing every request processed by the system.
Based on this insight, BlinkDB allows one to trade-off query accuracy for
response time, enabling interactive queries over massive data by running
queries on data samples and presenting results annotated with meaningful error
bars. To achieve this, BlinkDB uses two key ideas that differentiate it from
previous work in this area: (1) an adaptive optimization framework that builds
and maintains a set of multi-dimensional, multi-resolution samples from
original data over time, and (2) a dynamic sample selection strategy that
selects an appropriately sized sample based on a query's accuracy and/or
response time requirements. We have built an open-source version of BlinkDB and
validated its effectiveness using the well-known TPC-H benchmark as well as a
real-world analytic workload derived from Conviva Inc. Our experiments on a 100
node cluster show that BlinkDB can answer a wide range of queries from a
real-world query trace on up to 17 TBs of data in less than 2 seconds (over
100\times faster than Hive), within an error of 2 - 10%."
Samuel Madden,Madden_Samuel,arXiv:1109.6881,https://arxiv.org/abs/1109.6881,"Abstract:  Crowdsourcing markets like Amazon's Mechanical Turk (MTurk) make it possible
to task people with small jobs, such as labeling images or looking up phone
numbers, via a programmatic interface. MTurk tasks for processing datasets with
humans are currently designed with significant reimplementation of common
workflows and ad-hoc selection of parameters such as price to pay per task. We
describe how we have integrated crowds into a declarative workflow engine
called Qurk to reduce the burden on workflow designers. In this paper, we focus
on how to use humans to compare items for sorting and joining data, two of the
most common operations in DBMSs. We describe our basic query interface and the
user interface of the tasks we post to MTurk. We also propose a number of
optimizations, including task batching, replacing pairwise comparisons with
numerical ratings, and pre-filtering tables before joining them, which
dramatically reduce the overall cost of running sorts and joins on the crowd.
In an experiment joining two sets of images, we reduce the overall cost from
$67 in a naive implementation to about $3, without substantially affecting
accuracy or latency. In an end-to-end experiment, we reduced cost by a factor
of 14.5."
Aleksander Madry,Madry_Aleksander,arXiv:1811.02553,https://arxiv.org/abs/1811.02553,"Abstract:  We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. We propose a fine-grained
analysis of state-of-the-art methods based on key aspects of this framework:
gradient estimation, value prediction, optimization landscapes, and trust
region enforcement. We find that from this perspective, the behavior of deep
policy gradient algorithms often deviates from what their motivating framework
would predict. Our analysis suggests first steps towards solidifying the
foundations of these algorithms, and in particular indicates that we may need
to move beyond the current benchmark-centric evaluation methodology."
Aleksander Madry,Madry_Aleksander,arXiv:1811.00636,https://arxiv.org/abs/1811.00636,"Abstract:  A recent line of work has uncovered a new form of data poisoning: so-called
\emph{backdoor} attacks. These attacks are particularly dangerous because they
do not affect a network's behavior on typical, benign data. Rather, the network
only deviates from its expected output when triggered by a perturbation planted
by an adversary.
In this paper, we identify a new property of all known backdoor attacks,
which we call \emph{spectral signatures}. This property allows us to utilize
tools from robust statistics to thwart the attacks. We demonstrate the efficacy
of these signatures in detecting and removing poisoned examples on real image
sets and state of the art neural network architectures. We believe that
understanding spectral signatures is a crucial first step towards designing ML
systems secure against such backdoor attacks"
Aleksander Madry,Madry_Aleksander,arXiv:1809.03008,https://arxiv.org/abs/1809.03008,"Abstract:  We explore the concept of co-design in the context of neural network
verification. Specifically, we aim to train deep neural networks that not only
are robust to adversarial perturbations but also whose robustness can be
verified more easily. To this end, we identify two properties of network models
- weight sparsity and so-called ReLU stability - that turn out to significantly
impact the complexity of the corresponding verification task. We demonstrate
that improving weight sparsity alone already enables us to turn computationally
intractable verification problems into tractable ones. Then, improving ReLU
stability leads to an additional 4-13x speedup in verification times. An
important feature of our methodology is its ""universality,"" in the sense that
it can be used with a broad range of training procedures and verification
approaches."
Aleksander Madry,Madry_Aleksander,arXiv:1807.07978,https://arxiv.org/abs/1807.07978,"Abstract:  We study the problem of generating adversarial examples in a black-box
setting in which only loss-oracle access to a model is available. We introduce
a framework that conceptually unifies much of the existing work on black-box
attacks, and we demonstrate that the current state-of-the-art methods are
optimal in a natural sense. Despite this optimality, we show how to improve
black-box attacks by bringing a new element into the problem: gradient priors.
We give a bandit optimization-based algorithm that allows us to seamlessly
integrate any such priors, and we explicitly identify and incorporate two
examples. The resulting methods use two to four times fewer queries and fail
two to five times less often than the current state-of-the-art."
Aleksander Madry,Madry_Aleksander,arXiv:1805.12152,https://arxiv.org/abs/1805.12152,"Abstract:  We show that there exists an inherent tension between the goal of adversarial
robustness and that of standard generalization. Specifically, training robust
models may not only be more resource-consuming, but also lead to a reduction of
standard accuracy. We demonstrate that this trade-off between the standard
accuracy of a model and its robustness to adversarial perturbations provably
exists even in a fairly simple and natural setting. These findings also
corroborate a similar phenomenon observed in practice. Further, we argue that
this phenomenon is a consequence of robust classifiers learning fundamentally
different feature representations than standard classifiers. These differences,
in particular, seem to result in unexpected benefits: the representations
learned by robust models tend to align better with salient data characteristics
and human perception."
Aleksander Madry,Madry_Aleksander,arXiv:1805.11604,https://arxiv.org/abs/1805.11604,"Abstract:  Batch Normalization (BatchNorm) is a widely adopted technique that enables
faster and more stable training of deep neural networks (DNNs). Despite its
pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly
understood. The popular belief is that this effectiveness stems from
controlling the change of the layers' input distributions during training to
reduce the so-called ""internal covariate shift"". In this work, we demonstrate
that such distributional stability of layer inputs has little to do with the
success of BatchNorm. Instead, we uncover a more fundamental impact of
BatchNorm on the training process: it makes the optimization landscape
significantly smoother. This smoothness induces a more predictive and stable
behavior of the gradients, allowing for faster training."
Aleksander Madry,Madry_Aleksander,arXiv:1804.11285,https://arxiv.org/abs/1804.11285,"Abstract:  Machine learning models are often susceptible to adversarial perturbations of
their inputs. Even small perturbations can cause state-of-the-art classifiers
with high ""standard"" accuracy to produce an incorrect prediction with high
confidence. To better understand this phenomenon, we study adversarially robust
learning from the viewpoint of generalization. We show that already in a simple
natural data model, the sample complexity of robust learning can be
significantly larger than that of ""standard"" learning. This gap is information
theoretic and holds irrespective of the training algorithm or the model family.
We complement our theoretical results with experiments on popular image
classification datasets and show that a similar gap exists here as well. We
postulate that the difficulty of training robust classifiers stems, at least
partially, from this inherently larger sample complexity."
Aleksander Madry,Madry_Aleksander,arXiv:1712.08130,https://arxiv.org/abs/1712.08130,"Abstract:  Sparsity-based methods are widely used in machine learning, statistics, and
signal processing. There is now a rich class of structured sparsity approaches
that expand the modeling power of the sparsity paradigm and incorporate
constraints such as group sparsity, graph sparsity, or hierarchical sparsity.
While these sparsity models offer improved sample complexity and better
interpretability, the improvements come at a computational cost: it is often
challenging to optimize over the (non-convex) constraint sets that capture
various sparsity structures. In this paper, we make progress in this direction
in the context of separated sparsity -- a fundamental sparsity notion that
captures exclusion constraints in linearly ordered data such as time series.
While prior algorithms for computing a projection onto this constraint set
required quadratic time, we provide a perturbed Lagrangian relaxation approach
that computes provably exact projection in only nearly-linear time. Although
the sparsity constraint is non-convex, our perturbed Lagrangian approach is
still guaranteed to find a globally optimal solution. In experiments, our new
algorithms offer a 10$\times$ speed-up already on moderately-size inputs."
Aleksander Madry,Madry_Aleksander,arXiv:1712.02779,https://arxiv.org/abs/1712.02779,"Abstract:  We show that simple transformations, namely translations and rotations alone,
are sufficient to fool neural network-based vision models on a significant
fraction of inputs. This is in sharp contrast to previous work that relied on
more complicated optimization approaches that are unlikely to appear outside of
a truly adversarial setting. Moreover, fooling rotations and translations are
easy to find and require only a few black-box queries to the target model.
Overall, our findings emphasize the need for designing robust classifiers even
in natural, benign contexts."
Aleksander Madry,Madry_Aleksander,arXiv:1711.01085,https://arxiv.org/abs/1711.01085,"Abstract:  We present an $O((\log k)^2)$-competitive randomized algorithm for the
$k$-server problem on hierarchically separated trees (HSTs). This is the first
$o(k)$-competitive randomized algorithm for which the competitive ratio is
independent of the size of the underlying HST. Our algorithm is designed in the
framework of online mirror descent where the mirror map is a multiscale
entropy. When combined with Bartal's static HST embedding reduction, this leads
to an $O((\log k)^2 \log n)$-competitive algorithm on any $n$-point metric
space. We give a new dynamic HST embedding that yields an $O((\log k)^3 \log
\Delta)$-competitive algorithm on any metric space where the ratio of the
largest to smallest non-zero distance is at most $\Delta$."
Aleksander Madry,Madry_Aleksander,arXiv:1711.00970,https://arxiv.org/abs/1711.00970,"Abstract:  A basic, and still largely unanswered, question in the context of Generative
Adversarial Networks (GANs) is whether they are truly able to capture all the
fundamental characteristics of the distributions they are trained on. In
particular, evaluating the diversity of GAN distributions is challenging and
existing methods provide only a partial understanding of this issue. In this
paper, we develop quantitative and scalable tools for assessing the diversity
of GAN distributions. Specifically, we take a classification-based perspective
and view loss of diversity as a form of covariate shift introduced by GANs. We
examine two specific forms of such shift: mode collapse and boundary
distortion. In contrast to prior work, our methods need only minimal human
supervision and can be readily applied to state-of-the-art GANs on large,
canonical datasets. Examining popular GANs using our tools indicates that these
GANs have significant problems in reproducing the more distributional
properties of their training dataset."
Aleksander Madry,Madry_Aleksander,arXiv:1707.03478,https://arxiv.org/abs/1707.03478,"Abstract:  For over a decade now we have been witnessing the success of {\em massive
parallel computation} (MPC) frameworks, such as MapReduce, Hadoop, Dryad, or
Spark. One of the reasons for their success is the fact that these frameworks
are able to accurately capture the nature of large-scale computation. In
particular, compared to the classic distributed algorithms or PRAM models,
these frameworks allow for much more local computation. The fundamental
question that arises in this context is though: can we leverage this additional
power to obtain even faster parallel algorithms?
A prominent example here is the {\em maximum matching} problem---one of the
most classic graph problems. It is well known that in the PRAM model one can
compute a 2-approximate maximum matching in $O(\log{n})$ rounds. However, the
exact complexity of this problem in the MPC framework is still far from
understood. Lattanzi et al. showed that if each machine has $n^{1+\Omega(1)}$
memory, this problem can also be solved $2$-approximately in a constant number
of rounds. These techniques, as well as the approaches developed in the follow
up work, seem though to get stuck in a fundamental way at roughly $O(\log{n})$
rounds once we enter the near-linear memory regime. It is thus entirely
possible that in this regime, which captures in particular the case of sparse
graph computations, the best MPC round complexity matches what one can already
get in the PRAM model, without the need to take advantage of the extra local
computation power.
In this paper, we finally refute that perplexing possibility. That is, we
break the above $O(\log n)$ round complexity bound even in the case of {\em
slightly sublinear} memory per machine. In fact, our improvement here is {\em
almost exponential}: we are able to deliver a $(2+\epsilon)$-approximation to
maximum matching, for any fixed constant $\epsilon>0$, in $O((\log \log n)^2)$
rounds."
Aleksander Madry,Madry_Aleksander,arXiv:1706.09884,https://arxiv.org/abs/1706.09884,"Abstract:  While Generative Adversarial Networks (GANs) have demonstrated promising
performance on multiple vision tasks, their learning dynamics are not yet well
understood, both in theory and in practice. To address this issue, we study GAN
dynamics in a simple yet rich parametric model that exhibits several of the
common problematic convergence behaviors such as vanishing gradients, mode
collapse, and diverging or oscillatory behavior. In spite of the non-convex
nature of our model, we are able to perform a rigorous theoretical analysis of
its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN
with an optimal discriminator provably converges, while first order
approximations of the discriminator steps lead to unstable GAN dynamics and
mode collapse. Our result suggests that using first order discriminator steps
(the de-facto standard in most existing GAN setups) might be one of the factors
that makes GAN training challenging in practice."
Aleksander Madry,Madry_Aleksander,arXiv:1706.06083,https://arxiv.org/abs/1706.06083,"Abstract:  Recent work has demonstrated that neural networks are vulnerable to
adversarial examples, i.e., inputs that are almost indistinguishable from
natural data and yet classified incorrectly by the network. In fact, some of
the latest findings suggest that the existence of adversarial attacks may be an
inherent weakness of deep learning models. To address this problem, we study
the adversarial robustness of neural networks through the lens of robust
optimization. This approach provides us with a broad and unifying view on much
of the prior work on this topic. Its principled nature also enables us to
identify methods for both training and attacking neural networks that are
reliable and, in a certain sense, universal. In particular, they specify a
concrete security guarantee that would protect against any adversary. These
methods let us train networks with significantly improved resistance to a wide
range of adversarial attacks. They also suggest the notion of security against
a first-order adversary as a natural and broad security guarantee. We believe
that robustness against such well-defined classes of adversaries is an
important stepping stone towards fully resistant deep learning models."
Aleksander Madry,Madry_Aleksander,arXiv:1704.02310,https://arxiv.org/abs/1704.02310,"Abstract:  In this paper, we study matrix scaling and balancing, which are fundamental
problems in scientific computing, with a long line of work on them that dates
back to the 1960s. We provide algorithms for both these problems that, ignoring
logarithmic factors involving the dimension of the input matrix and the size of
its entries, both run in time $\widetilde{O}\left(m\log \kappa \log^2
(1/\epsilon)\right)$ where $\epsilon$ is the amount of error we are willing to
tolerate. Here, $\kappa$ represents the ratio between the largest and the
smallest entries of the optimal scalings. This implies that our algorithms run
in nearly-linear time whenever $\kappa$ is quasi-polynomial, which includes, in
particular, the case of strictly positive matrices. We complement our results
by providing a separate algorithm that uses an interior-point method and runs
in time $\widetilde{O}(m^{3/2} \log (1/\epsilon))$.
In order to establish these results, we develop a new second-order
optimization framework that enables us to treat both problems in a unified and
principled manner. This framework identifies a certain generalization of linear
system solving that we can use to efficiently minimize a broad class of
functions, which we call second-order robust. We then show that in the context
of the specific functions capturing matrix scaling and balancing, we can
leverage and generalize the work on Laplacian system solving to make the
algorithms obtained via this framework very efficient."
Aleksander Madry,Madry_Aleksander,arXiv:1608.06016,https://arxiv.org/abs/1608.06016,"Abstract:  We present an $\tilde{O}\left(m^{\frac{10}{7}}U^{\frac{1}{7}}\right)$-time
algorithm for the maximum $s$-$t$ flow problem and the minimum $s$-$t$ cut
problem in directed graphs with $m$ arcs and largest integer capacity $U$. This
matches the running time of the
$\tilde{O}\left((mU)^{\frac{10}{7}}\right)$-time algorithm of Mądry (FOCS
2013) in the unit-capacity case, and improves over it, as well as over the
$\tilde{O}\left(m \sqrt{n} \log U\right)$-time algorithm of Lee and Sidford
(FOCS 2014), whenever $U$ is moderately large and the graph is sufficiently
sparse. By well-known reductions, this also gives similar running time
improvements for the maximum-cardinality bipartite $b$-matching problem.
One of the advantages of our algorithm is that it is significantly simpler
than the ones presented in Madry (FOCS 2013) and Lee and Sidford (FOCS 2014).
In particular, these algorithms employ a sophisticated interior-point method
framework, while our algorithm is cast directly in the classic augmenting path
setting that almost all the combinatorial maximum flow algorithms use. At a
high level, the presented algorithm takes a primal dual approach in which each
iteration uses electrical flows computations both to find an augmenting $s$-$t$
flow in the current residual graph and to update the dual solution. We show
that by maintain certain careful coupling of these primal and dual solutions we
are always guaranteed to make significant progress."
Aleksander Madry,Madry_Aleksander,arXiv:1605.01717,https://arxiv.org/abs/1605.01717,"Abstract:  In this paper, we study a set of combinatorial optimization problems on
weighted graphs: the shortest path problem with negative weights, the weighted
perfect bipartite matching problem, the unit-capacity minimum-cost maximum flow
problem and the weighted perfect bipartite $b$-matching problem under the
assumption that $\Vert b\Vert_1=O(m)$. We show that each one of these four
problems can be solved in $\tilde{O}(m^{10/7}\log W)$ time, where $W$ is the
absolute maximum weight of an edge in the graph, which gives the first in over
25 years polynomial improvement in their sparse-graph time complexity.
At a high level, our algorithms build on the interior-point method-based
framework developed by Madry (FOCS 2013) for solving unit-capacity maximum flow
problem. We develop a refined way to analyze this framework, as well as provide
new variants of the underlying preconditioning and perturbation techniques.
Consequently, we are able to extend the whole interior-point method-based
approach to make it applicable in the weighted graph regime."
Aleksander Madry,Madry_Aleksander,arXiv:1501.00267,https://arxiv.org/abs/1501.00267,"Abstract:  We present a new algorithm for generating a uniformly random spanning tree in
an undirected graph. Our algorithm samples such a tree in expected
$\tilde{O}(m^{4/3})$ time. This improves over the best previously known bound
of $\min(\tilde{O}(m\sqrt{n}),O(n^{\omega}))$ -- that follows from the work of
Kelner and Mądry [FOCS'09] and of Colbourn et al. [J. Algorithms'96] --
whenever the input graph is sufficiently sparse.
At a high level, our result stems from carefully exploiting the interplay of
random spanning trees, random walks, and the notion of effective resistance, as
well as from devising a way to algorithmically relate these concepts to the
combinatorial structure of the graph. This involves, in particular,
establishing a new connection between the effective resistance metric and the
cut structure of the underlying graph."
Aleksander Madry,Madry_Aleksander,arXiv:1409.0034,https://arxiv.org/abs/1409.0034,"Abstract:  We present and study the Static-Routing-Resiliency problem, motivated by
routing on the Internet: Given a graph $G$, a unique destination vertex $d$,
and an integer constant $c>0$, does there exist a static and destination-based
routing scheme such that the correct delivery of packets from any source $s$ to
the destination $d$ is guaranteed so long as (1) no more than $c$ edges fail
and (2) there exists a physical path from $s$ to $d$? We embark upon a
systematic exploration of this fundamental question in a variety of models
(deterministic routing, randomized routing, with packet-duplication, with
packet-header-rewriting) and present both positive and negative results that
relate the edge-connectivity of a graph, i.e., the minimum number of edges
whose deletion partitions $G$, to its resiliency."
Aleksander Madry,Madry_Aleksander,arXiv:1403.7519,https://arxiv.org/abs/1403.7519,"Abstract:  We study the Maximum Budgeted Allocation problem, i.e., the problem of
selling a set of $m$ indivisible goods to $n$ players, each with a separate
budget, such that we maximize the collected revenue. Since the natural
assignment LP is known to have an integrality gap of $\frac{3}{4}$, which
matches the best known approximation algorithms, our main focus is to improve
our understanding of the stronger configuration LP relaxation. In this
direction, we prove that the integrality gap of the configuration LP is
strictly better than $\frac{3}{4}$, and provide corresponding polynomial time
roundings, in the following restrictions of the problem: (i) the Restricted
Budgeted Allocation problem, in which all the players have the same budget and
every item has the same value for any player it can be sold to, and (ii) the
graph MBA problem, in which an item can be assigned to at most 2 players.
Finally, we improve the best known upper bound on the integrality gap for the
general case from $\frac{5}{6}$ to $2\sqrt{2}-2\approx 0.828$ and also prove
hardness of approximation results for both cases."
Aleksander Madry,Madry_Aleksander,arXiv:1307.2205,https://arxiv.org/abs/1307.2205,"Abstract:  We present an $\tilde{O}(m^{10/7})=\tilde{O}(m^{1.43})$-time algorithm for
the maximum s-t flow and the minimum s-t cut problems in directed graphs with
unit capacities. This is the first improvement over the sparse-graph case of
the long-standing $O(m \min(\sqrt{m},n^{2/3}))$ time bound due to Even and
Tarjan [EvenT75]. By well-known reductions, this also establishes an
$\tilde{O}(m^{10/7})$-time algorithm for the maximum-cardinality bipartite
matching problem. That, in turn, gives an improvement over the celebrated
celebrated $O(m \sqrt{n})$ time bound of Hopcroft and Karp [HK73] whenever the
input graph is sufficiently sparse."
Aleksander Madry,Madry_Aleksander,arXiv:1110.1580,https://arxiv.org/abs/1110.1580,"Abstract:  We give the first polylogarithmic-competitive randomized online algorithm for
the $k$-server problem on an arbitrary finite metric space. In particular, our
algorithm achieves a competitive ratio of O(log^3 n log^2 k log log n) for any
metric space on n points. Our algorithm improves upon the deterministic
(2k-1)-competitive algorithm of Koutsoupias and Papadimitriou [J.ACM'95]
whenever n is sub-exponential in k."
Aleksander Madry,Madry_Aleksander,arXiv:1110.1358,https://arxiv.org/abs/1110.1358,"Abstract:  We study theoretical runtime guarantees for a class of optimization problems
that occur in a wide variety of inference problems. these problems are
motivated by the lasso framework and have applications in machine learning and
computer vision.
Our work shows a close connection between these problems and core questions
in algorithmic graph theory. While this connection demonstrates the
difficulties of obtaining runtime guarantees, it also suggests an approach of
using techniques originally developed for graph algorithms.
We then show that most of these problems can be formulated as a grouped least
squares problem, and give efficient algorithms for this formulation. Our
algorithms rely on routines for solving quadratic minimization problems, which
in turn are equivalent to solving linear systems. Finally we present some
experimental results on applying our approximation algorithm to image
processing problems."
Aleksander Madry,Madry_Aleksander,arXiv:1010.2921,https://arxiv.org/abs/1010.2921,"Abstract:  We introduce a new approach to computing an approximately maximum s-t flow in
a capacitated, undirected graph. This flow is computed by solving a sequence of
electrical flow problems. Each electrical flow is given by the solution of a
system of linear equations in a Laplacian matrix, and thus may be approximately
computed in nearly-linear time.
Using this approach, we develop the fastest known algorithm for computing
approximately maximum s-t flows. For a graph having n vertices and m edges, our
algorithm computes a (1-\epsilon)-approximately maximum s-t flow in time
\tilde{O}(mn^{1/3} \epsilon^{-11/3}). A dual version of our approach computes a
(1+\epsilon)-approximately minimum s-t cut in time
\tilde{O}(m+n^{4/3}\eps^{-8/3}), which is the fastest known algorithm for this
problem as well. Previously, the best dependence on m and n was achieved by the
algorithm of Goldberg and Rao (J. ACM 1998), which can be used to compute
approximately maximum s-t flows in time \tilde{O}(m\sqrt{n}\epsilon^{-1}), and
approximately minimum s-t cuts in time \tilde{O}(m+n^{3/2}\epsilon^{-3})."
Aleksander Madry,Madry_Aleksander,arXiv:1008.1975,https://arxiv.org/abs/1008.1975,"Abstract:  We present a general method of designing fast approximation algorithms for
cut-based minimization problems in undirected graphs. In particular, we develop
a technique that given any such problem that can be approximated quickly on
trees, allows approximating it almost as quickly on general graphs while only
losing a poly-logarithmic factor in the approximation guarantee.
To illustrate the applicability of our paradigm, we focus our attention on
the undirected sparsest cut problem with general demands and the balanced
separator problem. By a simple use of our framework, we obtain poly-logarithmic
approximation algorithms for these problems that run in time close to linear.
The main tool behind our result is an efficient procedure that decomposes
general graphs into simpler ones while approximately preserving the cut-flow
structure. This decomposition is inspired by the cut-based graph decomposition
of Räcke that was developed in the context of oblivious routing schemes, as
well as, by the construction of the ultrasparsifiers due to Spielman and Teng
that was employed to preconditioning symmetric diagonally-dominant matrices."
Aleksander Madry,Madry_Aleksander,arXiv:1003.5907,https://arxiv.org/abs/1003.5907,"Abstract:  We combine the work of Garg and Konemann, and Fleischer with ideas from
dynamic graph algorithms to obtain faster (1-eps)-approximation schemes for
various versions of the multicommodity flow problem. In particular, if eps is
moderately small and the size of every number used in the input instance is
polynomially bounded, the running times of our algorithms match - up to
poly-logarithmic factors and some provably optimal terms - the Omega(mn)
flow-decomposition barrier for single-commodity flow."
Aleksander Madry,Madry_Aleksander,arXiv:0908.1448,https://arxiv.org/abs/0908.1448,"Abstract:  In this paper, we set forth a new algorithm for generating approximately
uniformly random spanning trees in undirected graphs. We show how to sample
from a distribution that is within a multiplicative $(1+\delta)$ of uniform in
expected time $\TO(m\sqrt{n}\log 1/\delta)$. This improves the sparse graph
case of the best previously known worst-case bound of $O(\min \{mn,
n^{2.376}\})$, which has stood for twenty years.
To achieve this goal, we exploit the connection between random walks on
graphs and electrical networks, and we use this to introduce a new approach to
the problem that integrates discrete random walk-based techniques with
continuous linear algebraic methods. We believe that our use of electrical
networks and sparse linear system solvers in conjunction with random walks and
combinatorial partitioning techniques is a useful paradigm that will find
further applications in algorithmic graph theory."
Aleksander Madry,Madry_Aleksander,arXiv:0812.5101,https://arxiv.org/abs/0812.5101,"Abstract:  We give a 7/9 - Approximation Algorithm for the Maximum Traveling Salesman
Problem."
Aleksander Madry,Madry_Aleksander,arXiv:quant-ph/0605150,https://arxiv.org/abs/quant-ph/0605150,"Abstract:  It is well known that unconditionally secure bit commitment is impossible
even in the quantum world. In this paper a weak variant of quantum bit
commitment, introduced independently by Aharonov et al. [STOC, 2000] and Hardy
and Kent [Phys. Rev. Lett. 92 (2004)] is investigated. In this variant, the
parties require some nonzero probability of detecting a cheating, i.e. if Bob,
who commits a bit b to Alice, changes his mind during the revealing phase then
Alice detects the cheating with a positive probability (we call this property
binding); and if Alice gains information about the committed bit before the
revealing phase then Bob discovers this with positive probability (sealing). In
our paper we give quantum bit commitment scheme that is simultaneously binding
and sealing and we show that if a cheating gives epsilon advantage to a
malicious Alice then Bob can detect the cheating with a probability
Omega(epsilon^2). If Bob cheats then Alice's probability of detecting the
cheating is greater than some fixed constant lambda>0. This improves the
probabilities of cheating detections shown by Hardy and Kent and the scheme by
Aharonov et al. who presented a protocol that is either binding or sealing, but
not simultaneously both. To construct a cheat sensitive quantum bit commitment
scheme we use a protocol for a weak quantum one-out-of-two oblivious transfer."
Thomas Magnanti,Magnanti_Thomas,arXiv:1202.2654,https://arxiv.org/abs/1202.2654,"Abstract:  We introduce an algorithm design technique for a class of combinatorial
optimization problems with concave costs. This technique yields a strongly
polynomial primal-dual algorithm for a concave cost problem whenever such an
algorithm exists for the fixed-charge counterpart of the problem. For many
practical concave cost problems, the fixed-charge counterpart is a well-studied
combinatorial optimization problem. Our technique preserves constant factor
approximation ratios, as well as ratios that depend only on certain problem
parameters, and exact algorithms yield exact algorithms.
Using our technique, we obtain a new 1.61-approximation algorithm for the
concave cost facility location problem. For inventory problems, we obtain a new
exact algorithm for the economic lot-sizing problem with general concave
ordering costs, and a 4-approximation algorithm for the joint replenishment
problem with general concave individual ordering costs."
Thomas Magnanti,Magnanti_Thomas,arXiv:1201.3148,https://arxiv.org/abs/1201.3148,"Abstract:  We study the problem of minimizing a nonnegative separable concave function
over a compact feasible set. We approximate this problem to within a factor of
1+epsilon by a piecewise-linear minimization problem over the same feasible
set. Our main result is that when the feasible set is a polyhedron, the number
of resulting pieces is polynomial in the input size of the polyhedron and
linear in 1/epsilon. For many practical concave cost problems, the resulting
piecewise-linear cost problem can be formulated as a well-studied discrete
optimization problem. As a result, a variety of polynomial-time exact
algorithms, approximation algorithms, and polynomial-time heuristics for
discrete optimization problems immediately yield fully polynomial-time
approximation schemes, approximation algorithms, and polynomial-time heuristics
for the corresponding concave cost problems.
We illustrate our approach on two problems. For the concave cost
multicommodity flow problem, we devise a new heuristic and study its
performance using computational experiments. We are able to approximately solve
significantly larger test instances than previously possible, and obtain
solutions on average within 4.27% of optimality. For the concave cost facility
location problem, we obtain a new 1.4991+epsilon approximation algorithm."
Roger Mark,Mark_Roger,arXiv:1901.07042,https://arxiv.org/abs/1901.07042,"Abstract:  Chest radiography is an extremely powerful imaging modality, allowing for a
detailed inspection of a patient's thorax, but requiring specialized training
for proper interpretation. With the advent of high performance general purpose
computer vision algorithms, the accurate automated analysis of chest
radiographs is becoming increasingly of interest to researchers. However, a key
challenge in the development of these techniques is the lack of sufficient
data. Here we describe MIMIC-CXR, a large dataset of 371,920 chest x-rays
associated with 227,943 imaging studies sourced from the Beth Israel Deaconess
Medical Center between 2011 - 2016. Each imaging study can pertain to one or
more images, but most often are associated with two images: a frontal view and
a lateral view. Images are provided with 14 labels derived from a natural
language processing tool applied to the corresponding free-text radiology
reports. All images have been de-identified to protect patient privacy. The
dataset is made freely available to facilitate and encourage a wide range of
research in medical computer vision."
Roger Mark,Mark_Roger,arXiv:1709.03562,https://arxiv.org/abs/1709.03562,"Abstract:  Research has shown that false alarms constitute more than 80% of the alarms
triggered in the intensive care unit (ICU). The high false arrhythmia alarm
rate has severe implications such as disruption of patient care, caregiver
alarm fatigue, and desensitization from clinical staff to real life-threatening
alarms. A method to reduce the false alarm rate would therefore greatly benefit
patients as well as nurses in their ability to provide care. We here develop
and describe a robust false arrhythmia alarm reduction system for use in the
ICU. Building off of work previously described in the literature, we make use
of signal processing and machine learning techniques to identify true and false
alarms for five arrhythmia types. This baseline algorithm alone is able to
perform remarkably well, with a sensitivity of 0.908, a specificity of 0.838,
and a PhysioNet/CinC challenge score of 0.756. We additionally explore dynamic
time warping techniques on both the entire alarm signal as well as on a
beat-by-beat basis in an effort to improve performance of ventricular
tachycardia, which has in the literature been one of the hardest arrhythmias to
classify. Such an algorithm with strong performance and efficiency could
potentially be translated for use in the ICU to promote overall patient care
and recovery."
Wojciech Matusik,Matusik_Wojciech,arXiv:1902.02752,https://arxiv.org/abs/1902.02752,"Abstract:  Motivated by the recent potential of mass customization brought by
whole-garment knitting machines, we introduce the new problem of automatic
machine instruction generation using a single image of the desired physical
product, which we apply to machine knitting. We propose to tackle this problem
by directly learning to synthesize regular machine instructions from real
images. We create a cured dataset of real samples with their instruction
counterpart and propose to use synthetic images to augment it in a novel way.
We theoretically motivate our data mixing framework and show empirical results
suggesting that making real images look more synthetic is beneficial in our
problem setup."
Wojciech Matusik,Matusik_Wojciech,arXiv:1810.01054,https://arxiv.org/abs/1810.01054,"Abstract:  Physical simulators have been widely used in robot planning and control.
Among them, differentiable simulators are particularly favored, as they can be
incorporated into gradient-based optimization algorithms that are efficient in
solving inverse problems such as optimal control and motion planning.
Simulating deformable objects is, however, more challenging compared to rigid
body dynamics. The underlying physical laws of deformable objects are more
complex, and the resulting systems have orders of magnitude more degrees of
freedom and therefore they are significantly more computationally expensive to
simulate. Computing gradients with respect to physical design or controller
parameters is typically even more computationally challenging. In this paper,
we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical
simulator for deformable objects, ChainQueen, based on the Moving Least Squares
Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects
including contact and can be seamlessly incorporated into inference, control
and co-design systems. We demonstrate that our simulator achieves high
precision in both forward simulation and backward gradient computation. We have
successfully employed it in a diverse set of control tasks for soft robots,
including problems with nearly 3,000 decision variables."
Wojciech Matusik,Matusik_Wojciech,arXiv:1810.00706,https://arxiv.org/abs/1810.00706,"Abstract:  We present the first algorithm for designing volumetric Michell Trusses. Our
method uses a parametrization approach to generate trusses made of structural
elements aligned with the primary direction of an object's stress field. Such
trusses exhibit high strength-to-weight ratios. We demonstrate the structural
robustness of our designs via a posteriori physical simulation. We believe our
algorithm serves as an important complement to existing structural optimization
tools and as a novel standalone design tool itself."
Wojciech Matusik,Matusik_Wojciech,arXiv:1809.03355,https://arxiv.org/abs/1809.03355,"Abstract:  We introduce a saliency-based distortion layer for convolutional neural
networks that helps to improve the spatial sampling of input data for a given
task. Our differentiable layer can be added as a preprocessing block to
existing task networks and trained altogether in an end-to-end fashion. The
effect of the layer is to efficiently estimate how to sample from the original
data in order to boost task performance. For example, for an image
classification task in which the original data might range in size up to
several megapixels, but where the desired input images to the task network are
much smaller, our layer learns how best to sample from the underlying high
resolution data in a manner which preserves task-relevant information better
than uniform downsampling. This has the effect of creating distorted,
caricature-like intermediate images, in which idiosyncratic elements of the
image that improve task performance are zoomed and exaggerated. Unlike
alternative approaches such as spatial transformer networks, our proposed layer
is inspired by image saliency, computed efficiently from uniformly downsampled
data, and degrades gracefully to a uniform sampling strategy under uncertainty.
We apply our layer to improve existing networks for the tasks of human gaze
estimation and fine-grained object classification. Code for our method is
available in: this http URL"
Wojciech Matusik,Matusik_Wojciech,arXiv:1805.05553,https://arxiv.org/abs/1805.05553,"Abstract:  In this paper, we study the associations between human faces and voices.
Audiovisual integration, specifically the integration of facial and vocal
information is a well-researched area in neuroscience. It is shown that the
overlapping information between the two modalities plays a significant role in
perceptual tasks such as speaker identification. Through an online study on a
new dataset we created, we confirm previous findings that people can associate
unseen faces with corresponding voices and vice versa with greater than chance
accuracy. We computationally model the overlapping information between faces
and voices and show that the learned cross-modal representation contains enough
information to identify matching faces and voices with performance similar to
that of humans. Our representation exhibits correlations to certain demographic
attributes and features obtained from either visual or aural modality alone. We
release our dataset of audiovisual recordings and demographic annotations of
people reading out short text used in our studies."
Wojciech Matusik,Matusik_Wojciech,arXiv:1804.02684,https://arxiv.org/abs/1804.02684,"Abstract:  Video motion magnification techniques allow us to see small motions
previously invisible to the naked eyes, such as those of vibrating airplane
wings, or swaying buildings under the influence of the wind. Because the motion
is small, the magnification results are prone to noise or excessive blurring.
The state of the art relies on hand-designed filters to extract representations
that may not be optimal. In this paper, we seek to learn the filters directly
from examples using deep convolutional neural networks. To make training
tractable, we carefully design a synthetic dataset that captures small motion
well, and use two-frame input for training. We show that the learned filters
achieve high-quality results on real videos, with less ringing artifacts and
better noise characteristics than previous methods. While our model is not
trained with temporal filters, we found that the temporal filters can be used
with our extracted representations up to a moderate magnification, enabling a
frequency-based motion selection. Finally, we analyze the learned filters and
show that they behave similarly to the derivative filters used in previous
works. Our code, trained model, and datasets will be available online."
Wojciech Matusik,Matusik_Wojciech,arXiv:1706.03189,https://arxiv.org/abs/1706.03189,"Abstract:  In this paper we present a novel two-scale framework to optimize the
structure and the material distribution of an object given its functional
specifications. Our approach utilizes multi-material microstructures as
low-level building blocks of the object. We start by precomputing the material
property gamut -- the set of bulk material properties that can be achieved with
all material microstructures of a given size. We represent the boundary of this
material property gamut using a level set field. Next, we propose an efficient
and general topology optimization algorithm that simultaneously computes an
optimal object topology and spatially-varying material properties constrained
by the precomputed gamut. Finally, we map the optimal spatially-varying
material properties onto the microstructures with the corresponding properties
in order to generate a high-resolution printable structure. We demonstrate the
efficacy of our framework by designing, optimizing, and fabricating objects in
different material property spaces on the level of a trillion voxels, i.e
several orders of magnitude higher than what can be achieved with current
systems."
Wojciech Matusik,Matusik_Wojciech,arXiv:1705.03737,https://arxiv.org/abs/1705.03737,"Abstract:  Single-image-based view generation (SIVG) is important for producing 3D
stereoscopic content. Here, handling different spatial resolutions as input and
optimizing both reconstruction accuracy and processing speed is desirable.
Latest approaches are based on convolutional neural network (CNN), and they
generate promising results. However, their use of fully connected layers as
well as pre-trained VGG forces a compromise between reconstruction accuracy and
processing speed. In addition, this approach is limited to the use of a
specific spatial resolution. To remedy these problems, we propose exploiting
fully convolutional networks (FCN) for SIVG. We present two FCN architectures
for SIVG. The first one is based on combination of an FCN and a view-rendering
network called DeepView$_{ren}$. The second one consists of decoupled networks
for luminance and chrominance signals, denoted by DeepView$_{dec}$. To train
our solutions we present a large dataset of 2M stereoscopic images. Results
show that both of our architectures improve accuracy and speed over the state
of the art. DeepView$_{ren}$ generates competitive accuracy to the state of the
art, however, with the fastest processing speed of all. That is x5 times faster
speed and x24 times lower memory consumption compared to the state of the art.
DeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and
x12 times lower memory consumption. We evaluated our approach with both
objective and subjective studies."
Wojciech Matusik,Matusik_Wojciech,arXiv:1705.03281,https://arxiv.org/abs/1705.03281,"Abstract:  Shot boundary detection (SBD) is an important pre-processing step for video
manipulation. Here, each segment of frames is classified as either sharp,
gradual or no transition. Current SBD techniques analyze hand-crafted features
and attempt to optimize both detection accuracy and processing speed. However,
the heavy computations of optical flow prevents this. To achieve this aim, we
present an SBD technique based on spatio-temporal Convolutional Neural Networks
(CNN). Since current datasets are not large enough to train an accurate SBD
CNN, we present a new dataset containing more than 3.5 million frames of sharp
and gradual transitions. The transitions are generated synthetically using
image compositing models. Our dataset contain additional 70,000 frames of
important hard-negative no transitions. We perform the largest evaluation to
date for one SBD algorithm, on real and synthetic data, containing more than
4.85 million frames. In comparison to the state of the art, we outperform
dissolve gradual detection, generate competitive performance for sharp
detections and produce significant improvement in wipes. In addition, we are up
to 11 times faster than the state of the art."
Wojciech Matusik,Matusik_Wojciech,arXiv:1606.05814,https://arxiv.org/abs/1606.05814,"Abstract:  From scientific research to commercial applications, eye tracking is an
important tool across many domains. Despite its range of applications, eye
tracking has yet to become a pervasive technology. We believe that we can put
the power of eye tracking in everyone's palm by building eye tracking software
that works on commodity hardware such as mobile phones and tablets, without the
need for additional sensors or devices. We tackle this problem by introducing
GazeCapture, the first large-scale dataset for eye tracking, containing data
from over 1450 people consisting of almost 2.5M frames. Using GazeCapture, we
train iTracker, a convolutional neural network for eye tracking, which achieves
a significant reduction in error over previous approaches while running in real
time (10-15fps) on a modern mobile device. Our model achieves a prediction
error of 1.71cm and 2.53cm without calibration on mobile phones and tablets
respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further,
we demonstrate that the features learned by iTracker generalize well to other
datasets, achieving state-of-the-art results. The code, data, and models are
available at this http URL."
Muriel Medard,Medard_Muriel,arXiv:1901.11102,https://arxiv.org/abs/1901.11102,"Abstract:  We propose a decentralized spatial soft-core cache placement (SSCC) policy
for wireless networks. SSCC yields a spatially balanced sampling via negative
dependence across caches, and can be tuned to satisfy cache size constraints
with high probability. Given a desired cache hit probability, we compare the
95% confidence intervals of the required cache sizes for independent placement,
hard-core placement and SSCC policies. We demonstrate that in terms of the
required cache storage size, SSCC can provide up to more than 180% and 100%
gains with respect to the independent and hard-core placement policies,
respectively. SSCC can be used to enable proximity-based applications such as
device-to-device communications and peer-to-peer networking as it promotes the
item diversity and reciprocation among the nodes."
Muriel Medard,Medard_Muriel,arXiv:1808.05647,https://arxiv.org/abs/1808.05647,"Abstract:  We present the computational wiretap channel: Alice has some data x and wants
to share some computation h(x) with Bob. To do this, she sends f(x), where f is
some sufficient statistic for h. An eavesdropper, Eve, is interested in
computing another function g(x). We show that, under some conditions on f and
g, this channel can be approximated, from Eve's point of view, by the classic
Wyner wiretap channel."
Muriel Medard,Medard_Muriel,arXiv:1808.03153,https://arxiv.org/abs/1808.03153,"Abstract:  We propose a cumulative feedback-based ARQ (CF ARQ) protocol for a sliding
window of size 2 over packet erasure channels with unreliable feedback. We
exploit a matrix signal-flow graph approach to analyze probability-generating
functions of transmission and delay times. Contrasting its performance with
that of the uncoded baseline scheme for ARQ, developed by Ausavapattanakun and
Nosratinia, we demonstrate that CF ARQ can provide significantly less average
delay under bursty feedback, and gains up to about 20% in terms of throughput.
We also outline the benefits of CF ARQ under burst errors and asymmetric
channel conditions. The protocol is more predictable across statistics, hence
is more stable. This can help design robust systems when feedback is
unreliable. This feature may be preferable for meeting the strict end-to-end
latency and reliability requirements of future use cases of ultra-reliable
low-latency communications in 5G, such as mission-critical communications and
industrial control for critical control messaging."
Muriel Medard,Medard_Muriel,arXiv:1806.05776,https://arxiv.org/abs/1806.05776,"Abstract:  Future 5G systems will need to support ultra-reliable low-latency
communications scenarios. From a latency-reliability viewpoint, it is
inefficient to rely on average utility-based system design. Therefore, we
introduce the notion of guaranteeable delay which is the average delay plus
three standard deviations of the mean. We investigate the trade-off between
guaranteeable delay and throughput for point-to-point wireless erasure links
with unreliable and delayed feedback, by bringing together signal flow
techniques to the area of coding. We use tiny codes, i.e. sliding window by
coding with just 2 packets, and design three variations of selective-repeat ARQ
protocols, by building on the baseline scheme, i.e. uncoded ARQ, developed by
Ausavapattanakun and Nosratinia: (i) Hybrid ARQ with soft combining at the
receiver; (ii) cumulative feedback-based ARQ without rate adaptation; and (iii)
Coded ARQ with rate adaptation based on the cumulative feedback. Contrasting
the performance of these protocols with uncoded ARQ, we demonstrate that HARQ
performs only slightly better, cumulative feedback-based ARQ does not provide
significant throughput while it has better average delay, and Coded ARQ can
provide gains up to about 40% in terms of throughput. Coded ARQ also provides
delay guarantees, and is robust to various challenges such as imperfect and
delayed feedback, burst erasures, and round-trip time fluctuations. This
feature may be preferable for meeting the strict end-to-end latency and
reliability requirements of future use cases of ultra-reliable low-latency
communications in 5G, such as mission-critical communications and industrial
control for critical control messaging."
Muriel Medard,Medard_Muriel,arXiv:1805.11666,https://arxiv.org/abs/1805.11666,"Abstract:  In September 2017, McAffee Labs quarterly report estimated that brute force
attacks represent 20% of total network attacks, making them the most prevalent
type of attack ex-aequo with browser based vulnerabilities. These attacks have
sometimes catastrophic consequences, and understanding their fundamental limits
may play an important role in the risk assessment of password-secured systems,
and in the design of better security protocols. While some solutions exist to
prevent online brute-force attacks that arise from one single IP address,
attacks performed by botnets are more challenging. In this paper, we analyze
these distributed attacks by using a simplified model. Our aim is to understand
the impact of distribution and asynchronization on the overall computational
effort necessary to breach a system. Our result is based on Guesswork, a
measure of the number of password queries (guesses) before the correct one is
found in an optimal attack, which is a direct surrogate for the time and the
computational effort. We model the lack of synchronization by a worst-case
optimization in which the queries are received in the worst possible order,
resulting in a min-max formulation. We show that even without synchronization
and for sequences of growing length, the asymptotic optimal performance is
achievable by using randomized guesses drawn from an appropriate distribution.
Therefore, randomization is key for distributed asynchronous attacks. In other
words, asynchronous guessers can asymptotically perform brute-force attacks as
efficiently as synchronized guessers."
Muriel Medard,Medard_Muriel,arXiv:1805.03727,https://arxiv.org/abs/1805.03727,"Abstract:  Atomicity or strong consistency is one of the fundamental, most intuitive,
and hardest to provide primitives in distributed shared memory emulations. To
ensure survivability, scalability, and availability of a storage service in the
presence of failures, traditional approaches for atomic memory emulation, in
message passing environments, replicate the objects across multiple servers.
Compared to replication based algorithms, erasure code-based atomic memory
algorithms has much lower storage and communication costs, but usually, they
are harder to design. The difficulty of designing atomic memory algorithms
further grows, when the set of servers may be changed to ensure survivability
of the service over software and hardware upgrades, while avoiding service
interruptions. Atomic memory algorithms for performing server reconfiguration,
in the replicated systems, are very few, complex, and are still part of an
active area of research; reconfigurations of erasure-code based algorithms are
non-existent.
In this work, we present ARES, an algorithmic framework that allows
reconfiguration of the underlying servers, and is particularly suitable for
erasure-code based algorithms emulating atomic objects. ARES introduces new
configurations while keeping the service available. To use with ARES we also
propose a new, and to our knowledge, the first two-round erasure code based
algorithm TREAS, for emulating multi-writer, multi-reader (MWMR) atomic objects
in asynchronous, message-passing environments, with near-optimal communication
and storage costs. Our algorithms can tolerate crash failures of any client and
some fraction of servers, and yet, guarantee safety and liveness property.
Moreover, by bringing together the advantages of ARES and TREAS, we propose an
optimized algorithm where new configurations can be installed without the
objects values passing through the reconfiguration clients."
Muriel Medard,Medard_Muriel,arXiv:1805.00396,https://arxiv.org/abs/1805.00396,"Abstract:  Motivated by applications to delivery of dynamically updated, but correlated
data in settings such as content distribution networks, and distributed file
sharing systems, we study a single source multiple destination network coded
multicast problem in a cache-aided network. We focus on models where the caches
are primarily located near the destinations, and where the source has no cache.
The source observes a sequence of correlated frames, and is expected to do
frame-by-frame encoding with no access to prior frames. We present a novel
scheme that shows how the caches can be advantageously used to decrease the
overall cost of multicast, even though the source encodes without access to
past data. Our cache design and update scheme works with any choice of network
code designed for a corresponding cache-less network, is largely decentralized,
and works for an arbitrary network. We study a convex relation of the
optimization problem that results form the overall cost function. The results
of the optimization problem determines the rate allocation and caching
strategies. Numerous simulation results are presented to substantiate the
theory developed."
Muriel Medard,Medard_Muriel,arXiv:1802.07010,https://arxiv.org/abs/1802.07010,"Abstract:  We introduce a new algorithm for realizing Maximum Likelihood (ML) decoding
in discrete channels with or without memory. In it, the receiver rank orders
noise sequences from most likely to least likely. Subtracting noise from the
received signal in that order, the first instance that results in a member of
the code-book is the ML decoding. We name this algorithm GRAND for Guessing
Random Additive Noise Decoding.
We establish that GRAND is capacity-achieving when used with random
code-books. For rates below capacity we identify error exponents, and for rates
beyond capacity we identify success exponents. We determine the scheme's
complexity in terms of the number of computations the receiver performs. For
rates beyond capacity, this reveals thresholds for the number of guesses by
which if a member of the code-book is identified it is likely to be the
transmitted code-word.
We introduce an approximate ML decoding scheme where the receiver abandons
the search after a fixed number of queries, an approach we dub GRANDAB, for
GRAND with ABandonment. While not an ML decoder, we establish that the
algorithm GRANDAB is also capacity-achieving for an appropriate choice of
abandonment threshold, and characterize its complexity, error and success
exponents. Worked examples are presented for Markovian noise that indicate
these decoding schemes substantially out-perform the brute force decoding
approach."
Muriel Medard,Medard_Muriel,arXiv:1801.10500,https://arxiv.org/abs/1801.10500,"Abstract:  We propose two schemes for selective-repeat ARQ protocols over packet erasure
channels with unreliable feedback: (i) a hybrid ARQ protocol with soft
combining at the receiver, and (ii) a coded ARQ protocol, by building on the
uncoded baseline scheme for ARQ, developed by Ausavapattanakun and Nosratinia.
Our method leverages discrete-time queuing and coding theory to analyze the
performance of the proposed data transmission methods. We incorporate forward
error-correction to reduce in-order delivery delay, and exploit a matrix
signal-flow graph approach to analyze the throughput and delay of the
protocols. We demonstrate and contrast the performance of the coded protocols
with that of the uncoded scheme, illustrating the benefits of coded
transmissions."
Muriel Medard,Medard_Muriel,arXiv:1801.09021,https://arxiv.org/abs/1801.09021,"Abstract:  Given a collection of strings, each with an associated probability of
occurrence, the guesswork of each of them is their position in a list ordered
from most likely to least likely, breaking ties arbitrarily. Guesswork is
central to several applications in information theory: Average guesswork
provides a lower bound on the expected computational cost of a sequential
decoder to decode successfully the transmitted message; the complementary
cumulative distribution function of guesswork gives the error probability in
list decoding; the logarithm of guesswork is the number of bits needed in
optimal lossless one-to-one source coding; and guesswork is the number of
trials required of an adversary to breach a password protected system in a
brute-force attack. In this paper, we consider memoryless string-sources that
generate strings consisting of i.i.d. characters drawn from a finite alphabet,
and characterize their corresponding guesswork. Our main tool is the tilt
operation. We show that the tilt operation on a memoryless string-source
parametrizes an exponential family of memoryless string-sources, which we refer
to as the tilted family. We provide an operational meaning to the tilted
families by proving that two memoryless string-sources result in the same
guesswork on all strings of all lengths if and only if their respective
categorical distributions belong to the same tilted family. Establishing some
general properties of the tilt operation, we generalize the notions of weakly
typical set and asymptotic equipartition property to tilted weakly typical sets
of different orders. We use this new definition to characterize the large
deviations for all atypical strings and characterize the volume of weakly
typical sets of different orders. We subsequently build on this
characterization to prove large deviation bounds on guesswork and provide an
accurate approximation of its PMF."
Muriel Medard,Medard_Muriel,arXiv:1801.04462,https://arxiv.org/abs/1801.04462,"Abstract:  Let $T_{\epsilon}$ be the noise operator acting on Boolean functions $f:\{0,
1\}^n\mapsto \{0, 1\}$, where $\epsilon\in[0, 1/2]$ is the noise parameter.
Given $\alpha\geq1$ and the mean $\mathbb{E} f$, which Boolean function $f$
maximizes the $\alpha$-th moment $\mathbb{E}(T_\epsilon f)^\alpha$? Our
findings are: in the weak noise scenario, i.e., $\epsilon$ is small, the
maximum is achieved by the lexicographic function; in the strong noise
scenario, i.e., $\epsilon$ is close to 1/2, the maximum is achieved by Boolean
functions with the largest degree-1 Fourier weight; and when $\alpha$ is a
large integer, among balanced Boolean functions, the maximum is achieved by any
function which is 0 on all strings with fewer than $n/2$ 1's. Moreover, for any
convex function $\Phi$, we show that the maximum of $\mathbb{E}\Phi(T_\epsilon
f)$ is achieved by some monotone function. Analogous results are established in
more general contexts, such as Boolean functions defined on the discrete torus
$(\mathbb{Z}/p\mathbb{Z})^n$, as well as noise stability in a tree model. We
also discuss the relationships between this noise stability problem and the
problem of non-interactive correlation distillation, as well as
Courtade-Kumar's conjecture on the most informative Boolean function."
Muriel Medard,Medard_Muriel,arXiv:1712.09082,https://arxiv.org/abs/1712.09082,"Abstract:  We consider an abstraction of computational security in password protected
systems where a user draws a secret string of given length with i.i.d.
characters from a finite alphabet, and an adversary would like to identify the
secret string by querying, or guessing, the identity of the string. The concept
of a ""total entropy budget"" on the chosen word by the user is natural,
otherwise the chosen password would have arbitrary length and complexity. One
intuitively expects that a password chosen from the uniform distribution is
more secure. This is not the case, however, if we are considering only the
average guesswork of the adversary when the user is subject to a total entropy
budget. The optimality of the uniform distribution for the user's secret string
holds when we have also a budget on the guessing adversary. We suppose that the
user is subject to a ""total entropy budget"" for choosing the secret string,
whereas the computational capability of the adversary is determined by his
""total guesswork budget."" We study the regime where the adversary's chances are
exponentially small in guessing the secret string chosen subject to a total
entropy budget. We introduce a certain notion of uniformity and show that a
more uniform source will provide better protection against the adversary in
terms of his chances of success in guessing the secret string. In contrast, the
average number of queries that it takes the adversary to identify the secret
string is smaller for the more uniform secret string subject to the same total
entropy budget."
Muriel Medard,Medard_Muriel,arXiv:1712.00658,https://arxiv.org/abs/1712.00658,"Abstract:  In this paper, we address the scheduling problem in wireless ad hoc networks
by exploiting the computational advantage that comes when such scheduling
problems can be represented by claw-free conflict graphs where we consider a
wireless broadcast medium. It is possible to formulate a scheduling problem of
network coded flows as finding maximum weighted independent set (MWIS) in the
conflict graph of the network. Finding MWIS of a general graph is NP-hard
leading to an NP-hard complexity of scheduling. In a claw-free conflict graph,
MWIS can be found in polynomial time leading to a throughput-optimal
scheduling. We show that the conflict graph of certain wireless ad hoc networks
are claw-free. In order to obtain claw-free conflict graphs in general
networks, we suggest introducing additional conflicts (edges) while keeping the
decrease in MWIS size minimal. To this end, we introduce an iterative
optimization problem to decide where to introduce edges and investigate its
efficient implementation. Besides, we exemplify some physical modifications to
manipulate the conflict graph of a network and also propose a mixed scheduling
strategy for specific networks. We conclude that claw breaking method by adding
extra edges can perform nearly optimal under the necessary assumptions."
Muriel Medard,Medard_Muriel,arXiv:1711.01620,https://arxiv.org/abs/1711.01620,"Abstract:  In this paper, we address the scheduling problem in wireless ad hoc networks
by exploiting the computational advantage that comes when such scheduling
problems can be represented by claw-free conflict graphs. It is possible to
formulate a scheduling problem of network coded flows as finding maximum
weighted independent set (MWIS) in the conflict graph of the network. We
consider activation of hyperedges in a hypergraph to model a wireless broadcast
medium. We show that the conflict graph of certain wireless ad hoc networks are
claw-free. It is known that finding MWIS of a general graph is NP-hard, but in
a claw-free conflict graph, it is possible to apply Minty's or Faenza et al.'s
algorithms in polynomial time. We discuss our approach on some sample networks."
Muriel Medard,Medard_Muriel,arXiv:1710.00447,https://arxiv.org/abs/1710.00447,"Abstract:  We study the central problem in data privacy: how to share data with an
analyst while providing both privacy and utility guarantees to the user that
owns the data. In this setting, we present an estimation-theoretic analysis of
the privacy-utility trade-off (PUT). Here, an analyst is allowed to reconstruct
(in a mean-squared error sense) certain functions of the data (utility), while
other private functions should not be reconstructed with distortion below a
certain threshold (privacy). We demonstrate how $\chi^2$-information captures
the fundamental PUT in this case and provide bounds for the best PUT. We
propose a convex program to compute privacy-assuring mappings when the
functions to be disclosed and hidden are known a priori and the data
distribution is known. We derive lower bounds on the minimum mean-squared error
of estimating a target function from the disclosed data and evaluate the
robustness of our approach when an empirical distribution is used to compute
the privacy-assuring mappings instead of the true data distribution. We
illustrate the proposed approach through two numerical experiments."
Muriel Medard,Medard_Muriel,arXiv:1708.05474,https://arxiv.org/abs/1708.05474,"Abstract:  We study the trade-off between storage overhead and inter-cluster repair
bandwidth in clustered storage systems, while recovering from multiple node
failures within a cluster. A cluster is a collection of $m$ nodes, and there
are $n$ clusters. For data collection, we download the entire content from any
$k$ clusters. For repair of $t \geq 2$ nodes within a cluster, we take help
from $\ell$ local nodes, as well as $d$ helper clusters. We characterize the
optimal trade-off under functional repair, and also under exact repair for the
minimum storage and minimum inter-cluster bandwidth (MBR) operating points. Our
bounds show the following interesting facts: $1)$ When $t|(m-\ell)$ the
trade-off is the same as that under $t=1$, and thus there is no advantage in
jointly repairing multiple nodes, $2)$ When $t \nmid (m-\ell)$, the optimal
file-size at the MBR point under exact repair can be strictly less than that
under functional repair. $3)$ Unlike the case of $t=1$, increasing the number
of local helper nodes does not necessarily increase the system capacity under
functional repair."
Muriel Medard,Medard_Muriel,arXiv:1707.02789,https://arxiv.org/abs/1707.02789,"Abstract:  As parallelism becomes critically important in the semiconductor technology,
high-performance computing, and cloud applications, parallel network systems
will increasingly follow suit. Today, parallelism is an essential architectural
feature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet
systems are equipped with multiple parallel network interfaces. This creates
new network topology abstractions and new technology requirements: instead of a
single high capacity network link, multiple Ethernet end-points and interfaces
need to be considered together with multiple links in form of discrete parallel
paths. This new paradigm is enabling implementations of various new features to
improve overall system performance. In this paper, we analyze the performance
of parallel network systems with network coding. In particular, by using random
LNC (RLNC), - a code without the need for decoding, we can make use of the fact
that we have codes that are both distributed (removing the need for
coordination or optimization of resources) and composable (without the need to
exchange code information), leading to a fully stateless operation. We propose
a novel theoretical modeling framework, including derivation of the upper and
lower bounds as well as an expected value of the differential delay of parallel
paths, and the resulting queue size at the receiver. The results show a great
promise of network system parallelism in combination with RLNC: with a proper
set of design parameters, the differential delay and the buffer size at the
Ethernet receiver can be reduced significantly, while the cross-layer design
and routing can be greatly simplified."
Muriel Medard,Medard_Muriel,arXiv:1706.05883,https://arxiv.org/abs/1706.05883,"Abstract:  This paper considers the problem of channel coding over Gaussian intersymbol
interference (ISI) channels with a given metric decoding rule. Specifically, it
is assumed that the mismatched decoder has an incorrect assumption on the
impulse response function. The mismatch capacity is the highest achievable rate
for a given decoding rule. Existing lower bounds to the mismatch capacity for
channels and decoding metrics with memory (as in our model) are presented only
in the form of multi-letter expressions that have not been calculated in
practice. Consequently, they provide little insight on the mismatch problem. In
this paper, we derive computable single-letter lower bounds to the mismatch
capacity, and discuss some implications of our results. Our achievable rates
are based on two ensembles, the ensemble of codewords generated by an
autoregressive process, and the ensemble of codewords drawn uniformly over a
""type class"" of real-valued sequences. Computation of our achievable rates
demonstrates non-trivial behavior of the achievable rates as a function of the
mismatched parameters. As a simple application of our technique, we derive also
the random coding exponent associated with a mismatched decoder which assumes
that there is no ISI at all. Finally, we compare our results with universal
decoders which are designed outside the true class of channels that we consider
in this paper."
Muriel Medard,Medard_Muriel,arXiv:1705.09372,https://arxiv.org/abs/1705.09372,"Abstract:  We study a notion of guesswork, where multiple agents intend to launch a
coordinated brute-force attack to find a single binary secret string, and each
agent has access to side information generated through either a BEC or a BSC.
The average number of trials required to find the secret string grows
exponentially with the length of the string, and the rate of the growth is
called the guesswork exponent. We compute the guesswork exponent for several
multi-agent attacks. We show that a multi-agent attack reduces the guesswork
exponent compared to a single agent, even when the agents do not exchange
information to coordinate their attack, and try to individually guess the
secret string using a predetermined scheme in a decentralized fashion. Further,
we show that the guesswork exponent of two agents who do coordinate their
attack is strictly smaller than that of any finite number of agents
individually performing decentralized guesswork."
Muriel Medard,Medard_Muriel,arXiv:1705.08040,https://arxiv.org/abs/1705.08040,"Abstract:  This work introduces the particle-intensity channel (PIC) as a model for
molecular communication systems and characterizes the properties of the optimal
input distribution and the capacity limits for this system. In the PIC, the
transmitter encodes information, in symbols of a given duration, based on the
number of particles released, and the receiver detects and decodes the message
based on the number of particles detected during the symbol interval. In this
channel, the transmitter may be unable to control precisely the number of
particles released, and the receiver may not detect all the particles that
arrive. We demonstrate that the optimal input distribution for this channel
always has mass points at zero and the maximum number of particles that can be
released. We then consider diffusive particle transport, derive the capacity
expression when the input distribution is binary, and show conditions under
which the binary input is capacity-achieving. In particular, we demonstrate
that when the transmitter cannot generate particles at a high rate, the optimal
input distribution is binary."
Muriel Medard,Medard_Muriel,arXiv:1704.00820,https://arxiv.org/abs/1704.00820,"Abstract:  We explore properties and applications of the Principal Inertia Components
(PICs) between two discrete random variables $X$ and $Y$. The PICs lie in the
intersection of information and estimation theory, and provide a fine-grained
decomposition of the dependence between $X$ and $Y$. Moreover, the PICs
describe which functions of $X$ can or cannot be reliably inferred (in terms of
MMSE) given an observation of $Y$. We demonstrate that the PICs play an
important role in information theory, and they can be used to characterize
information-theoretic limits of certain estimation problems. In privacy
settings, we prove that the PICs are related to fundamental limits of perfect
privacy."
Muriel Medard,Medard_Muriel,arXiv:1703.01286,https://arxiv.org/abs/1703.01286,"Abstract:  Motivated by emerging applications to the edge computing paradigm, we
introduce a two-layer erasure-coded fault-tolerant distributed storage system
offering atomic access for read and write operations. In edge computing,
clients interact with an edge-layer of servers that is geographically near; the
edge-layer in turn interacts with a back-end layer of servers. The edge-layer
provides low latency access and temporary storage for client operations, and
uses the back-end layer for persistent storage. Our algorithm, termed Layered
Data Storage (LDS) algorithm, offers several features suitable for
edge-computing systems, works under asynchronous message-passing environments,
supports multiple readers and writers, and can tolerate $f_1 < n_1/2$ and $f_2
< n_2/3$ crash failures in the two layers having $n_1$ and $n_2$ servers,
respectively. We use a class of erasure codes known as regenerating codes for
storage of data in the back-end layer. The choice of regenerating codes,
instead of popular choices like Reed-Solomon codes, not only optimizes the cost
of back-end storage, but also helps in optimizing communication cost of read
operations, when the value needs to be recreated all the way from the back-end.
The two-layer architecture permits a modular implementation of atomicity and
erasure-code protocols; the implementation of erasure-codes is mostly limited
to interaction between the two layers. We prove liveness and atomicity of LDS,
and also compute performance costs associated with read and write operations.
Further, in a multi-object system running $N$ independent instances of LDS,
where only a small fraction of the objects undergo concurrent accesses at any
point during the execution, the overall storage cost is dominated by that of
persistent storage in the back-end layer, and is given by $\Theta(N)$."
Muriel Medard,Medard_Muriel,arXiv:1702.03012,https://arxiv.org/abs/1702.03012,"Abstract:  The principal mission of Multi-Source Multicast (MSM) is to disseminate all
messages from all sources in a network to all destinations. MSM is utilized in
numerous applications. In many of them, securing the messages disseminated is
critical. A common secure model is to consider a network where there is an
eavesdropper which is able to observe a subset of the network links, and seek a
code which keeps the eavesdropper ignorant regarding all the messages. While
this is solved when all messages are located at a single source, Secure MSM
(SMSM) is an open problem, and the rates required are hard to characterize in
general. In this paper, we consider Individual Security, which promises that
the eavesdropper has zero mutual information with each message individually. We
completely characterize the rate region for SMSM under individual security, and
show that such a security level is achievable at the full capacity of the
network, that is, the cut-set bound is the matching converse, similar to
non-secure MSM. Moreover, we show that the field size is similar to non-secure
MSM and does not have to be larger due to the security constraint."
Muriel Medard,Medard_Muriel,arXiv:1701.04909,https://arxiv.org/abs/1701.04909,"Abstract:  We study a generalization of the setting of regenerating codes, motivated by
applications to storage systems consisting of clusters of storage nodes. There
are $n$ clusters in total, with $m$ nodes per cluster. A data file is coded and
stored across the $mn$ nodes, with each node storing $\alpha$ symbols. For
availability of data, we require that the file be retrievable by downloading
the entire content from any subset of $k$ clusters. Nodes represent entities
that can fail. We distinguish between intra-cluster and inter-cluster bandwidth
(BW) costs during node repair. Node-repair in a cluster is accomplished by
downloading $\beta$ symbols each from any set of $d$ other clusters, dubbed
remote helper clusters, and also up to $\alpha$ symbols each from any set of
$\ell$ surviving nodes, dubbed local helper nodes, in the host cluster. We
first identify the optimal trade-off between storage-overhead and inter-cluster
repair-bandwidth under functional repair, and also present optimal exact-repair
code constructions for a class of parameters. The new trade-off is strictly
better than what is achievable via space-sharing existing coding solutions,
whenever $\ell > 0$. We then obtain sharp lower bounds on the necessary
intra-cluster repair BW to achieve optimal trade-off. Our bounds reveal the
interesting fact that, while it is beneficial to increase the number of local
helper nodes $\ell$ in order to improve the storage-vs-inter-cluster-repair-BW
trade-off, increasing $\ell$ not only increases intra-cluster BW in the
host-cluster, but also increases the intra-cluster BW in the remote helper
clusters. We also analyze resilience of the clustered storage system against
passive eavesdropping by providing file-size bounds and optimal code
constructions."
Muriel Medard,Medard_Muriel,arXiv:1611.05356,https://arxiv.org/abs/1611.05356,"Abstract:  Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses."
Muriel Medard,Medard_Muriel,arXiv:1609.00424,https://arxiv.org/abs/1609.00424,"Abstract:  The capability of mobile devices to use multiple interfaces to support a
single session is becoming more prevalent. Prime examples include the desire to
implement WiFi offloading and the introduction of 5G. Furthermore, an
increasing fraction of Internet traffic is becoming delay sensitive. These two
trends drive the need to investigate methods that enable communication over
multiple parallel heterogeneous networks, while also ensuring that delay
constraints are met. This paper approaches these challenges using a multi-path
streaming code that uses forward error correction to reduce the in-order
delivery delay of packets in networks with poor link quality and transient
connectivity. A simple analysis is developed that provides a good approximation
of the in-order delivery delay. Furthermore, numerical results help show that
the delay penalty of communicating over multiple paths is insignificant when
considering the potential throughput gains obtained through the fusion of
multiple networks."
Muriel Medard,Medard_Muriel,arXiv:1606.07383,https://arxiv.org/abs/1606.07383,"Abstract:  Several significant models have been developed that enable the study of
diffusion of signals across biological, social and engineered networks. Within
these established frameworks, the inverse problem of identifying the source of
the propagated signal is challenging, owing to the numerous alternative
possibilities for signal progression through the network. In real world
networks, the challenge of determining sources is compounded as the true
propagation dynamics are typically unknown, and when they have been directly
measured, they rarely conform to the assumptions of any of the well-studied
models. In this paper we introduce a method called Network Infusion (NI) that
has been designed to circumvent these issues, making source inference practical
for large, complex real world networks. The key idea is that to infer the
source node in the network, full characterization of diffusion dynamics, in
many cases, may not be necessary. This objective is achieved by creating a
diffusion kernel that well-approximates standard diffusion models, but lends
itself to inversion, by design, via likelihood maximization or error
minimization. We apply NI for both single-source and multi-source diffusion,
for both single-snapshot and multi-snapshot observations, and for both
homogeneous and heterogeneous diffusion setups. We prove the mean-field
optimality of NI for different scenarios, and demonstrate its effectiveness
over several synthetic networks. Moreover, we apply NI to a real-data
application, identifying news sources in the Digg social network, and
demonstrate the effectiveness of NI compared to existing methods. Finally, we
propose an integrative source inference framework that combines NI with a
distance centrality-based method, which leads to a robust performance in cases
where the underlying dynamics are unknown."
Muriel Medard,Medard_Muriel,arXiv:1606.04789,https://arxiv.org/abs/1606.04789,"Abstract:  We introduce Network Maximal Correlation (NMC) as a multivariate measure of
nonlinear association among random variables. NMC is defined via an
optimization that infers transformations of variables by maximizing aggregate
inner products between transformed variables. For finite discrete and jointly
Gaussian random variables, we characterize a solution of the NMC optimization
using basis expansion of functions over appropriate basis functions. For finite
discrete variables, we propose an algorithm based on alternating conditional
expectation to determine NMC. Moreover we propose a distributed algorithm to
compute an approximation of NMC for large and dense graphs using graph
partitioning. For finite discrete variables, we show that the probability of
discrepancy greater than any given level between NMC and NMC computed using
empirical distributions decays exponentially fast as the sample size grows. For
jointly Gaussian variables, we show that under some conditions the NMC
optimization is an instance of the Max-Cut problem. We then illustrate an
application of NMC in inference of graphical model for bijective functions of
jointly Gaussian variables. Finally, we show NMC's utility in a data
application of learning nonlinear dependencies among genes in a cancer dataset."
Muriel Medard,Medard_Muriel,arXiv:1605.05717,https://arxiv.org/abs/1605.05717,"Abstract:  Erasure codes offer an efficient way to decrease storage and communication
costs while implementing atomic memory service in asynchronous distributed
storage systems. In this paper, we provide erasure-code-based algorithms having
the additional ability to perform background repair of crashed nodes. A repair
operation of a node in the crashed state is triggered externally, and is
carried out by the concerned node via message exchanges with other active nodes
in the system. Upon completion of repair, the node re-enters active state, and
resumes participation in ongoing and future read, write, and repair operations.
To guarantee liveness and atomicity simultaneously, existing works assume
either the presence of nodes with stable storage, or presence of nodes that
never crash during the execution. We demand neither of these; instead we
consider a natural, yet practical network stability condition $N1$ that only
restricts the number of nodes in the crashed/repair state during broadcast of
any message.
We present an erasure-code based algorithm $RADON_C$ that is always live, and
guarantees atomicity as long as condition $N1$ holds. In situations when the
number of concurrent writes is limited, $RADON_C$ has significantly improved
storage and communication cost over a replication-based algorithm $RADON_R$,
which also works under $N1$. We further show how a slightly stronger network
stability condition $N2$ can be used to construct algorithms that never violate
atomicity. The guarantee of atomicity comes at the expense of having an
additional phase during the read and write operations."
Muriel Medard,Medard_Muriel,arXiv:1605.01748,https://arxiv.org/abs/1605.01748,"Abstract:  Erasure codes are increasingly being studied in the context of implementing
atomic memory objects in large scale asynchronous distributed storage systems.
When compared with the traditional replication based schemes, erasure codes
have the potential of significantly lowering storage and communication costs
while simultaneously guaranteeing the desired resiliency levels. In this work,
we propose the Storage-Optimized Data-Atomic (SODA) algorithm for implementing
atomic memory objects in the multi-writer multi-reader setting. SODA uses
Maximum Distance Separable (MDS) codes, and is specifically designed to
optimize the total storage cost for a given fault-tolerance requirement. For
tolerating $f$ server crashes in an $n$-server system, SODA uses an $[n, k]$
MDS code with $k=n-f$, and incurs a total storage cost of $\frac{n}{n-f}$. SODA
is designed under the assumption of reliable point-to-point communication
channels. The communication cost of a write and a read operation are
respectively given by $O(f^2)$ and $\frac{n}{n-f}(\delta_w+1)$, where
$\delta_w$ denotes the number of writes that are concurrent with the particular
read. In comparison with the recent CASGC algorithm, which also uses MDS codes,
SODA offers lower storage cost while pays more on the communication cost.
We also present a modification of SODA, called SODA$_{\text{err}}$, to handle
the case where some of the servers can return erroneous coded elements during a
read operation. Specifically, in order to tolerate $f$ server failures and $e$
error-prone coded elements, the SODA$_{\text{err}}$ algorithm uses an $[n, k]$
MDS code such that $k=n-2e-f$. SODA$_{\text{err}}$ also guarantees liveness and
atomicity, while maintaining an optimized total storage cost of
$\frac{n}{n-f-2e}$."
Muriel Medard,Medard_Muriel,arXiv:1605.01105,https://arxiv.org/abs/1605.01105,"Abstract:  We consider a communication problem in which an update of the source message
needs to be conveyed to one or more distant receivers that are interested in
maintaining specific linear functions of the source message. The setting is one
in which the updates are sparse in nature, and where neither the source nor the
receiver(s) is aware of the exact {\em difference vector}, but only know the
amount of sparsity that is present in the difference-vector. Under this
setting, we are interested in devising linear encoding and decoding schemes
that minimize the communication cost involved. We show that the optimal
solution to this problem is closely related to the notion of maximally
recoverable codes (MRCs), which were originally introduced in the context of
coding for storage systems. In the context of storage, MRCs guarantee optimal
erasure protection when the system is partially constrained to have local
parity relations among the storage nodes. In our problem, we show that optimal
solutions exist if and only if MRCs of certain kind (identified by the desired
linear functions) exist. We consider point-to-point and broadcast versions of
the problem, and identify connections to MRCs under both these settings. For
the point-to-point setting, we show that our linear-encoder based achievable
scheme is optimal even when non-linear encoding is permitted. The theory is
illustrated in the context of updating erasure coded storage nodes. We present
examples based on modern storage codes such as the minimum bandwidth
regenerating codes."
Muriel Medard,Medard_Muriel,arXiv:1604.03877,https://arxiv.org/abs/1604.03877,"Abstract:  Consider two correlated sources $X$ and $Y$ generated from a joint
distribution $p_{X,Y}$. Their Gács-Körner Common Information, a measure of
common information that exploits the combinatorial structure of the
distribution $p_{X,Y}$, leads to a source decomposition that exhibits the
latent common parts in $X$ and $Y$. Using this source decomposition we
construct an efficient distributed compression scheme, which can be efficiently
used in the network setting as well. Then, we relax the combinatorial
conditions on the source distribution, which results in an efficient scheme
with a helper node, which can be thought of as a front-end cache. This
relaxation leads to an inherent trade-off between the rate of the helper and
the rate reduction at the sources, which we capture by a notion of optimal
decomposition. We formulate this as an approximate Gács-Körner
optimization. We then discuss properties of this optimization, and provide
connections with the maximal correlation coefficient, as well as an efficient
algorithm, both through the application of spectral graph theory to the induced
bipartite graph of $p_{X,Y}$."
Muriel Medard,Medard_Muriel,arXiv:1602.04181,https://arxiv.org/abs/1602.04181,"Abstract:  Graph alignment refers to the problem of finding a bijective mapping across
vertices of two graphs such that, if two nodes are connected in the first
graph, their images are connected in the second graph. This problem arises in
many fields such as computational biology, social sciences, and computer vision
and is often cast as a quadratic assignment problem (QAP). Most standard graph
alignment methods consider an optimization that maximizes the number of matches
between the two graphs, ignoring the effect of mismatches. We propose a
generalized graph alignment formulation that considers both matches and
mismatches in a standard QAP formulation. This modification can have a major
impact in aligning graphs with different sizes and heterogenous edge densities.
Moreover, we propose two methods for solving the generalized graph alignment
problem based on spectral decomposition of matrices. We compare the performance
of proposed methods with some existing graph alignment algorithms including
Natalie2, GHOST, IsoRank, NetAlign, Klau's approach as well as a semidefinite
programming-based method over various synthetic and real graph models. Our
proposed method based on simultaneous alignment of multiple eigenvectors leads
to consistently good performance in different graph models. In particular, in
the alignment of regular graph structures which is one of the most difficult
graph alignment cases, our proposed method significantly outperforms other
methods."
Muriel Medard,Medard_Muriel,arXiv:1601.06882,https://arxiv.org/abs/1601.06882,"Abstract:  Consider a multi-source network coding problem with correlated sources. While
the fundamental limits are known, achieving them, in general, involves a
computational burden due to the complex decoding process. Efficient solutions,
on the other hand, are by large based on source and network coding separation,
thus imposing strict topological constraints on the networks which can be
solved.
In this work, we introduce a novel notion of separation of source and network
coding using Gács-Körner Common Information (CI). Unlike existing notions
of separation, the sufficient condition for this separation to hold depends on
the source structure rather than the network topology. Using the suggested
separation scheme, we tackle three important multi-source problems. The first
is the multi-source multicast. We construct efficient, zero error source codes,
and via properties of the CI completely characterize the resulting rate region.
The second is broadcast with side information. We establish a duality between
this problem and the classical problem of degraded message set broadcast, and
give two code constructions and their associated regions. Finally, we consider
the Ahlswede-Korner problem in a network, and give an efficient solution which
is tight under the CI constraints."
Muriel Medard,Medard_Muriel,arXiv:1601.04504,https://arxiv.org/abs/1601.04504,"Abstract:  The problem of storing permutations in a distributed manner arises in several
common scenarios, such as efficient updates of a large, encrypted, or
compressed data set. This problem may be addressed in either a combinatorial or
a coding approach. The former approach boils down to presenting large sets of
permutations with \textit{locality}, that is, any symbol of the permutation can
be computed from a small set of other symbols. In the latter approach, a
permutation may be coded in order to achieve locality. This paper focuses on
the combinatorial approach.
We provide upper and lower bounds for the maximal size of a set of
permutations with locality, and provide several simple constructions which
attain the upper bound. In cases where the upper bound is not attained, we
provide alternative constructions using Reed-Solomon codes, permutation
polynomials, and multi-permutations."
Muriel Medard,Medard_Muriel,arXiv:1510.00850,https://arxiv.org/abs/1510.00850,"Abstract:  A latent space model for a family of random graphs assigns real-valued
vectors to nodes of the graph such that edge probabilities are determined by
latent positions. Latent space models provide a natural statistical framework
for graph visualizing and clustering. A latent space model of particular
interest is the Random Dot Product Graph (RDPG), which can be fit using an
efficient spectral method; however, this method is based on a heuristic that
can fail, even in simple cases. Here, we consider a closely related latent
space model, the Logistic RDPG, which uses a logistic link function to map from
latent positions to edge likelihoods. Over this model, we show that
asymptotically exact maximum likelihood inference of latent position vectors
can be achieved using an efficient spectral method. Our method involves
computing top eigenvectors of a normalized adjacency matrix and scaling
eigenvectors using a regression step. The novel regression scaling step is an
essential part of the proposed method. In simulations, we show that our
proposed method is more accurate and more robust than common practices. We also
show the effectiveness of our approach over standard real networks of the
karate club and political blogs."
Muriel Medard,Medard_Muriel,arXiv:1509.00167,https://arxiv.org/abs/1509.00167,"Abstract:  We consider use of FEC to reduce in-order delivery delay over packet erasure
channels. We propose a class of streaming codes that is capacity achieving and
provides a superior throughput-delay trade-off compared to block codes by
introducing flexibility in where and when redundancy is placed. This
flexibility results in significantly lower in-order delay for a given
throughput for a wide range of network scenarios. Furthermore, a major
contribution of this paper is the combination of queuing and coding theory to
analyze the code's performance. Finally, we present simulation and experimental
results illustrating the code's benefits."
Muriel Medard,Medard_Muriel,arXiv:1507.05941,https://arxiv.org/abs/1507.05941,"Abstract:  Information theory is rapidly approaching its 70th birthday. What are
promising future directions for research in information theory? Where will
information theory be having the most impact in 10-20 years? What new and
emerging areas are ripe for the most impact, of the sort that information
theory has had on the telecommunications industry over the last 60 years? How
should the IEEE Information Theory Society promote high-risk new research
directions and broaden the reach of information theory, while continuing to be
true to its ideals and insisting on the intellectual rigor that makes its
breakthroughs so powerful? These are some of the questions that an ad hoc
committee (composed of the present authors) explored over the past two years.
We have discussed and debated these questions, and solicited detailed inputs
from experts in fields including genomics, biology, economics, and
neuroscience. This report is the result of these discussions."
Muriel Medard,Medard_Muriel,arXiv:1506.06154,https://arxiv.org/abs/1506.06154,"Abstract:  Satellite networks provide unique challenges that can restrict users' quality
of service. For example, high packet erasure rates and large latencies can
cause significant disruptions to applications such as video streaming or
voice-over-IP. Network coding is one promising technique that has been shown to
help improve performance, especially in these environments. However,
implementing any form of network code can be challenging. This paper will use
an example of a generation-based network code and a sliding-window network code
to help highlight the benefits and drawbacks of using one over the other.
In-order packet delivery delay, as well as network efficiency, will be used as
metrics to help differentiate between the two approaches. Furthermore, lessoned
learned during the course of our research will be provided in an attempt to
help the reader understand when and where network coding provides its benefits."
Muriel Medard,Medard_Muriel,arXiv:1506.01048,https://arxiv.org/abs/1506.01048,"Abstract:  Conventional TCP performance is significantly impaired under long latency
and/or constrained bandwidth. While small Pacific Island states on satellite
links experience this in the extreme, small populations and remoteness often
rule out submarine fibre connections and their communities struggle to reap the
benefits of the Internet. Network-coded TCP (TCP/NC) can increase goodput under
high latency and packet loss, but has not been used to tunnel conventional TCP
and UDP across satellite links before. We report on a feasibility study aimed
at determining expected goodput gain across such TCP/NC tunnels into island
targets on geostationary and medium earth orbit satellite links."
Muriel Medard,Medard_Muriel,arXiv:1503.08513,https://arxiv.org/abs/1503.08513,"Abstract:  We present information-theoretic definitions and results for analyzing
symmetric-key encryption schemes beyond the perfect secrecy regime, i.e. when
perfect secrecy is not attained. We adopt two lines of analysis, one based on
lossless source coding, and another akin to rate-distortion theory. We start by
presenting a new information-theoretic metric for security, called symbol
secrecy, and derive associated fundamental bounds. We then introduce
list-source codes (LSCs), which are a general framework for mapping a key
length (entropy) to a list size that an eavesdropper has to resolve in order to
recover a secret message. We provide explicit constructions of LSCs, and
demonstrate that, when the source is uniformly distributed, the highest level
of symbol secrecy for a fixed key length can be achieved through a construction
based on minimum-distance separable (MDS) codes. Using an analysis related to
rate-distortion theory, we then show how symbol secrecy can be used to
determine the probability that an eavesdropper correctly reconstructs functions
of the original plaintext. We illustrate how these bounds can be applied to
characterize security properties of symmetric-key encryption schemes, and, in
particular, extend security claims based on symbol secrecy to a functional
setting."
Muriel Medard,Medard_Muriel,arXiv:1502.07830,https://arxiv.org/abs/1502.07830,"Abstract:  A client/encoder edits a file, as modeled by an insertion-deletion (InDel)
process. An old copy of the file is stored remotely at a data-centre/decoder,
and is also available to the client. We consider the problem of throughput- and
computationally-efficient communication from the client to the data-centre, to
enable the server to update its copy to the newly edited file. We study two
models for the source files/edit patterns: the random pre-edit sequence
left-to-right random InDel (RPES-LtRRID) process, and the arbitrary pre-edit
sequence arbitrary InDel (APES-AID) process. In both models, we consider the
regime in which the number of insertions/deletions is a small (but constant)
fraction of the original file. For both models we prove information-theoretic
lower bounds on the best possible compression rates that enable file updates.
Conversely, our compression algorithms use dynamic programming (DP) and entropy
coding, and achieve rates that are approximately optimal."
Muriel Medard,Medard_Muriel,arXiv:1502.06601,https://arxiv.org/abs/1502.06601,"Abstract:  For general connections, the problem of finding network codes and optimizing
resources for those codes is intrinsically difficult and little is known about
its complexity. Most of the existing solutions rely on very restricted classes
of network codes in terms of the number of flows allowed to be coded together,
and are not entirely distributed. In this paper, we consider a new method for
constructing linear network codes for general connections of continuous flows
to minimize the total cost of edge use based on mixing. We first formulate the
minimumcost network coding design problem. To solve the optimization problem,
we propose two equivalent alternative formulations with discrete mixing and
continuous mixing, respectively, and develop distributed algorithms to solve
them. Our approach allows fairly general coding across flows and guarantees no
greater cost than any solution without network coding."
Muriel Medard,Medard_Muriel,arXiv:1502.06321,https://arxiv.org/abs/1502.06321,"Abstract:  The problem of finding network codes for general connections is inherently
difficult in capacity constrained networks. Resource minimization for general
connections with network coding is further complicated. Existing methods for
identifying solutions mainly rely on highly restricted classes of network
codes, and are almost all centralized. In this paper, we introduce linear
network mixing coefficients for code constructions of general connections that
generalize random linear network coding (RLNC) for multicast connections. For
such code constructions, we pose the problem of cost minimization for the
subgraph involved in the coding solution and relate this minimization to a
path-based Constraint Satisfaction Problem (CSP) and an edge-based CSP. While
CSPs are NP-complete in general, we present a path-based probabilistic
distributed algorithm and an edge-based probabilistic distributed algorithm
with almost sure convergence in finite time by applying Communication Free
Learning (CFL). Our approach allows fairly general coding across flows,
guarantees no greater cost than routing, and shows a possible distributed
implementation. Numerical results illustrate the performance improvement of our
approach over existing methods."
Muriel Medard,Medard_Muriel,arXiv:1502.00656,https://arxiv.org/abs/1502.00656,"Abstract:  In this paper, we study the wireline two-unicast-Z communication network over
directed acyclic graphs. The two-unicast-Z network is a two-unicast network
where the destination intending to decode the second message has apriori side
information of the first message. We make three contributions in this paper:
1. We describe a new linear network coding algorithm for two-unicast-Z
networks over directed acyclic graphs. Our approach includes the idea of
interference alignment as one of its key ingredients. For graphs of a bounded
degree, our algorithm has linear complexity in terms of the number of vertices,
and polynomial complexity in terms of the number of edges.
2. We prove that our algorithm achieves the rate-pair (1, 1) whenever it is
feasible in the network. Our proof serves as an alternative, albeit restricted
to two-unicast-Z networks over directed acyclic graphs, to an earlier result of
Wang et al. which studied necessary and sufficient conditions for feasibility
of the rate pair (1, 1) in two-unicast networks.
3. We provide a new proof of the classical max-flow min-cut theorem for
directed acyclic graphs."
Muriel Medard,Medard_Muriel,arXiv:1501.04905,https://arxiv.org/abs/1501.04905,"Abstract:  In non-coherent wideband fading channels where energy rather than spectrum is
the limiting resource, peaky and non-peaky signaling schemes have long been
considered species apart, as the first approaches asymptotically the capacity
of a wideband AWGN channel with the same average SNR, whereas the second
reaches a peak rate at some finite critical bandwidth and then falls to zero as
bandwidth grows to infinity. In this paper it is shown that this distinction is
in fact an artifact of the limited attention paid in the past to the product
between the bandwidth and the fraction of time it is in use. This fundamental
quantity, called bandwidth occupancy, measures average bandwidth usage over
time. For all signaling schemes with the same bandwidth occupancy, achievable
rates approach to the wideband AWGN capacity within the same gap as the
bandwidth occupancy approaches its critical value, and decrease to zero as the
occupancy goes to infinity. This unified analysis produces quantitative
closed-form expressions for the ideal bandwidth occupancy, recovers the
existing capacity results for (non-)peaky signaling schemes, and unveils a
trade-off between the accuracy of approximating capacity with a generalized
Taylor polynomial and the accuracy with which the optimal bandwidth occupancy
can be bounded."
Muriel Medard,Medard_Muriel,arXiv:1408.1440,https://arxiv.org/abs/1408.1440,"Abstract:  A large number of streaming applications use reliable transport protocols
such as TCP to deliver content over the Internet. However, head-of-line
blocking due to packet loss recovery can often result in unwanted behavior and
poor application layer performance. Transport layer coding can help mitigate
this issue by helping to recover from lost packets without waiting for
retransmissions. We consider the use of an on-line network code that inserts
coded packets at strategic locations within the underlying packet stream. If
retransmissions are necessary, additional coding packets are transmitted to
ensure the receiver's ability to decode. An analysis of this scheme is provided
that helps determine both the expected in-order packet delivery delay and its
variance. Numerical results are then used to determine when and how many coded
packets should be inserted into the packet stream, in addition to determining
the trade-offs between reducing the in-order delay and the achievable rate. The
analytical results are finally compared with experimental results to provide
insight into how to minimize the delay of existing transport layer protocols."
Muriel Medard,Medard_Muriel,arXiv:1407.4167,https://arxiv.org/abs/1407.4167,"Abstract:  This paper considers the communication and storage costs of emulating atomic
(linearizable) multi-writer multi-reader shared memory in distributed
message-passing systems. The paper contains three main contributions: (1) We
present a atomic shared-memory emulation algorithm that we call Coded Atomic
Storage (CAS). This algorithm uses erasure coding methods. In a storage system
with $N$ servers that is resilient to $f$ server failures, we show that the
communication cost of CAS is $\frac{N}{N-2f}$. The storage cost of CAS is
unbounded. (2) We present a modification of the CAS algorithm known as CAS with
Garbage Collection (CASGC). The CASGC algorithm is parametrized by an integer
$\delta$ and has a bounded storage cost. We show that in every execution where
the number of write operations that are concurrent with a read operation is no
bigger than $\delta$, the CASGC algorithm with parameter $\delta$ satisfies
atomicity and liveness. We explicitly characterize the storage cost of CASGC,
and show that it has the same communication cost as CAS. (3) We describe an
algorithm known as the Communication Cost Optimal Atomic Storage (CCOAS)
algorithm that achieves a smaller communication cost than CAS and CASGC. In
particular, CCOAS incurs read and write communication costs of $\frac{N}{N-f}$
measured in terms of number of object values. We also discuss drawbacks of
CCOAS as compared with CAS and CASGC."
Muriel Medard,Medard_Muriel,arXiv:1406.5786,https://arxiv.org/abs/1406.5786,"Abstract:  Coding techniques may be useful for data center data survivability as well as
for reducing traffic congestion. We present a queued cross-bar network (QCN)
method that can be used for traffic analysis of both replication/uncoded and
coded storage systems. We develop a framework for generating QCN rate regions
(RRs) by analyzing their conflict graph stable set polytopes (SSPs). In doing
so, we apply recent results from graph theory on the characterization of
particular graph SSPs. We characterize the SSP of QCN conflict graphs under a
variety of traffic patterns, allowing for their efficient RR computation. For
uncoded systems, we show how to compute RRs and find rate optimal scheduling
algorithms. For coded storage, we develop a RR upper bound, for which we
provide an intuitive interpretation. We show that the coded storage RR upper
bound is achievable in certain coded systems in which drives store sufficient
coded information, as well in certain dynamic coding systems. Numerical
illustrations show that coded storage can result in gains in RR volume of
approximately 50%, averaged across traffic patterns."
Muriel Medard,Medard_Muriel,arXiv:1405.5024,https://arxiv.org/abs/1405.5024,"Abstract:  The Guesswork problem was originally motivated by a desire to quantify
computational security for single user systems. Leveraging recent results from
its analysis, we extend the remit and utility of the framework to the
quantification of the computational security for multi-user systems. In
particular, assume that $V$ users independently select strings stochastically
from a finite, but potentially large, list. An inquisitor who does not know
which strings have been selected wishes to identify $U$ of them. The inquisitor
knows the selection probabilities of each user and is equipped with a method
that enables the testing of each (user, string) pair, one at a time, for
whether that string had been selected by that user.
Here we establish that, unless $U=V$, there is no general strategy that
minimizes the distribution of the number of guesses, but in the asymptote as
the strings become long we prove the following: by construction, there is an
asymptotically optimal class of strategies; the number of guesses required in
an asymptotically optimal strategy satisfies a large deviation principle with a
rate function, which is not necessarily convex, that can be determined from the
rate functions of optimally guessing individual users' strings; if all user's
selection statistics are identical, the exponential growth rate of the average
guesswork as the string-length increases is determined by the specific Rényi
entropy of the string-source with parameter $(V-U+1)/(V-U+2)$, generalizing the
known $V=U=1$ case; and that the Shannon entropy of the source is a lower bound
on the average guesswork growth rate for all $U$ and $V$, thus providing a
bound on computational security for multi-user systems. Examples are presented
to illustrate these results and their ramifications for systems design."
Muriel Medard,Medard_Muriel,arXiv:1405.2458,https://arxiv.org/abs/1405.2458,"Abstract:  We present a heuristic for designing vector non-linear network codes for
non-multicast networks, which we call quasi-linear network codes. The method
presented has two phases: finding an approximate linear network code over the
reals, and then quantizing it to a vector non-linear network code using a
fixed-point representation. Apart from describing the method, we draw some
links between some network parameters and the rate of the resulting code."
Muriel Medard,Medard_Muriel,arXiv:1405.1472,https://arxiv.org/abs/1405.1472,"Abstract:  The principal inertia components of the joint distribution of two random
variables $X$ and $Y$ are inherently connected to how an observation of $Y$ is
statistically related to a hidden variable $X$. In this paper, we explore this
connection within an information theoretic framework. We show that, under
certain symmetry conditions, the principal inertia components play an important
role in estimating one-bit functions of $X$, namely $f(X)$, given an
observation of $Y$. In particular, the principal inertia components bear an
interpretation as filter coefficients in the linear transformation of
$p_{f(X)|X}$ into $p_{f(X)|Y}$. This interpretation naturally leads to the
conjecture that the mutual information between $f(X)$ and $Y$ is maximized when
all the principal inertia components have equal value. We also study the role
of the principal inertia components in the Markov chain $B\rightarrow
X\rightarrow Y\rightarrow \widehat{B}$, where $B$ and $\widehat{B}$ are binary
random variables. We illustrate our results for the setting where $X$ and $Y$
are binary strings and $Y$ is the result of sending $X$ through an additive
noise binary channel."
Muriel Medard,Medard_Muriel,arXiv:1402.3074,https://arxiv.org/abs/1402.3074,"Abstract:  We consider scheduling strategies for point-to-multipoint (PMP) storage area
networks (SANs) that use network coded storage (NCS). In particular, we present
a simple SAN system model, two server scheduling algorithms for PMP networks,
and analytical expressions for internal and external blocking probability. We
point to select scheduling advantages in NCS systems under normal operating
conditions, where content requests can be temporarily denied owing to finite
system capacity from drive I/O access or storage redundancy limitations. NCS
can lead to improvements in throughput and blocking probability due to
increased immediate scheduling options, and complements other well documented
NCS advantages such as regeneration, and can be used as a guide for future
storage system design."
Muriel Medard,Medard_Muriel,arXiv:1402.1774,https://arxiv.org/abs/1402.1774,"Abstract:  We focus on the privacy-utility trade-off encountered by users who wish to
disclose some information to an analyst, that is correlated with their private
data, in the hope of receiving some utility. We rely on a general privacy
statistical inference framework, under which data is transformed before it is
disclosed, according to a probabilistic privacy mapping. We show that when the
log-loss is introduced in this framework in both the privacy metric and the
distortion metric, the privacy leakage and the utility constraint can be
reduced to the mutual information between private data and disclosed data, and
between non-private data and disclosed data respectively. We justify the
relevance and generality of the privacy metric under the log-loss by proving
that the inference threat under any bounded cost function can be upper-bounded
by an explicit function of the mutual information between private data and
disclosed data. We then show that the privacy-utility tradeoff under the
log-loss can be cast as the non-convex Privacy Funnel optimization, and we
leverage its connection to the Information Bottleneck, to provide a greedy
algorithm that is locally optimal. We evaluate its performance on the US census
dataset."
Muriel Medard,Medard_Muriel,arXiv:1402.1761,https://arxiv.org/abs/1402.1761,"Abstract:  An intuitive overview of the scalability of a variety of types of wireless
networks is presented. Simple heuris- tic arguments are demonstrated here for
scaling laws presented in other works, as well as for conditions not previously
considered in the literature. Unicast and multicast messages, topology,
hierarchy, and effects of reliability protocols are discussed. We show how two
key factors, bottlenecks and erasures, can often domi- nate the network scaling
behavior. Scaling of through- put or delay with the number of transmitting
nodes, the number of receiving nodes, and the file size is described."
Muriel Medard,Medard_Muriel,arXiv:1401.6670,https://arxiv.org/abs/1401.6670,"Abstract:  In this paper we close the gap between end-to-end diversity coding and
intra-session network coding for unicast connections resilient against single
link failures. In particular, we show that coding operations are sufficient to
perform at the source and receiver if the user data can be split into at most
two parts over the filed GF(2). Our proof is purely combinatorial and based on
standard graph and network flow techniques. It is a linear time construction
that defines the route of subflows A, B and A+B between the source and
destination nodes. The proposed resilient flow decomposition method generalizes
the 1+1 protection and the end-to-end diversity coding approaches while keeping
both of their benefits. It provides a simple yet resource efficient protection
method feasible in 2-connected backbone topologies. Since the core switches do
not need to be modified, this result can bring benefits to current transport
networks."
Muriel Medard,Medard_Muriel,arXiv:1401.4189,https://arxiv.org/abs/1401.4189,"Abstract:  The framework of network equivalence theory developed by Koetter et al.
introduces a notion of channel emulation to construct noiseless networks as
upper (resp. lower) bounding models, which can be used to calculate the outer
(resp. inner) bounds for the capacity region of the original noisy network.
Based on the network equivalence framework, this paper presents scalable upper
and lower bounding models for wireless networks with potentially many nodes. A
channel decoupling method is proposed to decompose wireless networks into
decoupled multiple-access channels (MACs) and broadcast channels (BCs). The
upper bounding model, consisting of only point-to-point bit pipes, is
constructed by firstly extending the ""one-shot"" upper bounding models developed
by Calmon et al. and then integrating them with network equivalence tools. The
lower bounding model, consisting of both point-to-point and point-to-points bit
pipes, is constructed based on a two-step update of the lower bounding models
to incorporate the broadcast nature of wireless transmission. The main
advantages of the proposed methods are their simplicity and the fact that they
can be extended easily to large networks with a complexity that grows linearly
with the number of nodes. It is demonstrated that the resulting upper and lower
bounds can approach the capacity in some setups."
Muriel Medard,Medard_Muriel,arXiv:1401.0543,https://arxiv.org/abs/1401.0543,"Abstract:  In a single hop broadcast packet erasure network, we demonstrate that it is
possible to provide multirate packet delivery outside of what is given by the
network min-cut. This is achieved by using a deterministic non-block-based
network coding scheme, which allows us to sidestep some of the limitations put
in place by the block coding model used to determine the network capacity.
Under the network coding scheme we outline, the sender is able to transmit
network coded packets above the channel rate of some receivers, while ensuring
that they still experience nonzero delivery rates. Interestingly, in this
generalised form of asynchronous network coded broadcast, receivers are not
required to obtain knowledge of all packets transmitted so far. Instead, causal
feedback from the receivers about packet erasures is used by the sender to
determine a network coded transmission that will allow at least one, but often
multiple receivers, to deliver their next needed packet.
Although the analysis of deterministic coding schemes is generally a
difficult problem, by making some approximations we are able to obtain
tractable estimates of the receivers' delivery rates, which are shown to match
reasonably well with simulation. Using these estimates, we design a fairness
algorithm that allocates the sender's resources so all receivers will
experience fair delivery rate performance."
Muriel Medard,Medard_Muriel,arXiv:1311.1053,https://arxiv.org/abs/1311.1053,"Abstract:  A string is sent over a noisy channel that erases some of its characters.
Knowing the statistical properties of the string's source and which characters
were erased, a listener that is equipped with an ability to test the veracity
of a string, one string at a time, wishes to fill in the missing pieces. Here
we characterize the influence of the stochastic properties of both the string's
source and the noise on the channel on the distribution of the number of
attempts required to identify the string, its guesswork. In particular, we
establish that the average noise on the channel is not a determining factor for
the average guesswork and illustrate simple settings where one recipient with,
on average, a better channel than another recipient, has higher average
guesswork. These results stand in contrast to those for the capacity of wiretap
channels and suggest the use of techniques such as friendly jamming with
pseudo-random sequences to exploit this guesswork behavior."
Muriel Medard,Medard_Muriel,arXiv:1310.6635,https://arxiv.org/abs/1310.6635,"Abstract:  We show preliminary results for the performance of Network Coded TCP (CTCP)
over large latency networks. While CTCP performs very well in networks with
relatively short RTT, the slow-start mechanism currently employed does not
adequately fill the available bandwidth when the RTT is large. Regardless, we
show that CTCP still outperforms current TCP variants (i.e., Cubic TCP and
Hybla TCP) for high packet loss rates (e.g., >2.5%). We then explore the
possibility of a modified congestion control mechanism based off of H-TCP that
opens the congestion window quickly to overcome the challenges of large latency
networks. Preliminary results are provided that show the combination of network
coding with an appropriate congestion control algorithm can provide gains on
the order of 20 times that of existing TCP variants. Finally, we provide a
discussion of the future work needed to increase CTCP's performance in these
networks."
Muriel Medard,Medard_Muriel,arXiv:1310.1512,https://arxiv.org/abs/1310.1512,"Abstract:  Lower bounds for the average probability of error of estimating a hidden
variable X given an observation of a correlated random variable Y, and Fano's
inequality in particular, play a central role in information theory. In this
paper, we present a lower bound for the average estimation error based on the
marginal distribution of X and the principal inertias of the joint distribution
matrix of X and Y. Furthermore, we discuss an information measure based on the
sum of the largest principal inertias, called k-correlation, which generalizes
maximal correlation. We show that k-correlation satisfies the Data Processing
Inequality and is convex in the conditional distribution of Y given X. Finally,
we investigate how to answer a fundamental question in inference and privacy:
given an observation Y, can we estimate a function f(X) of the hidden random
variable X with an average error below a certain threshold? We provide a
general method for answering this question using an approach based on
rate-distortion theory."
Muriel Medard,Medard_Muriel,arXiv:1308.5239,https://arxiv.org/abs/1308.5239,"Abstract:  Locally decodable channel codes form a special class of error-correcting
codes with the property that the decoder is able to reconstruct any bit of the
input message from querying only a few bits of a noisy codeword. It is well
known that such codes require significantly more redundancy (in particular have
vanishing rate) compared to their non-local counterparts. In this paper, we
define a dual problem, i.e. locally decodable source codes (LDSC). We consider
both almost lossless (block error) and lossy (bit error) cases. In almost
lossless case, we show that optimal compression (to entropy) is possible with
O(log n) queries to compressed string by the decompressor. We also show the
following converse bounds: 1) linear LDSC cannot achieve any rate below one,
with a bounded number of queries, 2) rate of any source coding with linear
decoder (not necessarily local) in one, 3) for 2 queries, any code construction
cannot have a rate below one. In lossy case, we show that any rate above rate
distortion is achievable with a bounded number of queries. We also show that,
rate distortion is achievable with any scaling number of queries. We provide an
achievability bound in the finite block-length regime and compare it with the
existing bounds in succinct data structures literature."
Muriel Medard,Medard_Muriel,arXiv:1307.5483,https://arxiv.org/abs/1307.5483,"Abstract:  We present a natural and low-complexity technique for achieving the capacity
of the Gaussian relay network in the high SNR regime. Specifically, we propose
the use of end-to-end structured lattice codes with the amplify-and-forward
strategy, where the source uses a nested lattice code to encode the messages
and the destination decodes the messages by lattice decoding. All intermediate
relays simply amplify and forward the received signals over the network to the
destination. We show that the end-to-end lattice-coded amplify-and-forward
scheme approaches the capacity of the layered Gaussian relay network in the
high SNR regime. Next, we extend our scheme to non-layered Gaussian relay
networks under the amplify-and-forward scheme, which can be viewed as a
Gaussian intersymbol interference (ISI) channel. Compared with other schemes,
our approach is significantly simpler and requires only the end-to-end design
of the lattice precoding and decoding. It does not require any knowledge of the
network topology or the individual channel gains."
Muriel Medard,Medard_Muriel,arXiv:1306.2249,https://arxiv.org/abs/1306.2249,"Abstract:  Existing mobile devices have the capability to use multiple network
technologies simultaneously to help increase performance; but they rarely, if
at all, effectively use these technologies in parallel. We first present
empirical data to help understand the mobile environment when three
heterogeneous networks are available to the mobile device (i.e., a WiFi
network, WiMax network, and an Iridium satellite network). We then propose a
reliable, multi-path protocol called Multi-Path TCP with Network Coding
(MPTCP/NC) that utilizes each of these networks in parallel. An analytical
model is developed and a mean-field approximation is derived that gives an
estimate of the protocol's achievable throughput. Finally, a comparison between
MPTCP and MPTCP/NC is presented using both the empirical data and mean-field
approximation. Our results show that network coding can provide users in mobile
environments a higher quality of service by enabling the use of multiple
network technologies and the capability to overcome packet losses due to lossy,
wireless network connections."
Muriel Medard,Medard_Muriel,arXiv:1305.6864,https://arxiv.org/abs/1305.6864,"Abstract:  In this paper, we show that coding can be used in storage area networks
(SANs) to improve various quality of service metrics under normal SAN operating
conditions, without requiring additional storage space. For our analysis, we
develop a model which captures modern characteristics such as constrained I/O
access bandwidth limitations. Using this model, we consider two important
cases: single-resolution (SR) and multi-resolution (MR) systems. For SR
systems, we use blocking probability as the quality of service metric and
propose the network coded storage (NCS) scheme as a way to reduce blocking
probability. The NCS scheme codes across file chunks in time, exploiting file
striping and file duplication. Under our assumptions, we illustrate cases where
SR NCS provides an order of magnitude savings in blocking probability. For MR
systems, we introduce saturation probability as a quality of service metric to
manage multiple user types, and we propose the uncoded resolution- aware
storage (URS) and coded resolution-aware storage (CRS) schemes as ways to
reduce saturation probability. In MR URS, we align our MR layout strategy with
traffic requirements. In MR CRS, we code videos across MR layers. Under our
assumptions, we illustrate that URS can in some cases provide an order of
magnitude gain in saturation probability over classic non-resolution aware
systems. Further, we illustrate that CRS provides additional saturation
probability savings over URS."
Muriel Medard,Medard_Muriel,arXiv:1304.0637,https://arxiv.org/abs/1304.0637,"Abstract:  Parallel transmission, as defined in high-speed Ethernet standards, enables
to use less expensive optoelectronics and offers backwards compatibility with
legacy Optical Transport Network (OTN) infrastructure. However, optimal
parallel transmission does not scale to large networks, as it requires
computationally expensive multipath routing algorithms to minimize differential
delay, and thus the required buffer size, optimize traffic splitting ratio, and
ensure frame synchronization. In this paper, we propose a novel framework for
high-speed Ethernet, which we refer to as network coded parallel transmission,
capable of effective buffer management and frame synchronization without the
need for complex multipath algorithms in the OTN layer. We show that using
network coding can reduce the delay caused by packet reordering at the
receiver, thus requiring a smaller overall buffer size, while improving the
network throughput. We design the framework in full compliance with high-speed
Ethernet standards specified in IEEE802.3ba and present solutions for network
encoding, data structure of coded parallel transmission, buffer management and
decoding at the receiver side. The proposed network coded parallel transmission
framework is simple to implement and represents a potential major breakthrough
in the system design of future high-speed Ethernet."
Muriel Medard,Medard_Muriel,arXiv:1303.4484,https://arxiv.org/abs/1303.4484,"Abstract:  We consider an \textit{Adaptive Random Convolutional Network Coding} (ARCNC)
algorithm to address the issue of field size in random network coding for
multicast, and study its memory and decoding delay performances through both
analysis and numerical simulations. ARCNC operates as a convolutional code,
with the coefficients of local encoding kernels chosen randomly over a small
finite field. The cardinality of local encoding kernels increases with time
until the global encoding kernel matrices at related sink nodes have full
rank.ARCNC adapts to unknown network topologies without prior knowledge, by
locally incrementing the dimensionality of the convolutional code. Because
convolutional codes of different constraint lengths can coexist in different
portions of the network, reductions in decoding delay and memory overheads can
be achieved. We show that this method performs no worse than random linear
network codes in terms of decodability, and can provide significant gains in
terms of average decoding delay or memory in combination, shuttle and random
geometric networks."
Muriel Medard,Medard_Muriel,arXiv:1301.6356,https://arxiv.org/abs/1301.6356,"Abstract:  Consider the situation where a word is chosen probabilistically from a finite
list. If an attacker knows the list and can inquire about each word in turn,
then selecting the word via the uniform distribution maximizes the attacker's
difficulty, its Guesswork, in identifying the chosen word. It is tempting to
use this property in cryptanalysis of computationally secure ciphers by
assuming coded words are drawn from a source's typical set and so, for all
intents and purposes, uniformly distributed within it. By applying recent
results on Guesswork, for i.i.d. sources it is this equipartition ansatz that
we investigate here. In particular, we demonstrate that the expected Guesswork
for a source conditioned to create words in the typical set grows, with word
length, at a lower exponential rate than that of the uniform approximation,
suggesting use of the approximation is ill-advised."
Muriel Medard,Medard_Muriel,arXiv:1212.2291,https://arxiv.org/abs/1212.2291,"Abstract:  We introduce CTCP, a reliable transport protocol using network coding. CTCP
is designed to incorporate TCP features such as congestion control,
reliability, and fairness while significantly improving on TCP's performance in
lossy, interference-limited and/or dynamic networks. A key advantage of
adopting a transport layer over a link layer approach is that it provides
backward compatibility with wireless equipment installed throughout existing
networks. We present a portable userspace implementation of CTCP and
extensively evaluate its performance in both testbed and production wireless
networks."
Muriel Medard,Medard_Muriel,arXiv:1210.2126,https://arxiv.org/abs/1210.2126,"Abstract:  We present a new information-theoretic definition and associated results,
based on list decoding in a source coding setting. We begin by presenting
list-source codes, which naturally map a key length (entropy) to list size. We
then show that such codes can be analyzed in the context of a novel
information-theoretic metric, \epsilon-symbol secrecy, that encompasses both
the one-time pad and traditional rate-based asymptotic metrics, but, like most
cryptographic constructs, can be applied in non-asymptotic settings. We derive
fundamental bounds for \epsilon-symbol secrecy and demonstrate how these bounds
can be achieved with MDS codes when the source is uniformly distributed. We
discuss applications and implementation issues of our codes."
Muriel Medard,Medard_Muriel,arXiv:1208.6125,https://arxiv.org/abs/1208.6125,"Abstract:  Efficient communication in wireless networks is typically challenged by the
possibility of interference among several transmitting nodes. Much important
research has been invested in decreasing the number of collisions in order to
obtain faster algorithms for communication in such networks.
This paper proposes a novel approach for wireless communication, which
embraces collisions rather than avoiding them, over an additive channel. It
introduces a coding technique called Bounded-Contention Coding (BCC) that
allows collisions to be successfully decoded by the receiving nodes into the
original transmissions and whose complexity depends on a bound on the
contention among the transmitters.
BCC enables deterministic local broadcast in a network with n nodes and at
most a transmitters with information of l bits each within O(a log n + al) bits
of communication with full-duplex radios, and O((a log n + al)(log n)) bits,
with high probability, with half-duplex radios. When combined with random
linear network coding, BCC gives global broadcast within O((D + a + log n)(a
log n + l)) bits, with high probability. This also holds in dynamic networks
that can change arbitrarily over time by a worst-case adversary. When no bound
on the contention is given, it is shown how to probabilistically estimate it
and obtain global broadcast that is adaptive to the true contention in the
network."
Muriel Medard,Medard_Muriel,arXiv:1208.3806,https://arxiv.org/abs/1208.3806,"Abstract:  In this paper we provide theoretical and simulation-based study of the
delivery delay performance of a number of existing throughput optimal coding
schemes and use the results to design a new dynamic rate adaptation scheme that
achieves improved overall throughput-delay performance.
Under a baseline rate control scheme, the receivers' delay performance is
examined. Based on their Markov states, the knowledge difference between the
sender and receiver, three distinct methods for packet delivery are identified:
zero state, leader state and coefficient-based delivery. We provide analyses of
each of these and show that, in many cases, zero state delivery alone presents
a tractable approximation of the expected packet delivery behaviour.
Interestingly, while coefficient-based delivery has so far been treated as a
secondary effect in the literature, we find that the choice of coefficients is
extremely important in determining the delay, and a well chosen encoding scheme
can, in fact, contribute a significant improvement to the delivery delay.
Based on our delivery delay model, we develop a dynamic rate adaptation
scheme which uses performance prediction models to determine the sender
transmission rate. Surprisingly, taking this approach leads us to the simple
conclusion that the sender should regulate its addition rate based on the total
number of undelivered packets stored at the receivers. We show that despite its
simplicity, our proposed dynamic rate adaptation scheme results in noticeably
improved throughput-delay performance over existing schemes in the literature."
Muriel Medard,Medard_Muriel,arXiv:1208.3212,https://arxiv.org/abs/1208.3212,"Abstract:  We analyze the performance of TCP and TCP with network coding (TCP/NC) in
lossy networks. We build upon the framework introduced by Padhye et al. and
characterize the throughput behavior of classical TCP and TCP/NC as a function
of erasure probability, round-trip time, maximum window size, and duration of
the connection. Our analytical results show that network coding masks random
erasures from TCP, thus preventing TCP's performance degradation in lossy
networks. It is further seen that TCP/NC has significant throughput gains over
TCP.
In addition, we show that TCP/NC may lead to cost reduction for wireless
network providers while maintaining a certain quality of service to their
users. We measure the cost in terms of number of base stations, which is highly
correlated to the energy, capital, and operational costs of a network provider.
We show that increasing the available bandwidth may not necessarily lead to
increase in throughput, particularly in lossy networks in which TCP does not
perform well. We show that using protocols such as TCP/NC, which are more
resilient to erasures, may lead to a throughput commensurate the bandwidth
dedicated to each user."
Muriel Medard,Medard_Muriel,arXiv:1205.3797,https://arxiv.org/abs/1205.3797,"Abstract:  This manuscript provides a model to characterize the energy savings of
network coded storage (NCS) in storage area networks (SANs). We consider
blocking probability of drives as our measure of performance. A mapping
technique to analyze SANs as independent M/G/K/K queues is presented, and
blocking probabilities for uncoded storage schemes and NCS are derived and
compared. We show that coding operates differently than the amalgamation of
file chunks and energy savings are shown to scale well with striping number. We
illustrate that for enterprise-level SANs energy savings of 20-50% can be
realized."
Muriel Medard,Medard_Muriel,arXiv:1204.0034,https://arxiv.org/abs/1204.0034,"Abstract:  A characterization of systematic network coding over multi-hop wireless
networks is key towards understanding the trade-off between complexity and
delay performance of networks that preserve the systematic structure. This
paper studies the case of a relay channel, where the source's objective is to
deliver a given number of data packets to a receiver with the aid of a relay.
The source broadcasts to both the receiver and the relay using one frequency,
while the relay uses another frequency for transmissions to the receiver,
allowing for a full-duplex operation of the relay. We analyze the decoding
complexity and delay performance of two types of relays: one that preserves the
systematic structure of the code from the source; another that does not. A
systematic relay forwards uncoded packets upon reception, but transmits coded
packets to the receiver after receiving the first coded packet from the source.
On the other hand, a non-systematic relay always transmits linear combinations
of previously received packets. We compare the performance of these two
alternatives by analytically characterizing the expected transmission
completion time as well as the number of uncoded packets forwarded by the
relay. Our numerical results show that, for a poor channel between the source
and the receiver, preserving the systematic structure at the relay (i) allows a
significant increase in the number of uncoded packets received by the receiver,
thus reducing the decoding complexity, and (ii) preserves close to optimal
delay performance."
Muriel Medard,Medard_Muriel,arXiv:1203.3258,https://arxiv.org/abs/1203.3258,"Abstract:  We present a framework for studying the problem of media streaming in
technology and cost heterogeneous environments. We first address the problem of
efficient streaming in a technology-heterogeneous setting. We employ random
linear network coding to simplify the packet selection strategies and alleviate
issues such as duplicate packet reception. Then, we study the problem of media
streaming from multiple cost-heterogeneous access networks. Our objective is to
characterize analytically the trade-off between access cost and user
experience. We model the Quality of user Experience (QoE) as the probability of
interruption in playback as well as the initial waiting time. We design and
characterize various control policies, and formulate the optimal control
problem using a Markov Decision Process (MDP) with a probabilistic constraint.
We present a characterization of the optimal policy using the
Hamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we
provide an exact and explicit characterization of a threshold policy and prove
its optimality using the HJB equation.
Our simulation results show that under properly designed control policy, the
existence of alternative access technology as a complement for a primary access
network can significantly improve the user experience without any bandwidth
over-provisioning."
Muriel Medard,Medard_Muriel,arXiv:1203.2841,https://arxiv.org/abs/1203.2841,"Abstract:  We study the cost of improving the goodput, or the useful data rate, to user
in a wireless network. We measure the cost in terms of number of base stations,
which is highly correlated to the energy cost as well as capital and
operational costs of a network provider.We show that increasing the available
bandwidth, or throughput, may not necessarily lead to increase in goodput,
particularly in lossy wireless networks in which TCP does not perform well. As
a result, much of the resources dedicated to the user may not translate to high
goodput, resulting in an inefficient use of the network resources. We show that
using protocols such as TCP/NC, which are more resilient to erasures and
failures in the network, may lead to a goodput commensurate the throughput
dedicated to each user. By increasing goodput, users' transactions are
completed faster; thus, the resources dedicated to these users can be released
to serve other requests or transactions. Consequently, we show that translating
efficiently throughput to goodput may bring forth better connection to users
while reducing the cost for the network providers."
Muriel Medard,Medard_Muriel,arXiv:1202.1801,https://arxiv.org/abs/1202.1801,"Abstract:  We design and analyze gossip algorithms for networks with correlated data. In
these networks, either the data to be distributed, the data already available
at the nodes, or both, are correlated. This model is applicable for a variety
of modern networks, such as sensor, peer-to-peer and content distribution
networks.
Although coding schemes for correlated data have been studied extensively,
the focus has been on characterizing the rate region in static memory-free
networks. In a gossip-based scheme, however, nodes communicate among each other
by continuously exchanging packets according to some underlying communication
model. The main figure of merit in this setting is the stopping time -- the
time required until nodes can successfully decode. While Gossip schemes are
practical, distributed and scalable, they have only been studied for
uncorrelated data.
We wish to close this gap by providing techniques to analyze network coded
gossip in (dynamic) networks with correlated data. We give a clean framework
for oblivious network models that applies to a multitude of network and
communication scenarios, specify a general setting for distributed correlated
data, and give tight bounds on the stopping times of network coded protocols in
this wide range of scenarios."
Muriel Medard,Medard_Muriel,arXiv:1202.0835,https://arxiv.org/abs/1202.0835,"Abstract:  This paper shows how to reduce the otherwise hard joint relay positioning and
flow optimization problem into a sequence a two simpler decoupled problems. We
consider a class of wireless multicast hypergraphs mainly characterized by
their hyperarc rate functions, that are increasing and convex in power, and
decreasing in distance between the transmit node and the farthest end node of
the hyperarc. The set-up consists of a single multicast flow session involving
a source, multiple destinations and a relay that can be positioned freely. The
first problem formulates the relay positioning problem in a purely geometric
sense, and once the optimal relay position is obtained the second problem
addresses the flow optimization. Furthermore, we present simple and efficient
algorithms to solve these problems."
Muriel Medard,Medard_Muriel,arXiv:1202.0784,https://arxiv.org/abs/1202.0784,"Abstract:  Throughput and per-packet delay can present strong trade-offs that are
important in the cases of delay sensitive applications.We investigate such
trade-offs using a random linear network coding scheme for one or more
receivers in single hop wireless packet erasure broadcast channels. We capture
the delay sensitivities across different types of network applications using a
class of delay metrics based on the norms of packet arrival times. With these
delay metrics, we establish a unified framework to characterize the rate and
delay requirements of applications and optimize system parameters. In the
single receiver case, we demonstrate the trade-off between average packet
delay, which we view as the inverse of throughput, and maximum ordered
inter-arrival delay for various system parameters. For a single broadcast
channel with multiple receivers having different delay constraints and feedback
delays, we jointly optimize the coding parameters and time-division scheduling
parameters at the transmitters. We formulate the optimization problem as a
Generalized Geometric Program (GGP). This approach allows the transmitters to
adjust adaptively the coding and scheduling parameters for efficient allocation
of network resources under varying delay constraints. In the case where the
receivers are served by multiple non-interfering wireless broadcast channels,
the same optimization problem is formulated as a Signomial Program, which is
NP-hard in general. We provide approximation methods using successive
formulation of geometric programs and show the convergence of approximations."
Muriel Medard,Medard_Muriel,arXiv:1110.3774,https://arxiv.org/abs/1110.3774,"Abstract:  In this paper, we introduce a time-stampless adaptive nonuniform sampling
(TANS) framework, in which time increments between samples are determined by a
function of the $m$ most recent increments and sample values. Since only past
samples are used in computing time increments, it is not necessary to save
sampling times (time stamps) for use in the reconstruction process. We focus on
two TANS schemes for discrete-time stochastic signals: a greedy method, and a
method based on dynamic programming. We analyze the performances of these
schemes by computing (or bounding) their trade-offs between sampling rate and
expected reconstruction distortion for autoregressive and Markovian signals.
Simulation results support the analysis of the sampling schemes. We show that,
by opportunistically adapting to local signal characteristics, TANS may lead to
improved power efficiency in some applications."
Muriel Medard,Medard_Muriel,arXiv:1110.0428,https://arxiv.org/abs/1110.0428,"Abstract:  We propose a joint source-channel-network coding scheme, based on compressive
sensing principles, for wireless networks with AWGN channels (that may include
multiple access and broadcast), with sources exhibiting temporal and spatial
dependencies. Our goal is to provide a reconstruction of sources within an
allowed distortion level at each receiver. We perform joint source-channel
coding at each source by randomly projecting source values to a lower
dimensional space. We consider sources that satisfy the restricted eigenvalue
(RE) condition as well as more general sources for which the randomness of the
network allows a mapping to lower dimensional spaces. Our approach relies on
using analog random linear network coding. The receiver uses compressive
sensing decoders to reconstruct sources. Our key insight is the fact that,
compressive sensing and analog network coding both preserve the source
characteristics required for compressive sensing decoding."
Muriel Medard,Medard_Muriel,arXiv:1109.2613,https://arxiv.org/abs/1109.2613,"Abstract:  The throughput benefits of random linear network codes have been studied
extensively for wirelined and wireless erasure networks. It is often assumed
that all nodes within a network perform coding operations. In
energy-constrained systems, however, coding subgraphs should be chosen to
control the number of coding nodes while maintaining throughput. In this paper,
we explore the strategic use of network coding in the wireless packet erasure
relay channel according to both throughput and energy metrics. In the relay
channel, a single source communicates to a single sink through the aid of a
half-duplex relay. The fluid flow model is used to describe the case where both
the source and the relay are coding, and Markov chain models are proposed to
describe packet evolution if only the source or only the relay is coding. In
addition to transmission energy, we take into account coding and reception
energies. We show that coding at the relay alone while operating in a rateless
fashion is neither throughput nor energy efficient. Given a set of system
parameters, our analysis determines the optimal amount of time the relay should
participate in the transmission, and where coding should be performed."
Muriel Medard,Medard_Muriel,arXiv:1108.2514,https://arxiv.org/abs/1108.2514,"Abstract:  We construct a simple network model to provide insight into network design
strategies. We show that the model can be used to address various approaches to
network coding, MAC, and multi-packet reception so that their effects on
network throughput can be evaluated. We consider several topology components
which exhibit the same non-monotonic saturation behavior found within the Katti
et. al. COPE experiments. We further show that fairness allocation by the MAC
can seriously impact performance and cause this non-monotonic saturation. Using
our model, we develop a MAC that provides monotonic saturation, higher
saturation throughput gains and fairness among flows rather than nodes. The
proposed model provides an estimate of the achievable gains for the cross-layer
design of network coding, multi-packet reception, and MAC showing that
super-additive throughput gains on the order of six times that of routing are
possible."
Muriel Medard,Medard_Muriel,arXiv:1108.2080,https://arxiv.org/abs/1108.2080,"Abstract:  Network coding achieves optimal throughput in multicast networks. However,
throughput optimality \emph{relies} on the network nodes or routers to code
\emph{correctly}. A Byzantine node may introduce junk packets in the network
(thus polluting downstream packets and causing the sinks to receive the wrong
data) or may choose coding coefficients in a way that significantly reduces the
throughput of the network.
Most prior work focused on the problem of Byzantine nodes polluting packets.
However, even if a Byzantine node does not pollute packets, he can still affect
significantly the throughput of the network by not coding correctly. No
previous work attempted to verify if a certain node \emph{coded correctly using
random coefficients} over \emph{all} of the packets he was supposed to code
over.
We provide two novel protocols (which we call PIP and Log-PIP) for detecting
whether a node coded correctly over all the packets received (i.e., according
to a random linear network coding algorithm). Our protocols enable any node in
the network to examine a packet received from another node by running a
""verification test"". With our protocols, the worst an adversary can do and
still pass the packet verification test is in fact equivalent to random linear
network coding, which has been shown to be optimal in multicast networks. Our
protocols resist collusion among nodes and are applicable to a variety of
settings.
Our topology simulations show that the throughput in the worst case for our
protocol is two to three times larger than the throughput in various
adversarial strategies allowed by prior work. We implemented our protocols in
C/C++ and Java, as well as incorporated them on the Android platform (Nexus
One). Our evaluation shows that our protocols impose modest overhead."
Muriel Medard,Medard_Muriel,arXiv:1107.3857,https://arxiv.org/abs/1107.3857,"Abstract:  We design a cross-layer approach to aid in develop- ing a cooperative
solution using multi-packet reception (MPR), network coding (NC), and medium
access (MAC). We construct a model for the behavior of the IEEE 802.11 MAC
protocol and apply it to key small canonical topology components and their
larger counterparts. The results obtained from this model match the available
experimental results with fidelity. Using this model, we show that fairness
allocation by the IEEE 802.11 MAC can significantly impede performance; hence,
we devise a new MAC that not only substantially improves throughput, but
provides fairness to flows of information rather than to nodes. We show that
cooperation between NC, MPR, and our new MAC achieves super-additive gains of
up to 6.3 times that of routing with the standard IEEE 802.11 MAC. Furthermore,
we extend the model to analyze our MAC's asymptotic and throughput behaviors as
the number of nodes increases or the MPR capability is limited to only a single
node. Finally, we show that although network performance is reduced under
substantial asymmetry or limited implementation of MPR to a central node, there
are some important practical cases, even under these conditions, where MPR, NC,
and their combination provide significant gains."
Muriel Medard,Medard_Muriel,arXiv:1106.0027,https://arxiv.org/abs/1106.0027,"Abstract:  We provide a geometric solution to the problem of optimal relay positioning
to maximize the multicast rate for low-SNR networks. The networks we consider,
consist of a single source, multiple receivers and the only intermediate and
locatable node as the relay. We construct network the hypergraph of the system
nodes from the underlying information theoretic model of low-SNR regime that
operates using superposition coding and FDMA in conjunction (which we call the
""achievable hypergraph model""). We make the following contributions. 1) We show
that the problem of optimal relay positioning maximizing the multicast rate can
be completely decoupled from the flow optimization by noticing and exploiting
geometric properties of multicast flow. 2) All the flow maximizing the
multicast rate is sent over at most two paths, in succession. The relay
position is dependent only on one path (out of the two), irrespective of the
number of receiver nodes in the system. Subsequently, we propose simple and
efficient geometric algorithms to compute the optimal relay position. 3)
Finally, we show that in our model at the optimal relay position, the
difference between the maximized multicast rate and the cut-set bound is
minimum. We solve the problem for all (Ps,Pr) pairs of source and relay
transmit powers and the path loss exponent \alpha greater than 2."
Muriel Medard,Medard_Muriel,arXiv:1104.2941,https://arxiv.org/abs/1104.2941,"Abstract:  We present a novel feedback protocol for wireless broadcast networks that
utilize linear network coding. We consider transmission of packets from one
source to many receivers over a single-hop broadcast erasure channel. Our
method utilizes a predictive model to request feedback only when the
probability that all receivers have completed decoding is significant. In
addition, our proposed NACK-based feedback mechanism enables all receivers to
request, within a single time slot, the number of retransmissions needed for
successful decoding. We present simulation results as well as analytical
results that show the favorable scalability of our technique as the number of
receivers, file size, and packet erasure probability increase. We also show the
robustness of this scheme to uncertainty in the predictive model, including
uncertainty in the number of receiving nodes and the packet erasure
probability, as well as to losses of the feedback itself. Our scheme, SMART, is
shown to perform nearly as well as an omniscient transmitter that requires no
feedback. Furthermore, SMART, is shown to outperform current state of the art
methods at any given erasure probability, file size, and numbers of receivers."
Muriel Medard,Medard_Muriel,arXiv:1103.6258,https://arxiv.org/abs/1103.6258,"Abstract:  We propose an efficient Adaptive Random Convolutional Network Coding (ARCNC)
algorithm to address the issue of field size in random network coding. ARCNC
operates as a convolutional code, with the coefficients of local encoding
kernels chosen randomly over a small finite field. The lengths of local
encoding kernels increase with time until the global encoding kernel matrices
at related sink nodes all have full rank. Instead of estimating the necessary
field size a priori, ARCNC operates in a small finite field. It adapts to
unknown network topologies without prior knowledge, by locally incrementing the
dimensionality of the convolutional code. Because convolutional codes of
different constraint lengths can coexist in different portions of the network,
reductions in decoding delay and memory overheads can be achieved with ARCNC.
We show through analysis that this method performs no worse than random linear
network codes in general networks, and can provide significant gains in terms
of average decoding delay in combination networks."
Muriel Medard,Medard_Muriel,arXiv:1103.0999,https://arxiv.org/abs/1103.0999,"Abstract:  The capacity of multiuser networks has been a long-standing problem in
information theory. Recently, Avestimehr et al. have proposed a deterministic
network model to approximate multiuser wireless networks. This model, known as
the ADT network model, takes into account the broadcast nature of wireless
medium and interference.
We show that the ADT network model can be described within the algebraic
network coding framework introduced by Koetter and Medard. We prove that the
ADT network problem can be captured by a single matrix, and show that the
min-cut of an ADT network is the rank of this matrix; thus, eliminating the
need to optimize over exponential number of cuts between two nodes to compute
the min-cut of an ADT network. We extend the capacity characterization for ADT
networks to a more general set of connections, including single
unicast/multicast connection and non-multicast connections such as multiple
multicast, disjoint multicast, and two-level multicast. We also provide
sufficiency conditions for achievability in ADT networks for any general
connection set. In addition, we show that random linear network coding, a
randomized distributed algorithm for network code construction, achieves the
capacity for the connections listed above. Furthermore, we extend the ADT
networks to those with random erasures and cycles (thus, allowing
bi-directional links).
In addition, we propose an efficient linear code construction for the
deterministic wireless multicast relay network model. Avestimehr et al.'s
proposed code construction is not guaranteed to be efficient and may
potentially involve an infinite block length. Unlike several previous coding
schemes, we do not attempt to find flows in the network. Instead, for a layered
network, we maintain an invariant where it is required that at each stage of
the code construction, certain sets of codewords are linearly independent."
Muriel Medard,Medard_Muriel,arXiv:1103.0361,https://arxiv.org/abs/1103.0361,"Abstract:  We define a notion of network capacity region of networks that generalizes
the notion of network capacity defined by Cannons et al. and prove its notable
properties such as closedness, boundedness and convexity when the finite field
is fixed. We show that the network routing capacity region is a computable
rational polytope and provide exact algorithms and approximation heuristics for
computing the region. We define the semi-network linear coding capacity region,
with respect to a fixed finite field, that inner bounds the corresponding
network linear coding capacity region, show that it is a computable rational
polytope, and provide exact algorithms and approximation heuristics. We show
connections between computing these regions and a polytope reconstruction
problem and some combinatorial optimization problems, such as the minimum cost
directed Steiner tree problem. We provide an example to illustrate our results.
The algorithms are not necessarily polynomial-time."
Muriel Medard,Medard_Muriel,arXiv:1103.0266,https://arxiv.org/abs/1103.0266,"Abstract:  Capacity scaling laws are analyzed in an underwater acoustic network with $n$
regularly located nodes on a square, in which both bandwidth and received
signal power can be limited significantly. A narrow-band model is assumed where
the carrier frequency is allowed to scale as a function of $n$. In the network,
we characterize an attenuation parameter that depends on the frequency scaling
as well as the transmission distance. Cut-set upper bounds on the throughput
scaling are then derived in both extended and dense networks having unit node
density and unit area, respectively. It is first analyzed that under extended
networks, the upper bound is inversely proportional to the attenuation
parameter, thus resulting in a highly power-limited network. Interestingly, it
is seen that the upper bound for extended networks is intrinsically related to
the attenuation parameter but not the spreading factor. On the other hand, in
dense networks, we show that there exists either a bandwidth or power
limitation, or both, according to the path-loss attenuation regimes, thus
yielding the upper bound that has three fundamentally different operating
regimes. Furthermore, we describe an achievable scheme based on the simple
nearest-neighbor multi-hop (MH) transmission. We show that under extended
networks, the MH scheme is order-optimal for all the operating regimes. An
achievability result is also presented in dense networks, where the operating
regimes that guarantee the order optimality are identified. It thus turns out
that frequency scaling is instrumental towards achieving the order optimality
in the regimes. Finally, these scaling results are extended to a random network
realization. As a result, vital information for fundamental limits of a variety
of underwater network scenarios is provided by showing capacity scaling laws."
Muriel Medard,Medard_Muriel,arXiv:1102.3569,https://arxiv.org/abs/1102.3569,"Abstract:  We resolve the question of optimality for a well-studied packetized
implementation of random linear network coding, called PNC. In PNC, in contrast
to the classical memoryless setting, nodes store received information in memory
to later produce coded packets that reflect this information. PNC is known to
achieve order optimal stopping times for the many-to-all multicast problem in
many settings.
We give a reduction that captures exactly how PNC and other network coding
protocols use the memory of the nodes. More precisely, we show that any such
protocol implementation induces a transformation which maps an execution of the
protocol to an instance of the classical memoryless setting. This allows us to
prove that, for any (non-adaptive dynamic) network, PNC converges with high
probability in optimal time. In other words, it stops at exactly the first time
in which in hindsight it was possible to route information from the sources to
each receiver individually.
Our technique also applies to variants of PNC, in which each node uses only a
finite buffer. We show that, even in this setting, PNC stops exactly within the
time in which in hindsight it was possible to route packets given the memory
constraint, i.e., that the memory used at each node never exceeds its buffer
size. This shows that PNC, even without any feedback or explicit memory
management, allows to keep minimal buffer sizes while maintaining its capacity
achieving performance."
Muriel Medard,Medard_Muriel,arXiv:1102.3204,https://arxiv.org/abs/1102.3204,"Abstract:  Random Linear Network Coding (RLNC) has emerged as a powerful tool for robust
high-throughput multicast. Projection analysis - a recently introduced
technique - shows that the distributed packetized RLNC protocol achieves
(order) optimal and perfectly pipelined information dissemination in many
settings. In the original approach to RNLC intermediate nodes code together all
available information. This requires intermediate nodes to keep considerable
data available for coding. Moreover, it results in a coding complexity that
grows linearly with the size of this data. While this has been identified as a
problem, approaches that combine queuing theory and network coding have
heretofore not provided a succinct representation of the memory needs of
network coding at intermediates nodes.
This paper shows the surprising result that, in all settings with a
continuous stream of data, network coding continues to perform optimally even
if only one packet per node is kept in active memory and used for computations.
This leads to an extremely simple RLNC protocol variant with drastically
reduced requirements on computational and memory resources. By extending the
projection analysis, we show that in all settings in which the RLNC protocol
was proven to be optimal its finite memory variant performs equally well. In
the same way as the original projection analysis, our technique applies in a
wide variety of network models, including highly dynamic topologies that can
change completely at any time in an adversarial fashion."
Muriel Medard,Medard_Muriel,arXiv:1101.5779,https://arxiv.org/abs/1101.5779,"Abstract:  We design a cross-layer approach to optimize the joint use of multi-packet
reception and network coding, in order to relieve congestion. We construct a
model for the behavior of the 802.11 MAC and apply it to several key canonical
topology components and their extensions to any number of nodes. The results
obtained from this model match the available experimental results, which are
for routing and opportunistic network coding, with fidelity. Using this model,
we show that fairness allocation by the MAC can seriously impact performance;
hence, we devise a new MAC that not only substantially improves throughput
relative to the current 802.11 MAC, but also provides fairness to flows of
information rather than to nodes. We show that the proper combination of
network coding, multi-packet reception, and our new MAC protocol achieves
super-additive throughput gains of up to 6.3 times that of routing alone with
the use of the standard 802.11 MAC. Finally, we extend the model to analyze the
asymptotic behavior of our new MAC as the number of nodes increases."
Muriel Medard,Medard_Muriel,arXiv:1012.0955,https://arxiv.org/abs/1012.0955,"Abstract:  In this paper, we demonstrate some applications of compressive sensing over
networks. We make a connection between compressive sensing and traditional
information theoretic techniques in source coding and channel coding. Our
results provide an explicit trade-off between the rate and the decoding
complexity. The key difference of compressive sensing and traditional
information theoretic approaches is at their decoding side. Although optimal
decoders to recover the original signal, compressed by source coding have high
complexity, the compressive sensing decoder is a linear or convex optimization.
First, we investigate applications of compressive sensing on distributed
compression of correlated sources. Here, by using compressive sensing, we
propose a compression scheme for a family of correlated sources with a
modularized decoder, providing a trade-off between the compression rate and the
decoding complexity. We call this scheme Sparse Distributed Compression. We use
this compression scheme for a general multicast network with correlated
sources. Here, we first decode some of the sources by a network decoding
technique and then, we use a compressive sensing decoder to obtain the whole
sources. Then, we investigate applications of compressive sensing on channel
coding. We propose a coding scheme that combines compressive sensing and random
channel coding for a high-SNR point-to-point Gaussian channel. We call this
scheme Sparse Channel Coding. We propose a modularized decoder providing a
trade-off between the capacity loss and the decoding complexity. At the
receiver side, first, we use a compressive sensing decoder on a noisy signal to
obtain a noisy estimate of the original signal and then, we apply a traditional
channel coding decoder to find the original signal."
Muriel Medard,Medard_Muriel,arXiv:1011.5496,https://arxiv.org/abs/1011.5496,"Abstract:  In this paper, we consider different aspects of the network functional
compression problem where computation of a function (or, some functions) of
sources located at certain nodes in a network is desired at receiver(s). The
rate region of this problem has been considered in the literature under certain
restrictive assumptions, particularly in terms of the network topology, the
functions and the characteristics of the sources. In this paper, we present
results that significantly relax these assumptions. Firstly, we consider this
problem for an arbitrary tree network and asymptotically lossless computation.
We show that, for depth one trees with correlated sources, or for general trees
with independent sources, a modularized coding scheme based on graph colorings
and Slepian-Wolf compression performs arbitrarily closely to rate lower bounds.
For a general tree network with independent sources, optimal computation to be
performed at intermediate nodes is derived. We introduce a necessary and
sufficient condition on graph colorings of any achievable coding scheme, called
coloring connectivity condition (C.C.C.).
Secondly, we investigate the effect of having several functions at the
receiver. In this problem, we derive a rate region and propose a coding scheme
based on graph colorings. Thirdly, we consider the functional compression
problem with feedback. We show that, in this problem, unlike Slepian-Wolf
compression, by having feedback, one may outperform rate bounds of the case
without feedback. Fourthly, we investigate functional computation problem with
distortion. We compute a rate-distortion region for this problem. Then, we
propose a simple suboptimal coding scheme with a non-trivial performance
guarantee. Finally, we introduce cases where finding minimum entropy colorings
and therefore, optimal coding schemes can be performed in polynomial time."
Muriel Medard,Medard_Muriel,arXiv:1011.3879,https://arxiv.org/abs/1011.3879,"Abstract:  We propose a secure scheme for wireless network coding, called the algebraic
watchdog. By enabling nodes to detect malicious behaviors probabilistically and
use overheard messages to police their downstream neighbors locally, the
algebraic watchdog delivers a secure global self-checking network. Unlike
traditional Byzantine detection protocols which are receiver-based, this
protocol gives the senders an active role in checking the node downstream. The
key idea is inspired by Marti et al.'s watchdog-pathrater, which attempts to
detect and mitigate the effects of routing misbehavior.
As an initial building block of a such system, we first focus on a two-hop
network. We present a graphical model to understand the inference process nodes
execute to police their downstream neighbors; as well as to compute, analyze,
and approximate the probabilities of misdetection and false detection. In
addition, we present an algebraic analysis of the performance using an
hypothesis testing framework that provides exact formulae for probabilities of
false detection and misdetection.
We then extend the algebraic watchdog to a more general network setting, and
propose a protocol in which we can establish trust in coded systems in a
distributed manner. We develop a graphical model to detect the presence of an
adversarial node downstream within a general multi-hop network. The structure
of the graphical model (a trellis) lends itself to well-known algorithms, such
as the Viterbi algorithm, which can compute the probabilities of misdetection
and false detection. We show analytically that as long as the min-cut is not
dominated by the Byzantine adversaries, upstream nodes can monitor downstream
neighbors and allow reliable communication with certain probability. Finally,
we present simulation results that support our analysis."
Muriel Medard,Medard_Muriel,arXiv:1008.3295,https://arxiv.org/abs/1008.3295,"Abstract:  We consider the broadcast relay channel (BRC), where a single source
transmits to multiple destinations with the help of a relay, in the limit of a
large bandwidth. We address the problem of optimal relay positioning and power
allocations at source and relay, to maximize the multicast rate from source to
all destinations. To solve such a network planning problem, we develop a
three-faceted approach based on an underlying information theoretic model,
computational geometric aspects, and network optimization tools. Firstly,
assuming superposition coding and frequency division between the source and the
relay, the information theoretic framework yields a hypergraph model of the
wideband BRC, which captures the dependency of achievable rate-tuples on the
network topology. As the relay position varies, so does the set of hyperarcs
constituting the hypergraph, rendering the combinatorial nature of optimization
problem. We show that the convex hull C of all nodes in the 2-D plane can be
divided into disjoint regions corresponding to distinct hyperarcs sets. These
sets are obtained by superimposing all k-th order Voronoi tessellation of C. We
propose an easy and efficient algorithm to compute all hyperarc sets, and prove
they are polynomially bounded. Using the switched hypergraph approach, we model
the original problem as a continuous yet non-convex network optimization
program. Ultimately, availing on the techniques of geometric programming and
$p$-norm surrogate approximation, we derive a good convex approximation. We
provide a detailed characterization of the problem for collinearly located
destinations, and then give a generalization for arbitrarily located
destinations. Finally, we show strong gains for the optimal relay positioning
compared to seemingly interesting positions."
Muriel Medard,Medard_Muriel,arXiv:1008.0420,https://arxiv.org/abs/1008.0420,"Abstract:  We analyze the performance of TCP and TCP with network coding (TCP/NC) in
lossy wireless networks. We build upon the simple framework introduced by
Padhye et al. and characterize the throughput behavior of classical TCP as well
as TCP/NC as a function of erasure rate, round-trip time, maximum window size,
and duration of the connection. Our analytical results show that network coding
masks erasures and losses from TCP, thus preventing TCP's performance
degradation in lossy networks, such as wireless networks. It is further seen
that TCP/NC has significant throughput gains over TCP. In addition, we simulate
TCP and TCP/NC to verify our analysis of the average throughput and the window
evolution. Our analysis and simulation results show very close concordance and
support that TCP/NC is robust against erasures. TCP/NC is not only able to
increase its window size faster but also to maintain a large window size
despite losses within the network, whereas TCP experiences window closing
essentially because losses are mistakenly attributed to congestion."
Alexandre Megretski,Megretski_Alexandre,arXiv:1703.04491,https://arxiv.org/abs/1703.04491,"Abstract:  In modern power systems, the operating point, at which the demand and supply
are balanced, may take different values due to changes in loads and renewable
generation levels. Understanding the dynamics of stressed power systems with a
range of operating points would be essential to assuring their reliable
operation, and possibly allow higher integration of renewable resources. This
letter introduces a non-traditional way to think about the stability assessment
problem of power systems. Instead of estimating the set of initial states
leading to a given operating condition, we characterize the set of operating
conditions that a power grid converges to from a given initial state under
changes in power injections and lines. We term this problem as ""inverse
stability"", a problem which is rarely addressed in the control and systems
literature, and hence, poorly understood. Exploiting quadratic approximations
of the system's energy function, we introduce an estimate of the inverse
stability region. Also, we briefly describe three important applications of the
inverse stability notion: (i) robust stability assessment of power systems
w.r.t. different renewable generation levels, (ii) stability-constrained
optimal power flow (sOPF), and (iii) stability-guaranteed corrective action
design."
Alexandre Megretski,Megretski_Alexandre,arXiv:1701.06652,https://arxiv.org/abs/1701.06652,"Abstract:  Model instability and poor prediction of long-term behavior are common
problems when modeling dynamical systems using nonlinear ""black-box""
techniques. Direct optimization of the long-term predictions, often called
simulation error minimization, leads to optimization problems that are
generally non-convex in the model parameters and suffer from multiple local
minima. In this work we present methods which address these problems through
convex optimization, based on Lagrangian relaxation, dissipation inequalities,
contraction theory, and semidefinite programming. We demonstrate the proposed
methods with a model order reduction task for electronic circuit design and the
identification of a pneumatic actuator from experiment."
Alexandre Megretski,Megretski_Alexandre,arXiv:1510.01705,https://arxiv.org/abs/1510.01705,"Abstract:  We consider baseband equivalent representation of transmission circuits, in
the form of a nonlinear dynamical system $\mathbf S$ in discrete time (DT)
defined by a series interconnection of a phase-amplitude modulator, a nonlinear
dynamical system $\mathbf F$ in continuous time (CT), and an ideal demodulator.
We show that when $\mathbf F$ is a CT Volterra series model, the resulting
$\mathbf S$ is a series interconnection of a DT Volterra series model of same
degree and memory depth, and an LTI system with special properties. The result
suggests a new, non-obvious, analytically motivated structure of digital
pre-compensation of analog nonlinear distortions such as those caused by power
amplifiers in digital communication systems. The baseband model and the
corresponding digital compensation structure readily extend to OFDM modulation.
MATLAB simulation is used to verify proposed baseband equivalent model and
demonstrate effectiveness of the new compensation scheme, as compared to the
standard Volterra series approach."
Alexandre Megretski,Megretski_Alexandre,arXiv:1411.1328,https://arxiv.org/abs/1411.1328,"Abstract:  We consider discrete-time (DT) systems S in which a DT input is first
tranformed to a continuous-time (CT) format by phase-amplitude modulation, then
modified by a non-linear CT dynamical transformation F, and finally converted
back to DT output using an ideal de-modulation scheme. Assuming that F belongs
to a special class of CT Volterra series models with fixed degree and memory
depth, we provide a complete characterization of S as a series connection of a
DT Volterra series model of fixed degree and memory depth, and an LTI system
with special properties. The result suggests a new, non-obvious, analytically
motivated structure of digital compensation of analog nonlinear distortions
(for example, those caused by power amplifiers) in digital communication
systems. Results from a MATLAB simulation are used to demonstrate effectiveness
of the new compensation scheme, as compared to the standard Volterra series
approach."
Alexandre Megretski,Megretski_Alexandre,arXiv:1305.5856,https://arxiv.org/abs/1305.5856,"Abstract:  We construct structured H-Infinity optimal model matching problems with
rational coefficients, in which the optimal solution is not rational, in the
sense that the cost does not achieve its maximal lower bound on the set of
rational matching models, but the same maximal lower bound can be reached by
using a continuous non-rational matching model."
Alexandre Megretski,Megretski_Alexandre,arXiv:1303.4175,https://arxiv.org/abs/1303.4175,"Abstract:  This paper introduces new techniques for using convex optimization to fit
input-output data to a class of stable nonlinear dynamical models. We present
an algorithm that guarantees consistent estimates of models in this class when
a small set of repeated experiments with suitably independent measurement noise
is available. Stability of the estimated models is guaranteed without any
assumptions on the input-output data. We first present a convex optimization
scheme for identifying stable state-space models from empirical moments. Next,
we provide a method for using repeated experiments to remove the effect of
noise on these moment and model estimates. The technique is demonstrated on a
simple simulated example."
Alexandre Megretski,Megretski_Alexandre,arXiv:1207.4967,https://arxiv.org/abs/1207.4967,"Abstract:  In this paper we prove optimality of a certain class of Analog to Digital
Converters (ADCs), which can be viewed as generalized Delta-Sigma Modulators
(DSMs), with respect to a performance measure that can be characterized as the
worst-case average intensity of the signal representation error. An analytic
expression for the ADC performance is given. Furthermore, our result proves
separation of quantization and control for this class of ADCs subject to some
technical conditions."
Alexandre Megretski,Megretski_Alexandre,arXiv:1204.3034,https://arxiv.org/abs/1204.3034,"Abstract:  This paper deals with the task of finding certified lower bounds for the
performance of Analog to Digital Converters (ADCs). A general ADC is modeled as
a causal, discrete-time dynamical system with outputs taking values in a finite
set. We define the performance of an ADC as the worst-case average intensity of
the filtered input matching error, defined as the difference between the input
and output of the ADC. The passband of the shaping filter used to filter the
error signal determines the frequency region of interest for minimizing the
error. The problem of finding a lower bound for the performance of an ADC is
formulated as a dynamic game problem in which the input signal to the ADC plays
against the output of the ADC. Furthermore, the performance measure must be
optimized in the presence of quantized disturbances (output of the ADC) that
can exceed the control variable (input of the ADC) in magnitude. We
characterize the optimal solution in terms of a Bellman-type inequality. A
numerical approach is presented to compute the value function in parallel with
the feedback law for generating the worst case input signal. The specific
structure of the problem is used to prove certain properties of the value
function that simplifies the iterative computation of a certified solution to
the Bellman inequality. The solution provides a certified lower bound on the
performance of any ADC with respect to the selected performance criteria."
Alexandre Megretski,Megretski_Alexandre,arXiv:1108.5622,https://arxiv.org/abs/1108.5622,"Abstract:  The paper proposes a control-theoretic framework for verification of
numerical software systems, and puts forward software verification as an
important application of control and systems theory. The idea is to transfer
Lyapunov functions and the associated computational techniques from control
systems analysis and convex optimization to verification of various software
safety and performance specifications. These include but are not limited to
absence of overflow, absence of division-by-zero, termination in finite time,
presence of dead-code, and certain user-specified assertions. Central to this
framework are Lyapunov invariants. These are properly constructed functions of
the program variables, and satisfy certain properties-resembling those of
Lyapunov functions-along the execution trace. The search for the invariants can
be formulated as a convex optimization problem. If the associated optimization
problem is feasible, the result is a certificate for the specification."
Alexandre Megretski,Megretski_Alexandre,arXiv:1108.0170,https://arxiv.org/abs/1108.0170,"Abstract:  The paper proposes a control-theoretic framework for verification of
numerical software systems, and puts forward software verification as an
important application of control and systems theory. The idea is to transfer
Lyapunov functions and the associated computational techniques from control
systems analysis and convex optimization to verification of various software
safety and performance specifications. These include but are not limited to
absence of overflow, absence of division-by-zero, termination in finite time,
presence of dead-code, and certain user-specified assertions. Central to this
framework are Lyapunov invariants. These are properly constructed functions of
the program variables, and satisfy certain properties-resembling those of
Lyapunov functions-along the execution trace. The search for the invariants can
be formulated as a convex optimization problem. If the associated optimization
problem is feasible, the result is a certificate for the specification."
Alexandre Megretski,Megretski_Alexandre,arXiv:1009.1670,https://arxiv.org/abs/1009.1670,"Abstract:  A new framework for nonlinear system identification is presented in terms of
optimal fitting of stable nonlinear state space equations to input/output/state
data, with a performance objective defined as a measure of robustness of the
simulation error with respect to equation errors. Basic definitions and
analytical results are presented. The utility of the method is illustrated on a
simple simulation example as well as experimental recordings from a live
neuron."
Alexandre Megretski,Megretski_Alexandre,arXiv:1008.2552,https://arxiv.org/abs/1008.2552,"Abstract:  Several variations of the classical Kalman-Yakubovich-Popov Lemma, as well
the associated minimax theorem are presented."
Alexandre Megretski,Megretski_Alexandre,arXiv:1005.2212,https://arxiv.org/abs/1005.2212,"Abstract:  The paper deals with the task of optimal design of Analog to Digital
Converters (ADCs). A general ADC is modeled as a causal discrete-time dynamical
system with outputs taking values in a finite set, and its performance is
defined as the worst-case average intensity of the filtered input matching
error. The design task can be viewed as that of optimal quantized decision
making in a system, with the objective being to optimize the performance
measure. An algorithm is proposed for designing optimal ADCs and certifying
their optimality. The algorithm is based on exploiting a special structure in
the underlying dynamic program, which makes it possible to find the optimal
value function, and hence the optimal quantization law exactly and
analytically. Moreover, the designed ADC is shown to have the classical
Delta-Sigma Modulator (DSM) structure with optimal quantization step spacing."
Alexandre Megretski,Megretski_Alexandre,arXiv:0903.5535,https://arxiv.org/abs/0903.5535,"Abstract:  We demonstrate the use of a new, control-oriented notion of finite state
approximation for a particular class of hybrid systems. Specifically, we
consider the problem of designing a stabilizing binary output feedback
switching controller for a pair of unstable homogeneous second order systems.
The constructive approach presented in this note, in addition to yielding an
explicit construction of a deterministic finite state approximate model of the
hybrid plant, allows us to efficiently establish a useable upper bound on the
quality of approximation, and leads to a discrete optimization problem whose
solution immediately provides a certifiably correct-by-design controller for
the original system. The resulting controller consists of a finite state
observer for the plant and a corresponding full state feedback switching
control law."
Alexandre Megretski,Megretski_Alexandre,arXiv:math/0610633,https://arxiv.org/abs/math/0610633,"Abstract:  This paper asks what classes of input signals are sufficient in order to
completely identify the input/output behavior of generic bilinear systems. The
main results are that step inputs are not sufficient, nor are single pulses,
but the family of all pulses (of a fixed amplitude but varying widths) do
suffice for identification."
Alexandre Megretski,Megretski_Alexandre,arXiv:quant-ph/0211111,https://arxiv.org/abs/quant-ph/0211111,"Abstract:  We develop a sufficient condition for the least-squares measurement (LSM), or
the square-root measurement, to minimize the probability of a detection error
when distinguishing between a collection of mixed quantum states. Using this
condition we derive the optimal measurement for state sets with a broad class
of symmetries.
We first consider geometrically uniform (GU) state sets with a possibly
nonabelian generating group, and show that if the generator satisfies a certain
constraint, then the LSM is optimal. In particular, for pure-state GU ensembles
the LSM is shown to be optimal. For arbitrary GU state sets we show that the
optimal measurement operators are GU with generator that can be computed very
efficiently in polynomial time, within any desired accuracy.
We then consider compound GU (CGU) state sets which consist of subsets that
are GU. When the generators satisfy a certain constraint, the LSM is again
optimal. For arbitrary CGU state sets the optimal measurement operators are
shown to be CGU with generators that can be computed efficiently in polynomial
time."
Alexandre Megretski,Megretski_Alexandre,arXiv:quant-ph/0205178,https://arxiv.org/abs/quant-ph/0205178,"Abstract:  We consider the problem of designing an optimal quantum detector to minimize
the probability of a detection error when distinguishing between a collection
of quantum states, represented by a set of density operators. We show that the
design of the optimal detector can be formulated as a semidefinite programming
problem. Based on this formulation, we derive a set of necessary and sufficient
conditions for an optimal quantum measurement. We then show that the optimal
measurement can be found by solving a standard (convex) semidefinite program
followed by the solution of a set of linear equations or, at worst, a standard
linear programming problem. By exploiting the many well-known algorithms for
solving semidefinite programs, which are guaranteed to converge to the global
optimum, the optimal measurement can be computed very efficiently in polynomial
time.
Using the semidefinite programming formulation, we also show that the rank of
each optimal measurement operator is no larger than the rank of the
corresponding density operator. In particular, if the quantum state ensemble is
a pure-state ensemble consisting of (not necessarily independent) rank-one
density operators, then we show that the optimal measurement is a pure-state
measurement consisting of rank-one measurement operators."
Silvio Micali,Micali_Silvio,arXiv:1607.01341,https://arxiv.org/abs/1607.01341,"Abstract:  A public ledger is a tamperproof sequence of data that can be read and
augmented by everyone. Public ledgers have innumerable and compelling uses.
They can secure, in plain sight, all kinds of transactions ---such as titles,
sales, and payments--- in the exact order in which they occur. Public ledgers
not only curb corruption, but also enable very sophisticated applications
---such as cryptocurrencies and smart contracts. They stand to revolutionize
the way a democratic society operates. As currently implemented, however, they
scale poorly and cannot achieve their potential.
Algorand is a truly democratic and efficient way to implement a public
ledger. Unlike prior implementations based on proof of work, it requires a
negligible amount of computation, and generates a transaction history that will
not ""fork"" with overwhelmingly high probability.
Algorand is based on (a novel and super fast) message-passing Byzantine
agreement.
For concreteness, we shall describe Algorand only as a money platform."
Silvio Micali,Micali_Silvio,arXiv:1411.5383,https://arxiv.org/abs/1411.5383,"Abstract:  Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic
connections are thought to be a prime candidate for how convergent pathways in
the brain compress information. However, to date, there is no complete
mathematical support for such implementations given the constraints of real
neural tissue. The fact that neurons are either excitatory or inhibitory
implies that every so implementable JL matrix must be sign-consistent (i.e.,
all entries in a single column must be either all non-negative or all
non-positive), and the fact that any given neuron connects to a relatively
small subset of other neurons implies that the JL matrix had better be sparse.
We construct sparse JL matrices that are sign-consistent, and prove that our
construction is essentially optimal. Our work answers a mathematical question
that was triggered by earlier work and is necessary to justify the existence of
JL compression in the brain, and emphasizes that inhibition is crucial if
neurons are to perform efficient, correlation-preserving compression."
Silvio Micali,Micali_Silvio,arXiv:1403.6413,https://arxiv.org/abs/1403.6413,"Abstract:  We analyze the Vickrey mechanism for auctions of multiple identical goods
when the players have both Knightian uncertainty over their own valuations and
incomplete preferences. In this model, the Vickrey mechanism is no longer
dominant-strategy, and we prove that all dominant-strategy mechanisms are
inadequate. However, we also prove that, in undominated strategies, the social
welfare produced by the Vickrey mechanism in the worst case is not only very
good, but also essentially optimal."
Silvio Micali,Micali_Silvio,arXiv:1403.6411,https://arxiv.org/abs/1403.6411,"Abstract:  We consider players that have very limited knowledge about their own
valuations. Specifically, the only information that a Knightian player $i$ has
about the profile of true valuations, $\theta^*$, consists of a set of
distributions, from one of which $\theta_i^*$ has been drawn.
We prove a ``robustness'' theorem for Knightian players in single-parameter
domains: every mechanism that is weakly dominant-strategy truthful for
classical players continues to be well-behaved for Knightian players that
choose undominated strategies."
Silvio Micali,Micali_Silvio,arXiv:1403.6410,https://arxiv.org/abs/1403.6410,"Abstract:  We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
The VCG mechanism guarantees very high social welfare both in single- and
multi-good auctions, so long as Knightian players do not select strategies that
are dominated. With such Knightian players, however, we prove that the VCG
mechanism guarantees very poor social welfare in unrestricted combinatorial
auctions."
Silvio Micali,Micali_Silvio,arXiv:1403.6409,https://arxiv.org/abs/1403.6409,"Abstract:  We consider auctions in which the players have very limited knowledge about
their own valuations. Specifically, the only information that a Knightian
player $i$ has about the profile of true valuations, $\theta^*$, consists of a
set of distributions, from one of which $\theta_i^*$ has been drawn.
We analyze the social-welfare performance of the VCG mechanism, for
unrestricted combinatorial auctions, when Knightian players that either (a)
choose a regret-minimizing strategy, or (b) resort to regret minimization only
to refine further their own sets of undominated strategies, if needed. We prove
that this performance is very good."
Silvio Micali,Micali_Silvio,arXiv:1403.6394,https://arxiv.org/abs/1403.6394,"Abstract:  We relate the strategy sets that a player ends up with after refining his own
strategies according to two very different models of rationality: namely,
utility maximization and regret minimization."
Silvio Micali,Micali_Silvio,arXiv:1112.1147,https://arxiv.org/abs/1112.1147,"Abstract:  We study single-good auctions in a setting where each player knows his own
valuation only within a constant multiplicative factor \delta{} in (0,1), and
the mechanism designer knows \delta. The classical notions of implementation in
dominant strategies and implementation in undominated strategies are naturally
extended to this setting, but their power is vastly different.
On the negative side, we prove that no dominant-strategy mechanism can
guarantee social welfare that is significantly better than that achievable by
assigning the good to a random player.
On the positive side, we provide tight upper and lower bounds for the
fraction of the maximum social welfare achievable in undominated strategies,
whether deterministically or probabilistically."
Rob Miller,Miller_Rob,arXiv:1703.06815,https://arxiv.org/abs/1703.06815,"Abstract:  We present PEC, an Event Calculus (EC) style action language for reasoning
about probabilistic causal and narrative information. It has an action language
style syntax similar to that of the EC variant Modular-E. Its semantics is
given in terms of possible worlds which constitute possible evolutions of the
domain, and builds on that of EFEC, an epistemic extension of EC. We also
describe an ASP implementation of PEC and show the sense in which this is sound
and complete."
Rob Miller,Miller_Rob,arXiv:1409.6680,https://arxiv.org/abs/1409.6680,"Abstract:  Constructing a good conference schedule for a large multi-track conference
needs to take into account the preferences and constraints of organizers,
authors, and attendees. Creating a schedule which has fewer conflicts for
authors and attendees, and thematically coherent sessions is a challenging
task.
Cobi introduced an alternative approach to conference scheduling by engaging
the community to play an active role in the planning process. The current Cobi
pipeline consists of committee-sourcing and author-sourcing to plan a
conference schedule. We further explore the design space of community-sourcing
by introducing attendee-sourcing -- a process that collects input from
conference attendees and encodes them as preferences and constraints for
creating sessions and schedule. For CHI 2014, a large multi-track conference in
human-computer interaction with more than 3,000 attendees and 1,000 authors, we
collected attendees' preferences by making available all the accepted papers at
the conference on a paper recommendation tool we built called Confer, for a
period of 45 days before announcing the conference program (sessions and
schedule). We compare the preferences marked on Confer with the preferences
collected from Cobi's author-sourcing approach. We show that attendee-sourcing
can provide insights beyond what can be discovered by author-sourcing. For CHI
2014, the results show value in the method and attendees' participation. It
produces data that provides more alternatives in scheduling and complements
data collected from other methods for creating coherent sessions and reducing
conflicts."
Rob Miller,Miller_Rob,arXiv:1407.3832,https://arxiv.org/abs/1407.3832,"Abstract:  This paper develops a Reasoning about Actions and Change framework integrated
with Default Reasoning, suitable as a Knowledge Representation and Reasoning
framework for Story Comprehension. The proposed framework, which is guided
strongly by existing knowhow from the Psychology of Reading and Comprehension,
is based on the theory of argumentation from AI. It uses argumentation to
capture appropriate solutions to the frame, ramification and qualification
problems and generalizations of these problems required for text comprehension.
In this first part of the study the work concentrates on the central problem of
integration (or elaboration) of the explicit information from the narrative in
the text with the implicit (in the readers mind) common sense world knowledge
pertaining to the topic(s) of the story given in the text. We also report on
our empirical efforts to gather background common sense world knowledge used by
humans when reading a story and to evaluate, through a prototype system, the
ability of our approach to capture both the majority and the variability of
understanding of a story by the human readers in the experiments."
Rob Miller,Miller_Rob,arXiv:cs/0003049,https://arxiv.org/abs/cs/0003049,"Abstract:  Planning is a natural domain of application for frameworks of reasoning about
actions and change. In this paper we study how one such framework, the Language
E, can form the basis for planning under (possibly) incomplete information. We
define two types of plans: weak and safe plans, and propose a planner, called
the E-Planner, which is often able to extend an initial weak plan into a safe
plan even though the (explicit) information available is incomplete, e.g. for
cases where the initial state is not completely known. The E-Planner is based
upon a reformulation of the Language E in argumentation terms and a natural
proof theory resulting from the reformulation. It uses an extension of this
proof theory by means of abduction for the generation of plans and adopts
argumentation-based techniques for extending weak plans into safe plans. We
provide representative examples illustrating the behaviour of the E-Planner, in
particular for cases where the status of fluents is incompletely known."
Rob Miller,Miller_Rob,arXiv:cs/0003034,https://arxiv.org/abs/cs/0003034,"Abstract:  E-RES is a system that implements the Language E, a logic for reasoning about
narratives of action occurrences and observations. E's semantics is
model-theoretic, but this implementation is based on a sound and complete
reformulation of E in terms of argumentation, and uses general computational
techniques of argumentation frameworks. The system derives sceptical
non-monotonic consequences of a given reformulated theory which exactly
correspond to consequences entailed by E's model-theory. The computation relies
on a complimentary ability of the system to derive credulous non-monotonic
consequences together with a set of supporting assumptions which is sufficient
for the (credulous) conclusion to hold. E-RES allows theories to contain
general action laws, statements about action occurrences, observations and
statements of ramifications (or universal laws). It is able to derive
consequences both forward and backward in time. This paper gives a short
overview of the theoretical basis of E-RES and illustrates its use on a variety
of examples. Currently, E-RES is being extended so that the system can be used
for planning."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1703.05348,https://arxiv.org/abs/1703.05348,"Abstract:  Theorems from Part 1 of this paper are generalized to {\psi}-mixing sources
in this paper. Application to Markoff chains and order m Markoff chains is
presented. The main result is the generalization of Theorem 1 in Part 1."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1703.05346,https://arxiv.org/abs/1703.05346,"Abstract:  In this paper, the problem of communication over an essentially unknown
channel, which is known to be able to communicate a source to a destination to
within a certain distortion level, is considered from a behavioral,
interconnection view-point. Rates of reliable communication are derived and
source-channel separation for communication with fidelity criteria is proved.
The results are then generalized to the multi-user setting under certain
assumptions. Other applications of this problem problem which follow from this
perspective are discussed."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1510.04214,https://arxiv.org/abs/1510.04214,"Abstract:  We consider a discrete-time Linear-Quadratic-Gaussian (LQG) control problem
in which Massey's directed information from the observed output of the plant to
the control input is minimized while required control performance is
attainable. This problem arises in several different contexts, including joint
encoder and controller design for data-rate minimization in networked control
systems. We show that the optimal control law is a Linear-Gaussian randomized
policy. We also identify the state space realization of the optimal policy,
which can be synthesized by an efficient algorithm based on semidefinite
programming. Our structural result indicates that the filter-controller
separation principle from the LQG control theory, and the sensor-filter
separation principle from the zero-delay rate-distortion theory for
Gauss-Markov sources hold simultaneously in the considered problem. A
connection to the data-rate theorem for mean-square stability by Nair and Evans
is also established."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1508.00237,https://arxiv.org/abs/1508.00237,"Abstract:  We consider a class of non-linear dynamics on a graph that contains and
generalizes various models from network systems and control and study
convergence to uniform agreement states using gradient methods. In particular,
under the assumption of detailed balance, we provide a method to formulate the
governing ODE system in gradient descent form of sum-separable energy
functions, which thus represent a class of Lyapunov functions; this class
coincides with Csiszár's information divergences. Our approach bases on a
transformation of the original problem to a mass-preserving transport problem
and it reflects a little-noticed general structure result for passive network
synthesis obtained by B.D.O. Anderson and P.J. Moylan in 1975. The proposed
gradient formulation extends known gradient results in dynamical systems
obtained recently by M. Erbar and J. Maas in the context of porous medium
equations. Furthermore, we exhibit a novel relationship between inhomogeneous
Markov chains and passive non-linear circuits through gradient systems, and
show that passivity of resistor elements is equivalent to strict convexity of
sum-separable stored energy. Eventually, we discuss our results at the
intersection of Markov chains and network systems under sinusoidal coupling."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1505.06130,https://arxiv.org/abs/1505.06130,"Abstract:  A randomized covering-packing duality between source and channel coding will
be discussed by considering the source coding problem of coding a source with a
certain distortion level and by considering a channel which communicates the
source within a certain distortion level. An operational view of source-channel
separation for communication with a fidelity criterion will be discussed in
brief."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1411.7632,https://arxiv.org/abs/1411.7632,"Abstract:  Sequential rate-distortion (SRD) theory provides a framework for studying the
fundamental trade-off between data-rate and data-quality in real-time
communication systems. In this paper, we consider the SRD problem for
multi-dimensional time-varying Gauss-Markov processes under mean-square
distortion criteria. We first revisit the sensor-estimator separation
principle, which asserts that considered SRD problem is equivalent to a joint
sensor and estimator design problem in which data-rate of the sensor output is
minimized while the estimator's performance satisfies the distortion criteria.
We then show that the optimal joint design can be performed by semidefinite
programming. A semidefinite representation of the corresponding SRD function is
obtained. Implications of the obtained result in the context of zero-delay
source coding theory and applications to networked control theory are also
discussed."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1402.1010,https://arxiv.org/abs/1402.1010,"Abstract:  In this theoretical study, we determine the maximum amount of work
extractable in finite time by a demon performing continuous measurements on a
quadratic Hamiltonian system subjected to thermal fluctuations, in terms of the
information extracted from the system. This is in contrast to many recent
studies that focus on demons' maximizing the extracted work over received
information, and operate close to equilibrium. The maximum work demon is found
to apply a high-gain continuous feedback using a Kalman-Bucy estimate of the
system state. A simple and concrete electrical implementation of the feedback
protocol is proposed, which allows for analytic expressions of the flows of
energy and entropy inside the demon. This let us show that any implementation
of the demon must necessarily include an external power source, which we prove
both from classical thermodynamics arguments and from a version of Landauer's
memory erasure argument extended to non-equilibrium linear systems."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1310.0425,https://arxiv.org/abs/1310.0425,"Abstract:  The hypothesis that high dimensional data tend to lie in the vicinity of a
low dimensional manifold is the basis of manifold learning. The goal of this
paper is to develop an algorithm (with accompanying complexity guarantees) for
fitting a manifold to an unknown probability distribution supported in a
separable Hilbert space, only using i.i.d samples from that distribution. More
precisely, our setting is the following. Suppose that data are drawn
independently at random from a probability distribution $P$ supported on the
unit ball of a separable Hilbert space $H$. Let $G(d, V, \tau)$ be the set of
submanifolds of the unit ball of $H$ whose volume is at most $V$ and reach
(which is the supremum of all $r$ such that any point at a distance less than
$r$ has a unique nearest point on the manifold) is at least $\tau$. Let $L(M,
P)$ denote mean-squared distance of a random point from the probability
distribution $P$ to $M$.
We obtain an algorithm that tests the manifold hypothesis in the following
sense.
The algorithm takes i.i.d random samples from $P$ as input, and determines
which of the following two is true (at least one must be):
(a) There exists $M \in G(d, CV, \frac{\tau}{C})$ such that $L(M, P) \leq C
\epsilon.$
(b) There exists no $M \in G(d, V/C, C\tau)$ such that $L(M, P) \leq
\frac{\epsilon}{C}.$
The answer is correct with probability at least $1-\delta$."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1302.5860,https://arxiv.org/abs/1302.5860,"Abstract:  This is a three part paper.
Optimality of source-channel separation for communication with a fidelity
criterion when the channel is compound as defined by Csiszar and Korner in
their book and general as defined by Verdu and Han, is proved in Part I. It is
assumed that random codes are permitted. The word ""universal"" in the title of
this paper refers to the fact that the channel model is compound. The proof
uses a layered black-box or a layered input-output view-point. In particular,
only the end-to-end description of the channel as being capable of
communicating a source to within a certain distortion level is used when
proving separation. This implies that the channel model does not play any role
for separation to hold as long as there is a source model. Further implications
of the layered black-box view-point are discussed.
Optimality of source-medium separation for multi-user communication with
fidelity criteria over a general, compound medium in the unicast setting is
proved in Part II, thus generalizing Part I to the unicast, multi-user setting.
Part III gets to an understanding of the question, ""Why is a channel which is
capable of communicating a source to within a certain distortion level, also
capable of communicating bits at any rate less than the infimum of the rates
needed to code the source to within the distortion level"": this lies at the
heart of why optimality of separation for communication with a fidelity
criterion holds. The perspective taken to get to this understanding is a
randomized covering-packing perspective, and the proof is operational."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1205.1907,https://arxiv.org/abs/1205.1907,"Abstract:  In this paper, we study distributed estimation and control problems over
graphs under partially nested information patterns. We show a duality result
that is very similar to the classical duality result between state estimation
and state feedback control with a classical information pattern, under the
condition that the disturbances entering different systems on the graph are
uncorrelated. The distributed estimation problem decomposes into $N$ separate
estimation problems, where $N$ is the number of interconnected subsystems over
the graph, and the solution to each subproblem is simply the optimal Kalman
filter. This also gives the solution to the distributed control problem due to
the duality of distributed estimation and control under partially nested
information pattern. We then consider a weighted distributed estimation
problem, where we get coupling between the estimators, and separation between
the estimators is not possible. We propose a solution based on linear quadratic
team decision theory, which provides a generalized Riccati equation for teams.
We show that the weighted estimation problem is the dual to a distributed state
feedback problem, where the disturbances entering the interconnected systems
are correlated."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1106.2055,https://arxiv.org/abs/1106.2055,"Abstract:  Given the possibility of communication systems failing catastrophically, we
investigate limits to communicating over channels that fail at random times.
These channels are finite-state semi-Markov channels. We show that
communication with arbitrarily small probability of error is not possible.
Making use of results in finite blocklength channel coding, we determine
sequences of blocklengths that optimize transmission volume communicated at
fixed maximum message error probabilities. We provide a partial ordering of
communication channels. A dynamic programming formulation is used to show the
structural result that channel state feedback does not improve performance."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1106.1401,https://arxiv.org/abs/1106.1401,"Abstract:  The paper proposes a framework for modeling and analysis of the dynamics of
supply, demand, and clearing prices in power system with real-time retail
pricing and information asymmetry. Real-time retail pricing is characterized by
passing on the real-time wholesale electricity prices to the end consumers, and
is shown to create a closed-loop feedback system between the physical layer and
the market layer of the power system. In the absence of a carefully designed
control law, such direct feedback between the two layers could increase
volatility and lower the system's robustness to uncertainty in demand and
generation. A new notion of generalized price-elasticity is introduced, and it
is shown that price volatility can be characterized in terms of the system's
maximal relative price elasticity, defined as the maximal ratio of the
generalized price-elasticity of consumers to that of the producers. As this
ratio increases, the system becomes more volatile, and eventually, unstable. As
new demand response technologies and distributed storage increase the
price-elasticity of demand, the architecture under examination is likely to
lead to increased volatility and possibly instability. This highlights the need
for assessing architecture systematically and in advance, in order to optimally
strike the trade-offs between volatility, economic efficiency, and system
reliability."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:1002.1300,https://arxiv.org/abs/1002.1300,"Abstract:  We prove that in order to communicate independent sources (this is the
unicast problem) between various users over an unknown medium to within various
distortion levels, it is sufficient to consider source-channel separation based
architectures: architectures which first compress the sources to within the
corresponding distortion levels followed by reliable communication over the
unknown medium. We are reducing the problem of universal rate-distortion
communication of independent sources over a network to the universal reliable
communication problem over networks. This is a reductionist view. We are not
solving the reliable communication problem in networks."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:0911.3872,https://arxiv.org/abs/0911.3872,"Abstract:  An operational perspective is used to understand the relationship between
source and channel coding. This is based on a direct reduction of one problem
to another that uses random coding (and hence common randomness) but unlike all
prior work, does not involve any functional computations, in particular, no
mutual-information computations. This result is then used to prove a universal
source-channel separation theorem in the rate-distortion context where
universality is in the sense of a compound ``general channel.''"
Sanjoy Mitter,Mitter_Sanjoy,arXiv:cs/0610146,https://arxiv.org/abs/cs/0610146,"Abstract:  In part I, we reviewed how Shannon's classical notion of capacity is not
sufficient to characterize a noisy communication channel if the channel is
intended to be used as part of a feedback loop to stabilize an unstable scalar
linear system. While classical capacity is not enough, a sense of capacity
(parametrized by reliability) called ""anytime capacity"" is both necessary and
sufficient for channel evaluation in this context. The rate required is the log
of the open-loop system gain and the required reliability comes from the
desired sense of stability. Sufficiency is maintained even in cases with noisy
observations and without any explicit feedback between the observer and the
controller. This established the asymptotic equivalence between scalar
stabilization problems and delay-universal communication problems with
feedback.
Here in part II, the vector-state generalizations are established and it is
the magnitudes of the unstable eigenvalues that play an essential role. To deal
with such systems, the concept of the anytime rate-region is introduced. This
is the region of rates that the channel can support while still meeting
potentially different anytime reliability targets for parallel message streams.
All the scalar results generalize on an eigenvalue by eigenvalue basis. When
there is no explicit feedback of the noisy channel outputs, the intrinsic delay
of the unstable system tells us what the feedback delay needs to be while
evaluating the anytime-rate-region for the channel. An example involving a
binary erasure channel is used to illustrate how differentiated service is
required in any separation-based control architecture."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:cs/0610143,https://arxiv.org/abs/cs/0610143,"Abstract:  Our understanding of information in systems has been based on the foundation
of memoryless processes. Extensions to stable Markov and auto-regressive
processes are classical. Berger proved a source coding theorem for the
marginally unstable Wiener process, but the infinite-horizon exponentially
unstable case has been open since Gray's 1970 paper. There were also no
theorems showing what is needed to communicate such processes across noisy
channels.
In this work, we give a fixed-rate source-coding theorem for the
infinite-horizon problem of coding an exponentially unstable Markov process.
The encoding naturally results in two distinct bitstreams that have
qualitatively different QoS requirements for communicating over a noisy medium.
The first stream captures the information that is accumulating within the
nonstationary process and requires sufficient anytime reliability from the
channel used to communicate the process. The second stream captures the
historical information that dissipates within the process and is essentially
classical. This historical information can also be identified with a natural
stable counterpart to the unstable process. A converse demonstrating the
fundamentally layered nature of unstable sources is given by means of
information-embedding ideas."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:cs/0610142,https://arxiv.org/abs/cs/0610142,"Abstract:  Shannon proved that if we can transmit bits reliably at rates larger than the
rate distortion function $R(D)$, then we can transmit this source to within a
distortion $D$. We answer the converse question ``If we can transmit a source
to within a distortion $D$, can we transmit bits reliably at rates less than
the rate distortion function?'' in the affirmative. This can be viewed as a
direct converse of the rate distortion theorem."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:cs/0609139,https://arxiv.org/abs/cs/0609139,"Abstract:  We introduce a general framework for treating channels with memory and
feedback. First, we generalize Massey's concept of directed information and use
it to characterize the feedback capacity of general channels. Second, we
present coding results for Markov channels. This requires determining
appropriate sufficient statistics at the encoder and decoder. Third, a dynamic
programming framework for computing the capacity of Markov channels is
presented. Fourth, it is shown that the average cost optimality equation (ACOE)
can be viewed as an implicit single-letter characterization of the capacity.
Fifth, scenarios with simple sufficient statistics are described."
Sanjoy Mitter,Mitter_Sanjoy,arXiv:cs/0601007,https://arxiv.org/abs/cs/0601007,"Abstract:  We review how Shannon's classical notion of capacity is not enough to
characterize a noisy communication channel if the channel is intended to be
used as part of a feedback loop to stabilize an unstable scalar linear system.
While classical capacity is not enough, another sense of capacity (parametrized
by reliability) called ``anytime capacity'' is shown to be necessary for the
stabilization of an unstable process. The required rate is given by the log of
the unstable system gain and the required reliability comes from the sense of
stability desired. A consequence of this necessity result is a sequential
generalization of the Schalkwijk/Kailath scheme for communication over the AWGN
channel with feedback.
In cases of sufficiently rich information patterns between the encoder and
decoder, adequate anytime capacity is also shown to be sufficient for there to
exist a stabilizing controller. These sufficiency results are then generalized
to cases with noisy observations, delayed control actions, and without any
explicit feedback between the observer and the controller. Both necessary and
sufficient conditions are extended to continuous time systems as well. We close
with comments discussing a hierarchy of difficulty for communication problems
and how these results establish where stabilization problems sit in that
hierarchy."
Robert Morris,Morris_Robert,arXiv:1901.11465,https://arxiv.org/abs/1901.11465,"Abstract:  A covering system is a finite collection of arithmetic progressions whose
union is the set of integers. The study of covering systems with distinct
moduli was initiated by Erdős in 1950, and over the following decades
numerous problems were posed regarding their properties. One particularly
notorious question, due to Erdős, asks whether there exist covering systems
whose moduli are distinct and all odd. We show that if in addition one assumes
the moduli are square-free, then there must be an even modulus."
Robert Morris,Morris_Robert,arXiv:1901.09950,https://arxiv.org/abs/1901.09950,"Abstract:  We report the discovery of TOI-172 b from the Transiting Exoplanet Survey
Satellite (TESS) mission, a massive hot Jupiter transiting a slightly evolved
G-star with a 9.48-day orbital period. This is the first planet to be confirmed
from analysis of only the TESS full frame images, because the host star was not
chosen as a two minute cadence target. From a global analysis of the TESS
photometry and follow-up observations carried out by the TESS follow-up working
group, TOI-172 (TIC 29857954) is a slightly evolved star with an effective
temperature of T$_{\rm eff}$ =$5644\pm50$~K, a mass of M$_{\star}$ =
$1.124^{+0.063}_{-0.060}$ M$_{\odot}$, radius of R$_{\star}$ =
$1.774^{+0.046}_{-0.043}$ R$_{\odot}$, a surface gravity of $\log$ g$_{\star}$
= $3.993^{+0.026}_{-0.027}$, and an age of $7.5^{+1.6}_{-1.4}$ Gyr. Its
planetary companion (TOI-172 b) has a radius of R$_{\rm P}$ =
$0.965^{+0.033}_{-0.030}$ R$_{\rm J}$, a mass of M$_{\rm P}$ =
$5.41^{+0.21}_{-0.20}$ M$_{\rm J}$, and is on an eccentric orbit (e =
$0.3805^{+0.0092}_{-0.0090}$). TOI-172 b is one of the few known massive giant
planets on a highly eccentric short-period orbit. Future study of the
atmosphere of this planet and its system architecture offer opportunities to
understand the formation and evolution of similar systems."
Robert Morris,Morris_Robert,arXiv:1811.03547,https://arxiv.org/abs/1811.03547,"Abstract:  Since their introduction by Erdős in 1950, covering systems (that is,
finite collections of arithmetic progressions that cover the integers) have
been extensively studied, and numerous questions and conjectures have been
posed regarding the existence of covering systems with various properties. In
particular, Erdős asked if the moduli can be distinct and all arbitrarily
large, Erdős and Selfridge asked if the moduli can be distinct and all odd,
and Schinzel conjectured that in any covering system there exists a pair of
moduli, one of which divides the other.
Another beautiful conjecture, proposed by Erdős and Graham in 1980,
states that if the moduli are distinct elements of the interval $[n,Cn]$, and
$n$ is sufficiently large, then the density of integers uncovered by the union
is bounded below by a constant (depending only on $C$). This conjecture was
confirmed (in a strong form) by Filaseta, Ford, Konyagin, Pomerance and Yu in
2007, who moreover asked whether the same conclusion holds if the moduli are
distinct and sufficiently large, and $\sum_{i=1}^k \frac{1}{d_i} < C$. Although
this condition turns out not to be sufficiently strong to imply the desired
conclusion, as the main result of this paper we will give an essentially best
possible condition which is sufficient.
Our method has a number of further applications. Most importantly, we prove
the conjecture of Schinzel stated above, which was made in 1967. We moreover
give an alternative (somewhat simpler) proof of a breakthrough result of Hough,
who resolved Erdős' minimum modulus problem, with an improved bound on the
smallest difference. Finally, we make further progress on the problem of
Erdős and Selfridge."
Robert Morris,Morris_Robert,arXiv:1811.03080,https://arxiv.org/abs/1811.03080,"Abstract:  We count orientations of $G(n,p)$ avoiding certain classes of oriented
graphs. In particular, we study $T_r(n,p)$, the number of orientations of the
binomial random graph $G(n,p)$ in which every copy of $K_r$ is transitive, and
$S_r(n,p)$, the number of orientations of $G(n,p)$ containing no strongly
connected copy of $K_r$. We give the correct order of growth of $\log T_r(n,p)$
and $\log S_r(n,p)$ up to polylogarithmic factors; for orientations with no
cyclic triangle, this significantly improves a result of Allen, Kohayakawa,
Mota and Parente. We also discuss the problem for a single forbidden oriented
graph, and state a number of open problems and conjectures."
Robert Morris,Morris_Robert,arXiv:1810.02341,https://arxiv.org/abs/1810.02341,"Abstract:  We report the first confirmation of a hot Jupiter discovered by the
Transiting Exoplanet Survey Satellite (TESS) mission: HD 202772A b. The transit
signal was detected in the data from TESS Sector 1, and was confirmed to be of
planetary origin through radial-velocity measurements. HD 202772A b is orbiting
a mildly evolved star with a period of 3.3 days. With an apparent magnitude of
V = 8.3, the star is among the brightest known to host a hot Jupiter. Based on
the 27days of TESS photometry, and radial velocity data from the CHIRON and
HARPS spectrographs, the planet has a mass of 1.008+/-0.074 M_J and radius of
1.562+/-0.053 R_J , making it an inflated gas giant. HD 202772A b is a rare
example of a transiting hot Jupiter around a quickly evolving star. It is also
one of the most strongly irradiated hot Jupiters currently known."
Robert Morris,Morris_Robert,arXiv:1806.08931,https://arxiv.org/abs/1806.08931,"Abstract:  In the $r$-neighbour bootstrap process on a graph $G$, vertices are infected
(in each time step) if they have at least $r$ already-infected neighbours.
Motivated by its close connections to models from statistical physics, such as
the Ising model of ferromagnetism, and kinetically constrained spin models of
the liquid-glass transition, the most extensively-studied case is the
two-neighbour bootstrap process on the two-dimensional grid $[n]^2$. Around 15
years ago, in a major breakthrough, Holroyd determined the sharp threshold for
percolation in this model, and his bounds were subsequently sharpened further
by Gravner and Holroyd, and by Gravner, Holroyd and Morris.
In this paper we strengthen the lower bound of Gravner, Holroyd and Morris by
proving that the critical probability $p_c\big( [n]^2,2 \big)$ for percolation
in the two-neighbour model on $[n]^2$ satisfies \[p_c\big( [n]^2,2 \big) =
\frac{\pi^2}{18\log n} - \frac{\Theta(1)}{(\log n)^{3/2}}\,.\] The proof of
this result requires a very precise understanding of the typical growth of a
critical droplet, and involves a number of technical innovations. We expect
these to have other applications, for example, to the study of more general
two-dimensional cellular automata, and to the $r$-neighbour process in higher
dimensions."
Robert Morris,Morris_Robert,arXiv:1806.03706,https://arxiv.org/abs/1806.03706,"Abstract:  The method of hypergraph containers, introduced recently by Balogh, Morris,
and Samotij, and independently by Saxton and Thomason, has proved to be an
extremely useful tool in the study of various monotone graph properties. In
particular, a fairly straightforward application of this technique allows one
to locate, for each non-bipartite graph $H$, the threshold at which the
distribution of edges in a typical $H$-free graph with a given number of edges
undergoes a transition from 'random-like' to 'structured'. On the other hand,
for non-monotone hereditary graph properties the standard version of this
method does not allow one to establish even the existence of such a threshold.
In this paper we introduce a refinement of the container method that takes
into account the asymmetry between edges and non-edges in a sparse member of a
hereditary graph property. As an application, we determine the approximate
structure of a typical graph with $n$ vertices, $m$ edges, and no induced copy
of the $4$-cycle, for each function $m = m(n)$ satisfying $n^{4/3} (\log n)^4
\leqslant m \ll n^2$. We show that almost all such graphs $G$ have the
following property: the vertex set of $G$ can be partitioned into an
'almost-independent' set (a set with $o(m)$ edges) and an 'almost-clique' (a
set inducing a subgraph with density $1-o(1)$). The lower bound on $m$ is
optimal up to a polylogarithmic factor, as standard arguments show that if $n
\ll m \ll n^{4/3}$, then almost all such graphs are 'random-like'. As a further
consequence, we deduce that the random graph $G(n,p)$ conditioned to contain no
induced $4$-cycles undergoes phase transitions at $p = n^{-2/3 + o(1)}$ and $p
= n^{-1/3 + o(1)}$."
Robert Morris,Morris_Robert,arXiv:1801.04584,https://arxiv.org/abs/1801.04584,"Abstract:  In this survey we describe a recently-developed technique for bounding the
number (and controlling the typical structure) of finite objects with forbidden
substructures. This technique exploits a subtle clustering phenomenon exhibited
by the independent sets of uniform hypergraphs whose edges are sufficiently
evenly distributed; more precisely, it provides a relatively small family of
'containers' for the independent sets, each of which contains few edges. We
attempt to convey to the reader a general high-level overview of the method,
focusing on a small number of illustrative applications in areas such as
extremal graph theory, Ramsey theory, additive combinatorics, and discrete
geometry, and avoiding technical details as much as possible."
Robert Morris,Morris_Robert,arXiv:1801.01934,https://arxiv.org/abs/1801.01934,"Abstract:  Kinetically constrained models (KCM) are reversible interacting particle
systems on $\mathbb Z^d$ with continuous time Markov dynamics of Glauber type,
which represent a natural stochastic (and non-monotone) counterpart of the
family of cellular automata known as $\mathcal U$-bootstrap percolation. KCM
also display some of the peculiar features of the so-called ""glassy dynamics"",
and as such they are extensively used in the physics literature to model the
liquid-glass transition, a major and longstanding open problem in condensed
matter physics.
We consider two-dimensional KCM with update rule $\mathcal U$, and focus on
proving universality results for the mean infection time of the origin, in the
same spirit as those recently established in the setting of $\mathcal
U$-bootstrap percolation. We first identify what we believe are the correct
universality classes, which turn out to be different from those of $\mathcal
U$-bootstrap percolation. We then prove universal upper bounds on the mean
infection time within each class, which we conjecture to be sharp up to
logarithmic corrections. In certain cases, including all supercritical models,
and the well-known Duarte model, our conjecture has recently been confirmed in
[MMT]. In fact, in these cases our upper bound is sharp up to a constant factor
in the exponent. For certain classes of update rules, it turns out that the
infection time of the KCM diverges much faster than for the corresponding
$\mathcal U$-bootstrap process when the equilibrium density of infected sites
goes to zero. This is due to the occurrence of energy barriers which determine
the dominant behaviour for KCM, but which do not matter for the monotone
bootstrap dynamics."
Robert Morris,Morris_Robert,arXiv:1712.07179,https://arxiv.org/abs/1712.07179,"Abstract:  We present a short, self-contained, and purely combinatorial proof of
Linnik's theorem: for any $\varepsilon > 0$ there exists a constant
$C_\varepsilon$ such that for any $N$, there are at most $C_\varepsilon$ primes
$p \leqslant N$ such that the least positive quadratic non-residue modulo $p$
exceeds $N^\varepsilon$."
Robert Morris,Morris_Robert,arXiv:1710.06758,https://arxiv.org/abs/1710.06758,"Abstract:  We present the Kepler Object of Interest (KOI) catalog of transiting
exoplanets based on searching four years of Kepler time series photometry (Data
Release 25, Q1-Q17). The catalog contains 8054 KOIs of which 4034 are planet
candidates with periods between 0.25 and 632 days. Of these candidates, 219 are
new and include two in multi-planet systems (KOI-82.06 and KOI-2926.05), and
ten high-reliability, terrestrial-size, habitable zone candidates. This catalog
was created using a tool called the Robovetter which automatically vets the
DR25 Threshold Crossing Events (TCEs, Twicken et al. 2016). The Robovetter also
vetted simulated data sets and measured how well it was able to separate TCEs
caused by noise from those caused by low signal-to-noise transits. We discusses
the Robovetter and the metrics it uses to sort TCEs. For orbital periods less
than 100 days the Robovetter completeness (the fraction of simulated transits
that are determined to be planet candidates) across all observed stars is
greater than 85%. For the same period range, the catalog reliability (the
fraction of candidates that are not due to instrumental or stellar noise) is
greater than 98%. However, for low signal-to-noise candidates between 200 and
500 days around FGK dwarf stars, the Robovetter is 76.7% complete and the
catalog is 50.5% reliable. The KOI catalog, the transit fits and all of the
simulated data used to characterize this catalog are available at the NASA
Exoplanet Archive."
Robert Morris,Morris_Robert,arXiv:1705.09585,https://arxiv.org/abs/1705.09585,"Abstract:  Individuals on social media may reveal themselves to be in various states of
crisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting crisis
from social media text automatically and accurately can have profound
consequences. However, detecting a general state of crisis without explaining
why has limited applications. An explanation in this context is a coherent,
concise subset of the text that rationalizes the crisis detection. We explore
several methods to detect and explain crisis using a combination of neural and
non-neural techniques. We evaluate these techniques on a unique data set
obtained from Koko, an anonymous emotional support network available through
various messaging applications. We annotate a small subset of the samples
labeled with crisis with corresponding explanations. Our best technique
significantly outperforms the baseline for detection and explanation."
Robert Morris,Morris_Robert,arXiv:1610.06479,https://arxiv.org/abs/1610.06479,"Abstract:  We study survival among two competing types in two settings: a planar growth
model related to two-neighbour bootstrap percolation, and a system of urns with
graph-based interactions. In the planar growth model, uncoloured sites are
given a colour at rate $0$, $1$ or $\infty$, depending on whether they have
zero, one, or at least two neighbours of that colour. In the urn scheme, each
vertex of a graph $G$ has an associated urn containing some number of either
blue or red balls (but not both). At each time step, a ball is chosen uniformly
at random from all those currently present in the system, a ball of the same
colour is added to each neighbouring urn, and balls in the same urn but of
different colours annihilate on a one-for-one basis. We show that, for every
connected graph $G$ and every initial configuration, only one colour survives
almost surely. As a corollary, we deduce that in the two-type growth model on
$\mathbb{Z}^2$, one of the colours only infects a finite number of sites with
probability one. We also discuss generalisations to higher dimensions and
multi-type processes, and list a number of open problems and conjectures."
Robert Morris,Morris_Robert,arXiv:1608.03857,https://arxiv.org/abs/1608.03857,"Abstract:  Consider a random sequence of $N$ integers, each chosen uniformly and
independently from the set $\{1,\dots,x\}$. Motivated by applications to
factorisation algorithms such as Dixon's algorithm, the quadratic sieve, and
the number field sieve, Pomerance in 1994 posed the following problem: how
large should $N$ be so that, with high probability, this sequence contains a
subsequence, the product of whose elements is a perfect square? Pomerance
determined asymptotically the logarithm of the threshold for this event, and
conjectured that it in fact exhibits a sharp threshold in $N$. More recently,
Croot, Granville, Pemantle and Tetali determined the threshold up to a factor
of $4/\pi + o(1)$ as $x \to \infty$, and made a conjecture regarding the
location of the sharp threshold.
In this paper we prove both of these conjectures, by determining the sharp
threshold for making squares. Our proof combines techniques from combinatorics,
probability and analytic number theory; in particular, we use the so-called
method of self-correcting martingales in order to control the size of the
2-core of the random hypergraph that encodes the prime factors of our random
numbers. Our method also gives a new (and completely different) proof of the
upper bound in the main theorem of Croot, Granville, Pemantle and Tetali."
Robert Morris,Morris_Robert,arXiv:1604.06140,https://arxiv.org/abs/1604.06140,"Abstract:  We present results of the final Kepler Data Processing Pipeline search for
transiting planet signals in the full 17-quarter primary mission data set. The
search includes a total of 198,709 stellar targets, of which 112,046 were
observed in all 17 quarters and 86,663 in fewer than 17 quarters. We report on
17,230 targets for which at least one transit signature is identified that
meets the specified detection criteria: periodicity, minimum of three observed
transit events, detection statistic (i.e., signal-to-noise ratio) in excess of
the search threshold, and passing grade on three statistical transit
consistency tests. Light curves for which a transit signal is identified are
iteratively searched for additional signatures after a limb-darkened transiting
planet model is fitted to the data and transit events are removed. The search
for additional planets adds 16,802 transit signals for a total of 34,032; this
far exceeds the number of transit signatures identified in prior pipeline runs.
There was a strategic emphasis on completeness over reliability for the final
Kepler transit search. A comparison of the transit signals against a set of
3402 well-established, high-quality Kepler Objects of Interest yields a
recovery rate of 99.8%. The high recovery rate must be weighed against a large
number of false-alarm detections. We examine characteristics of the planet
population implied by the transiting planet model fits with an emphasis on
detections that would represent small planets orbiting in the habitable zone of
their host stars."
Robert Morris,Morris_Robert,arXiv:1603.05237,https://arxiv.org/abs/1603.05237,"Abstract:  The class of critical bootstrap percolation models in two dimensions was
recently introduced by Bollobás, Smith and Uzzell, and the critical threshold
for percolation was determined up to a constant factor for all such models by
the authors of this paper. Here we develop and refine the techniques introduced
in that paper in order to determine a sharp threshold for the Duarte model.
This resolves a question of Mountford from 1995, and is the first result of its
type for a model with drift."
Robert Morris,Morris_Robert,arXiv:1508.06267,https://arxiv.org/abs/1508.06267,"Abstract:  We consider a dynamical process on a graph $G$, in which vertices are
infected (randomly) at a rate which depends on the number of their neighbours
that are already infected. This model includes bootstrap percolation and
first-passage percolation as its extreme points. We give a precise description
of the evolution of this process on the graph $\mathbb{Z}^2$, significantly
sharpening results of Dehghanpour and Schonmann. In particular, we determine
the typical infection time up to a constant factor for almost all natural
values of the parameters, and in a large range we obtain a stronger, sharp
threshold."
Robert Morris,Morris_Robert,arXiv:1508.03875,https://arxiv.org/abs/1508.03875,"Abstract:  The chromatic threshold $\delta_\chi(H,p)$ of a graph $H$ with respect to the
random graph $G(n,p)$ is the infimum over $d > 0$ such that the following holds
with high probability: the family of $H$-free graphs $G \subset G(n,p)$ with
minimum degree $\delta(G) \ge dpn$ has bounded chromatic number. The study of
$\delta_\chi(H) :=\delta_\chi(H,1)$ was initiated in 1973 by Erdős and
Simonovits. Recently $\delta_\chi(H)$ was determined for all graphs $H$. It is
known that $\delta_\chi(H,p) =\delta_\chi(H)$ for all fixed $p \in (0,1)$, but
that typically $\delta_\chi(H,p) \ne \delta_\chi(H)$ if $p = o(1)$.
Here we study the problem for sparse random graphs. We determine
$\delta_\chi(H,p)$ for most functions $p = p(n)$ when $H\in\{K_3,C_5\}$, and
also for all graphs $H$ with $\chi(H) \not\in \{3,4\}$."
Robert Morris,Morris_Robert,arXiv:1508.03870,https://arxiv.org/abs/1508.03870,"Abstract:  The chromatic threshold $\delta_\chi(H,p)$ of a graph $H$ with respect to the
random graph $G(n,p)$ is the infimum over $d > 0$ such that the following holds
with high probability: the family of $H$-free graphs $G \subset G(n,p)$ with
minimum degree $\delta(G) \ge dpn$ has bounded chromatic number. The study of
the parameter $\delta_\chi(H) := \delta_\chi(H,1)$ was initiated in 1973 by
Erdős and Simonovits, and was recently determined for all graphs $H$. In
this paper we show that $\delta_\chi(H,p) = \delta_\chi(H)$ for all fixed $p
\in (0,1)$, but that typically $\delta_\chi(H,p) \ne \delta_\chi(H)$ if $p =
o(1)$. We also make significant progress towards determining $\delta_\chi(H,p)$
for all graphs $H$ in the range $p = n^{-o(1)}$. In sparser random graphs the
problem is somewhat more complicated, and is studied in a separate paper."
Robert Morris,Morris_Robert,arXiv:1502.02038,https://arxiv.org/abs/1502.02038,"Abstract:  \We present the sixth catalog of Kepler candidate planets based on nearly 4
years of high precision photometry. This catalog builds on the legacy of
previous catalogs released by the Kepler project and includes 1493 new Kepler
Objects of Interest (KOIs) of which 554 are planet candidates, and 131 of these
candidates have best fit radii <1.5 R_earth. This brings the total number of
KOIs and planet candidates to 7305 and 4173 respectively. We suspect that many
of these new candidates at the low signal-to-noise limit may be false alarms
created by instrumental noise, and discuss our efforts to identify such
objects. We re-evaluate all previously published KOIs with orbital periods of
>50 days to provide a consistently vetted sample that can be used to improve
planet occurrence rate calculations. We discuss the performance of our planet
detection algorithms, and the consistency of our vetting products. The full
catalog is publicly available at the NASA Exoplanet Archive."
Robert Morris,Morris_Robert,arXiv:1501.07286,https://arxiv.org/abs/1501.07286,"Abstract:  The Kepler mission discovered 2842 exoplanet candidates with 2 years of data.
We provide updates to the Kepler planet candidate sample based upon 3 years
(Q1-Q12) of data. Through a series of tests to exclude false-positives,
primarily caused by eclipsing binary stars and instrumental systematics, 855
additional planetary candidates have been discovered, bringing the total number
known to 3697. We provide revised transit parameters and accompanying posterior
distributions based on a Markov Chain Monte Carlo algorithm for the cumulative
catalogue of Kepler Objects of Interest. There are now 130 candidates in the
cumulative catalogue that receive less than twice the flux the Earth receives
and more than 1100 have a radius less than 1.5 Rearth. There are now a dozen
candidates meeting both criteria, roughly doubling the number of candidate
Earth analogs. A majority of planetary candidates have a high probability of
being bonafide planets, however, there are populations of likely
false-positives. We discuss and suggest additional cuts that can be easily
applied to the catalogue to produce a set of planetary candidates with good
fidelity. The full catalogue is publicly available at the NASA Exoplanet
Archive."
Robert Morris,Morris_Robert,arXiv:1501.04075,https://arxiv.org/abs/1501.04075,"Abstract:  We prove that the probability of crossing a large square in quenched Voronoi
percolation converges to 1/2 at criticality, confirming a conjecture of
Benjamini, Kalai and Schramm from 1999. The main new tools are a quenched
version of the box-crossing property for Voronoi percolation at criticality,
and an Efron-Stein type bound on the variance of the probability of the
crossing event in terms of the sum of the squares of the influences. As a
corollary of the proof, we moreover obtain that the quenched crossing event at
criticality is almost surely noise sensitive."
Robert Morris,Morris_Robert,arXiv:1406.6961,https://arxiv.org/abs/1406.6961,"Abstract:  In 1987, Kolaitis, Prömel and Rothschild proved that, for every fixed $r
\in \mathbb{N}$, almost every $n$-vertex $K_{r+1}$-free graph is $r$-partite.
In this paper we extend this result to all functions $r = r(n)$ with $r
\leqslant (\log n)^{1/4}$. The proof combines a new (close to sharp)
supersaturation version of the Erdős-Simonovits stability theorem, the
hypergraph container method, and a counting technique developed by Balogh,
Bollobás and Simonovits."
Robert Morris,Morris_Robert,arXiv:1406.6680,https://arxiv.org/abs/1406.6680,"Abstract:  We study the class of monotone, two-state, deterministic cellular automata,
in which sites are activated (or `infected') by certain configurations of
nearby infected sites. These models have close connections to statistical
physics, and several specific examples have been extensively studied in recent
years by both mathematicians and physicists. This general setting was first
studied only recently, however, by Bollobás, Smith and Uzzell, who showed
that the family of all such `bootstrap percolation' models on $\mathbb{Z}^2$
can be naturally partitioned into three classes, which they termed subcritical,
critical and supercritical.
In this paper we determine the order of the threshold for percolation
(complete occupation) for every critical bootstrap percolation model in two
dimensions. This `universality' theorem includes as special cases results of
Aizenman and Lebowitz, Gravner and Griffeath, Mountford, and van Enter and
Hulshof, significantly strengthens bounds of Bollobás, Smith and Uzzell, and
complements recent work of Balister, Bollobás, Przykucki and Smith on
subcritical models."
Robert Morris,Morris_Robert,arXiv:1404.5258,https://arxiv.org/abs/1404.5258,"Abstract:  We show that, for $pn \to \infty$, the largest set in a $p$-random sub-family
of the power set of $\{1, \ldots, n\}$ containing no $k$-chain has size $( k -
1 + o(1) ) p \binom{n}{n/2}$ with high probability. This confirms a conjecture
of Osthus, and has been proved independently by Balogh, Mycroft and Treglown."
Robert Morris,Morris_Robert,arXiv:1310.3236,https://arxiv.org/abs/1310.3236,"Abstract:  We study sum-free sets in sparse random subsets of even order abelian groups.
In particular, we determine the sharp threshold for the following property: the
largest such set is contained in some maximum-size sum-free subset of the
group. This theorem extends recent work of Balogh, Morris and Samotij, who
resolved the case G = Z_{2n}, and who obtained a weaker threshold (up to a
constant factor) in general."
Robert Morris,Morris_Robert,arXiv:1309.2927,https://arxiv.org/abs/1309.2927,"Abstract:  One of the most basic questions one can ask about a graph $H$ is: how many
$H$-free graphs on $n$ vertices are there? For non-bipartite $H$, the answer to
this question has been well-understood since 1986, when Erdős, Frankl and
Rödl proved that there are $2^{(1 + o(1)) ex(n,H)}$ such graphs. For
bipartite graphs, however, much less is known: even the weaker bound
$2^{O(ex(n,H))}$ has been proven in only a few special cases: for cycles of
length four and six, and for some complete bipartite graphs.
For even cycles, Bondy and Simonovits proved in the 1970s that ex$(n,C_{2l})
= O( n^{1 + 1/l} )$, and this bound is conjectured to be sharp up to the
implicit constant. In this paper we prove that the number of $C_{2l}$-free
graphs on $n$ vertices is at most $2^{O(n^{1 + 1/l})}$, confirming a conjecture
of Erdős. Our proof uses the hypergraph container method, which was
developed recently (and independently) by Balogh, Morris and Samotij, and by
Saxton and Thomason, together with a new 'balanced supersaturation theorem' for
even cycles. We moreover show that there are at least $2^{(1 + c)ex(n,C_6)}$
$C_6$-free graphs on $n$ vertices for some $c > 0$ and infinitely many values
of $n$, disproving a well-known and natural conjecture. As a further
application of our method, we essentially resolve the so-called Turán problem
on the Erdős-Rényi random graph $G(n,p)$ for both even cycles and
complete bipartite graphs."
Robert Morris,Morris_Robert,arXiv:1307.5967,https://arxiv.org/abs/1307.5967,"Abstract:  Two central topics of study in combinatorics are the so-called evolution of
random graphs, introduced by the seminal work of Erdős and Rényi, and the
family of $H$-free graphs, that is, graphs which do not contain a subgraph
isomorphic to a given (usually small) graph $H$. A widely studied problem that
lies at the interface of these two areas is that of determining how the
structure of a typical $H$-free graph with $n$ vertices and $m$ edges changes
as $m$ grows from $0$ to $\text{ex}(n,H)$. In this paper, we resolve this
problem in the case when $H$ is a clique, extending a classical result of
Kolaitis, Prömel, and Rothschild. In particular, we prove that for every $r
\ge 2$, there is an explicit constant $\theta_r$ such that, letting $m_r =
\theta_r n^{2-\frac{2}{r+2}} (\log n)^{1/\left[\binom{r+1}{2}-1\right]}$, the
following holds for every positive constant $\varepsilon$. If $m \ge
(1+\varepsilon) m_r$, then almost all $K_{r+1}$-free $n$-vertex graphs with $m$
edges are $r$-partite, whereas if $n \ll m \le (1-\varepsilon)m_r$, then almost
all of them are not $r$-partite."
Robert Morris,Morris_Robert,arXiv:1306.0461,https://arxiv.org/abs/1306.0461,"Abstract:  The Ramsey number r(K_s,Q_n) is the smallest positive integer N such that
every red-blue colouring of the edges of the complete graph K_N on N vertices
contains either a red n-dimensional hypercube, or a blue clique on s vertices.
Answering a question of Burr and Erdős from 1983, and improving on recent
results of Conlon, Fox, Lee and Sudakov, and of the current authors, we show
that r(K_s,Q_n) = (s-1) (2^n - 1) + 1 for every s \in \N and every sufficiently
large n \in \N."
Robert Morris,Morris_Robert,arXiv:1305.5587,https://arxiv.org/abs/1305.5587,"Abstract:  Since the discovery of the first exoplanet we have known that other planetary
systems can look quite unlike our own. However, until recently we have only
been able to probe the upper range of the planet size distribution. The high
precision of the Kepler space telescope has allowed us to detect planets that
are the size of Earth and somewhat smaller, but no previous planets have been
found that are smaller than those we see in our own Solar System. Here we
report the discovery of a planet significantly smaller than Mercury. This tiny
planet is the innermost of three planets that orbit the Sun-like host star,
which we have designated Kepler-37. Owing to its extremely small size, similar
to that of Earth's Moon, and highly irradiated surface, Kepler-37b is probably
a rocky planet with no atmosphere or water, similar to Mercury."
Robert Morris,Morris_Robert,arXiv:1305.3079,https://arxiv.org/abs/1305.3079,"Abstract:  We study the number of $k$-element sets $A \subset \{1,\ldots,N\}$ with $|A +
A| \leq K|A|$ for some (fixed) $K > 0$. Improving results of the first author
and of Alon, Balogh, Samotij and the second author, we determine this number up
to a factor of $2^{o(k)} N^{o(1)}$ for most $N$ and $k$. As a consequence of
this and a further new result concerning the number of sets $A \subset
\mathbf{Z}/N\mathbf{Z}$ with $|A +A| \leq c |A|^2$, we deduce that the random
Cayley graph on $\mathbf{Z}/N\mathbf{Z}$ with edge density~$\frac{1}{2}$ has no
clique or independent set of size greater than $\big( 2 + o(1) \big) \log_2 N$,
asymptotically the same as for the Erdős-Rényi random graph. This
improves a result of the first author from 2003 in which a bound of $160 \log_2
N$ was obtained. As a second application, we show that if the elements of $A
\subset \mathbf{N}$ are chosen at random, each with probability $1/2$, then the
probability that $A+A$ misses exactly $k$ elements of $\mathbf{N}$ is equal to
$\big( 2 + o(1) \big)^{-k/2}$ as $k \to \infty$."
Robert Morris,Morris_Robert,arXiv:1303.0858,https://arxiv.org/abs/1303.0858,"Abstract:  We present high precision photometry of Kepler-41, a giant planet in a 1.86
day orbit around a G6V star that was recently confirmed through radial velocity
measurements. We have developed a new method to confirm giant planets solely
from the photometric light curve, and we apply this method herein to Kepler-41
to establish the validity of this technique. We generate a full phase
photometric model by including the primary and secondary transits, ellipsoidal
variations, Doppler beaming and reflected/emitted light from the planet. Third
light contamination scenarios that can mimic a planetary transit signal are
simulated by injecting a full range of dilution values into the model, and we
re-fit each diluted light curve model to the light curve. The resulting
constraints on the maximum occultation depth and stellar density combined with
stellar evolution models rules out stellar blends and provides a measurement of
the planet's mass, size, and temperature. We expect about two dozen Kepler
giant planets can be confirmed via this method."
Robert Morris,Morris_Robert,arXiv:1302.6279,https://arxiv.org/abs/1302.6279,"Abstract:  The areas of Ramsey theory and random graphs have been closely linked ever
since Erdős' famous proof in 1947 that the 'diagonal' Ramsey numbers $R(k)$
grow exponentially in $k$. In the early 1990s, the triangle-free process was
introduced as a model which might potentially provide good lower bounds for the
'off-diagonal' Ramsey numbers $R(3,k)$. In this model, edges of $K_n$ are
introduced one-by-one at random and added to the graph if they do not create a
triangle; the resulting final (random) graph is denoted $G_{n,\triangle}$. In
2009, Bohman succeeded in following this process for a positive fraction of its
duration, and thus obtained a second proof of Kim's celebrated result that
$R(3,k) = \Theta \big( k^2 / \log k \big)$.
In this paper we improve the results of both Bohman and Kim, and follow the
triangle-free process all the way to its asymptotic end. In particular, we
shall prove that $$e\big( G_{n,\triangle} \big) \,=\, \left(
\frac{1}{2\sqrt{2}} + o(1) \right) n^{3/2} \sqrt{\log n },$$ with high
probability as $n \to \infty$. We also obtain several pseudorandom properties
of $G_{n,\triangle}$, and use them to bound its independence number, which
gives as an immediate corollary $$R(3,k) \, \ge \, \left( \frac{1}{4} - o(1)
\right) \frac{k^2}{\log k}.$$ This significantly improves Kim's lower bound,
and is within a factor of $4 + o(1)$ of the best known upper bound, proved by
Shearer over 25 years ago."
Robert Morris,Morris_Robert,arXiv:1302.3840,https://arxiv.org/abs/1302.3840,"Abstract:  The Ramsey number r(K_3,Q_n) is the smallest integer N such that every
red-blue colouring of the edges of the complete graph K_N contains either a red
n-dimensional hypercube, or a blue triangle. Almost thirty years ago, Burr and
Erdős conjectured that r(K_3,Q_n) = 2^{n+1} - 1 for every n \in \N, but the
first non-trivial upper bound was obtained only recently, by Conlon, Fox, Lee
and Sudakov, who proved that r(K_3,Q_n) \le 7000 \cdot 2^n. Here we show that
r(K_3,Q_n) = (1 + o(1)) 2^{n+1} as n \to \infty."
Robert Morris,Morris_Robert,arXiv:1204.6530,https://arxiv.org/abs/1204.6530,"Abstract:  Many important theorems in combinatorics, such as Szemerédi's theorem on
arithmetic progressions and the Erdős-Stone Theorem in extremal graph
theory, can be phrased as statements about independent sets in uniform
hypergraphs. In recent years, an important trend in the area has been to extend
such classical results to the so-called sparse random setting. This line of
research culminated recently in the breakthroughs of Conlon and Gowers and of
Schacht, who developed general tools for solving problems of this type.
In this paper, we provide a third, completely different approach to proving
extremal and structural results in sparse random sets. We give a structural
characterization of the independent sets in a large class of uniform
hypergraphs by showing that every independent set is almost contained in one of
a small number of relatively sparse sets. We then derive many interesting
results as fairly straightforward consequences of this abstract theorem. In
particular, we prove the well-known conjecture of Kohayakawa, \L uczak and
Rödl, a probabilistic embedding lemma for sparse graphs. We also give
alternative proofs of many of the results of Conlon and Gowers and Schacht, and
obtain their natural counting versions, which in some cases are considerably
stronger. We moreover prove a sparse version of the Erdős-Frankl-Rödl
Theorem on the number of H-free graphs and extend a result of Rödl and
Ruciński on Ramsey properties in sparse random graphs to the general,
non-symmetric setting.
We remark that similar results have been discovered independently by Saxton
and Thomason, and that, in parallel to this work, Conlon, Gowers, Samotij and
Schacht have proved a sparse analogue of the counting lemma for subgraphs of
the random graph G(n,p), which may be viewed as a version of the K\L R
conjecture that is stronger in some ways and weaker in others."
Robert Morris,Morris_Robert,arXiv:1204.3481,https://arxiv.org/abs/1204.3481,"Abstract:  One of the hallmarks of emotional intelligence is the ability to regulate
emotions. Research suggests that cognitive reappraisal - a technique that
involves reinterpreting the meaning of a thought or situation - can
down-regulate negative emotions, without incurring significant psychological or
physiological costs. Habitual use of this strategy is also linked to many key
indices of physical and emotional health. Unfortunately, this technique is not
always easy to apply. Thinking flexibly about stressful thoughts and situations
requires creativity and poise, faculties that often elude us when we need them
the most. In this paper, we propose an assistive technology that coordinates
collective intelligence on demand, to help individuals reappraise stressful
thoughts and situations. In two experiments, we assess key features of our
design and we demonstrate the feasibility of crowdsourcing empathetic
reappraisals with on demand workforces, such as Amazon's Mechanical Turk."
Robert Morris,Morris_Robert,arXiv:1203.1383,https://arxiv.org/abs/1203.1383,"Abstract:  With the unprecedented photometric precision of the Kepler Spacecraft,
significant systematic and stochastic errors on transit signal levels are
observable in the Kepler photometric data. These errors, which include
discontinuities, outliers, systematic trends and other instrumental signatures,
obscure astrophysical signals. The Presearch Data Conditioning (PDC) module of
the Kepler data analysis pipeline tries to remove these errors while preserving
planet transits and other astrophysically interesting signals. The completely
new noise and stellar variability regime observed in Kepler data poses a
significant problem to standard cotrending methods such as SYSREM and TFA.
Variable stars are often of particular astrophysical interest so the
preservation of their signals is of significant importance to the astrophysical
community. We present a Bayesian Maximum A Posteriori (MAP) approach where a
subset of highly correlated and quiet stars is used to generate a cotrending
basis vector set which is in turn used to establish a range of ""reasonable""
robust fit parameters. These robust fit parameters are then used to generate a
Bayesian Prior and a Bayesian Posterior Probability Distribution Function (PDF)
which when maximized finds the best fit that simultaneously removes systematic
effects while reducing the signal distortion and noise injection which commonly
afflicts simple least-squares (LS) fitting. A numerical and empirical approach
is taken where the Bayesian Prior PDFs are generated from fits to the light
curve distributions themselves."
Robert Morris,Morris_Robert,arXiv:1203.1382,https://arxiv.org/abs/1203.1382,"Abstract:  Kepler provides light curves of 156,000 stars with unprecedented precision.
However, the raw data as they come from the spacecraft contain significant
systematic and stochastic errors. These errors, which include discontinuities,
systematic trends, and outliers, obscure the astrophysical signals in the light
curves. To correct these errors is the task of the Presearch Data Conditioning
(PDC) module of the Kepler data analysis pipeline. The original version of PDC
in Kepler did not meet the extremely high performance requirements for the
detection of miniscule planet transits or highly accurate analysis of stellar
activity and rotation. One particular deficiency was that astrophysical
features were often removed as a side-effect to removal of errors. In this
paper we introduce the completely new and significantly improved version of PDC
which was implemented in Kepler SOC 8.0. This new PDC version, which utilizes a
Bayesian approach for removal of systematics, reliably corrects errors in the
light curves while at the same time preserving planet transits and other
astrophysically interesting signals. We describe the architecture and the
algorithms of this new PDC module, show typical errors encountered in Kepler
data, and illustrate the corrections using real light curve examples."
Robert Morris,Morris_Robert,arXiv:1202.5852,https://arxiv.org/abs/1202.5852,"Abstract:  New transiting planet candidates are identified in sixteen months (May 2009 -
September 2010) of data from the Kepler spacecraft. Nearly five thousand
periodic transit-like signals are vetted against astrophysical and instrumental
false positives yielding 1,091 viable new planet candidates, bringing the total
count up to over 2,300. Improved vetting metrics are employed, contributing to
higher catalog reliability. Most notable is the noise-weighted robust averaging
of multi-quarter photo-center offsets derived from difference image analysis
which identifies likely background eclipsing binaries. Twenty-two months of
photometry are used for the purpose of characterizing each of the new
candidates. Ephemerides (transit epoch, T_0, and orbital period, P) are
tabulated as well as the products of light curve modeling: reduced radius
(Rp/R*), reduced semi-major axis (d/R*), and impact parameter (b). The largest
fractional increases are seen for the smallest planet candidates (197% for
candidates smaller than 2Re compared to 52% for candidates larger than 2Re) and
those at longer orbital periods (123% for candidates outside of 50-day orbits
versus 85% for candidates inside of 50-day orbits). The gains are larger than
expected from increasing the observing window from thirteen months (Quarter 1--
Quarter 5) to sixteen months (Quarter 1 -- Quarter 6). This demonstrates the
benefit of continued development of pipeline analysis software. The fraction of
all host stars with multiple candidates has grown from 17% to 20%, and the
paucity of short-period giant planets in multiple systems is still evident. The
progression toward smaller planets at longer orbital periods with each new
catalog release suggests that Earth-size planets in the Habitable Zone are
forthcoming if, indeed, such planets are abundant."
Robert Morris,Morris_Robert,arXiv:1202.5200,https://arxiv.org/abs/1202.5200,"Abstract:  In this paper we study sum-free subsets of the set $\{1,...,n\}$, that is,
subsets of the first $n$ positive integers which contain no solution to the
equation $x + y = z$. Cameron and Erdős conjectured in 1990 that the number
of such sets is $O(2^{n/2})$. This conjecture was confirmed by Green and,
independently, by Sapozhenko. Here we prove a refined version of their theorem,
by showing that the number of sum-free subsets of $[n]$ of size $m$ is
$2^{O(n/m)} {\lceil n/2 \rceil \choose m}$, for every $1 \le m \le \lceil n/2
\rceil$. For $m \ge \sqrt{n}$, this result is sharp up to the constant implicit
in the $O(\cdot)$. Our proof uses a general bound on the number of independent
sets of size $m$ in 3-uniform hypergraphs, proved recently by the authors, and
new bounds on the number of integer partitions with small sumset."
Robert Morris,Morris_Robert,arXiv:1201.6654,https://arxiv.org/abs/1201.6654,"Abstract:  In this paper we study sum-free sets of order $m$ in finite Abelian groups.
We prove a general theorem on 3-uniform hypergraphs, which allows us to deduce
structural results in the sparse setting from stability results in the dense
setting. As a consequence, we determine the typical structure and asymptotic
number of sum-free sets of order $m$ in Abelian groups $G$ whose order is
divisible by a prime $q$ with $q \equiv 2 \pmod 3$, for every $m \ge C(q)
\sqrt{n \log n}$, thus extending and refining a theorem of Green and Ruzsa. In
particular, we prove that almost all sum-free subsets of size $m$ are contained
in a maximum-size sum-free subset of $G$. We also give a completely
self-contained proof of this statement for Abelian groups of even order, which
uses spectral methods and a new bound on the number of independent sets of size
$m$ in an $(n,d,\lambda)$-graph."
Robert Morris,Morris_Robert,arXiv:1201.5412,https://arxiv.org/abs/1201.5412,"Abstract:  We present a method to confirm the planetary nature of objects in systems
with multiple transiting exoplanet candidates. This method involves a
Fourier-Domain analysis of the deviations in the transit times from a constant
period that result from dynamical interactions within the system. The
combination of observed anti-correlations in the transit times and mass
constraints from dynamical stability allow us to claim the discovery of four
planetary systems Kepler-25, Kepler-26, Kepler-27, and Kepler-28, containing
eight planets and one additional planet candidate."
Robert Morris,Morris_Robert,arXiv:1110.1375,https://arxiv.org/abs/1110.1375,"Abstract:  Photometric observations made by the NASA Kepler Mission have led to a
dramatic increase in the number of main-sequence and subgiant stars with
detected solar-like oscillations. We present an ensemble asteroseismic analysis
of 76 solar-type stars. Using frequencies determined from the Kepler
time-series photometry, we have measured three asteroseismic parameters that
characterize the oscillations: the large frequency separation (\Delta \nu), the
small frequency separation between modes of l=0 and l=2 (\delta \nu_02), and
the dimensionless offset (\epsilon). These measurements allow us to construct
asteroseismic diagrams, namely the so-called C-D diagram of \delta \nu_02
versus \Delta \nu, and the recently re-introduced {\epsilon} diagram. We
compare the Kepler results with previously observed solar-type stars and with
theoretical models. The positions of stars in these diagrams places constraints
on their masses and ages. Additionally, we confirm the observational
relationship between {\epsilon} and T_eff that allows for the unambiguous
determination of radial order and should help resolve the problem of mode
identification in F stars."
Robert Morris,Morris_Robert,arXiv:1109.3198,https://arxiv.org/abs/1109.3198,"Abstract:  Kepler-16 is an eccentric low-mass eclipsing binary with a circumbinary
transiting planet. Here we investigate the angular momentum of the primary
star, based on Kepler photometry and Keck spectroscopy. The primary star's
rotation period is 35.1 +/- 1.0 days, and its projected obliquity with respect
to the stellar binary orbit is 1.6 +/- 2.4 degrees. Therefore the three largest
sources of angular momentum---the stellar orbit, the planetary orbit, and the
primary's rotation---are all closely aligned. This finding supports a formation
scenario involving accretion from a single disk. Alternatively, tides may have
realigned the stars despite their relatively wide separation (0.2 AU), a
hypothesis that is supported by the agreement between the measured rotation
period and the ""pseudosynchronous"" period of tidal evolution theory. The
rotation period, chromospheric activity level, and fractional light variations
suggest a main-sequence age of 2-4 Gyr. Evolutionary models of low-mass stars
can match the observed masses and radii of the primary and secondary stars to
within about 3%."
Robert Morris,Morris_Robert,arXiv:1108.1746,https://arxiv.org/abs/1108.1746,"Abstract:  The chromatic threshold delta_chi(H) of a graph H is the infimum of d>0 such
that there exists C=C(H,d) for which every H-free graph G with minimum degree
at least d|G| satisfies chi(G)<C. We prove that delta_chi(H) \in {(r-3)/(r-2),
(2r-5)/(2r-3), (r-2)/(r-1)} for every graph H with chi(H)=r>2. We moreover
characterise the graphs H with a given chromatic threshold, and thus determine
delta_chi(H) for every graph H. This answers a question of Erdős and
Simonovits [Discrete Math. 5 (1973), 323-334], and confirms a conjecture of
{\L}uczak and Thomassé [preprint (2010), 18pp]."
Robert Morris,Morris_Robert,arXiv:1108.0310,https://arxiv.org/abs/1108.0310,"Abstract:  We prove that the Poisson Boolean model, also known as the Gilbert disc
model, is noise sensitive at criticality. This is the first such result for a
Continuum Percolation model, and the first for which the critical probability
p_c \ne 1/2. Our proof uses a version of the Benjamini-Kalai-Schramm Theorem
for biased product measures. A quantitative version of this result was recently
proved by Keller and Kindler. We give a simple deduction of the
non-quantitative result from the unbiased version. We also develop a quite
general method of approximating Continuum Percolation models by discrete models
with p_c bounded away from zero; this method is based on an extremal result on
non-uniform hypergraphs."
Robert Morris,Morris_Robert,arXiv:1107.1410,https://arxiv.org/abs/1107.1410,"Abstract:  In $\HH$-bootstrap percolation, a set $A \subset V(\HH)$ of initially
'infected' vertices spreads by infecting vertices which are the only uninfected
vertex in an edge of the hypergraph $\HH$. A particular case of this is the
$H$-bootstrap process, in which $\HH$ encodes copies of $H$ in a graph $G$. We
find the minimum size of a set $A$ that leads to complete infection when $G$
and $H$ are powers of complete graphs and $\HH$ encodes induced copies of $H$
in $G$. The proof uses linear algebra, a technique that is new in bootstrap
percolation, although standard in the study of weakly saturated graphs, which
are equivalent to (edge) $H$-bootstrap percolation on a complete graph."
Robert Morris,Morris_Robert,arXiv:1107.1381,https://arxiv.org/abs/1107.1381,"Abstract:  Graph bootstrap percolation is a deterministic cellular automaton which was
introduced by Bollobás in 1968, and is defined as follows. Given a graph $H$,
and a set $G \subset E(K_n)$ of initially `infected' edges, we infect, at each
time step, a new edge $e$ if there is a copy of $H$ in $K_n$ such that $e$ is
the only not-yet infected edge of $H$. We say that $G$ percolates in the
$H$-bootstrap process if eventually every edge of $K_n$ is infected. The
extremal questions for this model, when $H$ is the complete graph $K_r$, were
solved (independently) by Alon, Kalai and Frankl almost thirty years ago. In
this paper we study the random questions, and determine the critical
probability $p_c(n,K_r)$ for the $K_r$-process up to a poly-logarithmic factor.
In the case $r = 4$ we prove a stronger result, and determine the threshold for
$p_c(n,K_4)$."
Robert Morris,Morris_Robert,arXiv:1107.1379,https://arxiv.org/abs/1107.1379,"Abstract:  We consider generalizations of the classical secretary problem, also known as
the problem of optimal choice, to posets where the only information we have is
the size of the poset and the number of maximal elements. We show that, given
this information, there is an algorithm that is successful with probability at
least $\frac{1}{e}$. We conjecture that if there are $k$ maximal elements and
$k \geq 2$ then this can be improved to $\sqrt[k-1]{\frac{1}{k}}$, and prove
this conjecture for posets of width $k$. We also show that no better bound is
possible."
Robert Morris,Morris_Robert,arXiv:1103.2541,https://arxiv.org/abs/1103.2541,"Abstract:  We report the distribution of planets as a function of planet radius (R_p),
orbital period (P), and stellar effective temperature (Teff) for P < 50 day
orbits around GK stars. These results are based on the 1,235 planets (formally
""planet candidates"") from the Kepler mission that include a nearly complete set
of detected planets as small as 2 Earth radii (Re). For each of the 156,000
target stars we assess the detectability of planets as a function of R_p and P.
We also correct for the geometric probability of transit, R*/a. We consider
first stars within the ""solar subset"" having Teff = 4100-6100 K, logg =
4.0-4.9, and Kepler magnitude Kp < 15 mag. We include only those stars having
noise low enough to permit detection of planets down to 2 Re. We count planets
in small domains of R_p and P and divide by the included target stars to
calculate planet occurrence in each domain. Occurrence of planets varies by
more than three orders of magnitude and increases substantially down to the
smallest radius (2 Re) and out to the longest orbital period (50 days, ~0.25
AU) in our study. For P < 50 days, the radius distribution is given by a power
law, df/dlogR= k R^\alpha. This rapid increase in planet occurrence with
decreasing planet size agrees with core-accretion, but disagrees with
population synthesis models. We fit occurrence as a function of P to a power
law model with an exponential cutoff below a critical period P_0. For smaller
planets, P_0 has larger values, suggesting that the ""parking distance"" for
migrating planets moves outward with decreasing planet size. We also measured
planet occurrence over Teff = 3600-7100 K, spanning M0 to F2 dwarfs. The
occurrence of 2-4 Re planets in the Kepler field increases with decreasing
Teff, making these small planets seven times more abundant around cool stars
than the hottest stars in our sample. [abridged]"
Robert Morris,Morris_Robert,arXiv:1103.2041,https://arxiv.org/abs/1103.2041,"Abstract:  We characterize the structure of maximum-size sum-free subsets of a random
subset of an Abelian group $G$. In particular, we determine the threshold $p_c
\approx \sqrt{\log n / n}$ above which, with high probability as $|G| \to
\infty$, each such subset is contained in a maximum-size sum-free subset of
$G$, whenever $q$ divides $|G|$ for some (fixed) prime $q$ with $q \equiv 2
\pmod 3$. Moreover, in the special case $G = \ZZ_{2n}$, we determine a sharp
threshold for the above property. The proof uses recent 'transference' theorems
of Conlon and Gowers, together with stability theorems for sum-free subsets of
Abelian groups."
Robert Morris,Morris_Robert,arXiv:1010.3326,https://arxiv.org/abs/1010.3326,"Abstract:  In r-neighbour bootstrap percolation on a graph G, a (typically random) set A
of initially 'infected' vertices spreads by infecting (at each time step)
vertices with at least r already-infected neighbours. This process may be
viewed as a monotone version of the Glauber dynamics of the Ising model, and
has been extensively studied on the d-dimensional grid $[n]^d$. The elements of
the set A are usually chosen independently, with some density p, and the main
question is to determine $p_c([n]^d,r)$, the density at which percolation
(infection of the entire vertex set) becomes likely.
In this paper we prove, for every pair $d \ge r \ge 2$, that there is a
constant L(d,r) such that $p_c([n]^d,r) = [(L(d,r) + o(1)) / log_(r-1)
(n)]^{d-r+1}$ as $n \to \infty$, where $log_r$ denotes an r-times iterated
logarithm. We thus prove the existence of a sharp threshold for percolation in
any (fixed) number of dimensions. Moreover, we determine L(d,r) for every pair
(d,r)."
Robert Morris,Morris_Robert,arXiv:1002.3881,https://arxiv.org/abs/1002.3881,"Abstract:  Two-dimensional bootstrap percolation is a cellular automaton in which sites
become 'infected' by contact with two or more already infected nearest
neighbors. We consider these dynamics, which can be interpreted as a monotone
version of the Ising model, on an n x n square, with sites initially infected
independently with probability p. The critical probability p_c is the smallest
p for which the probability that the entire square is eventually infected
exceeds 1/2. Holroyd determined the sharp first-order approximation: p_c \sim
\pi^2/(18 log n) as n \to \infty. Here we sharpen this result, proving that the
second term in the expansion is -(log n)^{-3/2+ o(1)}, and moreover determining
it up to a poly(log log n)-factor. The exponent -3/2 corrects numerical
predictions from the physics literature."
Robert Morris,Morris_Robert,arXiv:0907.3097,https://arxiv.org/abs/0907.3097,"Abstract:  In r-neighbour bootstrap percolation on a graph G, a set of initially
infected vertices A \subset V(G) is chosen independently at random, with
density p, and new vertices are subsequently infected if they have at least r
infected neighbours. The set A is said to percolate if eventually all vertices
are infected. Our aim is to understand this process on the grid, [n]^d, for
arbitrary functions n = n(t), d = d(t) and r = r(t), as t -> infinity. The main
question is to determine the critical probability p_c([n]^d,r) at which
percolation becomes likely, and to give bounds on the size of the critical
window. In this paper we study this problem when r = 2, for all functions n and
d satisfying d \gg log n.
The bootstrap process has been extensively studied on [n]^d when d is a fixed
constant and 2 \leq r \leq d, and in these cases p_c([n]^d,r) has recently been
determined up to a factor of 1 + o(1) as n -> infinity. At the other end of the
scale, Balogh and Bollobas determined p_c([2]^d,2) up to a constant factor, and
Balogh, Bollobas and Morris determined p_c([n]^d,d) asymptotically if d > (log
log n)^{2+\eps}, and gave much sharper bounds for the hypercube.
Here we prove the following result: let \lambda be the smallest positive root
of the equation \sum_{k=0}^\infty (-1)^k \lambda^k / (2^{k^2-k} k!) = 0, so
\lambda \approx 1.166. Then
(16\lambda / d^2) (1 + (log d / \sqrt{d})) 2^{-2\sqrt{d}} < p_c([2]^d,2) <
(16\lambda / d^2) (1 + (5(log d)^2 / \sqrt{d})) 2^{-2\sqrt{d}} if d is
sufficiently large, and moreover we determine a sharp threshold for the
critical probability p_c([n]^d,2) for every function n = n(d) with d \gg log n."
Robert Morris,Morris_Robert,arXiv:0906.3724,https://arxiv.org/abs/0906.3724,"Abstract:  Isoperimetric inequalities have been studied since antiquity, and in recent
decades they have been studied extensively on discrete objects, such as the
hypercube. An important special case of this problem involves bounding the size
of the shadow of a set system, and the basic question was solved by Kruskal (in
1963) and Katona (in 1968). In this paper we introduce the concept of the
shadow \d\G of a collection \G of ordered graphs, and prove the following,
simple-sounding statement: if n \in \N is sufficiently large, |V(G)| = n for
each G \in \G, and |\G| < n, then |\d \G| \ge |\G|. As a consequence, we
substantially strengthen a result of Balogh, Bollobás and Morris on
hereditary properties of ordered graphs: we show that if ¶is such a property,
and |¶_k| < k for some sufficiently large k \in \N, then |¶_n| is decreasing
for k \le n < \infty."
Robert Morris,Morris_Robert,arXiv:0905.1942,https://arxiv.org/abs/0905.1942,"Abstract:  A hereditary property of graphs is a collection of graphs which is closed
under taking induced subgraphs. The speed of ¶is the function n \mapsto
|¶_n|, where ¶_n denotes the graphs of order n in ¶. It was shown by
Alekseev, and by Bollobas and Thomason, that if ¶is a hereditary property of
graphs then |¶_n| = 2^{(1 - 1/r + o(1))n^2/2}, where r = r(¶) \in \N is the
so-called `colouring number' of ¶. However, their results tell us very little
about the structure of a typical graph G \in ¶.
In this paper we describe the structure of almost every graph in a hereditary
property of graphs, ¶. As a consequence, we derive essentially optimal bounds
on the speed of ¶, improving the Alekseev-Bollobas-Thomason Theorem, and also
generalizing results of Balogh, Bollobas and Simonovits."
Robert Morris,Morris_Robert,arXiv:0809.0353,https://arxiv.org/abs/0809.0353,"Abstract:  We study zero-temperature Glauber dynamics on \Z^d, which is a dynamic
version of the Ising model of ferromagnetism. Spins are initially chosen
according to a Bernoulli distribution with density p, and then the states are
continuously (and randomly) updated according to the majority rule. This
corresponds to the sudden quenching of a ferromagnetic system at high
temperature with an external field, to one at zero temperature with no external
field. Define p_c(\Z^d) to be the infimum over p such that the system fixates
at '+' with probability 1. It is a folklore conjecture that p_c(\Z^d) = 1/2 for
every 2 \le d \in \N. We prove that p_c(\Z^d) \to 1/2 as d \to \infty."
Robert Morris,Morris_Robert,arXiv:0806.4485,https://arxiv.org/abs/0806.4485,"Abstract:  By bootstrap percolation we mean the following deterministic process on a
graph $G$. Given a set $A$ of vertices ""infected"" at time 0, new vertices are
subsequently infected, at each time step, if they have at least
$r\in\mathbb{N}$ previously infected neighbors. When the set $A$ is chosen at
random, the main aim is to determine the critical probability $p_c(G,r)$ at
which percolation (infection of the entire graph) becomes likely to occur. This
bootstrap process has been extensively studied on the $d$-dimensional grid
$[n]^d$: with $2\leq r\leq d$ fixed, it was proved by Cerf and Cirillo (for
$d=r=3$), and by Cerf and Manzo (in general), that
\[p_c([n]^d,r)=\Theta\biggl(\frac{1}{\log_{(r-1)}n}\biggr)^{d-r+1},\] where
$\log_{(r)}$ is an $r$-times iterated logarithm. However, the exact threshold
function is only known in the case $d=r=2$, where it was shown by Holroyd to be
$(1+o(1))\frac{\pi^2}{18\log n}$. In this paper we shall determine the exact
threshold in the crucial case $d=r=3$, and lay the groundwork for solving the
problem for all fixed $d$ and $r$."
Robert Morris,Morris_Robert,arXiv:math/0702373,https://arxiv.org/abs/math/0702373,"Abstract:  In majority bootstrap percolation on a graph G, an infection spreads
according to the following deterministic rule: if at least half of the
neighbours of a vertex v are already infected, then v is also infected, and
infected vertices remain infected forever. Percolation occurs if eventually
every vertex is infected.
The elements of the set of initially infected vertices, A \subset V(G), are
normally chosen independently at random, each with probability p, say. This
process has been extensively studied on the sequence of torus graphs [n]^d, for
n = 1,2,..., where d = d(n) is either fixed or a very slowly growing function
of n. For example, Cerf and Manzo showed that the critical probability is o(1)
if d(n) < log*(n), i.e., if p = p(n) is bounded away from zero then the
probability of percolation on [n]^d tends to one as n goes to infinity.
In this paper we study the case when the growth of d to infinity is not
excessively slow; in particular, we show that the critical probability is 1/2 +
o(1) if d > (loglog(n))^2 logloglog(n), and give much stronger bounds in the
case that G is the hypercube, [2]^d."
Robert Morris,Morris_Robert,arXiv:math/0702371,https://arxiv.org/abs/math/0702371,"Abstract:  A collection of unlabelled tournaments P is called a hereditary property if
it is closed under isomorphism and under taking induced sub-tournaments. The
speed of P is the function n -> |P_n|, where P_n = {T \in P : |V(T)| = n}. In
this paper, we prove that there is a jump in the possible speeds of a
hereditary property of tournaments, from polynomial to exponential speed.
Moreover, we determine the minimal exponential speed, |P_n| = c^(n + o(n)),
where c = 1.47... is the largest real root of the polynomial x^3 = x^2 + 1, and
the unique hereditary property with this speed."
Robert Morris,Morris_Robert,arXiv:math/0702370,https://arxiv.org/abs/math/0702370,"Abstract:  In standard bootstrap percolation, a subset A of the n x n grid is initially
infected. A new site is then infected if at least two of its neighbours are
infected, and an infected site stays infected forever. The set A is said to
percolate if eventually the entire grid is infected. A percolating set is said
to be minimal if none of its subsets percolate. Answering a question of
Bollobas, we show that there exists a minimal percolating set of size 4n^2/33 +
o(n^2), but there does not exist one larger than (n + 2)^2/6."
Robert Morris,Morris_Robert,arXiv:math/0702369,https://arxiv.org/abs/math/0702369,"Abstract:  Suppose the edges of the complete graph on n vertices are coloured using r
colours; how large a k-connected subgraph are we guaranteed to find, which uses
only at most s of the colours? This question is due to Bollobas, and the case s
= 1 was considered in a preivous paper by the same authors. Here we consider
the case s >= 2, and mention some open problems."
Robert Morris,Morris_Robert,arXiv:math/0702354,https://arxiv.org/abs/math/0702354,"Abstract:  We consider the following question of Bollobas: given an r-colouring of the
edges of the complete graph on n vertices, how large a k-connected subgraph can
we find using only one colour? We solve this problem asymptotically when r-1 is
a prime power, and exactly for 2- and 3-colourings."
Robert Morris,Morris_Robert,arXiv:math/0702352,https://arxiv.org/abs/math/0702352,"Abstract:  An ordered graph is a graph together with a linear order on its vertices. A
hereditary property of ordered graphs is a collection of ordered graphs closed
under taking induced ordered subgraphs. If P is a property of ordered graphs,
then the function which counts the number of ordered graphs in P with exactly n
vertices is called the speed of P.
In this paper we determine the possible speeds of a hereditary property of
ordered graphs, up to the speed 2^(n-1). In particular, we prove that there
exists a jump from polynomial speed to speed F(n), the Fibonacci numbers, and
that there exists an infinite sequence of subsequent jumps, from p(n)F(n,k) to
F(n,k+1) (where p(n) is a polynomial and F(n,k) are the generalized Fibonacci
numbers) converging to 2^(n-1). Our results generalize a theorem of Kaiser and
Klazar, who proved that the same jumps occur for hereditary properties of
permutations."
Robert Morris,Morris_Robert,arXiv:math/0702351,https://arxiv.org/abs/math/0702351,"Abstract:  In this paper we use the Klazar-Marcus-Tardos method to prove that if a
hereditary property of partitions P has super-exponential speed, then for every
k-permutation pi, P contains the partition of [2k] with parts {i, pi(i) + k},
where 1 <= i <= k. We also prove a similar jump, from exponential to factorial,
in the possible speeds of monotone properties of ordered graphs, and of
hereditary properties of ordered graphs not containing large complete, or
complete bipartite ordered graphs.
Our results generalize the Stanley-Wilf Conjecture on the number of
n-permutations avoiding a fixed permutation, which was recently proved by the
combined results of Klazar and of Marcus and Tardos. Our main results follow
from a generalization to ordered hypergraphs of the theorem of Marcus and
Tardos."
Robert Morris,Morris_Robert,arXiv:math/0702350,https://arxiv.org/abs/math/0702350,"Abstract:  A hereditary property of combinatorial structures is a collection of
structures (e.g. graphs, posets) which is closed under isomorphism, closed
under taking induced substructures (e.g. induced subgraphs), and contains
arbitrarily large structures. Given a property P, we write P_n for the
collection of distinct (i.e., non-isomorphic) structures in a property P with n
vertices, and call the function n -> |P_n| the speed (or unlabelled speed) of
P. Also, we write P^n for the collection of distinct labelled structures in P
with vertices labelled 1,...,n, and call the function n -> |P^n| the labelled
speed of P.
The possible labelled speeds of a hereditary property of graphs have been
extensively studied, and the aim of this paper is to investigate the possible
speeds of other combinatorial structures, namely posets and oriented graphs.
More precisely, we show that (for sufficiently large n), the labelled speed of
a hereditary property of posets is either 1, or exactly a polynomial, or at
least 2^n - 1. We also show that there is an initial jump in the possible
unlabelled speeds of hereditary properties of posets, tournaments and directed
graphs, from bounded to linear speed, and give a sharp lower bound on the
possible linear speeds in each case."
Robert Morris,Morris_Robert,arXiv:math/0702348,https://arxiv.org/abs/math/0702348,"Abstract:  A family of sets F is said to be union-closed if A \cup B is in F for every A
and B in F. Frankl's conjecture states that given any finite union-closed
family of sets, not all empty, there exists an element contained in at least
half of the sets. Here we prove that the conjecture holds for families
containing three 3-subsets of a 5-set, four 3-subsets of a 6-set, or eight
4-subsets of a 6-set, extending work of Poonen and Vaughan. As an application
we prove the conjecture in the case that the largest set has at most nine
elements, extending a result of Gao and Yu. We also pose several open
questions."
Robert Morris,Morris_Robert,arXiv:cs/0603076,https://arxiv.org/abs/cs/0603076,"Abstract:  Nontechnical users who own increasingly ubiquitous network-enabled personal
devices such as laptops, digital cameras, and smart phones need a simple,
intuitive, and secure way to share information and services between their
devices. User Information Architecture, or UIA, is a novel naming and
peer-to-peer connectivity architecture addressing this need. Users assign UIA
names by ""introducing"" devices to each other on a common local-area network,
but these names remain securely bound to their target as devices migrate.
Multiple devices owned by the same user, once introduced, automatically merge
their namespaces to form a distributed ""personal cluster"" that the owner can
access or modify from any of his devices. Instead of requiring users to
allocate globally unique names from a central authority, UIA enables users to
assign their own ""user-relative"" names both to their own devices and to other
users. With UIA, for example, Alice can always access her iPod from any of her
own personal devices at any location via the name ""ipod"", and her friend Bob
can access her iPod via a relative name like ""ipod.Alice""."
Stefanie Mueller,Mueller_Stefanie,arXiv:1809.05491,https://arxiv.org/abs/1809.05491,"Abstract:  We present a system that allows users to visualize complex human motion via
3D motion sculptures---a representation that conveys the 3D structure swept by
a human body as it moves through space. Given an input video, our system
computes the motion sculptures and provides a user interface for rendering it
in different styles, including the options to insert the sculpture back into
the original video, render it in a synthetic scene or physically print it.
To provide this end-to-end workflow, we introduce an algorithm that estimates
that human's 3D geometry over time from a set of 2D images and develop a
3D-aware image-based rendering approach that embeds the sculpture back into the
scene. By automating the process, our system takes motion sculpture creation
out of the realm of professional artists, and makes it applicable to a wide
range of existing video material.
By providing viewers with 3D information, motion sculptures reveal space-time
motion information that is difficult to perceive with the naked eye, and allow
viewers to interpret how different parts of the object interact over time. We
validate the effectiveness of this approach with user studies, finding that our
motion sculpture visualizations are significantly more informative about motion
than existing stroboscopic and space-time visualization methods."
Farnaz Niroui,Niroui_Farnaz,arXiv:1803.01192,https://arxiv.org/abs/1803.01192,"Abstract:  Halide perovskites are promising semiconductors for inexpensive,
high-performance optoelectronics. Despite a remarkable defect tolerance
compared to conventional semiconductors, perovskite thin films still show
substantial microscale heterogeneity in key properties such as luminescence
efficiency and device performance. This behavior has been attributed to spatial
fluctuations in the population of sub-bandgap electronic states that act as
trap-mediated non-radiative recombination sites. However, the origin of the
variations, trap states and extent of the defect tolerance remains a topic of
debate, and a precise understanding is critical to the rational design of
defect management strategies. By combining scanning X-ray diffraction beamlines
at two different synchrotrons with high-resolution transmission electron
microscopy, we reveal levels of heterogeneity on the ten-micrometer scale
(super-grains) and even ten-nanometer scale (sub-grain domains). We find that
local strain is associated with enhanced defect concentrations, and
correlations between the local structure and time-resolved photoluminescence
reveal that these strain-related defects are the cause of non-radiative
recombination. We reveal a direct connection between defect concentrations and
non-radiative losses, as well as complex heterogeneity across multiple length
scales, shedding new light on the presence and influence of structural defects
in halide perovskites."
Alan Oppenheim,Oppenheim_Alan,arXiv:1802.04672,https://arxiv.org/abs/1802.04672,"Abstract:  The theoretical basis for conventional acquisition of bandlimited signals
typically relies on uniform time sampling and assumes infinite-precision
amplitude values. In this paper, we explore signal representation and recovery
based on uniform amplitude sampling with assumed infinite precision timing
information. The approach is based on the delta-ramp encoder which consists of
applying a one-level level-crossing detector to the result of adding an
appropriate sawtooth-like waveform to the input signal. The output samples are
the time instants of these level crossings, thus representing a time-encoded
version of the input signal. For theoretical purposes, this system can be
equivalently analyzed by reversibly transforming through ramp addition a
nonmonotonic input signal into a monotonic one which is then uniformly sampled
in amplitude. The monotonic function is then represented by the times at which
the signal crosses a predefined and equally-spaced set of amplitude values. We
refer to this technique as amplitude sampling. The time sequence generated can
be interpreted alternatively as nonuniform time sampling of the original source
signal. We derive duality and frequency-domain properties for the functions
involved in the transformation. Iterative algorithms are proposed and
implemented for recovery of the original source signal. As indicated in the
simulations, the proposed iterative amplitude-sampling algorithm achieves a
faster convergence rate than frame-based reconstruction for nonuniform
sampling. The performance can also be improved by appropriate choice of the
parameters while maintaining the same sampling density."
Alan Oppenheim,Oppenheim_Alan,arXiv:1802.04634,https://arxiv.org/abs/1802.04634,"Abstract:  Analog-to-digital (A/D) converters are the common interface between analog
signals and the domain of digital discrete-time signal processing. In essence,
this domain simultaneously incorporates quantization both in amplitude and
time, i.e. amplitude quantization and uniform time sampling. Thus, we view A/D
conversion as a sampling process in both the time and amplitude domains based
on the observation that the underlying continuous-time signals representing
digital sequences can be sampled in a lattice---i.e. at points restricted to
lie in a uniform grid both in time and amplitude. We refer to them as lattice
functions. This is in contrast with the traditional approach based on the
classical sampling theorem and quantization error analysis. The latter has been
mainly addressed with the help of probabilistic models, or deterministic ones
either confined to very particular scenarios or considering worst-case
assumptions. In this paper, we provide a deterministic theoretical analysis and
framework for the functions involved in digital discrete-time processing. We
show that lattice functions possess a rich analytic structure in the context of
integral-valued entire functions of exponential type. We derive set and
spectral properties of this class of functions. This allows us to prove in a
deterministic way and for general bandlimited functions a fundamental bound on
the spectrum of the quantization error that is independent of the resolution of
the quantizer."
Alan Oppenheim,Oppenheim_Alan,arXiv:1511.00379,https://arxiv.org/abs/1511.00379,"Abstract:  Designing and implementing systems as an interconnection of smaller
subsystems is a common practice for modularity and standardization of
components and design algorithms. Although not typically cast in this
framework, many of these approaches can be viewed within the mathematical
context of functional composition. This paper re-interprets and generalizes
within the functional composition framework one such approach known as filter
sharpening, i.e. interconnecting filter modules which have significant
approximation error in order to obtain improved filter characteristics. More
specifically, filter sharpening is approached by determining the composing
polynomial to minimize the infinity-norm of the approximation error, utilizing
the First Algorithm of Remez. This is applied both to sharpening for FIR,
even-symmetric filters and for the more general case of subfilters that have
complex-valued frequency responses including causal IIR filters and for
continuous-time filters. Within the framework of functional composition, this
paper also explores the use of functional decomposition to approximate a
desired system as a composition of simpler functions based on a two-norm on the
approximation error. Among the potential advantages of this decomposition is
the ability for modular implementation in which the inner component of the
functional decomposition represents the subfilters and the outer the
interconnection."
Alan Oppenheim,Oppenheim_Alan,arXiv:1509.05895,https://arxiv.org/abs/1509.05895,"Abstract:  This paper presents two novel regularization methods motivated in part by the
geometric significance of biorthogonal bases in signal processing applications.
These methods, in particular, draw upon the structural relevance of
orthogonality and biorthogonality principles and are presented from the
perspectives of signal processing, convex programming, continuation methods and
nonlinear projection operators. Each method is specifically endowed with either
a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy
and numerical stability. An example involving a basis comprised of real
exponential signals illustrates the utility of the proposed methods on an
ill-conditioned inverse problem and the results are compared to standard
regularization techniques from the signal processing literature."
Alan Oppenheim,Oppenheim_Alan,arXiv:1502.01220,https://arxiv.org/abs/1502.01220,"Abstract:  This paper presents a general framework for generating greedy algorithms for
solving convex constraint satisfaction problems for sparse solutions by mapping
the satisfaction problem into one of graph traversal on a rooted tree of
unknown topology. For every pre-walk of the tree an initial set of generally
dense feasible solutions is processed in such a way that the sparsity of each
solution increases with each generation unveiled. The specific computation
performed at any particular child node is shown to correspond to an embedding
of a polytope into the polytope received from that nodes parent. Several issues
related to pre-walk order selection, computational complexity and tractability,
and the use of heuristic and/or side information is discussed. An example of a
single-path, depth-first algorithm on a tree with randomized vertex reduction
and a run-time path selection algorithm is presented in the context of sparse
lowpass filter design."
Terry Orlando,Orlando_Terry,arXiv:1809.05215,https://arxiv.org/abs/1809.05215,"Abstract:  Quantum coherence and control is foundational to the science and engineering
of quantum systems. In van der Waals (vdW) materials, the collective coherent
behavior of carriers has been probed successfully by transport measurements.
However, temporal coherence and control, as exemplified by manipulating a
single quantum degree of freedom, remains to be verified. Here we demonstrate
such coherence and control of a superconducting circuit incorporating
graphene-based Josephson junctions. Furthermore, we show that this device can
be operated as a voltage-tunable transmon qubit, whose spectrum reflects the
electronic properties of massless Dirac fermions traveling ballistically. In
addition to the potential for advancing extensible quantum computing
technology, our results represent a new approach to studying vdW materials
using microwave photons in coherent quantum circuits."
Terry Orlando,Orlando_Terry,arXiv:1803.09813,https://arxiv.org/abs/1803.09813,"Abstract:  The prospect of computational hardware with quantum advantage relies
critically on the quality of quantum gate operations. Imperfect two-qubit gates
is a major bottleneck for achieving scalable quantum information processors.
Here, we propose a generalizable and extensible scheme for a two-qubit coupler
switch that controls the qubit-qubit coupling by modulating the coupler
frequency. Two-qubit gate operations can be implemented by operating the
coupler in the dispersive regime, which is non-invasive to the qubit states. We
investigate the performance of the scheme by simulating a universal two-qubit
gate on a superconducting quantum circuit, and find that errors from known
parasitic effects are strongly suppressed. The scheme is compatible with
existing high-coherence hardware, thereby promising a higher gate fidelity with
current technologies."
Terry Orlando,Orlando_Terry,arXiv:1801.00467,https://arxiv.org/abs/1801.00467,"Abstract:  In the cavity-QED architecture, photon number fluctuations from residual
cavity photons cause qubit dephasing due to the AC Stark effect. These unwanted
photons originate from a variety of sources, such as thermal radiation,
leftover measurement photons, and crosstalk. Using a capacitively-shunted flux
qubit coupled to a transmission line cavity, we demonstrate a method that
identifies and distinguishes coherent and thermal photons based on
noise-spectral reconstruction from time-domain spin-locking relaxometry. Using
these measurements, we attribute the limiting dephasing source in our system to
thermal photons, rather than coherent photons. By improving the cryogenic
attenuation on lines leading to the cavity, we successfully suppress residual
thermal photons and achieve $T_1$-limited spin-echo decay time. The
spin-locking noise spectroscopy technique can readily be applied to other qubit
modalities for identifying general asymmetric non-classical noise spectra."
Terry Orlando,Orlando_Terry,arXiv:1612.08462,https://arxiv.org/abs/1612.08462,"Abstract:  Dynamical error suppression techniques are commonly used to improve coherence
in quantum systems. They reduce dephasing errors by applying control pulses
designed to reverse erroneous coherent evolution driven by environmental noise.
However, such methods cannot correct for irreversible processes such as energy
relaxation. In this work, we investigate a complementary, stochastic approach
to reducing errors: instead of deterministically reversing the unwanted qubit
evolution, we use control pulses to shape the noise environment dynamically. In
the context of superconducting qubits, we implement a pumping sequence to
reduce the number of unpaired electrons (quasiparticles) in close proximity to
the device. We report a 70% reduction in the quasiparticle density, resulting
in a threefold enhancement in qubit relaxation times, and a comparable
reduction in coherence variability."
Terry Orlando,Orlando_Terry,arXiv:1606.09262,https://arxiv.org/abs/1606.09262,"Abstract:  We assess independently the impact of high-temperature substrate annealing
and metal deposition conditions on the coherence of transmon qubits in the
standard 2D circuit-QED architecture. We restrict our study to devices made
with aluminum interdigital capacitors on sapphire substrates. We record more
than an order-of-magnitude improvement in the relaxation time of devices made
with an annealed substrate, independent of whether a conventional evaporator or
molecular beam epitaxy chamber was used to deposit the aluminum. We also infer
similar levels of flux noise and photon shot noise through dephasing
measurements on these devices. Our results indicate that substrate annealing
plays a primary role in fabricating low-loss qubits, consistent with the
hypothesis that substrate-air and substrate-metal interfaces are essential
factors limiting the qubit lifetimes in superconducting circuits."
Terry Orlando,Orlando_Terry,arXiv:1409.6031,https://arxiv.org/abs/1409.6031,"Abstract:  We present measurements of coherence and successive decay dynamics of higher
energy levels of a superconducting transmon qubit. By applying consecutive
$\pi$-pulses for each sequential transition frequency, we excite the qubit from
the ground state up to its fourth excited level and characterize the decay and
coherence of each state. We find the decay to proceed mainly sequentially, with
relaxation times in excess of 20 $\mu$s for all transitions. We also provide a
direct measurement of the charge dispersion of these levels by analyzing
beating patterns in Ramsey fringes. The results demonstrate the feasibility of
using higher levels in transmon qubits for encoding quantum information."
Terry Orlando,Orlando_Terry,arXiv:1211.2175,https://arxiv.org/abs/1211.2175,"Abstract:  We present a new method for determining pulse imperfections and improving the
single-gate fidelity in a superconducting qubit. By applying consecutive
positive and negative $\pi$ pulses, we amplify the qubit evolution due to
microwave pulse distortion, which causes the qubit state to rotate around an
axis perpendicular to the intended rotation axis. Measuring these rotations as
a function of pulse period allows us to reconstruct the shape of the microwave
pulse arriving at the sample. Using the extracted response to predistort the
input signal, we are able to improve the pulse shapes and to reach an average
single-qubit gate fidelity higher than 99.8%."
Terry Orlando,Orlando_Terry,arXiv:1204.6377,https://arxiv.org/abs/1204.6377,"Abstract:  We implement dynamical decoupling techniques to mitigate noise and enhance
the lifetime of an entangled state that is formed in a superconducting flux
qubit coupled to a microscopic two-level system. By rapidly changing the
qubit's transition frequency relative to the two-level system, we realize a
refocusing pulse that reduces dephasing due to fluctuations in the transition
frequencies, thereby improving the coherence time of the entangled state. The
coupling coherence is further enhanced when applying multiple refocusing
pulses, in agreement with our $1/f$ noise model. The results are applicable to
any two-qubit system with transverse coupling, and they highlight the potential
of decoupling techniques for improving two-qubit gate fidelities, an essential
prerequisite for implementing fault-tolerant quantum computing."
Terry Orlando,Orlando_Terry,arXiv:1201.6341,https://arxiv.org/abs/1201.6341,"Abstract:  We have investigated the driven dynamics of a superconducting flux qubit that
is tunably coupled to a microwave resonator. We find that the qubit experiences
an oscillating field mediated by off-resonant driving of the resonator, leading
to strong modifications of the qubit Rabi frequency. This opens an additional
noise channel, and we find that low-frequency noise in the coupling parameter
causes a reduction of the coherence time during driven evolution. The noise can
be mitigated with the rotary-echo pulse sequence, which, for driven systems, is
analogous to the Hahn-echo sequence."
Terry Orlando,Orlando_Terry,arXiv:1201.5665,https://arxiv.org/abs/1201.5665,"Abstract:  We report a direct measurement of the low-frequency noise spectrum in a
superconducting flux qubit. Our method uses the noise sensitivity of a
free-induction Ramsey interference experiment, comprising free evolution in the
presence of noise for a fixed period of time followed by single-shot
qubit-state measurement. Repeating this procedure enables Fourier-transform
noise spectroscopy with access to frequencies up to the achievable repetition
rate, a regime relevant to dephasing in ensemble-averaged time-domain
measurements such as Ramsey interferometry. Rotating the qubit's quantization
axis allows us to measure two types of noise: effective flux noise and
effective critical-current or charge noise. For both noise sources, we observe
that the very same 1/f-type power laws measured at considerably higher
frequencies (0.2-20 MHz) are consistent with the noise in the 0.01-100-Hz range
measured here. We find no evidence of temperature dependence of the noises over
65-200 mK, and also no evidence of time-domain correlations between the two
noises. These methods and results are pertinent to the dephasing of all
superconducting qubits."
Terry Orlando,Orlando_Terry,arXiv:0805.1552,https://arxiv.org/abs/0805.1552,"Abstract:  The energy-level structure of a quantum system plays a fundamental role in
determining its behavior and manifests itself in a discrete absorption and
emission spectrum. Conventionally, spectra are probed via frequency
spectroscopy whereby the frequency \nu of a harmonic driving field is varied to
fulfill the conditions \Delta E = h \nu, where the driving field is resonant
with the level separation \Delta E (h is Planck's constant). Although this
technique has been successfully employed in a variety of physical systems,
including natural and artificial atoms and molecules, its application is not
universally straightforward, and becomes extremely challenging for frequencies
in the range of 10's and 100's of gigahertz. Here we demonstrate an alternative
approach, whereby a harmonic driving field sweeps the atom through its
energy-level avoided crossings at a fixed frequency, surmounting many of the
limitations of the conventional approach. Spectroscopic information is obtained
from the amplitude dependence of the system response. The resulting
``spectroscopy diamonds'' contain interference patterns and population
inversion that serve as a fingerprint of the atom's spectrum. By analyzing
these features, we determine the energy spectrum of a manifold of states with
energies from 0.01 to 120 GHz \times h in a superconducting artificial atom,
using a driving frequency near 0.1 GHz. This approach provides a means to
manipulate and characterize systems over a broad bandwidth, using only a single
driving frequency that may be orders of magnitude smaller than the energy
scales being probed."
Terry Orlando,Orlando_Terry,arXiv:cond-mat/0512691,https://arxiv.org/abs/cond-mat/0512691,"Abstract:  We demonstrate Mach-Zehnder-type interferometry in a superconducting flux
qubit. The qubit is a tunable artificial atom, whose ground and excited states
exhibit an avoided crossing. Strongly driving the qubit with harmonic
excitation sweeps it through the avoided crossing two times per period. As the
induced Landau-Zener transitions act as coherent beamsplitters, the accumulated
phase between transitions, which varies with microwave amplitude, results in
quantum interference fringes for n=1...20 photon transitions. The
generalization of optical Mach-Zehnder interferometry, performed in qubit phase
space, provides an alternative means to manipulate and characterize the qubit
in the strongly-driven regime."
Terry Orlando,Orlando_Terry,arXiv:cond-mat/0501283,https://arxiv.org/abs/cond-mat/0501283,"Abstract:  We have implemented a resonant circuit that uses a SQUID as a flux-sensitive
Josephson inductor for qubit readout. In contrast to the conventional switching
current measurement that generates undesired quasi-particles when the SQUID
switches to the voltage state, our approach keeps the readout SQUID biased
along the supercurrent branch during the measurement. By incorporating the
SQUID inductor in a high-Q resonant circuit, we can distinguish the two flux
states of a niobium persistent-current (PC) qubit by observing a shift in the
resonant frequency of both the magnitude and the phase spectra. The readout
circuit was also characterized in the nonlinear regime to investigate its
potential use as a nonlinear amplifier."
Terry Orlando,Orlando_Terry,arXiv:quant-ph/0403090,https://arxiv.org/abs/quant-ph/0403090,"Abstract:  A scalable superconducting architecture for adiabatic quantum computers is
proposed. The architecture is based on time-independent, nearest-neighbor
interqubit couplings: it can handle any problem in the class NP even in the
presence of measurement errors, noise, and decoherence. The implementation of
this architecture with superconducting persistent-current qubits and the
natural robustness of such an implementation to manufacturing imprecision and
decoherence are discussed."
Terry Orlando,Orlando_Terry,arXiv:cond-mat/9710224,https://arxiv.org/abs/cond-mat/9710224,"Abstract:  We present analytical and numerical studies of pinned superconducting states
of open-ended Josephson ladder arrays, neglecting inductances but taking edge
effects into account. Treating the edge effects perturbatively, we find
analytical approximations for three of these superconducting states -- the
no-vortex, fully-frustrated and single-vortex states -- as functions of the dc
bias current $I$ and the frustration $f$. Bifurcation theory is used to derive
formulas for the depinning currents and critical frustrations at which the
superconducting states disappear or lose dynamical stability as $I$ and $f$ are
varied. These results are combined to yield a zero-temperature stability
diagram of the system with respect to $I$ and $f$. To highlight the effects of
the edges, we compare this dynamical stability diagram to the thermodynamic
phase diagram for the infinite system where edges have been neglected. We
briefly indicate how to extend our methods to include self-inductances."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1901.08511,https://arxiv.org/abs/1901.08511,"Abstract:  We consider solving convex-concave saddle point problems. We focus on two
variants of gradient decent-ascent algorithms, Extra-gradient (EG) and
Optimistic Gradient (OGDA) methods, and show that they admit a unified analysis
as approximations of the classical proximal point method for solving
saddle-point problems. This viewpoint enables us to generalize EG (in terms of
extrapolation steps) and OGDA (in terms of parameters) and obtain new
convergence rate results for these algorithms for the bilinear case as well as
the strongly convex-concave case."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1901.08022,https://arxiv.org/abs/1901.08022,"Abstract:  We study the problem of minimizing a strongly convex and smooth function when
we have noisy estimates of its gradient. We propose a novel multistage
accelerated algorithm that is universally optimal in the sense that it achieves
the optimal rate both in the deterministic and stochastic case and operates
without knowledge of noise characteristics. The algorithm consists of stages
that use a stochastic version of Nesterov's accelerated algorithm with a
specific restart and parameters selected to achieve the fastest reduction in
the bias-variance terms in the convergence rate bounds."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1812.11420,https://arxiv.org/abs/1812.11420,"Abstract:  We offer a parsimonious model to investigate how strategic wind producers
sell energy under stochastic production constraints, where the extent of
heterogeneity of wind energy availability varies according to wind farm
locations. The main insight of our analysis is that increasing heterogeneity in
resource availability improves social welfare, as a function of its effects
both on improving diversification and on reducing withholding by firms. We show
that this insight is quite robust for any concave and downward-sloping inverse
demand function. The model is also used to analyze the effect of heterogeneity
on firm profits and opportunities for collusion. Finally, we analyze the
impacts of improving public information and weather forecasting; enhanced
public forecasting increases welfare, but it is not always in the best
interests of strategic producers."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1809.02162,https://arxiv.org/abs/1809.02162,"Abstract:  In this paper, we study the problem of escaping from saddle points in smooth
nonconvex optimization problems subject to a convex set $\mathcal{C}$. We
propose a generic framework that yields convergence to a second-order
stationary point of the problem, if the convex set $\mathcal{C}$ is simple for
a quadratic objective function. Specifically, our results hold if one can find
a $\rho$-approximate solution of a quadratic program subject to $\mathcal{C}$
in polynomial time, where $\rho<1$ is a positive constant that depends on the
structure of the set $\mathcal{C}$. Under this condition, we show that the
sequence of iterates generated by the proposed framework reaches an
$(\epsilon,\gamma)$-second order stationary point (SOSP) in at most
$\mathcal{O}(\max\{\epsilon^{-2},\rho^{-3}\gamma^{-3}\})$ iterations. We
further characterize the overall complexity of reaching an SOSP when the convex
set $\mathcal{C}$ can be written as a set of quadratic constraints and the
objective function Hessian has a specific structure over the convex set
$\mathcal{C}$. Finally, we extend our results to the stochastic setting and
characterize the number of stochastic gradient and Hessian evaluations to reach
an $(\epsilon,\gamma)$-SOSP."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1809.01485,https://arxiv.org/abs/1809.01485,"Abstract:  This paper considers a novel framework to detect communities in a graph from
the observation of signals at its nodes. We model the observed signals as noisy
outputs of an unknown network process -- represented as a graph filter -- that
is excited by a set of low-rank inputs. Rather than learning the precise
parameters of the graph itself, the proposed method retrieves the community
structure directly; Furthermore, as in blind system identification methods, it
does not require knowledge of the system excitation. The paper shows that
communities can be detected by applying spectral clustering to the low-rank
output covariance matrix obtained from the graph signals. The performance
analysis indicates that the community detection accuracy depends on the
spectral properties of the graph filter considered. Furthermore, we show that
the accuracy can be improved via a low-rank matrix decomposition method when
the excitation signals are known. Numerical experiments demonstrate that our
approach is effective for analyzing network data from diffusion, consumers, and
social dynamics."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1808.10590,https://arxiv.org/abs/1808.10590,"Abstract:  We study a routing game in an environment with multiple heterogeneous
information systems and an uncertain state that affects edge costs of a
congested network. Each information system sends a noisy signal about the state
to its subscribed traveler population. Travelers make route choices based on
their private beliefs about the state and other populations' signals. The
question then arises, ""How does the presence of asymmetric and incomplete
information affect the travelers' equilibrium route choices and costs?"" We
develop a systematic approach to characterize the equilibrium structure, and
determine the effect of population sizes on the relative value of information
(i.e. difference in expected traveler costs) between any two populations. This
effect can be evaluated using a population-specific size threshold. One
population enjoys a strictly positive value of information in comparison to the
other if and only if its size is below the corresponding threshold. We also
consider the situation when travelers may choose an information system based on
its value, and characterize the set of equilibrium adoption rates delineating
the sizes of subscribed traveler populations. The resulting routing strategies
are such that all travelers face an identical expected cost, and no traveler
has the incentive to change her subscription."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1807.04428,https://arxiv.org/abs/1807.04428,"Abstract:  Semidefinite programming (SDP) with equality constraints arise in many
optimization and machine learning problems, such as Max-Cut, community
detection and robust PCA. Although SDPs can be solved to arbitrary precision in
polynomial time, generic convex solvers do not scale well with the dimension of
the problem. In order to address this issue, Burer and Monteiro
\cite{burer2003nonlinear} proposed to reduce the dimension of the problem by
appealing to a low-rank factorization, and solve the subsequent non-convex
problem instead. It is well-understood that the resulting non-convex problem
acts as a reliable surrogate to the original SDP, and can be efficiently solved
using the block-coordinate maximization method. Despite its simplicity,
remarkable success, and wide use in practice, the theoretical understanding of
the convergence of this method is limited. We prove that the block-coordinate
maximization algorithm applied to the non-convex Burer-Monteiro approach enjoys
a global sublinear rate without any assumptions on the problem, and a local
linear convergence rate despite no local maxima is locally strongly concave. We
illustrate our results through examples and numerical experiments."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1805.10579,https://arxiv.org/abs/1805.10579,"Abstract:  We study the trade-offs between convergence rate and robustness to gradient
errors in designing a first-order algorithm. We focus on gradient descent (GD)
and accelerated gradient (AG) methods for minimizing strongly convex functions
when the gradient has random errors in the form of additive white noise. With
gradient errors, the function values of the iterates need not converge to the
optimal value; hence, we define the robustness of an algorithm to noise as the
asymptotic expected suboptimality of the iterate sequence to input noise power.
For this robustness measure, we provide exact expressions for the quadratic
case using tools from robust control theory and tight upper bounds for the
smooth strongly convex case using Lyapunov functions certified through matrix
inequalities. We use these characterizations within an optimization problem
which selects parameters of each of the algorithms to achieve a particular
trade-off between rate and robustness. Our results show that AG can achieve
acceleration while being more robust to random gradient errors. This behavior
is quite different than previously reported in the deterministic gradient noise
setting. Our framework also leads to practical algorithms that can perform
better than other state-of-the-art methods in the presence of random gradient
noise."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1803.08200,https://arxiv.org/abs/1803.08200,"Abstract:  We consider coordinate descent (CD) methods with exact line search on convex
quadratic problems. Our main focus is to study the performance of the CD method
that use random permutations in each epoch and compare it to the performance of
the CD methods that use deterministic orders and random sampling with
replacement. We focus on a class of convex quadratic problems with a diagonally
dominant Hessian matrix, for which we show that using random permutations
instead of random with-replacement sampling improves the performance of the CD
method in the worst-case. Furthermore, we prove that as the Hessian matrix
becomes more diagonally dominant, the performance improvement attained by using
random permutations increases. We also show that for this problem class, using
any fixed deterministic order yields a superior performance than using random
permutations. We present detailed theoretical analyses with respect to three
different convergence criteria that are used in the literature and support our
theoretical results with numerical experiments."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1802.00080,https://arxiv.org/abs/1802.00080,"Abstract:  The study of strategic behavior in large scale networks via standard game
theoretical methods is a challenging, if not intractable, task. In this paper,
we propose a way to approximate games played over networks of increasing size,
by using the graph limiting concept of graphon. To this end, we introduce the
new class of graphon games for populations of infinite size. As a first
contribution, we investigate properties of the Nash equilibrium of this newly
defined class of games, including existence, uniqueness and comparative
statics. As a second contribution, we illustrate how graphon games can be used
to approximate strategic behavior in large but finite network games by assuming
that the network is randomly drawn according to the graphon and we derive
precise bounds for the distance between graphon and sampled network game
equilibria in terms of the population size. Finally, we derive a closed form
expression for the Nash equilibrium of linear quadratic graphon games and we
illustrate its relation to Bonacich centrality."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1712.08277,https://arxiv.org/abs/1712.08277,"Abstract:  We provide a unified variational inequality framework for the study of
fundamental properties of the Nash equilibrium in network games. We identify
several conditions on the underlying network (in terms of spectral norm,
infinity norm and minimum eigenvalue of its adjacency matrix) that guarantee
existence, uniqueness, convergence and continuity of equilibrium in general
network games with multidimensional and possibly constrained strategy sets. We
delineate the relations between these conditions and characterize classes of
networks that satisfy each of these conditions."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1706.08693,https://arxiv.org/abs/1706.08693,"Abstract:  We investigate the sensitivity of the Nash equilibrium of constrained network
aggregative games to changes in exogenous parameters affecting the cost
function of the players. This setting is motivated by two applications. The
first is the analysis of interventions by a social planner with a networked
objective function while the second is network routing games with atomic
players and information constraints. By exploiting a primal reformulation of a
sensitivity analysis result for variational inequalities, we provide a
characterization of the sensitivity of the Nash equilibrium that depends on
primal variables only. To derive this result we assume strong monotonicity of
the mapping associated with the game. As the second main result, we derive
sufficient conditions that guarantee this strong monotonicity property in
network aggregative games. These two characterizations allows us to
systematically study changes in the Nash equilibrium due to perturbations or
parameter variations in the two applications mentioned above."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1706.01131,https://arxiv.org/abs/1706.01131,"Abstract:  We study the optimal pricing strategy of a monopolist selling homogeneous
goods to customers over multiple periods. The customers choose their time of
purchase to maximize their payoff that depends on their valuation of the
product, the purchase price, and the utility they derive from past purchases of
others, termed the network effect. We first show that the optimal price
sequence is non-decreasing. Therefore, by postponing purchase to future rounds,
customers trade-off a higher utility from the network effects with a higher
price. We then show that a customer's equilibrium strategy can be characterized
by a threshold rule in which at each round a customer purchases the product if
and only if her valuation exceeds a certain threshold. This implies that
customers face an inference problem regarding the valuations of others, i.e.,
observing that a customer has not yet purchased the product, signals that her
valuation is below a threshold. We consider a block model of network
interactions, where there are blocks of buyers subject to the same network
effect. A natural benchmark, this model allows us to provide an explicit
characterization of the optimal price sequence asymptotically as the number of
agents goes to infinity, which notably is linearly increasing in time with a
slope that depends on the network effect through a scalar given by the sum of
entries of the inverse of the network weight matrix. Our characterization shows
that increasing the ""imbalance"" in the network defined as the difference
between the in and out degree of the nodes increases the revenue of the
monopolist. We further study the effects of price discrimination and show that
in earlier periods monopolist offers lower prices to blocks with higher
Bonacich centrality to encourage them to purchase, which in turn further
incentivizes other customers to buy in subsequent periods."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1603.03461,https://arxiv.org/abs/1603.03461,"Abstract:  We consider a multi agent optimization problem where a set of agents
collectively solves a global optimization problem with the objective function
given by the sum of locally known convex functions. We focus on the case when
information exchange among agents takes place over a directed network and
propose a distributed subgradient algorithm in which each agent performs local
processing based on information obtained from his incoming neighbors. Our
algorithm uses weight balancing to overcome the asymmetries caused by the
directed communication network, i.e., agents scale their outgoing information
with dynamically updated weights that converge to balancing weights of the
graph. We show that both the objective function values and the consensus
violation, at the ergodic average of the estimates generated by the algorithm,
converge with rate $O(\frac{\log T}{\sqrt{T}})$, where $T$ is the number of
iterations. A special case of our algorithm provides a new distributed method
to compute average consensus over directed graphs."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1601.02039,https://arxiv.org/abs/1601.02039,"Abstract:  To systematically study the implications of additional information about
routes provided to certain users (e.g., via GPS-based route guidance systems),
we introduce a new class of congestion games in which users have differing
information sets about the available edges and can only use routes consisting
of edges in their information set. After defining the notion of Information
Constrained Wardrop Equilibrium (ICWE) for this class of congestion games and
studying its basic properties, we turn to our main focus: whether additional
information can be harmful (in the sense of generating greater equilibrium
costs/delays). We formulate this question in the form of Informational Braes'
Paradox (IBP), which extends the classic Braess' Paradox in traffic equilibria,
and asks whether users receiving additional information can become worse off.
We provide a comprehensive answer to this question showing that in any network
in the series of linearly independent (SLI) class, which is a strict subset of
series-parallel networks, IBP cannot occur, and in any network that is not in
the SLI class, there exists a configuration of edge-specific cost functions for
which IBP will occur. In the process, we establish several properties of the
SLI class of networks, which include the characterization of the complement of
the SLI class in terms of embedding a specific set of networks, and also an
algorithm which determines whether a graph is SLI in linear time. We further
prove that the worst-case inefficiency performance of ICWE is no worse than the
standard Wardrop equilibrium."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1601.00194,https://arxiv.org/abs/1601.00194,"Abstract:  We propose a distributed algorithm based on Alternating Direction Method of
Multipliers (ADMM) to minimize the sum of locally known convex functions using
communication over a network. This optimization problem emerges in many
applications in distributed machine learning and statistical estimation. We
show that when functions are convex, both the objective function values and the
feasibility violation converge with rate $O(\frac{1}{T})$, where $T$ is the
number of iterations. We then show that if the functions are strongly convex
and have Lipschitz continuous gradients, the sequence generated by our
algorithm converges linearly to the optimal solution. In particular, an
$\epsilon$-optimal solution can be computed with $O(\sqrt{\kappa_f} \log
(1/\epsilon))$ iterations, where $\kappa_f$ is the condition number of the
problem. Our analysis also highlights the effect of network structure on the
convergence rate through maximum and minimum degree of nodes as well as the
algebraic connectivity of the network."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1510.06055,https://arxiv.org/abs/1510.06055,"Abstract:  We consider an SIS-type epidemic process that evolves on a known graph. We
assume that a fixed curing budget can be allocated at each instant to the nodes
of the graph, towards the objective of minimizing the expected extinction time
of the epidemic. We provide a lower bound on the optimal expected extinction
time as a function of the available budget, the epidemic parameters, the
maximum degree, and the CutWidth of the graph. For graphs with large CutWidth
(close to the largest possible), and under a budget which is sublinear in the
number of nodes, our lower bound scales exponentially with the size of the
graph."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1510.06054,https://arxiv.org/abs/1510.06054,"Abstract:  We consider the propagation of a contagion process (epidemic) on a network
and study the problem of dynamically allocating a fixed curing budget to the
nodes of the graph, at each time instant. For bounded degree graphs, we provide
a lower bound on the expected time to extinction under any such dynamic
allocation policy, in terms of a combinatorial quantity that we call the
resistance of the set of initially infected nodes, the available budget, and
the number of nodes n. Specifically, we consider the case of bounded degree
graphs, with the resistance growing linearly in n. We show that if the curing
budget is less than a certain multiple of the resistance, then the expected
time to extinction grows exponentially with n. As a corollary, if all nodes are
initially infected and the CutWidth of the graph grows linearly, while the
curing budget is less than a certain multiple of the CutWidth, then the
expected time to extinction grows exponentially in n. The combination of the
latter with our prior work establishes a fairly sharp phase transition on the
expected time to extinction (sub-linear versus exponential) based on the
relation between the CutWidth and the curing budget."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1506.06394,https://arxiv.org/abs/1506.06394,"Abstract:  We introduce a new class of (dynamical) systems that inherently capture
cascading effects (viewed as consequential effects) and are naturally amenable
to combinations. We develop an axiomatic general theory around those systems,
and guide the endeavor towards an understanding of cascading failure. The
theory evolves as an interplay of lattices and fixed points, and its results
may be instantiated to commonly studied models of cascade effects.
We characterize the systems through their fixed points, and equip them with
two operators. We uncover properties of the operators, and express global
systems through combinations of local systems. We enhance the theory with a
notion of failure, and understand the class of shocks inducing a system to
failure. We develop a notion of mu-rank to capture the energy of a system, and
understand the minimal amount of effort required to fail a system, termed
resilience. We deduce a dual notion of fragility and show that the combination
of systems sets a limit on the amount of fragility inherited."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1506.02081,https://arxiv.org/abs/1506.02081,"Abstract:  Motivated by applications to distributed optimization over networks and
large-scale data processing in machine learning, we analyze the deterministic
incremental aggregated gradient method for minimizing a finite sum of smooth
functions where the sum is strongly convex. This method processes the functions
one at a time in a deterministic order and incorporates a memory of previous
gradient values to accelerate convergence. Empirically it performs well in
practice; however, no theoretical analysis with explicit rate results was
previously given in the literature to our knowledge, in particular most of the
recent efforts concentrated on the randomized versions. In this paper, we show
that this deterministic algorithm has global linear convergence and
characterize the convergence rate. We also consider an aggregated method with
momentum and demonstrate its linear convergence. Our proofs rely on a careful
choice of a Lyapunov function that offers insight into the algorithm's behavior
and simplifies the proofs considerably."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1411.2647,https://arxiv.org/abs/1411.2647,"Abstract:  We present a distributed asynchronous algorithm for approximating a single
component of the solution to a system of linear equations $Ax = b$, where $A$
is a positive definite real matrix, and $b \in \mathbb{R}^n$. This is
equivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that
the spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann
series characterization of the component $x_i$, and is based on residual
updates. We analyze our algorithm within the context of a cloud computation
model, in which the computation is split into small update tasks performed by
small processors with shared access to a distributed file system. We prove a
robust asymptotic convergence result when the spectral radius $\rho(|G|) < 1$,
regardless of the precise order and frequency in which the update tasks are
performed. We provide convergence rate bounds which depend on the order of
update tasks performed, analyzing both deterministic update rules via counting
weighted random walks, as well as probabilistic update rules via concentration
bounds. The probabilistic analysis requires analyzing the product of random
matrices which are drawn from distributions that are time and path dependent.
We specifically consider the setting where $n$ is large, yet $G$ is sparse,
e.g., each row has at most $d$ nonzero entries. This is motivated by
applications in which $G$ is derived from the edge structure of an underlying
graph. Our results prove that if the local neighborhood of the graph does not
grow too quickly as a function of $n$, our algorithm can provide significant
reduction in computation cost as opposed to any algorithm which computes the
global solution vector $x$. Our algorithm obtains an $\epsilon \|x\|_2$
additive approximation for $x_i$ in constant time with respect to the size of
the matrix when the maximum row sparsity $d = O(1)$ and $1/(1-\|G\|_2) = O(1)$."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1410.5284,https://arxiv.org/abs/1410.5284,"Abstract:  Motivated by machine learning problems over large data sets and distributed
optimization over networks, we develop and analyze a new method called
incremental Newton method for minimizing the sum of a large number of strongly
convex functions. We show that our method is globally convergent for a variable
stepsize rule. We further show that under a gradient growth condition,
convergence rate is linear for both variable and constant stepsize rules. By
means of an example, we show that without the gradient growth condition,
incremental Newton method cannot achieve linear convergence. Our analysis can
be extended to study other incremental methods: in particular, we obtain a
linear convergence rate result for the incremental Gauss-Newton algorithm under
a variable stepsize rule."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1407.2241,https://arxiv.org/abs/1407.2241,"Abstract:  We provide a dynamic policy for the rapid containment of a contagion process
modeled as an SIS epidemic on a bounded degree undirected graph with n nodes.
We show that if the budget $r$ of curing resources available at each time is
${\Omega}(W)$, where $W$ is the CutWidth of the graph, and also of order
${\Omega}(\log n)$, then the expected time until the extinction of the epidemic
is of order $O(n/r)$, which is within a constant factor from optimal, as well
as sublinear in the number of nodes. Furthermore, if the CutWidth increases
only sublinearly with n, a sublinear expected time to extinction is possible
with a sublinearly increasing budget $r$."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1312.1986,https://arxiv.org/abs/1312.1986,"Abstract:  In this paper, we present a novel iterative Monte Carlo method for
approximating the stationary probability of a single state of a positive
recurrent Markov chain. We utilize the characterization that the stationary
probability of a state $i$ is inversely proportional to the expected return
time of a random walk beginning at $i$. Our method obtains an
$\epsilon$-multiplicative close estimate with probability greater than $1 -
\alpha$ using at most $\tilde{O}\left(t_{\text{mix}} \ln(1/\alpha) / \pi_i
\epsilon^2 \right)$ simulated random walk steps on the Markov chain across all
iterations, where $t_{\text{mix}}$ is the standard mixing time and $\pi_i$ is
the stationary probability. In addition, the estimate at each iteration is
guaranteed to be an upper bound with high probability, and is decreasing in
expectation with the iteration count, allowing us to monitor the progress of
the algorithm and design effective termination criteria. We propose a
termination criteria which guarantees a $\epsilon (1 + 4 \ln(2)
t_{\text{mix}})$ multiplicative error performance for states with stationary
probability larger than $\Delta$, while providing an additive error for states
with stationary probability less than $\Delta \in (0,1)$. The algorithm along
with this termination criteria uses at most
$\tilde{O}\left(\frac{\ln(1/\alpha)}{\epsilon^2}
\min\left(\frac{t_{\text{mix}}}{\pi_i}, \frac{1}{\epsilon
\Delta}\right)\right)$ simulated random walk steps, which is bounded by a
constant with respect to the Markov Chain. We provide a tight analysis of our
algorithm based on a locally weighted variant of the mixing time. Our results
naturally extend for countably infinite state space Markov chains via Lyapunov
function analysis."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1310.0005,https://arxiv.org/abs/1310.0005,"Abstract:  This paper proposes a new measure of node centrality in social networks, the
Harmonic Influence Centrality, which emerges naturally in the study of social
influence over networks. Using an intuitive analogy between social and
electrical networks, we introduce a distributed message passing algorithm to
compute the Harmonic Influence Centrality of each node. Although its design is
based on theoretical results which assume the network to have no cycle, the
algorithm can also be successfully applied on general graphs."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1307.8254,https://arxiv.org/abs/1307.8254,"Abstract:  We consider a network of agents that are cooperatively solving a global
optimization problem, where the objective function is the sum of privately
known local objective functions of the agents and the decision variables are
coupled via linear constraints. Recent literature focused on special cases of
this formulation and studied their distributed solution through either
subgradient based methods with O(1/sqrt(k)) rate of convergence (where k is the
iteration number) or Alternating Direction Method of Multipliers (ADMM) based
methods, which require a synchronous implementation and a globally known order
on the agents. In this paper, we present a novel asynchronous ADMM based
distributed method for the general formulation and show that it converges at
the rate O(1/k)."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1307.3586,https://arxiv.org/abs/1307.3586,"Abstract:  We introduce the notion of exchangeable equilibria of a symmetric bimatrix
game, defined as those correlated equilibria in which players' strategy choices
are conditionally independently and identically distributed given some hidden
variable. We give several game-theoretic interpretations and a version of the
""revelation principle"". Geometrically, the set of exchangeable equilibria is
convex and lies between the symmetric Nash equilibria and the symmetric
correlated equilibria. Exchangeable equilibria can achieve higher expected
utility than symmetric Nash equilibria."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1211.0654,https://arxiv.org/abs/1211.0654,"Abstract:  We study a model for cascade effects over finite networks based on a
deterministic binary linear threshold model. Our starting point is a networked
coordination game where each agent's payoff is the sum of the payoffs coming
from pairwise interactions with each of the neighbors. We first establish that
the best response dynamics in this networked game is equivalent to the linear
threshold dynamics with heterogeneous thresholds over the agents. While the
previous literature has studied such linear threshold models under the
assumption that each agent may change actions at most once, a study of best
response dynamics in such networked games necessitates an analysis that allows
for multiple switches in actions. In this paper, we develop such an analysis
and construct a combinatorial framework to understand the behavior of the
model. To this end, we establish that the agents behavior cycles among
different actions in the limit and provide three sets of results.
We first characterize the limiting behavioral properties of the dynamics. We
determine the length of the limit cycles and reveal bounds on the time steps
required to reach such cycles for different network structures. We then study
the complexity of decision/counting problems that arise within the context.
Specifically, we consider the tractability of counting the number of limit
cycles and fixed-points, and deciding the reachability of action profiles. We
finally propose a measure of network resilience that captures the nature of the
involved dynamics. We prove bounds and investigate the resilience of different
network structures under this measure."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1210.2289,https://arxiv.org/abs/1210.2289,"Abstract:  We present a distributed proximal-gradient method for optimizing the average
of convex functions, each of which is the private local objective of an agent
in a network with time-varying topology. The local objectives have distinct
differentiable components, but they share a common nondifferentiable component,
which has a favorable structure suitable for effective computation of the
proximal operator. In our method, each agent iteratively updates its estimate
of the global minimum by optimizing its local objective function, and
exchanging estimates with others via communication in the network. Using
Nesterov-type acceleration techniques and multiple communication steps per
iteration, we show that this method converges at the rate 1/k (where k is the
number of communication rounds between the agents), which is faster than the
convergence rate of the existing distributed methods for solving this problem.
The superior convergence rate of our method is also verified by numerical
experiments."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1209.1122,https://arxiv.org/abs/1209.1122,"Abstract:  We consider an infinite collection of agents who make decisions,
sequentially, about an unknown underlying binary state of the world. Each
agent, prior to making a decision, receives an independent private signal whose
distribution depends on the state of the world. Moreover, each agent also
observes the decisions of its last K immediate predecessors. We study
conditions under which the agent decisions converge to the correct value of the
underlying state. We focus on the case where the private signals have bounded
information content and investigate whether learning is possible, that is,
whether there exist decision rules for the different agents that result in the
convergence of their sequence of individual decisions to the correct state of
the world. We first consider learning in the almost sure sense and show that it
is impossible, for any value of K. We then explore the possibility of
convergence in probability of the decisions to the correct state. Here, a
distinction arises: if K equals 1, learning in probability is impossible under
any decision rule, while for K greater or equal to 2, we design a decision rule
that achieves it. We finally consider a new model, involving forward looking
strategic agents, each of which maximizes the discounted sum (over all agents)
of the probabilities of a correct decision. (The case, studied in previous
literature, of myopic agents who maximize the probability of their own decision
being correct is an extreme special case.) We show that for any value of K, for
any equilibrium of the associated Bayesian game, and under the assumption that
each private signal has bounded information content, learning in probability
fails to obtain."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1203.3258,https://arxiv.org/abs/1203.3258,"Abstract:  We present a framework for studying the problem of media streaming in
technology and cost heterogeneous environments. We first address the problem of
efficient streaming in a technology-heterogeneous setting. We employ random
linear network coding to simplify the packet selection strategies and alleviate
issues such as duplicate packet reception. Then, we study the problem of media
streaming from multiple cost-heterogeneous access networks. Our objective is to
characterize analytically the trade-off between access cost and user
experience. We model the Quality of user Experience (QoE) as the probability of
interruption in playback as well as the initial waiting time. We design and
characterize various control policies, and formulate the optimal control
problem using a Markov Decision Process (MDP) with a probabilistic constraint.
We present a characterization of the optimal policy using the
Hamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we
provide an exact and explicit characterization of a threshold policy and prove
its optimality using the HJB equation.
Our simulation results show that under properly designed control policy, the
existence of alternative access technology as a complement for a primary access
network can significantly improve the user experience without any bandwidth
over-provisioning."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1109.6505,https://arxiv.org/abs/1109.6505,"Abstract:  This paper examines the value of storage in securing reliability of a system
with uncertain supply and demand, and supply friction. The storage is
frictionless as a supply source, but once used, it cannot be filled up
instantaneously. The focus application is a power supply network in which the
base supply and demand are assumed to match perfectly, while deviations from
the base are modeled as random shocks with stochastic arrivals. Due to
friction, the random surge shocks cannot be tracked by the main supply sources.
Storage, when available, can be used to compensate, fully or partially, for the
surge in demand or loss of supply. The problem of optimal utilization of
storage with the objective of maximizing system reliability is formulated as
minimization of the expected discounted cost of blackouts over an infinite
horizon. It is shown that when the stage cost is linear in the size of the
blackout, the optimal policy is myopic in the sense that all shocks are
compensated by storage up to the available level of storage. However, when the
stage cost is strictly convex, it may be optimal to curtail some of the demand
and allow a small current blackout in the interest of maintaining a higher
level of reserve to avoid a large blackout in the future. The value of storage
capacity in improving system's reliability, as well as the effects of the
associated optimal policies under different stage costs on the probability
distribution of blackouts are examined."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1107.4386,https://arxiv.org/abs/1107.4386,"Abstract:  Except for special classes of games, there is no systematic framework for
analyzing the dynamical properties of multi-agent strategic interactions.
Potential games are one such special but restrictive class of games that allow
for tractable dynamic analysis. Intuitively, games that are ""close"" to a
potential game should share similar properties. In this paper, we formalize and
develop this idea by quantifying to what extent the dynamic features of
potential games extend to ""near-potential"" games. We study convergence of three
commonly studied classes of adaptive dynamics: discrete-time better/best
response, logit response, and discrete-time fictitious play dynamics. For
better/best response dynamics, we focus on the evolution of the sequence of
pure strategy profiles and show that this sequence converges to a (pure)
approximate equilibrium set, whose size is a function of the ""distance"" from a
close potential game. We then study logit response dynamics and provide a
characterization of the stationary distribution of this update rule in terms of
the distance of the game from a close potential game and the corresponding
potential function. We further show that the stochastically stable strategy
profiles are pure approximate equilibria. Finally, we turn attention to
fictitious play, and establish that the sequence of empirical frequencies of
player actions converges to a neighborhood of (mixed) equilibria of the game,
where the size of the neighborhood increases with distance of the game to a
potential game. Thus, our results suggest that games that are close to a
potential game inherit the dynamical properties of potential games. Since a
close potential game to a given game can be found by solving a convex
optimization problem, our approach also provides a systematic framework for
studying convergence behavior of adaptive learning dynamics in arbitrary finite
strategic form games."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1101.5617,https://arxiv.org/abs/1101.5617,"Abstract:  We study the optimal pricing strategies of a monopolist selling a divisible
good (service) to consumers that are embedded in a social network. A key
feature of our model is that consumers experience a (positive) local network
effect. In particular, each consumer's usage level depends directly on the
usage of her neighbors in the social network structure. Thus, the monopolist's
optimal pricing strategy may involve offering discounts to certain agents, who
have a central position in the underlying network.
First, we consider a setting where the monopolist can offer individualized
prices and derive an explicit characterization of the optimal price for each
consumer as a function of her network position. In particular, we show that it
is optimal for the monopolist to charge each agent a price that is proportional
to her Bonacich centrality in the social network. In the second part of the
paper, we discuss the optimal strategy of a monopolist that can only choose a
single uniform price for the good and derive an algorithm polynomial in the
number of agents to compute such a price. Thirdly, we assume that the
monopolist can offer the good in two prices, full and discounted, and study the
problem of determining which set of consumers should be given the discount. We
show that the problem is NP-hard, however we provide an explicit
characterization of the set of agents that should be offered the discounted
price. Next, we describe an approximation algorithm for finding the optimal set
of agents. We show that if the profit is nonnegative under any feasible price
allocation, the algorithm guarantees at least 88% of the optimal profit.
Finally, we highlight the value of network information by comparing the profits
of a monopolist that does not take into account the network effects when
choosing her pricing policy to those of a monopolist that uses this information
optimally."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1011.3062,https://arxiv.org/abs/1011.3062,"Abstract:  In this paper we study the generalized version of weighted matching in
bipartite networks. Consider a weighted matching in a bipartite network in
which the nodes derive value from the split of the matching edge assigned to
them if they are matched. The value a node derives from the split depends both
on the split as well as the partner the node is matched to. We assume that the
value of a split to the node is continuous and strictly increasing in the part
of the split assigned to the node. A stable weighted matching is a matching and
splits on the edges in the matching such that no two adjacent nodes in the
network can split the edge between them so that both of them can derive a
higher value than in the matching. We extend the weighted matching problem to
this general case and study the existence of a stable weighted matching. We
also present an algorithm that converges to a stable weighted matching. The
algorithm generalizes the Hungarian algorithm for bipartite matching. Faster
algorithms can be made when there is more structure on the value functions."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1011.2515,https://arxiv.org/abs/1011.2515,"Abstract:  In this paper we show that when individuals in a bipartite network
exclusively choose partners and exchange valued goods with their partners, then
there exists a set of exchanges that are pair-wise stable. Pair-wise stability
implies that no individual breaks her partnership and no two neighbors in the
network can form a new partnership while breaking other partnerships if any so
that at least one of them improves her payoff and the other one does at least
as good. We consider a general class of continuous, strictly convex and
strongly monotone preferences over bundles of goods for individuals. Thus, this
work extends the general equilibrium framework from markets to networks with
exclusive exchanges. We present the complete existence proof using the
existence of a generalized stable matching in
\cite{Generalized-Stable-Matching}. The existence proof can be extended to
problems in social games as in \cite{Matching-Equilibrium} and
\cite{Social-Games}."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1010.2871,https://arxiv.org/abs/1010.2871,"Abstract:  The authors have decided to withdraw this submission.
Clarifications/corrections, if any, may follow at a later date."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1009.2653,https://arxiv.org/abs/1009.2653,"Abstract:  We study a tractable opinion dynamics model that generates long-run
disagreements and persistent opinion fluctuations. Our model involves an
inhomogeneous stochastic gossip process of continuous opinion dynamics in a
society consisting of two types of agents: regular agents, who update their
beliefs according to information that they receive from their social neighbors;
and stubborn agents, who never update their opinions. When the society contains
stubborn agents with different opinions, the belief dynamics never lead to a
consensus (among the regular agents). Instead, beliefs in the society fail to
converge almost surely, the belief profile keeps on fluctuating in an ergodic
fashion, and it converges in law to a non-degenerate random vector. The
structure of the network and the location of the stubborn agents within it
shape the opinion dynamics. The expected belief vector evolves according to an
ordinary differential equation coinciding with the Kolmogorov backward equation
of a continuous-time Markov chain with absorbing states corresponding to the
stubborn agents and converges to a harmonic vector, with every regular agent's
value being the weighted average of its neighbors' values, and boundary
conditions corresponding to the stubborn agents'. Expected cross-products of
the agents' beliefs allow for a similar characterization in terms of coupled
Markov chains on the network. We prove that, in large-scale societies which are
highly fluid, meaning that the product of the mixing time of the Markov chain
on the graph describing the social network and the relative size of the
linkages to stubborn agents vanishes as the population size grows large, a
condition of \emph{homogeneous influence} emerges, whereby the stationary
beliefs' marginal distributions of most of the regular agents have
approximately equal first and second moments."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1005.3045,https://arxiv.org/abs/1005.3045,"Abstract:  This document consists of two parts: the second part was submitted earlier as
a new proof of Nash's theorem, and the first part is a note explaining a
problem found in that proof. We are indebted to Sergiu Hart and Eran Shmaya for
their careful study which led to their simultaneous discovery of this error. So
far the error has not been fixed, but many of the results and techniques of the
paper remain valid, so we will continue to make it available online.
Abstract for the original paper:
We give a novel proof of the existence of Nash equilibria in all finite games
without using fixed point theorems or path following arguments. Our approach
relies on a new notion intermediate between Nash and correlated equilibria
called exchangeable equilibria, which are correlated equilibria with certain
symmetry and factorization properties. We prove these exist by a duality
argument, using Hart and Schmeidler's proof of correlated equilibrium existence
as a first step.
In an appropriate limit exchangeable equilibria converge to the convex hull
of Nash equilibria, proving that these exist as well. Exchangeable equilibria
are defined in terms of symmetries of the game, so this method automatically
proves the stronger statement that a symmetric game has a symmetric Nash
equilibrium. The case without symmetries follows by a symmetrization argument."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1005.2633,https://arxiv.org/abs/1005.2633,"Abstract:  Most existing work uses dual decomposition and subgradient methods to solve
Network Utility Maximization (NUM) problems in a distributed manner, which
suffer from slow rate of convergence properties. This work develops an
alternative distributed Newton-type fast converging algorithm for solving
network utility maximization problems with self-concordant utility functions.
By using novel matrix splitting techniques, both primal and dual updates for
the Newton step can be computed using iterative schemes in a decentralized
manner with limited information exchange. Similarly, the stepsize can be
obtained via an iterative consensus-based averaging scheme. We show that even
when the Newton direction and the stepsize in our method are computed within
some error (due to finite truncation of the iterative schemes), the resulting
objective function value still converges superlinearly to an explicitly
characterized error neighborhood. Simulation results demonstrate significant
convergence rate improvement of our algorithm relative to the existing
subgradient methods based on dual decomposition."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1005.2405,https://arxiv.org/abs/1005.2405,"Abstract:  In this paper we introduce a novel flow representation for finite games in
strategic form. This representation allows us to develop a canonical direct sum
decomposition of an arbitrary game into three components, which we refer to as
the potential, harmonic and nonstrategic components. We analyze natural classes
of games that are induced by this decomposition, and in particular, focus on
games with no harmonic component and games with no potential component. We show
that the first class corresponds to the well-known potential games. We refer to
the second class of games as harmonic games, and study the structural and
equilibrium properties of this new class of games. Intuitively, the potential
component of a game captures interactions that can equivalently be represented
as a common interest game, while the harmonic part represents the conflicts
between the interests of the players. We make this intuition precise, by
studying the properties of these two classes, and show that indeed they have
quite distinct and remarkable characteristics. For instance, while finite
potential games always have pure Nash equilibria, harmonic games generically
never do. Moreover, we show that the nonstrategic component does not affect the
equilibria of a game, but plays a fundamental role in their efficiency
properties, thus decoupling the location of equilibria and their payoff-related
properties. Exploiting the properties of the decomposition framework, we obtain
explicit expressions for the projections of games onto the subspaces of
potential and harmonic games. This enables an extension of the properties of
potential and harmonic games to ""nearby"" games. We exemplify this point by
showing that the set of approximate equilibria of an arbitrary game can be
characterized through the equilibria of its projection onto the set of
potential games."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1004.3523,https://arxiv.org/abs/1004.3523,"Abstract:  We study the design of media streaming applications in the presence of
multiple heterogeneous wireless access methods with different throughputs and
costs. Our objective is to analytically characterize the trade-off between the
usage cost and the Quality of user Experience (QoE), which is represented by
the probability of interruption in media playback and the initial waiting time.
We model each access network as a server that provides packets to the user
according to a Poisson process with a certain rate and cost. Blocks are coded
using random linear codes to alleviate the duplicate packet reception problem.
Users must take decisions on how many packets to buffer before playout, and
which networks to access during playout. We design, analyze and compare several
control policies with a threshold structure. We formulate the problem of
finding the optimal control policy as an MDP with a probabilistic constraint.
We present the HJB equation for this problem by expanding the state space, and
exploit it as a verification method for optimality of the proposed control law."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1004.0969,https://arxiv.org/abs/1004.0969,"Abstract:  We study distributed algorithms for solving global optimization problems in
which the objective function is the sum of local objective functions of agents
and the constraint set is given by the intersection of local constraint sets of
agents. We assume that each agent knows only his own local objective function
and constraint set, and exchanges information with the other agents over a
randomly varying network topology to update his information state. We assume a
state-dependent communication model over this topology: communication is
Markovian with respect to the states of the agents and the probability with
which the links are available depends on the states of the agents. In this
paper, we study a projected multi-agent subgradient algorithm under
state-dependent communication. The algorithm involves each agent performing a
local averaging to combine his estimate with the other agents' estimates,
taking a subgradient step along his local objective function, and projecting
the estimates on his local constraint set. The state-dependence of the
communication introduces significant challenges and couples the study of
information exchange with the analysis of subgradient steps and projection
errors. We first show that the multi-agent subgradient algorithm when used with
a constant stepsize may result in the agent estimates to diverge with
probability one. Under some assumptions on the stepsize sequence, we provide
convergence rate bounds on a ""disagreement metric"" between the agent estimates.
Our bounds are time-nonhomogeneous in the sense that they depend on the initial
starting time. Despite this, we show that agent estimates reach an almost sure
consensus and converge to the same optimal solution of the global optimization
problem with probability one under different assumptions on the local
constraint sets and the stepsize sequence."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1002.0035,https://arxiv.org/abs/1002.0035,"Abstract:  We exhibit the rich structure of the set of correlated equilibria by
analyzing the simplest of polynomial games: the mixed extension of matching
pennies. We show that while the correlated equilibrium set is convex and
compact, the structure of its extreme points can be quite complicated. In
finite games the ratio of extreme correlated to extreme Nash equilibria can be
greater than exponential in the size of the strategy spaces. In polynomial
games there can exist extreme correlated equilibria which are not finitely
supported; we construct a large family of examples using techniques from
ergodic theory. We show that in general the set of correlated equilibrium
distributions of a polynomial game cannot be described by conditions on
finitely many moments (means, covariances, etc.), in marked contrast to the set
of Nash equilibria which is always expressible in terms of finitely many
moments."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:1001.3171,https://arxiv.org/abs/1001.3171,"Abstract:  We focus on a particular form of network coding, reverse carpooling, in a
wireless network where the potentially coded transmitted messages are to be
decoded immediately upon reception. The network is fixed and known, and the
system performance is measured in terms of the number of wireless broadcasts
required to meet multiple unicast demands. Motivated by the structure of the
coding scheme, we formulate the problem as a linear program by introducing a
flow variable for each triple of connected nodes. This allows us to have a
formulation polynomial in the number of nodes. Using dual decomposition and
projected subgradient method, we present a decentralized algorithm to obtain
optimal routing schemes in presence of coding opportunities. We show that the
primal sub-problem can be expressed as a shortest path problem on an
\emph{edge-graph}, and the proposed algorithm requires each node to exchange
information only with its neighbors."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0906.5007,https://arxiv.org/abs/0906.5007,"Abstract:  We provide a model to investigate the tension between information aggregation
and spread of misinformation in large societies (conceptualized as networks of
agents communicating with each other). Each individual holds a belief
represented by a scalar. Individuals meet pairwise and exchange information,
which is modeled as both individuals adopting the average of their pre-meeting
beliefs. When all individuals engage in this type of information exchange, the
society will be able to effectively aggregate the initial information held by
all individuals. There is also the possibility of misinformation, however,
because some of the individuals are ""forceful,"" meaning that they influence the
beliefs of (some) of the other individuals they meet, but do not change their
own opinion. The paper characterizes how the presence of forceful agents
interferes with information aggregation. Under the assumption that even
forceful agents obtain some information (however infrequent) from some others
(and additional weak regularity conditions), we first show that beliefs in this
class of societies converge to a consensus among all individuals. This
consensus value is a random variable, however, and we characterize its
behavior. Our main results quantify the extent of misinformation in the society
by either providing bounds or exact results (in some special cases) on how far
the consensus value can be from the benchmark without forceful agents (where
there is efficient information aggregation). The worst outcomes obtain when
there are several forceful agents and forceful agents themselves update their
beliefs only on the basis of information they obtain from individuals most
likely to have received their own information previously."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0812.4279,https://arxiv.org/abs/0812.4279,"Abstract:  We present several new characterizations of correlated equilibria in games
with continuous utility functions. These have the advantage of being more
computationally and analytically tractable than the standard definition in
terms of departure functions. We use these characterizations to construct
effective algorithms for approximating a single correlated equilibrium or the
entire set of correlated equilibria of a game with polynomial utility
functions."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0812.3447,https://arxiv.org/abs/0812.3447,"Abstract:  A wireless packet network is considered in which each user transmits a stream
of packets to its destination. The transmit power of each user interferes with
the transmission of all other users. A convex cost function of the completion
times of the user packets is minimized by optimally allocating the users'
transmission power subject to their respective power constraints. At all ranges
of SINR, completion time minimization can be formulated as a convex
optimization problem and hence can be efficiently solved. In particular,
although the feasible rate region of the wireless network is non-convex, its
corresponding completion time region is shown to be convex. When channel
knowledge is imperfect, robust power control is considered based on the channel
fading distribution subject to outage probability constraints. The problem is
shown to be convex when the fading distribution is log-concave in exponentiated
channel power gains; e.g., when each user is under independent Rayleigh,
Nakagami, or log-normal fading. Applying the optimization frameworks in a
wireless cellular network, the average completion time is significantly reduced
as compared to full power transmission."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0810.1267,https://arxiv.org/abs/0810.1267,"Abstract:  We consider the problem of rate allocation in a fading Gaussian
multiple-access channel with fixed transmission powers. The goal is to maximize
a general concave utility function of the expected achieved rates of the users.
There are different approaches to this problem in the literature. From an
information theoretic point of view, rates are allocated only by using the
channel state information. The queueing theory approach utilizes the global
queue-length information for rate allocation to guarantee throughput optimality
as well as maximizing a utility function of the rates. In this work, we make a
connection between these two approaches by showing that the information
theoretic capacity region of a multiple-access channel and its stability region
are equivalent. Moreover, our numerical results show that a simple greedy
policy which does not use the queue-length information can outperform
queue-length based policies in terms of convergence rate and fairness."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0810.1260,https://arxiv.org/abs/0810.1260,"Abstract:  We consider the problem of rate and power allocation in a fading
multiple-access channel. Our objective is to obtain rate and power allocation
policies that maximize a utility function defined over average transmission
rates. In contrast with the literature, which focuses on the linear case, we
present results for general concave utility functions. We consider two cases.
In the first case, we assume that power control is possible and channel
statistics are known. In this case, we show that the optimal policies can be
obtained greedily by maximizing a linear utility function at each channel
state. In the second case, we assume that power control is not possible and
channel statistics are not available. In this case, we define a greedy rate
allocation policy and provide upper bounds on the performance difference
between the optimal and the greedy policy. Our bounds highlight the dependence
of the performance difference on the channel variations and the structure of
the utility function."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0810.1253,https://arxiv.org/abs/0810.1253,"Abstract:  We consider the problem of rate allocation in a fading Gaussian
multiple-access channel (MAC) with fixed transmission powers. Our goal is to
maximize a general concave utility function of transmission rates over the
throughput capacity region. In contrast to earlier works in this context that
propose solutions where a potentially complex optimization problem must be
solved in every decision instant, we propose a low-complexity approximate rate
allocation policy and analyze the effect of temporal channel variations on its
utility performance. To the best of our knowledge, this is the first work that
studies the tracking capabilities of an approximate rate allocation scheme
under fading channel conditions. We build on an earlier work to present a new
rate allocation policy for a fading MAC that implements a low-complexity
approximate gradient projection iteration for each channel measurement, and
explicitly characterize the effect of the speed of temporal channel variations
on the tracking neighborhood of our policy. We further improve our results by
proposing an alternative rate allocation policy for which tighter bounds on the
size of the tracking neighborhood are derived. These proposed rate allocation
policies are computationally efficient in our setting since they implement a
single gradient projection iteration per channel measurement and each such
iteration relies on approximate projections which has polynomial-complexity in
the number of users."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0810.1248,https://arxiv.org/abs/0810.1248,"Abstract:  We consider the problem of rate allocation in a Gaussian multiple-access
channel, with the goal of maximizing a utility function over transmission
rates. In contrast to the literature which focuses on linear utility functions,
we study general concave utility functions. We present a gradient projection
algorithm for this problem. Since the constraint set of the problem is
described by exponentially many constraints, methods that use exact projections
are computationally intractable. Therefore, we develop a new method that uses
approximate projections. We use the polymatroid structure of the capacity
region to show that the approximate projection can be implemented by a
recursive algorithm in time polynomial in the number of users. We further
propose another algorithm for implementing the approximate projections using
rate-splitting and show improved bounds on its convergence time."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0810.1234,https://arxiv.org/abs/0810.1234,"Abstract:  We consider the problem of rate and power allocation in a multiple-access
channel. Our objective is to obtain rate and power allocation policies that
maximize a general concave utility function of average transmission rates on
the information theoretic capacity region of the multiple-access channel. Our
policies does not require queue-length information. We consider several
different scenarios. First, we address the utility maximization problem in a
nonfading channel to obtain the optimal operating rates, and present an
iterative gradient projection algorithm that uses approximate projection. By
exploiting the polymatroid structure of the capacity region, we show that the
approximate projection can be implemented in time polynomial in the number of
users. Second, we consider resource allocation in a fading channel. Optimal
rate and power allocation policies are presented for the case that power
control is possible and channel statistics are available. For the case that
transmission power is fixed and channel statistics are unknown, we propose a
greedy rate allocation policy and provide bounds on the performance difference
of this policy and the optimal policy in terms of channel variations and
structure of the utility function. We present numerical results that
demonstrate superior convergence rate performance for the greedy policy
compared to queue-length based policies. In order to reduce the computational
complexity of the greedy policy, we present approximate rate allocation
policies which track the greedy policy within a certain neighborhood that is
characterized in terms of the speed of fading."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0803.1202,https://arxiv.org/abs/0803.1202,"Abstract:  We consider a convex unconstrained optimization problem that arises in a
network of agents whose goal is to cooperatively optimize the sum of the
individual agent objective functions through local computations and
communications. For this problem, we use averaging algorithms to develop
distributed subgradient methods that can operate over a time-varying topology.
Our focus is on the convergence rate of these methods and the degradation in
performance when only quantized information is available. Based on our recent
results on the convergence time of distributed averaging algorithms, we derive
improved upper bounds on the convergence rate of the unquantized subgradient
method. We then propose a distributed subgradient method under the additional
constraint that agents can only store and communicate quantized information,
and we provide bounds on its convergence rate that highlight the dependence on
the number of quantization levels."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0802.3922,https://arxiv.org/abs/0802.3922,"Abstract:  We present distributed algorithms that can be used by multiple agents to
align their estimates with a particular value over a network with time-varying
connectivity. Our framework is general in that this value can represent a
consensus value among multiple agents or an optimal solution of an optimization
problem, where the global objective function is a combination of local agent
objective functions. Our main focus is on constrained problems where the
estimate of each agent is restricted to lie in a different constraint set.
To highlight the effects of constraints, we first consider a constrained
consensus problem and present a distributed ``projected consensus algorithm''
in which agents combine their local averaging operation with projection on
their individual constraint sets. This algorithm can be viewed as a version of
an alternating projection method with weights that are varying over time and
across agents. We establish convergence and convergence rate results for the
projected consensus algorithm. We next study a constrained optimization problem
for optimizing the sum of local objective functions of the agents subject to
the intersection of their local constraint sets. We present a distributed
``projected subgradient algorithm'' which involves each agent performing a
local averaging operation, taking a subgradient step to minimize its own
objective function, and projecting on its constraint set. We show that, with an
appropriately selected stepsize rule, the agent estimates generated by this
algorithm converge to the same optimal solution for the cases when the weights
are constant and equal, and when the weights are time-varying but all agents
have the same constraint set."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0711.4179,https://arxiv.org/abs/0711.4179,"Abstract:  We consider distributed iterative algorithms for the averaging problem over
time-varying topologies. Our focus is on the convergence time of such
algorithms when complete (unquantized) information is available, and on the
degradation of performance when only quantized information is available. We
study a large and natural class of averaging algorithms, which includes the
vast majority of algorithms proposed to date, and provide tight polynomial
bounds on their convergence time. We also describe an algorithm within this
class whose convergence time is the best among currently available averaging
algorithms for time-varying topologies. We then propose and analyze distributed
averaging algorithms under the additional constraint that agents can only store
and communicate quantized information, so that they can only converge to the
average of the initial values of the agents within some error. We establish
bounds on the error and tight bounds on the convergence time, as a function of
the number of quantization levels."
Asuman Ozdaglar,Ozdaglar_Asuman,arXiv:0707.3462,https://arxiv.org/abs/0707.3462,"Abstract:  In this paper, we study nonzero-sum separable games, which are continuous
games whose payoffs take a sum-of-products form. Included in this subclass are
all finite games and polynomial games. We investigate the structure of
equilibria in separable games. We show that these games admit finitely
supported Nash equilibria. Motivated by the bounds on the supports of mixed
equilibria in two-player finite games in terms of the ranks of the payoff
matrices, we define the notion of the rank of an n-player continuous game and
use this to provide bounds on the cardinality of the support of equilibrium
strategies. We present a general characterization theorem that states that a
continuous game has finite rank if and only if it is separable. Using our rank
results, we present an efficient algorithm for computing approximate equilibria
of two-player separable games with fixed strategy spaces in time polynomial in
the rank of the game."
Tomas Palacios,Palacios_Tomas,arXiv:1812.07111,https://arxiv.org/abs/1812.07111,"Abstract:  When the Fermi level matches the Dirac point in graphene, the reduced charge
screening can dramatically enhance electron-electron (e-e) scattering to
produce a strongly interacting Dirac liquid. While the dominance of e-e
scattering already leads to novel behaviors, such as electron hydrodynamic
flow, further exotic phenomena have been predicted to arise specifically from
the unique kinematics of e-e scattering in massless Dirac systems. Here, we use
optoelectronic probes, which are highly sensitive to the kinematics of electron
scattering, to uncover a giant intrinsic photocurrent response in pristine
graphene. This photocurrent emerges exclusively at the charge neutrality point
and vanishes abruptly at non-zero charge densities. Moreover, it is observed at
places with broken reflection symmetry, and it is selectively enhanced at free
graphene edges with sharp bends. Our findings reveal that the photocurrent
relaxation is strongly suppressed by a drastic change of fast photocarrier
kinematics in graphene when its Fermi level matches the Dirac point. The
emergence of robust photocurrents in neutral Dirac materials promises new
energy-harvesting functionalities and highlights intriguing electron dynamics
in the optoelectronic response of Dirac fluids."
Tomas Palacios,Palacios_Tomas,arXiv:1805.12432,https://arxiv.org/abs/1805.12432,"Abstract:  The effect of a two dimensional (2D) graphene layer (GL) on top of the
silicon nitride (SiN) passivation layer of AlGaN/GaN
metal-insulator-semiconductor high-electron-mobility transistors (MIS-HEMTs)
has been systematically analyzed. Results showed that in the devices without
the GL, the maximum drain current density (I_D,max) and the maximum
transconductance (g_m,max) decreased gradually as the mist exposure time
increased, up to 23% and 10%, respectively. Moreover, the gate lag ratio (GLR)
increased around 10% during mist exposure. In contrast, devices with a GL
showed a robust behavior and not significant changes in the electrical
characteristics in both DC and pulsed conditions. The origin of these behaviors
has been discussed and the results pointed to the GL as the key factor for
improving the moisture resistance of the SiN passivation layer."
Tomas Palacios,Palacios_Tomas,arXiv:1705.00590,https://arxiv.org/abs/1705.00590,"Abstract:  A Weyl semimetal (WSM) is a novel topological phase of matter, in which Weyl
fermions (WFs) arise as pseudo-magnetic monopoles in its momentum space. The
chirality of the WFs, given by the sign of the monopole charge, is central to
the Weyl physics, since it directly serves as the sign of the topological
number and gives rise to exotic properties such as Fermi arcs and the chiral
anomaly. Despite being the defining property of a WSM, the chirality of the WFs
has never been experimentally measured. Here, we directly detect the chirality
of the WFs by measuring the photocurrent in response to circularly polarized
mid-infrared light. The resulting photocurrent is determined by both the
chirality of WFs and that of the photons. Our results pave the way for
realizing a wide range of theoretical proposals for studying and controlling
the WFs and their associated quantum anomalies by optical and electrical means.
More broadly, the two chiralities, analogous to the two valleys in 2D
materials, lead to a new degree of freedom in a 3D crystal with potential novel
pathways to store and carry information."
Tomas Palacios,Palacios_Tomas,arXiv:1610.04692,https://arxiv.org/abs/1610.04692,"Abstract:  Single photon emitters play a central role in many photonic quantum
technologies. A promising class of single photon emitters consists of atomic
color centers in wide-bandgap crystals, such as diamond silicon carbide and
hexagonal boron nitride. However, it is currently not possible to grow these
materials as sub-micron thick films on low-refractive index substrates, which
is necessary for mature photonic integrated circuit technologies. Hence, there
is great interest in identifying quantum emitters in technologically mature
semiconductors that are compatible with suitable heteroepitaxies. Here, we
demonstrate robust single photon emitters based on defects in gallium nitride
(GaN), the most established and well understood semiconductor that can emit
light over the entire visible spectrum. We show that the emitters have
excellent photophysical properties including a brightness in excess of 500x10^3
counts/s. We further show that the emitters can be found in a variety of GaN
wafers, thus offering reliable and scalable platform for further technological
development. We propose a theoretical model to explain the origin of these
emitters based on cubic inclusions in hexagonal gallium nitride. Our results
constitute a feasible path to scalable, integrated on-chip quantum technologies
based on GaN."
Tomas Palacios,Palacios_Tomas,arXiv:1512.04492,https://arxiv.org/abs/1512.04492,"Abstract:  Diverse parallel stitched two-dimensional heterostructures are synthesized,
including metal-semiconductor (graphene-MoS2), semiconductor-semiconductor
(WS2-MoS2), and insulator-semiconductor (hBN-MoS2), directly through selective
sowing of aromatic molecules as the seeds in chemical vapor deposition (CVD)
method. Our methodology enables the large-scale fabrication of lateral
heterostructures with arbitrary patterns, and clean and precisely aligned
interfaces, which offers tremendous potential for its application in integrated
circuits."
Tomas Palacios,Palacios_Tomas,arXiv:1401.6878,https://arxiv.org/abs/1401.6878,"Abstract:  Graphene, owing to its ability to support plasmon polariton waves in the
terahertz frequency range, enables the miniaturization of antennas to allow
wireless communications among nanosystems. One of the main challenges in the
demonstration of graphene antennas is finding suitable terahertz sources to
feed the antenna. This paper estimates the performance of a graphene RF
plasmonic micro-antenna fed with a photoconductive source. The terahertz source
is modeled and, by means of a full-wave EM solver, the radiated power of the
device is estimated with respect to material, laser illumination and antenna
geometry parameters. The results show that the proposed device radiates
terahertz pulses with an average power up to 1$\mu$W, proving the feasibility
of feeding miniaturized graphene antennas with photoconductive materials."
Tomas Palacios,Palacios_Tomas,arXiv:1401.4798,https://arxiv.org/abs/1401.4798,"Abstract:  We demonstrate second order optical nonlinearity in a silicon architecture
through heterogeneous integration of single-crystalline gallium nitride (GaN)
on silicon (100) substrates. By engineering GaN microrings for dual resonance
around 1560 nm and 780 nm, we achieve efficient, tunable second harmonic
generation at 780 nm. The \{chi}(2) nonlinear susceptibility is measured to be
as high as 16 plus minus 7 pm/V. Because GaN has a wideband transparency window
covering ultraviolet, visible and infrared wavelengths, our platform provides a
viable route for the on-chip generation of optical wavelengths in both the far
infrared and near-UV through a combination of \{chi}(2) enabled
sum-/difference-frequency processes."
Tomas Palacios,Palacios_Tomas,arXiv:1302.4027,https://arxiv.org/abs/1302.4027,"Abstract:  2D nanoelectronics based on single-layer MoS2 offers great advantages for
both conventional and ubiquitous applications. This paper discusses the
large-scale CVD growth of single-layer MoS2 and fabrication of devices and
circuits for the first time. Both digital and analog circuits are fabricated to
demonstrate its capability for mixed-signal applications."
Tomas Palacios,Palacios_Tomas,arXiv:1208.1078,https://arxiv.org/abs/1208.1078,"Abstract:  Two-dimensional (2D) materials, such as molybdenum disulfide (MoS2), have
been shown to exhibit excellent electrical and optical properties. The
semiconducting nature of MoS2 allows it to overcome the shortcomings of
zero-bandgap graphene, while still sharing many of graphene's advantages for
electronic and optoelectronic applications. Discrete electronic and
optoelectronic components, such as field-effect transistors, sensors and
photodetectors made from few-layer MoS2 show promising performance as potential
substitute of Si in conventional electronics and of organic and amorphous Si
semiconductors in ubiquitous systems and display applications. An important
next step is the fabrication of fully integrated multi-stage circuits and logic
building blocks on MoS2 to demonstrate its capability for complex digital logic
and high-frequency ac applications. This paper demonstrates an inverter, a NAND
gate, a static random access memory, and a five-stage ring oscillator based on
a direct-coupled transistor logic technology. The circuits comprise between two
to twelve transistors seamlessly integrated side-by-side on a single sheet of
bilayer MoS2. Both enhancement-mode and depletion-mode transistors were
fabricated thanks to the use of gate metals with different work functions."
Tomas Palacios,Palacios_Tomas,arXiv:1112.4831,https://arxiv.org/abs/1112.4831,"Abstract:  In this letter, we analyze the carrier transit delay in graphene field-effect
transistors (GFETs). GFETs are fabricated at the wafer-scale on sapphire
substrate. For a device with a gate length of 210 nm, a current gain cut-off
frequency fT of 18 GHz and 22 GHz is obtained before and after de-embedding.
The extraction of the internal (Cgs,i, Cgd,i) and external capacitances (Cgs,ex
and Cgd,ex) from the scaling behavior of the gate capacitances Cgs and Cgd
allows the intrinsic ({\tau}_int), extrinsic ({\tau}_ext) and parasitic delays
({\tau}_par) to be obtained. In addition, the extraction of the intrinsic delay
provides a new way to directly estimate carrier velocity from the experimental
data while the breakdown of the total delay into intrinsic, extrinsic, and
parasitic components can offer valuable information for optimizing RF GFETs
structures."
Tomas Palacios,Palacios_Tomas,arXiv:1108.2021,https://arxiv.org/abs/1108.2021,"Abstract:  In this letter, we demonstrate the first BN/Graphene/BN field effect
transistor for RF applications. The BN/Graphene/BN structure can preserve the
high mobility of graphene, even when it is sandwiched between a substrate and a
gate dielectric. Field effect transistors (FETs) using a bilayer graphene
channel have been fabricated with a gate length LG=450 nm. A current density in
excess of 1 A/mm and DC transconductance close to 250 mS/mm are achieved for
both electron and hole conductions. RF characterization is performed for the
first time on this device structure, giving a current-gain cut-off frequency
fT=33 GHz and an fT.LG product of 15 GHz.um. The improved performance obtained
by the BN/Graphene/BN structure is very promising to enable the next generation
of high frequency graphene RF electronics."
Pablo Parrilo,Parrilo_Pablo,arXiv:1812.05475,https://arxiv.org/abs/1812.05475,"Abstract:  The package SOS implements sums-of-squares (SOS) decompositions in Macaulay2.
It is based on methods to rationalize SOS decompositions due to Parrilo and
Peyrl. The package features a data type for SOS decompositions, support for
external SDP solvers, and optimization over varieties."
Pablo Parrilo,Parrilo_Pablo,arXiv:1807.04428,https://arxiv.org/abs/1807.04428,"Abstract:  Semidefinite programming (SDP) with equality constraints arise in many
optimization and machine learning problems, such as Max-Cut, community
detection and robust PCA. Although SDPs can be solved to arbitrary precision in
polynomial time, generic convex solvers do not scale well with the dimension of
the problem. In order to address this issue, Burer and Monteiro
\cite{burer2003nonlinear} proposed to reduce the dimension of the problem by
appealing to a low-rank factorization, and solve the subsequent non-convex
problem instead. It is well-understood that the resulting non-convex problem
acts as a reliable surrogate to the original SDP, and can be efficiently solved
using the block-coordinate maximization method. Despite its simplicity,
remarkable success, and wide use in practice, the theoretical understanding of
the convergence of this method is limited. We prove that the block-coordinate
maximization algorithm applied to the non-convex Burer-Monteiro approach enjoys
a global sublinear rate without any assumptions on the problem, and a local
linear convergence rate despite no local maxima is locally strongly concave. We
illustrate our results through examples and numerical experiments."
Pablo Parrilo,Parrilo_Pablo,arXiv:1801.00070,https://arxiv.org/abs/1801.00070,"Abstract:  We show that existence of a global polynomial Lyapunov function for a
homogeneous polynomial vector field or a planar polynomial vector field (under
a mild condition) implies existence of a polynomial Lyapunov function that is a
sum of squares (sos) and that the negative of its derivative is also a sum of
squares. This result is extended to show that such sos-based certificates of
stability are guaranteed to exist for all stable switched linear systems. For
this class of systems, we further show that if the derivative inequality of the
Lyapunov function has an sos certificate, then the Lyapunov function itself is
automatically a sum of squares. These converse results establish cases where
semidefinite programming is guaranteed to succeed in finding proofs of Lyapunov
inequalities. Finally, we demonstrate some merits of replacing the sos
requirement on a polynomial Lyapunov function with an sos requirement on its
top homogeneous component. In particular, we show that this is a weaker
algebraic requirement in addition to being cheaper to impose computationally."
Pablo Parrilo,Parrilo_Pablo,arXiv:1710.04287,https://arxiv.org/abs/1710.04287,"Abstract:  In this paper we consider a parametric family of quadratically constrained
quadratic programs (QCQP) and their associated semidefinite programming (SDP)
relaxations. Given a value of the parameters at which the SDP relaxation is
exact, we study conditions (and quantitative bounds) under which the relaxation
will continue to be exact as the parameter moves in a neighborhood around it.
More generally, our results can be used to analyze SDP relaxations of
polynomial optimization problems. Our framework captures several estimation
problems such as low rank approximation, camera triangulation, rotation
synchronization and approximate matrix completion. The SDP relaxation correctly
solves these problems under noiseless observations, and our results guarantee
that the relaxation will continue to solve them in the low noise regime."
Pablo Parrilo,Parrilo_Pablo,arXiv:1705.00812,https://arxiv.org/abs/1705.00812,"Abstract:  The matrix logarithm, when applied to Hermitian positive definite matrices,
is concave with respect to the positive semidefinite order. This operator
concavity property leads to numerous concavity and convexity results for other
matrix functions, many of which are of importance in quantum information
theory. In this paper we show how to approximate the matrix logarithm with
functions that preserve operator concavity and can be described using the
feasible regions of semidefinite optimization problems of fairly small size.
Such approximations allow us to use off-the-shelf semidefinite optimization
solvers for convex optimization problems involving the matrix logarithm and
related functions, such as the quantum relative entropy. The basic ingredients
of our approach apply, beyond the matrix logarithm, to functions that are
operator concave and operator monotone. As such, we introduce strategies for
constructing semidefinite approximations that we expect will be useful, more
generally, for studying the approximation power of functions with small
semidefinite representations."
Pablo Parrilo,Parrilo_Pablo,arXiv:1608.08311,https://arxiv.org/abs/1608.08311,"Abstract:  We study stability criteria for discrete-time switched systems and provide a
meta-theorem that characterizes all Lyapunov theorems of a certain canonical
type. For this purpose, we investigate the structure of sets of LMIs that
provide a sufficient condition for stability. Various such conditions have been
proposed in the literature in the past fifteen years. We prove in this note
that a family of languagetheoretic conditions recently provided by the authors
encapsulates all the possible LMI conditions, thus putting a conclusion to this
research effort. As a corollary, we show that it is PSPACE-complete to
recognize whether a particular set of LMIs implies stability of a switched
system. Finally, we provide a geometric interpretation of these conditions, in
terms of existence of an invariant set."
Pablo Parrilo,Parrilo_Pablo,arXiv:1608.02090,https://arxiv.org/abs/1608.02090,"Abstract:  We propose a new method for simplifying semidefinite programs (SDP) inspired
by symmetry reduction. Specifically, we show if an orthogonal projection
satisfies certain invariance conditions, restricting to its range yields an
equivalent primal-dual pair over a lower-dimensional symmetric cone---namely,
the cone-of-squares of a Jordan subalgebra of symmetric matrices. We then give
a simple algorithm for minimizing the rank of this projection and hence the
dimension of this cone. Through the theory of Jordan algebras, the proposed
method easily extends to linear programming, second-order cone programming,
and, more generally, symmetric cone optimization."
Pablo Parrilo,Parrilo_Pablo,arXiv:1606.02253,https://arxiv.org/abs/1606.02253,"Abstract:  This is an experimental case study in real algebraic geometry, aimed at
computing the image of a semialgebraic subset of 3-space under a polynomial map
into the plane. For general instances, the boundary of the image is given by
two highly singular curves. We determine these curves and show how they
demarcate the ""flattened soccer ball"". We explore cylindrical algebraic
decompositions, by working through concrete examples. Maps onto convex polygons
and connections to convex optimization are also discussed."
Pablo Parrilo,Parrilo_Pablo,arXiv:1604.02618,https://arxiv.org/abs/1604.02618,"Abstract:  We introduce a novel representation of structured polynomial ideals, which we
refer to as chordal networks. The sparsity structure of a polynomial system is
often described by a graph that captures the interactions among the variables.
Chordal networks provide a computationally convenient decomposition into
simpler (triangular) polynomial sets, while preserving the underlying graphical
structure. We show that many interesting families of polynomial ideals admit
compact chordal network representations (of size linear in the number of
variables), even though the number of components is exponentially large.
Chordal networks can be computed for arbitrary polynomial systems using a
refinement of the chordal elimination algorithm from [Cifuentes-Parrilo-2016].
Furthermore, they can be effectively used to obtain several properties of the
variety, such as its dimension, cardinality, and equidimensional components, as
well as an efficient probabilistic test for radical ideal membership. We apply
our methods to examples from algebraic statistics and vector addition systems;
for these instances, algorithms based on chordal networks outperform existing
techniques by orders of magnitude."
Pablo Parrilo,Parrilo_Pablo,arXiv:1511.06751,https://arxiv.org/abs/1511.06751,"Abstract:  We study sum of squares (SOS) relaxations to optimize polynomial functions
over a set $V\cap R^n$, where $V$ is a complex algebraic variety. We propose a
new methodology that, rather than relying on some algebraic description,
represents $V$ with a generic set of complex samples. This approach depends
only on the geometry of $V$, avoiding representation issues such as
multiplicity and choice of generators. It also takes advantage of the
coordinate ring structure to reduce the size of the corresponding semidefinite
program (SDP). In addition, the input can be given as a straight-line program.
Our methods are particularly appealing for varieties that are easy to sample
from but for which the defining equations are complicated, such as $SO(n)$,
Grassmannians or rank $k$ tensors. For arbitrary varieties we can obtain the
required samples by using the tools of numerical algebraic geometry. In this
way we connect the areas of SOS optimization and numerical algebraic geometry."
Pablo Parrilo,Parrilo_Pablo,arXiv:1510.08562,https://arxiv.org/abs/1510.08562,"Abstract:  The incremental gradient method is a prominent algorithm for minimizing a
finite sum of smooth convex functions, used in many contexts including
large-scale data processing applications and distributed optimization over
networks. It is a first-order method that processes the functions one at a time
based on their gradient information. The incremental Newton method, on the
other hand, is a second-order variant which exploits additionally the curvature
information of the underlying functions and can therefore be faster. In this
paper, we focus on the case when the objective function is strongly convex and
present fast convergence results for the incremental gradient and incremental
Newton methods under the constant and diminishing stepsizes. For a decaying
stepsize rule $\alpha_k = \Theta(1/k^s)$ with $s \in (0,1]$, we show that the
distance of the IG iterates to the optimal solution converges at rate ${\cal
O}(1/k^{s})$ (which translates into ${\cal O}(1/k^{2s})$ rate in the
suboptimality of the objective value). For $s>1/2$, this improves the previous
${\cal O}(1/\sqrt{k})$ results in distances obtained for the case when
functions are non-smooth. We show that to achieve the fastest ${\cal O}(1/k)$
rate, incremental gradient needs a stepsize that requires tuning to the strong
convexity parameter whereas the incremental Newton method does not. The results
are based on viewing the incremental gradient method as a gradient descent
method with gradient errors, devising efficient upper bounds for the gradient
error to derive inequalities that relate distances of the consecutive iterates
to the optimal solution and finally applying Chung's lemmas from the stochastic
approximation literature to these inequalities to determine their asymptotic
behavior. In addition, we construct examples to show tightness of our rate
results."
Pablo Parrilo,Parrilo_Pablo,arXiv:1510.08560,https://arxiv.org/abs/1510.08560,"Abstract:  We analyze the convergence rate of the random reshuffling (RR) method, which
is a randomized first-order incremental algorithm for minimizing a finite sum
of convex component functions. RR proceeds in cycles, picking a uniformly
random order (permutation) and processing the component functions one at a time
according to this order, i.e., at each cycle, each component function is
sampled without replacement from the collection. Though RR has been numerically
observed to outperform its with-replacement counterpart stochastic gradient
descent (SGD), characterization of its convergence rate has been a long
standing open question. In this paper, we answer this question by showing that
when the component functions are quadratics or smooth and the sum function is
strongly convex, RR with iterate averaging and a diminishing stepsize
$\alpha_k=\Theta(1/k^s)$ for $s\in (1/2,1)$ converges at rate
$\Theta(1/k^{2s})$ with probability one in the suboptimality of the objective
value, thus improving upon the $\Omega(1/k)$ rate of SGD. Our analysis draws on
the theory of Polyak-Ruppert averaging and relies on decoupling the dependent
cycle gradient error into an independent term over cycles and another term
dominated by $\alpha_k^2$. This allows us to apply law of large numbers to an
appropriately weighted version of the cycle gradient errors, where the weights
depend on the stepsize. We also provide high probability convergence rate
estimates that shows decay rate of different terms and allows us to propose a
modification of RR with convergence rate ${\cal O}(\frac{1}{k^2})$."
Pablo Parrilo,Parrilo_Pablo,arXiv:1507.03046,https://arxiv.org/abs/1507.03046,"Abstract:  We present an efficient algorithm to compute permanents, mixed discriminants
and hyperdeterminants of structured matrices and multidimensional arrays
(tensors). We describe the sparsity structure of an array in terms of a graph,
and we assume that its treewidth, denoted as $\omega$, is small. Our algorithm
requires $O(n 2^\omega)$ arithmetic operations to compute permanents, and
$O(n^2 + n 3^\omega)$ for mixed discriminants and hyperdeterminants. We finally
show that mixed volume computation continues to be hard under bounded treewidth
assumptions."
Pablo Parrilo,Parrilo_Pablo,arXiv:1506.02081,https://arxiv.org/abs/1506.02081,"Abstract:  Motivated by applications to distributed optimization over networks and
large-scale data processing in machine learning, we analyze the deterministic
incremental aggregated gradient method for minimizing a finite sum of smooth
functions where the sum is strongly convex. This method processes the functions
one at a time in a deterministic order and incorporates a memory of previous
gradient values to accelerate convergence. Empirically it performs well in
practice; however, no theoretical analysis with explicit rate results was
previously given in the literature to our knowledge, in particular most of the
recent efforts concentrated on the randomized versions. In this paper, we show
that this deterministic algorithm has global linear convergence and
characterize the convergence rate. We also consider an aggregated method with
momentum and demonstrate its linear convergence. Our proofs rely on a careful
choice of a Lyapunov function that offers insight into the algorithm's behavior
and simplifies the proofs considerably."
Pablo Parrilo,Parrilo_Pablo,arXiv:1503.01207,https://arxiv.org/abs/1503.01207,"Abstract:  Let G be a finite abelian group. This paper is concerned with nonnegative
functions on G that are sparse with respect to the Fourier basis. We establish
combinatorial conditions on subsets S and T of Fourier basis elements under
which nonnegative functions with Fourier support S are sums of squares of
functions with Fourier support T. Our combinatorial condition involves
constructing a chordal cover of a graph related to G and S (the Cayley graph
Cay($\hat{G}$,S)) with maximal cliques related to T. Our result relies on two
main ingredients: the decomposition of sparse positive semidefinite matrices
with a chordal sparsity pattern, as well as a simple but key observation
exploiting the structure of the Fourier basis elements of G.
We apply our general result to two examples. First, in the case where $G =
\mathbb{Z}_2^n$, by constructing a particular chordal cover of the half-cube
graph, we prove that any nonnegative quadratic form in n binary variables is a
sum of squares of functions of degree at most $\lceil n/2 \rceil$, establishing
a conjecture of Laurent. Second, we consider nonnegative functions of degree d
on $\mathbb{Z}_N$ (when d divides N). By constructing a particular chordal
cover of the d'th power of the N-cycle, we prove that any such function is a
sum of squares of functions with at most $3d\log(N/d)$ nonzero Fourier
coefficients. Dually this shows that a certain cyclic polytope in
$\mathbb{R}^{2d}$ with N vertices can be expressed as a projection of a section
of the cone of psd matrices of size $3d\log(N/d)$. Putting $N=d^2$ gives a
family of polytopes $P_d \subset \mathbb{R}^{2d}$ with LP extension complexity
$\text{xc}_{LP}(P_d) = \Omega(d^2)$ and SDP extension complexity
$\text{xc}_{PSD}(P_d) = O(d\log(d))$. To the best of our knowledge, this is the
first explicit family of polytopes in increasing dimensions where
$\text{xc}_{PSD}(P_d) = o(\text{xc}_{LP}(P_d))$."
Pablo Parrilo,Parrilo_Pablo,arXiv:1411.7632,https://arxiv.org/abs/1411.7632,"Abstract:  Sequential rate-distortion (SRD) theory provides a framework for studying the
fundamental trade-off between data-rate and data-quality in real-time
communication systems. In this paper, we consider the SRD problem for
multi-dimensional time-varying Gauss-Markov processes under mean-square
distortion criteria. We first revisit the sensor-estimator separation
principle, which asserts that considered SRD problem is equivalent to a joint
sensor and estimator design problem in which data-rate of the sensor output is
minimized while the estimator's performance satisfies the distortion criteria.
We then show that the optimal joint design can be performed by semidefinite
programming. A semidefinite representation of the corresponding SRD function is
obtained. Implications of the obtained result in the context of zero-delay
source coding theory and applications to networked control theory are also
discussed."
Pablo Parrilo,Parrilo_Pablo,arXiv:1411.1745,https://arxiv.org/abs/1411.1745,"Abstract:  Chordal structure and bounded treewidth allow for efficient computation in
numerical linear algebra, graphical models, constraint satisfaction and many
other areas. In this paper, we begin the study of how to exploit chordal
structure in computational algebraic geometry, and in particular, for solving
polynomial systems. The structure of a system of polynomial equations can be
described in terms of a graph. By carefully exploiting the properties of this
graph (in particular, its chordal completions), more efficient algorithms can
be developed. To this end, we develop a new technique, which we refer to as
chordal elimination, that relies on elimination theory and Gröbner bases. By
maintaining graph structure throughout the process, chordal elimination can
outperform standard Gröbner basis algorithms in many cases. The reason is
that all computations are done on ""smaller"" rings, of size equal to the
treewidth of the graph. In particular, for a restricted class of ideals, the
computational complexity is linear in the number of variables. Chordal
structure arises in many relevant applications. We demonstrate the suitability
of our methods in examples from graph colorings, cryptography, sensor
localization and differential equations."
Pablo Parrilo,Parrilo_Pablo,arXiv:1410.5284,https://arxiv.org/abs/1410.5284,"Abstract:  Motivated by machine learning problems over large data sets and distributed
optimization over networks, we develop and analyze a new method called
incremental Newton method for minimizing the sum of a large number of strongly
convex functions. We show that our method is globally convergent for a variable
stepsize rule. We further show that under a gradient growth condition,
convergence rate is linear for both variable and constant stepsize rules. By
means of an example, we show that without the gradient growth condition,
incremental Newton method cannot achieve linear convergence. Our analysis can
be extended to study other incremental methods: in particular, we obtain a
linear convergence rate result for the incremental Gauss-Newton algorithm under
a variable stepsize rule."
Pablo Parrilo,Parrilo_Pablo,arXiv:1410.2841,https://arxiv.org/abs/1410.2841,"Abstract:  We consider the problem of jointly estimating the attitude and spin-rate of a
spinning spacecraft. Psiaki (J. Astronautical Sci., 57(1-2):73--92, 2009) has
formulated a family of optimization problems that generalize the classical
least-squares attitude estimation problem, known as Wahba's problem, to the
case of a spinning spacecraft. If the rotation axis is fixed and known, but the
spin-rate is unknown (such as for nutation-damped spin-stabilized spacecraft)
we show that Psiaki's problem can be reformulated exactly as a type of
tractable convex optimization problem called a semidefinite optimization
problem. This reformulation allows us to globally solve the problem using
standard numerical routines for semidefinite optimization. It also provides a
natural semidefinite relaxation-based approach to more complicated variations
on the problem."
Pablo Parrilo,Parrilo_Pablo,arXiv:1409.4379,https://arxiv.org/abs/1409.4379,"Abstract:  Given a polytope P in $\mathbb{R}^n$, we say that P has a positive
semidefinite lift (psd lift) of size d if one can express P as the linear
projection of an affine slice of the positive semidefinite cone
$\mathbf{S}^d_+$. If a polytope P has symmetry, we can consider equivariant psd
lifts, i.e. those psd lifts that respect the symmetry of P. One of the simplest
families of polytopes with interesting symmetries are regular polygons in the
plane, which have played an important role in the study of linear programming
lifts (or extended formulations). In this paper we study equivariant psd lifts
of regular polygons. We first show that the standard Lasserre/sum-of-squares
hierarchy for the regular N-gon requires exactly ceil(N/4) iterations and thus
yields an equivariant psd lift of size linear in N. In contrast we show that
one can construct an equivariant psd lift of the regular 2^n-gon of size 2n-1,
which is exponentially smaller than the psd lift of the sum-of-squares
hierarchy. Our construction relies on finding a sparse sum-of-squares
certificate for the facet-defining inequalities of the regular 2^n-gon, i.e.,
one that only uses a small (logarithmic) number of monomials. Since any
equivariant LP lift of the regular 2^n-gon must have size 2^n, this gives the
first example of a polytope with an exponential gap between sizes of
equivariant LP lifts and equivariant psd lifts. Finally we prove that our
construction is essentially optimal by showing that any equivariant psd lift of
the regular N-gon must have size at least logarithmic in N."
Pablo Parrilo,Parrilo_Pablo,arXiv:1408.4685,https://arxiv.org/abs/1408.4685,"Abstract:  We develop a practical semidefinite programming (SDP) facial reduction
procedure that utilizes computationally efficient approximations of the
positive semidefinite cone. The proposed method simplifies SDPs with no
strictly feasible solution (a frequent output of parsers) by solving a sequence
of easier optimization problems and could be a useful pre-processing technique
for SDP solvers. We demonstrate effectiveness of the method on SDPs arising in
practice, and describe our publicly-available software implementation. We also
show how to find maximum rank matrices in our PSD cone approximations (which
helps us find maximal simplifications), and we give a post-processing procedure
for dual solution recovery that generally applies to facial-reduction-based
pre-processing techniques. Finally, we show how approximations can be chosen to
preserve problem sparsity."
Pablo Parrilo,Parrilo_Pablo,arXiv:1407.4095,https://arxiv.org/abs/1407.4095,"Abstract:  Let M be a p-by-q matrix with nonnegative entries. The positive semidefinite
rank (psd rank) of M is the smallest integer k for which there exist positive
semidefinite matrices $A_i, B_j$ of size $k \times k$ such that $M_{ij} =
\text{trace}(A_i B_j)$. The psd rank has many appealing geometric
interpretations, including semidefinite representations of polyhedra and
information-theoretic applications. In this paper we develop and survey the
main mathematical properties of psd rank, including its geometry, relationships
with other rank notions, and computational and algorithmic aspects."
Pablo Parrilo,Parrilo_Pablo,arXiv:1404.3240,https://arxiv.org/abs/1404.3240,"Abstract:  The nonnegative rank of a matrix A is the smallest integer r such that A can
be written as the sum of r rank-one nonnegative matrices. The nonnegative rank
has received a lot of attention recently due to its application in
optimization, probability and communication complexity. In this paper we study
a class of atomic rank functions defined on a convex cone which generalize
several notions of ""positive"" ranks such as nonnegative rank or cp-rank (for
completely positive matrices). The main contribution of the paper is a new
method to obtain lower bounds for such ranks which improve on previously known
bounds. Additionally the bounds we propose can be computed by semidefinite
programming. The idea of the lower bound relies on an atomic norm approach
where the atoms are self-scaled according to the vector (or matrix, in the case
of nonnegative rank) of interest. This results in a lower bound that is
invariant under scaling and that is at least as good as other existing
norm-based bounds.
We mainly focus our attention on the two important cases of nonnegative rank
and cp-rank where our bounds satisfying interesting properties: For the
nonnegative rank we show that our lower bound can be interpreted as a
non-combinatorial version of the fractional rectangle cover number, while the
sum-of-squares relaxation is closely related to the Lovász theta number of
the rectangle graph of the matrix. We also prove that the lower bound inherits
many of the structural properties satisfied by the nonnegative rank such as
invariance under diagonal scaling, subadditivity, etc. We also apply our method
to obtain lower bounds on the cp-rank for completely positive matrices. In this
case we prove that our lower bound is always greater than or equal the plain
rank lower bound, and we show that it has interesting connections with
combinatorial lower bounds based on edge-clique cover number."
Pablo Parrilo,Parrilo_Pablo,arXiv:1403.4914,https://arxiv.org/abs/1403.4914,"Abstract:  We study the convex hull of $SO(n)$, thought of as the set of $n\times n$
orthogonal matrices with unit determinant, from the point of view of
semidefinite programming. We show that the convex hull of $SO(n)$ is doubly
spectrahedral, i.e. both it and its polar have a description as the
intersection of a cone of positive semidefinite matrices with an affine
subspace. Our spectrahedral representations are explicit, and are of minimum
size, in the sense that there are no smaller spectrahedral representations of
these convex bodies."
Pablo Parrilo,Parrilo_Pablo,arXiv:1403.4330,https://arxiv.org/abs/1403.4330,"Abstract:  Distributed control problems under some specific information constraints can
be formulated as (possibly infinite dimensional) convex optimization problems.
The underlying motivation of this work is to develop an understanding of the
optimal decision making architecture for such problems. In this paper, we
particularly focus on the N-player triangular LQG problems and show that the
optimal output feedback controllers have attractive state space realizations.
The optimal controller can be synthesized using a set of stabilizing solutions
to 2N linearly coupled algebraic Riccati equations, which turn out to be easily
solvable under reasonable assumptions."
Pablo Parrilo,Parrilo_Pablo,arXiv:1312.6662,https://arxiv.org/abs/1312.6662,"Abstract:  A central question in optimization is to maximize (or minimize) a linear
function over a given polytope P. To solve such a problem in practice one needs
a concise description of the polytope P. In this paper we are interested in
representations of P using the positive semidefinite cone: a positive
semidefinite lift (psd lift) of a polytope P is a representation of P as the
projection of an affine slice of the positive semidefinite cone
$\mathbf{S}^d_+$. Such a representation allows linear optimization problems
over P to be written as semidefinite programs of size d. Such representations
can be beneficial in practice when d is much smaller than the number of facets
of the polytope P. In this paper we are concerned with so-called equivariant
psd lifts (also known as symmetric psd lifts) which respect the symmetries of
the polytope P. We present a representation-theoretic framework to study
equivariant psd lifts of a certain class of symmetric polytopes known as
orbitopes. Our main result is a structure theorem where we show that any
equivariant psd lift of size d of an orbitope is of sum-of-squares type where
the functions in the sum-of-squares decomposition come from an invariant
subspace of dimension smaller than d^3. We use this framework to study two
well-known families of polytopes, namely the parity polytope and the cut
polytope, and we prove exponential lower bounds for equivariant psd lifts of
these polytopes."
Pablo Parrilo,Parrilo_Pablo,arXiv:1311.2571,https://arxiv.org/abs/1311.2571,"Abstract:  There has been a lot of interest recently in proving lower bounds on the size
of linear programs needed to represent a given polytope P. In a breakthrough
paper Fiorini et al. [Proceedings of 44th ACM Symposium on Theory of Computing
2012, pages 95-106] showed that any linear programming formulation of
maximum-cut must have exponential size. A natural question to ask is whether
one can prove such strong lower bounds for semidefinite programming
formulations. In this paper we take a step towards this goal and we prove
strong lower bounds for a certain class of SDP formulations, namely SDPs over
the Cartesian product of fixed-size positive semidefinite cones. In practice
this corresponds to semidefinite programs with a block-diagonal structure and
where blocks have constant size d. We show that any such extended formulation
of the cut polytope must have exponential size (when d is fixed). The result of
Fiorini et al. for LP formulations is obtained as a special case when d=1. For
blocks of size d=2 the result rules out any small formulations using
second-order cone programming. Our study of SDP lifts over Cartesian product of
fixed-size positive semidefinite cones is motivated mainly from practical
considerations where it is well known that such SDPs can be solved more
efficiently than general SDPs. The proof of our lower bound relies on new
results about the sparsity pattern of certain matrices with small psd rank,
combined with an induction argument inspired from the recent paper by Kaibel
and Weltge [arXiv:1307.3543] on the LP extension complexity of the correlation
polytope."
Pablo Parrilo,Parrilo_Pablo,arXiv:1310.4716,https://arxiv.org/abs/1310.4716,"Abstract:  SOSTOOLS v3.00 is the latest release of the freely available MATLAB toolbox
for formulating and solving sum of squares (SOS) optimization problems. Such
problems arise naturally in the analysis and control of nonlinear dynamical
systems, but also in other areas such as combinatorial optimization. Highlights
of the new release include the ability to create polynomial matrices and
formulate polynomial matrix inequalities, compatibility with MuPAD, the new
MATLAB symbolic engine, as well as the multipoly toolbox v2.01. SOSTOOLS v3.00
can interface with five semidefinite programming solvers, and includes ten
demonstration examples."
Pablo Parrilo,Parrilo_Pablo,arXiv:1308.6833,https://arxiv.org/abs/1308.6833,"Abstract:  We consider polynomial differential equations and make a number of
contributions to the questions of (i) complexity of deciding stability, (ii)
existence of polynomial Lyapunov functions, and (iii) existence of sum of
squares (sos) Lyapunov functions.
(i) We show that deciding local or global asymptotic stability of cubic
vector fields is strongly NP-hard. Simple variations of our proof are shown to
imply strong NP-hardness of several other decision problems: testing local
attractivity of an equilibrium point, stability of an equilibrium point in the
sense of Lyapunov, invariance of the unit ball, boundedness of trajectories,
convergence of all trajectories in a ball to a given equilibrium point,
existence of a quadratic Lyapunov function, local collision avoidance, and
existence of a stabilizing control law.
(ii) We present a simple, explicit example of a globally asymptotically
stable quadratic vector field on the plane which does not admit a polynomial
Lyapunov function (joint work with M. Krstic). For the subclass of homogeneous
vector fields, we conjecture that asymptotic stability implies existence of a
polynomial Lyapunov function, but show that the minimum degree of such a
Lyapunov function can be arbitrarily large even for vector fields in fixed
dimension and degree. For the same class of vector fields, we further establish
that there is no monotonicity in the degree of polynomial Lyapunov functions.
(iii) We show via an explicit counterexample that if the degree of the
polynomial Lyapunov function is fixed, then sos programming may fail to find a
valid Lyapunov function even though one exists. On the other hand, if the
degree is allowed to increase, we prove that existence of a polynomial Lyapunov
function for a planar or a homogeneous vector field implies existence of a
polynomial Lyapunov function that is sos and that the negative of its
derivative is also sos."
Pablo Parrilo,Parrilo_Pablo,arXiv:1308.2162,https://arxiv.org/abs/1308.2162,"Abstract:  In this paper we show how to construct inner and outer convex approximations
of a polytope from an approximate cone factorization of its slack matrix. This
provides a robust generalization of the famous result of Yannakakis that
polyhedral lifts of a polytope are controlled by (exact) nonnegative
factorizations of its slack matrix. Our approximations behave well under
polarity and have efficient representations using second order cones. We
establish a direct relationship between the quality of the factorization and
the quality of the approximations, and our results extend to generalized slack
matrices that arise from a polytope contained in a polyhedron."
Pablo Parrilo,Parrilo_Pablo,arXiv:1307.3586,https://arxiv.org/abs/1307.3586,"Abstract:  We introduce the notion of exchangeable equilibria of a symmetric bimatrix
game, defined as those correlated equilibria in which players' strategy choices
are conditionally independently and identically distributed given some hidden
variable. We give several game-theoretic interpretations and a version of the
""revelation principle"". Geometrically, the set of exchangeable equilibria is
convex and lies between the symmetric Nash equilibria and the symmetric
correlated equilibria. Exchangeable equilibria can achieve higher expected
utility than symmetric Nash equilibria."
Pablo Parrilo,Parrilo_Pablo,arXiv:1301.1327,https://arxiv.org/abs/1301.1327,"Abstract:  Model-based compressed sensing refers to compressed sensing with extra
structure about the underlying sparse signal known a priori. Recent work has
demonstrated that both for deterministic and probabilistic models imposed on
the signal, this extra information can be successfully exploited to enhance
recovery performance. In particular, weighted $\ell_1$-minimization with
suitable choice of weights has been shown to improve performance in the so
called non-uniform sparse model of signals. In this paper, we consider a full
generalization of the non-uniform sparse model with very mild assumptions. We
prove that when the measurements are obtained using a matrix with i.i.d
Gaussian entries, weighted $\ell_1$-minimization successfully recovers the
sparse signal from its measurements with overwhelming probability. We also
provide a method to choose these weights for any general signal model from the
non-uniform sparse class of signal models."
Pablo Parrilo,Parrilo_Pablo,arXiv:1211.0835,https://arxiv.org/abs/1211.0835,"Abstract:  Rejoinder to ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290]."
Pablo Parrilo,Parrilo_Pablo,arXiv:1210.6970,https://arxiv.org/abs/1210.6970,"Abstract:  The nonnegative rank of an entrywise nonnegative matrix A of size mxn is the
smallest integer r such that A can be written as A=UV where U is mxr and V is
rxn and U and V are both nonnegative. The nonnegative rank arises in different
areas such as combinatorial optimization and communication complexity.
Computing this quantity is NP-hard in general and it is thus important to find
efficient bounding techniques especially in the context of the aforementioned
applications. In this paper we propose a new lower bound on the nonnegative
rank which, unlike most existing lower bounds, does not explicitly rely on the
matrix sparsity pattern and applies to nonnegative matrices with arbitrary
support. The idea involves computing a certain nuclear norm with nonnegativity
constraints which allows to lower bound the nonnegative rank, in the same way
the standard nuclear norm gives lower bounds on the standard rank. Our lower
bound is expressed as the solution of a copositive programming problem and can
be relaxed to obtain polynomial-time computable lower bounds using semidefinite
programming. We compare our lower bound with existing ones, and we show
examples of matrices where our lower bound performs better than currently known
ones."
Pablo Parrilo,Parrilo_Pablo,arXiv:1208.1443,https://arxiv.org/abs/1208.1443,"Abstract:  We give explicit polynomial-sized (in $n$ and $k$) semidefinite
representations of the hyperbolicity cones associated with the elementary
symmetric polynomials of degree $k$ in $n$ variables. These convex cones form a
family of non-polyhedral outer approximations of the non-negative orthant that
preserve low-dimensional faces while successively discarding high-dimensional
faces. More generally we construct explicit semidefinite representations
(polynomial-sized in $k,m$, and $n$) of the hyperbolicity cones associated with
$k$th directional derivatives of polynomials of the form $p(x) =
\det(\sum_{i=1}^{n}A_i x_i)$ where the $A_i$ are $m\times m$ symmetric
matrices. These convex cones form an analogous family of outer approximations
to any spectrahedral cone. Our representations allow us to use semidefinite
programming to solve the linear cone programs associated with these convex
cones as well as their (less well understood) dual cones."
Pablo Parrilo,Parrilo_Pablo,arXiv:1204.1220,https://arxiv.org/abs/1204.1220,"Abstract:  In this paper we establish links between, and new results for, three problems
that are not usually considered together. The first is a matrix decomposition
problem that arises in areas such as statistical modeling and signal
processing: given a matrix $X$ formed as the sum of an unknown diagonal matrix
and an unknown low rank positive semidefinite matrix, decompose $X$ into these
constituents. The second problem we consider is to determine the facial
structure of the set of correlation matrices, a convex set also known as the
elliptope. This convex body, and particularly its facial structure, plays a
role in applications from combinatorial optimization to mathematical finance.
The third problem is a basic geometric question: given points
$v_1,v_2,...,v_n\in \R^k$ (where $n > k$) determine whether there is a centered
ellipsoid passing \emph{exactly} through all of the points.
We show that in a precise sense these three problems are equivalent.
Furthermore we establish a simple sufficient condition on a subspace $U$ that
ensures any positive semidefinite matrix $L$ with column space $U$ can be
recovered from $D+L$ for any diagonal matrix $D$ using a convex
optimization-based heuristic known as minimum trace factor analysis. This
result leads to a new understanding of the structure of rank-deficient
correlation matrices and a simple condition on a set of points that ensures
there is a centered ellipsoid passing through them."
Pablo Parrilo,Parrilo_Pablo,arXiv:1201.3227,https://arxiv.org/abs/1201.3227,"Abstract:  We study stability criteria for discrete time switching systems. We
investigate the structure of sets of LMIs that are a sufficient condition for
stability (i.e., such that any switching system which satisfies these LMIs is
stable). We provide an exact characterization of these sets. As a corollary, we
show that it is PSPACE-complete to recognize whether a particular set of LMIs
implies the stability of a switching system."
Pablo Parrilo,Parrilo_Pablo,arXiv:1111.7221,https://arxiv.org/abs/1111.7221,"Abstract:  We propose a novel and natural architecture for decentralized control that is
applicable whenever the underlying system has the structure of a partially
ordered set (poset). This controller architecture is based on the concept of
Moebius inversion for posets, and enjoys simple and appealing separation
properties, since the closed-loop dynamics can be analyzed in terms of
decoupled subsystems. The controller structure provides rich and interesting
connections between concepts from order theory such as Moebius inversion and
control-theoretic concepts such as state prediction, correction, and
separability. In addition, using our earlier results on H_2-optimal
decentralized control for arbitrary posets, we prove that the H_2-optimal
controller in fact possesses the proposed structure, thereby establishing the
optimality of the new controller architecture."
Pablo Parrilo,Parrilo_Pablo,arXiv:1111.4587,https://arxiv.org/abs/1111.4587,"Abstract:  Our first contribution in this paper is to prove that three natural sum of
squares (sos) based sufficient conditions for convexity of polynomials, via the
definition of convexity, its first order characterization, and its second order
characterization, are equivalent. These three equivalent algebraic conditions,
henceforth referred to as sos-convexity, can be checked by semidefinite
programming whereas deciding convexity is NP-hard. If we denote the set of
convex and sos-convex polynomials in $n$ variables of degree $d$ with
$\tilde{C}_{n,d}$ and $\tilde{\Sigma C}_{n,d}$ respectively, then our main
contribution is to prove that $\tilde{C}_{n,d}=\tilde{\Sigma C}_{n,d}$ if and
only if $n=1$ or $d=2$ or $(n,d)=(2,4)$. We also present a complete
characterization for forms (homogeneous polynomials) except for the case
$(n,d)=(3,4)$ which is joint work with G. Blekherman and is to be published
elsewhere. Our result states that the set $C_{n,d}$ of convex forms in $n$
variables of degree $d$ equals the set $\Sigma C_{n,d}$ of sos-convex forms if
and only if $n=2$ or $d=2$ or $(n,d)=(3,4)$. To prove these results, we present
in particular explicit examples of polynomials in
$\tilde{C}_{2,6}\setminus\tilde{\Sigma C}_{2,6}$ and
$\tilde{C}_{3,4}\setminus\tilde{\Sigma C}_{3,4}$ and forms in
$C_{3,6}\setminus\Sigma C_{3,6}$ and $C_{4,4}\setminus\Sigma C_{4,4}$, and a
general procedure for constructing forms in $C_{n,d+2}\setminus\Sigma
C_{n,d+2}$ from nonnegative but not sos forms in $n$ variables and degree $d$.
Although for disparate reasons, the remarkable outcome is that convex
polynomials (resp. forms) are sos-convex exactly in cases where nonnegative
polynomials (resp. forms) are sums of squares, as characterized by Hilbert."
Pablo Parrilo,Parrilo_Pablo,arXiv:1111.3427,https://arxiv.org/abs/1111.3427,"Abstract:  We introduce the framework of path-complete graph Lyapunov functions for
approximation of the joint spectral radius. The approach is based on the
analysis of the underlying switched system via inequalities imposed among
multiple Lyapunov functions associated to a labeled directed graph. Inspired by
concepts in automata theory and symbolic dynamics, we define a class of graphs
called path-complete graphs, and show that any such graph gives rise to a
method for proving stability of the switched system. This enables us to derive
several asymptotically tight hierarchies of semidefinite programming
relaxations that unify and generalize many existing techniques such as common
quadratic, common sum of squares, and maximum/minimum-of-quadratics Lyapunov
functions. We compare the quality of approximation obtained by certain classes
of path-complete graphs including a family of dual graphs and all path-complete
graphs with two nodes on an alphabet of two matrices. We provide approximation
guarantees for several families of path-complete graphs, such as the De Bruijn
graphs, establishing as a byproduct a constructive converse Lyapunov theorem
for maximum/minimum-of-quadratics Lyapunov functions."
Pablo Parrilo,Parrilo_Pablo,arXiv:1111.3164,https://arxiv.org/abs/1111.3164,"Abstract:  In this paper we address the basic geometric question of when a given convex
set is the image under a linear map of an affine slice of a given closed convex
cone. Such a representation or 'lift' of the convex set is especially useful if
the cone admits an efficient algorithm for linear optimization over its affine
slices. We show that the existence of a lift of a convex set to a cone is
equivalent to the existence of a factorization of an operator associated to the
set and its polar via elements in the cone and its dual. This generalizes a
theorem of Yannakakis that established a connection between polyhedral lifts of
a polytope and nonnegative factorizations of its slack matrix. Symmetric lifts
of convex sets can also be characterized similarly. When the cones live in a
family, our results lead to the definition of the rank of a convex set with
respect to this family. We present results about this rank in the context of
cones of positive semidefinite matrices. Our methods provide new tools for
understanding cone lifts of convex sets."
Pablo Parrilo,Parrilo_Pablo,arXiv:1111.1498,https://arxiv.org/abs/1111.1498,"Abstract:  We develop a complete state-space solution to H_2-optimal decentralized
control of poset-causal systems with state-feedback. Our solution is based on
the exploitation of a key separability property of the problem, that enables an
efficient computation of the optimal controller by solving a small number of
uncoupled standard Riccati equations. Our approach gives important insight into
the structure of optimal controllers, such as controller degree bounds that
depend on the structure of the poset. A novel element in our state-space
characterization of the controller is a remarkable pair of transfer functions,
that belong to the incidence algebra of the poset, are inverses of each other,
and are intimately related to prediction of the state along the different paths
on the poset. The results are illustrated by a numerical example."
Pablo Parrilo,Parrilo_Pablo,arXiv:1107.4386,https://arxiv.org/abs/1107.4386,"Abstract:  Except for special classes of games, there is no systematic framework for
analyzing the dynamical properties of multi-agent strategic interactions.
Potential games are one such special but restrictive class of games that allow
for tractable dynamic analysis. Intuitively, games that are ""close"" to a
potential game should share similar properties. In this paper, we formalize and
develop this idea by quantifying to what extent the dynamic features of
potential games extend to ""near-potential"" games. We study convergence of three
commonly studied classes of adaptive dynamics: discrete-time better/best
response, logit response, and discrete-time fictitious play dynamics. For
better/best response dynamics, we focus on the evolution of the sequence of
pure strategy profiles and show that this sequence converges to a (pure)
approximate equilibrium set, whose size is a function of the ""distance"" from a
close potential game. We then study logit response dynamics and provide a
characterization of the stationary distribution of this update rule in terms of
the distance of the game from a close potential game and the corresponding
potential function. We further show that the stochastically stable strategy
profiles are pure approximate equilibria. Finally, we turn attention to
fictitious play, and establish that the sequence of empirical frequencies of
player actions converges to a neighborhood of (mixed) equilibria of the game,
where the size of the neighborhood increases with distance of the game to a
potential game. Thus, our results suggest that games that are close to a
potential game inherit the dynamical properties of potential games. Since a
close potential game to a given game can be found by solving a convex
optimization problem, our approach also provides a systematic framework for
studying convergence behavior of adaptive learning dynamics in arbitrary finite
strategic form games."
Pablo Parrilo,Parrilo_Pablo,arXiv:1012.1908,https://arxiv.org/abs/1012.1908,"Abstract:  We show that unless P=NP, there exists no polynomial time (or even
pseudo-polynomial time) algorithm that can decide whether a multivariate
polynomial of degree four (or higher even degree) is globally convex. This
solves a problem that has been open since 1992 when N. Z. Shor asked for the
complexity of deciding convexity for quartic polynomials. We also prove that
deciding strict convexity, strong convexity, quasiconvexity, and
pseudoconvexity of polynomials of even degree four or higher is strongly
NP-hard. By contrast, we show that quasiconvexity and pseudoconvexity of odd
degree polynomials can be decided in polynomial time."
Pablo Parrilo,Parrilo_Pablo,arXiv:1012.0623,https://arxiv.org/abs/1012.0623,"Abstract:  The structural properties of graphs are usually characterized in terms of
invariants, which are functions of graphs that do not depend on the labeling of
the nodes. In this paper we study convex graph invariants, which are graph
invariants that are convex functions of the adjacency matrix of a graph. Some
examples include functions of a graph such as the maximum degree, the MAXCUT
value (and its semidefinite relaxation), and spectral invariants such as the
sum of the $k$ largest eigenvalues. Such functions can be used to construct
convex sets that impose various structural constraints on graphs, and thus
provide a unified framework for solving a number of interesting graph problems
via convex optimization. We give a representation of all convex graph
invariants in terms of certain elementary invariants, and describe methods to
compute or approximate convex graph invariants tractably. We also compare
convex and non-convex invariants, and discuss connections to robust
optimization. Finally we use convex graph invariants to provide efficient
convex programming solutions to graph problems such as the deconvolution of the
composition of two graphs into the individual components, hypothesis testing
between graph families, and the generation of graphs with certain desired
structural properties."
Pablo Parrilo,Parrilo_Pablo,arXiv:1012.0621,https://arxiv.org/abs/1012.0621,"Abstract:  In applications throughout science and engineering one is often faced with
the challenge of solving an ill-posed inverse problem, where the number of
available measurements is smaller than the dimension of the model to be
estimated. However in many practical situations of interest, models are
constrained structurally so that they only have a few degrees of freedom
relative to their ambient dimension. This paper provides a general framework to
convert notions of simplicity into convex penalty functions, resulting in
convex optimization solutions to linear, underdetermined inverse problems. The
class of simple models considered are those formed as the sum of a few atoms
from some (possibly infinite) elementary atomic set; examples include
well-studied cases such as sparse vectors and low-rank matrices, as well as
several others including sums of a few permutations matrices, low-rank tensors,
orthogonal matrices, and atomic measures. The convex programming formulation is
based on minimizing the norm induced by the convex hull of the atomic set; this
norm is referred to as the atomic norm. The facial structure of the atomic norm
ball carries a number of favorable properties that are useful for recovering
simple models, and an analysis of the underlying convex geometry provides sharp
estimates of the number of generic measurements required for exact and robust
recovery of models from partial information. These estimates are based on
computing the Gaussian widths of tangent cones to the atomic norm ball. When
the atomic set has algebraic structure the resulting optimization problems can
be solved or approximated via semidefinite programming. The quality of these
approximations affects the number of measurements required for recovery. Thus
this work extends the catalog of simple models that can be recovered from
limited linear information via tractable convex programming."
Pablo Parrilo,Parrilo_Pablo,arXiv:1010.2871,https://arxiv.org/abs/1010.2871,"Abstract:  The authors have decided to withdraw this submission.
Clarifications/corrections, if any, may follow at a later date."
Pablo Parrilo,Parrilo_Pablo,arXiv:1008.1290,https://arxiv.org/abs/1008.1290,"Abstract:  Suppose we observe samples of a subset of a collection of random variables.
No additional information is provided about the number of latent variables, nor
of the relationship between the latent and observed variables. Is it possible
to discover the number of latent components, and to learn a statistical model
over the entire collection of variables? We address this question in the
setting in which the latent and observed variables are jointly Gaussian, with
the conditional statistics of the observed variables conditioned on the latent
variables being specified by a graphical model. As a first step we give natural
conditions under which such latent-variable Gaussian graphical models are
identifiable given marginal statistics of only the observed variables.
Essentially these conditions require that the conditional graphical model among
the observed variables is sparse, while the effect of the latent variables is
""spread out"" over most of the observed variables. Next we propose a tractable
convex program based on regularized maximum-likelihood for model selection in
this latent-variable setting; the regularizer uses both the $\ell_1$ norm and
the nuclear norm. Our modeling framework can be viewed as a combination of
dimensionality reduction (to identify latent variables) and graphical modeling
(to capture remaining statistical structure not attributable to the latent
variables), and it consistently estimates both the number of latent components
and the conditional graphical model structure among the observed variables.
These results are applicable in the high-dimensional setting in which the
number of latent/observed variables grows with the number of samples of the
observed variables. The geometric properties of the algebraic varieties of
sparse matrices and of low-rank matrices play an important role in our
analysis."
Pablo Parrilo,Parrilo_Pablo,arXiv:1005.3045,https://arxiv.org/abs/1005.3045,"Abstract:  This document consists of two parts: the second part was submitted earlier as
a new proof of Nash's theorem, and the first part is a note explaining a
problem found in that proof. We are indebted to Sergiu Hart and Eran Shmaya for
their careful study which led to their simultaneous discovery of this error. So
far the error has not been fixed, but many of the results and techniques of the
paper remain valid, so we will continue to make it available online.
Abstract for the original paper:
We give a novel proof of the existence of Nash equilibria in all finite games
without using fixed point theorems or path following arguments. Our approach
relies on a new notion intermediate between Nash and correlated equilibria
called exchangeable equilibria, which are correlated equilibria with certain
symmetry and factorization properties. We prove these exist by a duality
argument, using Hart and Schmeidler's proof of correlated equilibrium existence
as a first step.
In an appropriate limit exchangeable equilibria converge to the convex hull
of Nash equilibria, proving that these exist as well. Exchangeable equilibria
are defined in terms of symmetries of the game, so this method automatically
proves the stronger statement that a symmetric game has a symmetric Nash
equilibrium. The case without symmetries follows by a symmetrization argument."
Pablo Parrilo,Parrilo_Pablo,arXiv:1005.2405,https://arxiv.org/abs/1005.2405,"Abstract:  In this paper we introduce a novel flow representation for finite games in
strategic form. This representation allows us to develop a canonical direct sum
decomposition of an arbitrary game into three components, which we refer to as
the potential, harmonic and nonstrategic components. We analyze natural classes
of games that are induced by this decomposition, and in particular, focus on
games with no harmonic component and games with no potential component. We show
that the first class corresponds to the well-known potential games. We refer to
the second class of games as harmonic games, and study the structural and
equilibrium properties of this new class of games. Intuitively, the potential
component of a game captures interactions that can equivalently be represented
as a common interest game, while the harmonic part represents the conflicts
between the interests of the players. We make this intuition precise, by
studying the properties of these two classes, and show that indeed they have
quite distinct and remarkable characteristics. For instance, while finite
potential games always have pure Nash equilibria, harmonic games generically
never do. Moreover, we show that the nonstrategic component does not affect the
equilibria of a game, but plays a fundamental role in their efficiency
properties, thus decoupling the location of equilibria and their payoff-related
properties. Exploiting the properties of the decomposition framework, we obtain
explicit expressions for the projections of games onto the subspaces of
potential and harmonic games. This enables an extension of the properties of
potential and harmonic games to ""nearby"" games. We exemplify this point by
showing that the set of approximate equilibria of an arbitrary game can be
characterized through the equilibria of its projection onto the set of
potential games."
Pablo Parrilo,Parrilo_Pablo,arXiv:1002.0035,https://arxiv.org/abs/1002.0035,"Abstract:  We exhibit the rich structure of the set of correlated equilibria by
analyzing the simplest of polynomial games: the mixed extension of matching
pennies. We show that while the correlated equilibrium set is convex and
compact, the structure of its extreme points can be quite complicated. In
finite games the ratio of extreme correlated to extreme Nash equilibria can be
greater than exponential in the size of the strategy spaces. In polynomial
games there can exist extreme correlated equilibria which are not finitely
supported; we construct a large family of examples using techniques from
ergodic theory. We show that in general the set of correlated equilibrium
distributions of a polynomial game cannot be described by conditions on
finitely many moments (means, covariances, etc.), in marked contrast to the set
of Nash equilibria which is always expressible in terms of finitely many
moments."
Pablo Parrilo,Parrilo_Pablo,arXiv:0909.0808,https://arxiv.org/abs/0909.0808,"Abstract:  The purpose of this note is to survey a methodology to solve systems of
polynomial equations and inequalities. The techniques we discuss use the
algebra of multivariate polynomials with coefficients over a field to create
large-scale linear algebra or semidefinite programming relaxations of many
kinds of feasibility or optimization questions. We are particularly interested
in problems arising in combinatorial optimization."
Pablo Parrilo,Parrilo_Pablo,arXiv:0907.4518,https://arxiv.org/abs/0907.4518,"Abstract:  The theta bodies of a polynomial ideal are a series of semidefinite
programming relaxations of the convex hull of the real variety of the ideal. In
this paper we construct the theta bodies of the vanishing ideal of cycles in a
binary matroid. Applied to cuts in graphs, this yields a new hierarchy of
semidefinite programming relaxations of the cut polytope of the graph. If the
binary matroid avoids certain minors we can characterize when the first theta
body in the hierarchy equals the cycle polytope of the matroid. Specialized to
cuts in graphs, this result solves a problem posed by Lovász."
Pablo Parrilo,Parrilo_Pablo,arXiv:0907.2267,https://arxiv.org/abs/0907.2267,"Abstract:  In this paper, we consider the optimal design of photonic crystal band
structures for two-dimensional square lattices. The mathematical formulation of
the band gap optimization problem leads to an infinite-dimensional Hermitian
eigenvalue optimization problem parametrized by the dielectric material and the
wave vector. To make the problem tractable, the original eigenvalue problem is
discretized using the finite element method into a series of finite-dimensional
eigenvalue problems for multiple values of the wave vector parameter. The
resulting optimization problem is large-scale and non-convex, with low
regularity and non-differentiable objective. By restricting to appropriate
eigenspaces, we reduce the large-scale non-convex optimization problem via
reparametrization to a sequence of small-scale convex semidefinite programs
(SDPs) for which modern SDP solvers can be efficiently applied. Numerical
results are presented for both transverse magnetic (TM) and transverse electric
(TE) polarizations at several frequency bands. The optimized structures exhibit
patterns which go far beyond typical physical intuition on periodic media
design."
Pablo Parrilo,Parrilo_Pablo,arXiv:0906.2220,https://arxiv.org/abs/0906.2220,"Abstract:  Suppose we are given a matrix that is formed by adding an unknown sparse
matrix to an unknown low-rank matrix. Our goal is to decompose the given matrix
into its sparse and low-rank components. Such a problem arises in a number of
applications in model and system identification, and is NP-hard in general. In
this paper we consider a convex optimization formulation to splitting the
specified matrix into its components, by minimizing a linear combination of the
$\ell_1$ norm and the nuclear norm of the components. We develop a notion of
\emph{rank-sparsity incoherence}, expressed as an uncertainty principle between
the sparsity pattern of a matrix and its row and column spaces, and use it to
characterize both fundamental identifiability as well as (deterministic)
sufficient conditions for exact recovery. Our analysis is geometric in nature,
with the tangent spaces to the algebraic varieties of sparse and low-rank
matrices playing a prominent role. When the sparse and low-rank matrices are
drawn from certain natural random ensembles, we show that the sufficient
conditions for exact recovery are satisfied with high probability. We conclude
with simulation results on synthetic matrix decomposition problems."
Pablo Parrilo,Parrilo_Pablo,arXiv:0904.3986,https://arxiv.org/abs/0904.3986,"Abstract:  In this paper, we show the optimality of a certain class of
disturbance-affine control policies in the context of one-dimensional,
constrained, multi-stage robust optimization. Our results cover the finite
horizon case, with minimax (worst-case) objective, and convex state costs plus
linear control costs. We develop a new proof methodology, which explores the
relationship between the geometrical properties of the feasible set of
solutions and the structure of the objective function. Apart from providing an
elegant and conceptually simple proof technique, the approach also entails very
fast algorithms for the case of piecewise affine state costs, which we explore
in connection with a classical inventory management application."
Pablo Parrilo,Parrilo_Pablo,arXiv:0903.1287,https://arxiv.org/abs/0903.1287,"Abstract:  A multivariate polynomial $p(x)=p(x_1,...,x_n)$ is sos-convex if its Hessian
$H(x)$ can be factored as $H(x)= M^T(x) M(x)$ with a possibly nonsquare
polynomial matrix $M(x)$. It is easy to see that sos-convexity is a sufficient
condition for convexity of $p(x)$. Moreover, the problem of deciding
sos-convexity of a polynomial can be cast as the feasibility of a semidefinite
program, which can be solved efficiently. Motivated by this computational
tractability, it has been recently speculated whether sos-convexity is also a
necessary condition for convexity of polynomials. In this paper, we give a
negative answer to this question by presenting an explicit example of a
trivariate homogeneous polynomial of degree eight that is convex but not
sos-convex. Interestingly, our example is found with software using sum of
squares programming techniques and the duality theory of semidefinite
optimization. As a byproduct of our numerical procedure, we obtain a simple
method for searching over a restricted family of nonnegative polynomials that
are not sums of squares."
Pablo Parrilo,Parrilo_Pablo,arXiv:0812.4279,https://arxiv.org/abs/0812.4279,"Abstract:  We present several new characterizations of correlated equilibria in games
with continuous utility functions. These have the advantage of being more
computationally and analytically tractable than the standard definition in
terms of departure functions. We use these characterizations to construct
effective algorithms for approximating a single correlated equilibrium or the
entire set of correlated equilibria of a game with polynomial utility
functions."
Pablo Parrilo,Parrilo_Pablo,arXiv:0809.3480,https://arxiv.org/abs/0809.3480,"Abstract:  Inspired by a question of Lovász, we introduce a hierarchy of nested
semidefinite relaxations of the convex hull of real solutions to an arbitrary
polynomial ideal, called theta bodies of the ideal. For the stable set problem
in a graph, the first theta body in this hierarchy is exactly Lovász's theta
body of the graph. We prove that theta bodies are, up to closure, a version of
Lasserre's relaxations for real solutions to ideals, and that they can be
computed explicitly using combinatorial moment matrices. Theta bodies provide a
new canonical set of semidefinite relaxations for the max cut problem. For
vanishing ideals of finite point sets, we give several equivalent
characterizations of when the first theta body equals the convex hull of the
points. We also determine the structure of the first theta body for all ideals."
Pablo Parrilo,Parrilo_Pablo,arXiv:0806.2469,https://arxiv.org/abs/0806.2469,"Abstract:  Stochastic games are an important class of problems that generalize Markov
decision processes to game theoretic scenarios. We consider finite state
two-player zero-sum stochastic games over an infinite time horizon with
discounted rewards. The players are assumed to have infinite strategy spaces
and the payoffs are assumed to be polynomials. In this paper we restrict our
attention to a special class of games for which the single-controller
assumption holds. It is shown that minimax equilibria and optimal strategies
for such games may be obtained via semidefinite programming."
Pablo Parrilo,Parrilo_Pablo,arXiv:0802.3922,https://arxiv.org/abs/0802.3922,"Abstract:  We present distributed algorithms that can be used by multiple agents to
align their estimates with a particular value over a network with time-varying
connectivity. Our framework is general in that this value can represent a
consensus value among multiple agents or an optimal solution of an optimization
problem, where the global objective function is a combination of local agent
objective functions. Our main focus is on constrained problems where the
estimate of each agent is restricted to lie in a different constraint set.
To highlight the effects of constraints, we first consider a constrained
consensus problem and present a distributed ``projected consensus algorithm''
in which agents combine their local averaging operation with projection on
their individual constraint sets. This algorithm can be viewed as a version of
an alternating projection method with weights that are varying over time and
across agents. We establish convergence and convergence rate results for the
projected consensus algorithm. We next study a constrained optimization problem
for optimizing the sum of local objective functions of the agents subject to
the intersection of their local constraint sets. We present a distributed
``projected subgradient algorithm'' which involves each agent performing a
local averaging operation, taking a subgradient step to minimize its own
objective function, and projecting on its constraint set. We show that, with an
appropriately selected stepsize rule, the agent estimates generated by this
algorithm converge to the same optimal solution for the cases when the weights
are constant and equal, and when the weights are time-varying but all agents
have the same constraint set."
Pablo Parrilo,Parrilo_Pablo,arXiv:0712.2887,https://arxiv.org/abs/0712.2887,"Abstract:  We provide an asymptotically tight, computationally efficient approximation
of the joint spectral radius of a set of matrices using sum of squares (SOS)
programming. The approach is based on a search for an SOS polynomial that
proves simultaneous contractibility of a finite set of matrices. We provide a
bound on the quality of the approximation that unifies several earlier results
and is independent of the number of matrices. Additionally, we present a
comparison between our approximation scheme and earlier techniques, including
the use of common quadratic Lyapunov functions and a method based on matrix
liftings. Theoretical results and numerical investigations show that our
approach yields tighter approximations."
Pablo Parrilo,Parrilo_Pablo,arXiv:0709.0955,https://arxiv.org/abs/0709.0955,"Abstract:  We show how to exploit symmetries of a graph to efficiently compute the
fastest mixing Markov chain on the graph (i.e., find the transition
probabilities on the edges to minimize the second-largest eigenvalue modulus of
the transition probability matrix). Exploiting symmetry can lead to significant
reduction in both the number of variables and the size of matrices in the
corresponding semidefinite program, thus enable numerical solution of
large-scale instances that are otherwise computationally infeasible. We obtain
analytic or semi-analytic results for particular classes of graphs, such as
edge-transitive and distance-transitive graphs. We describe two general
approaches for symmetry exploitation, based on orbit theory and
block-diagonalization, respectively. We also establish the connection between
these two approaches."
Pablo Parrilo,Parrilo_Pablo,arXiv:0707.3462,https://arxiv.org/abs/0707.3462,"Abstract:  In this paper, we study nonzero-sum separable games, which are continuous
games whose payoffs take a sum-of-products form. Included in this subclass are
all finite games and polynomial games. We investigate the structure of
equilibria in separable games. We show that these games admit finitely
supported Nash equilibria. Motivated by the bounds on the supports of mixed
equilibria in two-player finite games in terms of the ranks of the payoff
matrices, we define the notion of the rank of an n-player continuous game and
use this to provide bounds on the cardinality of the support of equilibrium
strategies. We present a general characterization theorem that states that a
continuous game has finite rank if and only if it is separable. Using our rank
results, we present an efficient algorithm for computing approximate equilibria
of two-player separable games with fixed strategy spaces in time polynomial in
the rank of the game."
Pablo Parrilo,Parrilo_Pablo,arXiv:0706.4138,https://arxiv.org/abs/0706.4138,"Abstract:  The affine rank minimization problem consists of finding a matrix of minimum
rank that satisfies a given system of linear equality constraints. Such
problems have appeared in the literature of a diverse set of fields including
system identification and control, Euclidean embedding, and collaborative
filtering. Although specific instances can often be solved with specialized
algorithms, the general affine rank minimization problem is NP-hard. In this
paper, we show that if a certain restricted isometry property holds for the
linear transformation defining the constraints, the minimum rank solution can
be recovered by solving a convex optimization problem, namely the minimization
of the nuclear norm over the given affine space. We present several random
ensembles of equations where the restricted isometry property holds with
overwhelming probability. The techniques used in our analysis have strong
parallels in the compressed sensing framework. We discuss how affine rank
minimization generalizes this pre-existing concept and outline a dictionary
relating concepts from cardinality minimization to those of rank minimization."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0702005,https://arxiv.org/abs/math/0702005,"Abstract:  The $k$-ellipse is the plane algebraic curve consisting of all points whose
sum of distances from $k$ given points is a fixed number. The polynomial
equation defining the $k$-ellipse has degree $2^k$ if $k$ is odd and degree
$2^k{-}\binom{k}{k/2}$ if $k$ is even. We express this polynomial equation as
the determinant of a symmetric matrix of linear polynomials. Our representation
extends to weighted $k$-ellipses and $k$-ellipsoids in arbitrary dimensions,
and it leads to new geometric applications of semidefinite programming."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0609532,https://arxiv.org/abs/math/0609532,"Abstract:  Let V(n) be the minimum number of monochromatic 3-term arithmetic
progressions in any 2-coloring of {1,2,...,n}. We show that (1675/32768) n^2
(1+o(1)) <= V(n) <= (117/2192) n^2(1+o(1)). As a consequence, we find that V(n)
is strictly greater than the corresponding number for Schur triples (which is
(1/22) n^2 (1+o(1)). Additionally, we disprove the conjecture that V(n) =
(1/16) n^2(1+o(1)), as well as a more general conjecture."
Pablo Parrilo,Parrilo_Pablo,arXiv:quant-ph/0608161,https://arxiv.org/abs/quant-ph/0608161,"Abstract:  One of the most basic computational problems is the task of finding a desired
item in an ordered list of N items. While the best classical algorithm for this
problem uses log_2 N queries to the list, a quantum computer can solve the
problem using a constant factor fewer queries. However, the precise value of
this constant is unknown. By characterizing a class of quantum query algorithms
for ordered search in terms of a semidefinite program, we find new quantum
algorithms for small instances of the ordered search problem. Extending these
algorithms to arbitrarily large instances using recursion, we show that there
is an exact quantum ordered search algorithm using 4 log_{605} N \approx 0.433
log_2 N queries, which improves upon the previously best known exact algorithm."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0603313,https://arxiv.org/abs/math/0603313,"Abstract:  Contraction analysis is a stability theory for nonlinear systems where
stability is defined incrementally between two arbitrary trajectories. It
provides an alternative framework in which to study uncertain interconnections
or systems with external inputs, where it offers several significant advantages
when compared with traditional Lyapunov analysis. Contraction-based methods are
particularly useful for analyzing systems with uncertain parameters and for
proving synchronization properties of nonlinear oscillators. Existence of a
contraction metric for a given system is a necessary and sufficient condition
for global exponential convergence of system trajectories. For systems with
polynomial or rational dynamics, the search for contraction metrics can be made
fully algorithmic through the use of convex optimization and sum of squares
(SOS) programming. The search process is made computationally tractable by
relaxing matrix definiteness constraints, whose feasibility indicate existence
of a contraction metric, into SOS constraints on polynomial matrices. We
illustrate the results through examples from the literature, emphasizing the
advantages and contrasting the differences between the contraction approach and
traditional Lyapunov techniques."
Pablo Parrilo,Parrilo_Pablo,arXiv:quant-ph/0407143,https://arxiv.org/abs/quant-ph/0407143,"Abstract:  We discuss the problem of determining whether the state of several quantum
mechanical subsystems is entangled. As in previous work on two subsystems we
introduce a procedure for checking separability that is based on finding state
extensions with appropriate properties and may be implemented as a semidefinite
program. The main result of this work is to show that there is a series of
tests of this kind such that if a multiparty state is entangled this will
eventually be detected by one of the tests. The procedure also provides a means
of constructing entanglement witnesses that could in principle be measured in
order to demonstrate that the state is entangled."
Pablo Parrilo,Parrilo_Pablo,arXiv:quant-ph/0308032,https://arxiv.org/abs/quant-ph/0308032,"Abstract:  We introduce a new family of separability criteria that are based on the
existence of extensions of a bipartite quantum state $\rho$ to a larger number
of parties satisfying certain symmetry properties. It can be easily shown that
all separable states have the required extensions, so the non-existence of such
an extension for a particular state implies that the state is entangled. One of
the main advantages of this approach is that searching for the extension can be
cast as a convex optimization problem known as a semidefinite program (SDP).
Whenever an extension does not exist, the dual optimization constructs an
explicit entanglement witness for the particular state. These separability
tests can be ordered in a hierarchical structure whose first step corresponds
to the well-known Positive Partial Transpose (Peres-Horodecki) criterion, and
each test in the hierarchy is at least as powerful as the preceding one. This
hierarchy is complete, in the sense that any entangled state is guaranteed to
fail a test at some finite point in the hierarchy, thus showing it is
entangled. The entanglement witnesses corresponding to each step of the
hierarchy have well-defined and very interesting algebraic properties that in
turn allow for a characterization of the interior of the set of positive maps.
Coupled with some recent results on the computational complexity of the
separability problem, which has been shown to be NP-hard, this hierarchy of
tests gives a complete and also computationally and theoretically appealing
characterization of mixed bipartite entangled states."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0304104,https://arxiv.org/abs/math/0304104,"Abstract:  In 1958 Lax conjectured that hyperbolic polynomials in three variables are
determinants of linear combinations of three symmetric matrices. This
conjecture is equivalent to a recent observation of Helton and Vinnikov."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0211450,https://arxiv.org/abs/math/0211450,"Abstract:  We investigate the representation of symmetric polynomials as a sum of
squares. Since this task is solved using semidefinite programming tools we
explore the geometric, algebraic, and computational implications of the
presence of discrete symmetries in semidefinite programs. It is shown that
symmetry exploitation allows a significant reduction in both matrix size and
number of decision variables. This result is applied to semidefinite programs
arising from the computation of sum of squares decompositions for multivariate
polynomials. The results, reinterpreted from an invariant-theoretic viewpoint,
provide a novel representation of a class of nonnegative symmetric polynomials.
The main theorem states that an invariant sum of squares polynomial is a sum of
inner products of pairs of matrices, whose entries are invariant polynomials.
In these pairs, one of the matrices is computed based on the real irreducible
representations of the group, and the other is a sum of squares matrix. The
reduction techniques enable the numerical solution of large-scale instances,
otherwise computationally infeasible to solve."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0205278,https://arxiv.org/abs/math/0205278,"Abstract:  A geometric inequality among three triangles, originating in circle packing
problems, is introduced. In order to prove it, we reduce the original
formulation to the nonnegativity of a particular polynomial in four real
indeterminates. Techniques based on sum of squares decompositions, semidefinite
programming, and symmetry reduction are then applied to provide an easily
verifiable nonnegativity certificate."
Pablo Parrilo,Parrilo_Pablo,arXiv:quant-ph/0112007,https://arxiv.org/abs/quant-ph/0112007,"Abstract:  We show how to design families of operational criteria that distinguish
entangled from separable quantum states. The simplest of these tests
corresponds to the well-known Peres-Horodecki positive partial transpose (PPT)
criterion, and the more complicated tests are strictly stronger. The new
criteria are tractable due to powerful computational and theoretical methods
for the class of convex optimization problems known as semidefinite programs.
We successfully applied the results to many low-dimensional states from the
literature where the PPT test fails. As a byproduct of the criteria, we provide
an explicit construction of the corresponding entanglement witnesses."
Pablo Parrilo,Parrilo_Pablo,arXiv:math/0103170,https://arxiv.org/abs/math/0103170,"Abstract:  We compare algorithms for global optimization of polynomial functions in many
variables. It is demonstrated that existing algebraic methods (Gröbner bases,
resultants, homotopy methods) are dramatically outperformed by a relaxation
technique, due to N.Z. Shor and the first author, which involves sums of
squares and semidefinite programming. This opens up the possibility of using
semidefinite programming relaxations arising from the Positivstellensatz for a
wide range of computational problems in real algebraic geometry.
This paper was presented at the Workshop on Algorithmic and Quantitative
Aspects of Real Algebraic Geometry in Mathematics and Computer Science, held at
DIMACS, Rutgers University, March 12-16, 2001."
David Perreault,Perreault_David,arXiv:1603.05289,https://arxiv.org/abs/1603.05289,"Abstract:  Ad hoc electrical networks are formed by connecting power sources and loads
without pre-determining the network topology. These systems are well-suited to
addressing the lack of electricity in rural areas because they can be assembled
and modified by non-expert users without central oversight. There are two core
aspects to ad hoc system design: 1) designing source and load units such that
the microgrid formed from the arbitrary interconnection of many units is always
stable and 2) developing control strategies to autonomously manage the
microgrid (i.e., perform power dispatch and voltage regulation) in a
decentralized manner and under large uncertainty. To address these challenges
we apply a number of nonlinear control techniques---including Brayton-Moser
potential theory and primal-dual dynamics---to obtain conditions under which an
ad hoc dc microgrid will have a suitable and asymptotically stable equilibrium
point. Further, we propose a new decentralized control scheme that coordinates
many sources to achieve a specified power dispatch from each. A simulated
comparison to previous research is included."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1901.09100,https://arxiv.org/abs/1901.09100,"Abstract:  We characterize the communication complexity of the following distributed
estimation problem. Alice and Bob observe infinitely many iid copies of
$\rho$-correlated unit-variance (Gaussian or $\pm1$ binary) random variables,
with unknown $\rho\in[-1,1]$. By interactively exchanging $k$ bits, Bob wants
to produce an estimate $\hat\rho$ of $\rho$. We show that the best possible
performance (optimized over interaction protocol $\Pi$ and estimator $\hat
\rho$) satisfies $\inf_{\Pi,\hat\rho}\sup_\rho \mathbb{E} [|\rho-\hat\rho|^2] =
\Theta(\tfrac{1}{k})$. Furthermore, we show that the best possible unbiased
estimator achieves performance of $1+o(1)\over {2k\ln 2}$. Curiously, thus,
restricting communication to $k$ bits results in (order-wise) similar minimax
estimation error as restricting to $k$ samples. Our results also imply an
$\Omega(n)$ lower bound on the information complexity of the Gap-Hamming
problem, for which we show a direct information-theoretic proof.
Notably, the protocol achieving (almost) optimal performance is one-way
(non-interactive). For one-way protocols we also prove the
$\Omega(\tfrac{1}{k})$ bound even when $\rho$ is restricted to any small open
sub-interval of $[-1,1]$ (i.e. a local minimax lower bound). %We do not know if
this local behavior remains true in the interactive setting. Our proof
techniques rely on symmetric strong data-processing inequalities, various
tensorization techniques from information-theoretic interactive
common-randomness extraction, and (for the local lower bound) on the
Otto-Villani estimate for the Wasserstein-continuity of trajectories of the
Ornstein-Uhlenbeck semigroup."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1901.06732,https://arxiv.org/abs/1901.06732,"Abstract:  Consider a (multiple-access) wireless communication system where users are
connected to a unique base station over a shared-spectrum radio links. Each
user has a fixed number $k$ of bits to send to the base station, and his signal
gets attenuated by a random channel gain (quasi-static fading). In this paper
we consider the many-user asymptotics of Chen-Chen-Guo'2017, where the number
of users grows linearly with the blocklength. In addition, we adopt a per-user
probability of error criterion of Polyanskiy'2017 (as opposed to classical
joint-error probability criterion). Under these two settings we derive bounds
on the optimal required energy-per-bit for reliable multi-access communication.
We confirm the curious behaviour (previously observed for non-fading MAC) of
the possibility of perfect multi-user interference cancellation for user
densities below a critical threshold. Further we demonstrate the suboptimality
of standard solutions such as orthogonalization (i.e., TDMA/FDMA) and treating
interference as noise (i.e. pseudo-random CDMA without multi-user detection)."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1812.03031,https://arxiv.org/abs/1812.03031,"Abstract:  Let $X$ and $Y$ be dependent random variables. This paper considers the
problem of designing a scalar quantizer for $Y$ to maximize the mutual
information between the quantizer's output and $X$, and develops fundamental
properties and bounds for this form of quantization, which is connected to the
log-loss distortion criterion. The main focus is the regime of low $I(X;Y)$,
where it is shown that, if $X$ is binary, a constant fraction of the mutual
information can always be preserved using $\mathcal{O}(\log(1/I(X;Y)))$
quantization levels, and there exist distributions for which this many
quantization levels are necessary. Furthermore, for larger finite alphabets $2
< |\mathcal{X}| < \infty$, it is established that an $\eta$-fraction of the
mutual information can be preserved using roughly $(\log(| \mathcal{X} |
/I(X;Y)))^{\eta\cdot(|\mathcal{X}| - 1)}$ quantization levels."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1811.03946,https://arxiv.org/abs/1811.03946,"Abstract:  We study a generalization of the well-known model of broadcasting on trees to
the case of directed acyclic graphs (DAGs). At time $0$, a source vertex $X$
sends out a uniform bit along binary symmetric channels to a set of vertices
called layer $1$. Each vertex except $X$ is assumed to have indegree $d$. At
time $k\geq1$, vertices at layer $k$ apply $d$-input Boolean processing
functions to their received bits and send out the results to vertices at layer
$k+1$. We say that broadcasting is possible if we can reconstruct $X$ with
probability of error bounded away from $1/2$ using knowledge of all vertices at
an arbitrarily deep layer $k$. This question is also related to models of
reliable computation and storage, and information flow in biological networks.
In this paper, we study randomly constructed DAGs, for which we show that
broadcasting is only possible if the noise level is below a certain (degree and
function dependent) critical threshold. For $d\geq3$, and random DAGs with
layer sizes $\Omega(\log k)$ and majority processing functions, we identify the
critical threshold. For $d=2$, we establish a similar result for NAND
processing functions. We also prove a partial converse for odd $d\geq3$
illustrating that the identified thresholds are impossible to improve by
selecting different processing functions if the decoder is restricted to using
a single vertex.
Finally, for any noise level, we construct explicit DAGs (using expander
graphs) with bounded degree and layer sizes $\Theta(\log k)$ admitting
reconstruction. In particular, we show that such DAGs can be generated in
deterministic quasi-polynomial time or randomized polylogarithmic time in the
depth. These results portray a doubly-exponential advantage for storing a bit
in bounded degree DAGs compared to trees, where $d=1$ but layer sizes need to
grow exponentially with depth in order for broadcasting to be possible."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1810.11589,https://arxiv.org/abs/1810.11589,"Abstract:  This paper studies the problem of estimating the differential entropy
$h(S+Z)$, where $S$ and $Z$ are independent $d$-dimensional random variables
with $Z\sim\mathcal{N}(0,\sigma^2 \mathrm{I}_d)$. The distribution of $S$ is
unknown, but $n$ independently and identically distributed (i.i.d) samples from
it are available. The question is whether having access to samples of $S$ as
opposed to samples of $S+Z$ can improve estimation performance. We show that
the answer is positive.
More concretely, we first show that despite the regularizing effect of noise,
the number of required samples still needs to scale exponentially in $d$. This
result is proven via a random-coding argument that reduces the question to
estimating the Shannon entropy on a $2^{O(d)}$-sized alphabet. Next, for a
fixed $d$ and $n$ large enough, it is shown that a simple plugin estimator,
given by the differential entropy of the empirical distribution from $S$
convolved with the Gaussian density, achieves the loss of $O\left((\log
n)^{d/4}/\sqrt{n}\right)$. Note that the plugin estimator amounts here to the
differential entropy of a $d$-dimensional Gaussian mixture, for which we
propose an efficient Monte Carlo computation algorithm. At the same time,
estimating $h(S+Z)$ via popular differential entropy estimators (based on
kernel density estimation (KDE) or k nearest neighbors (kNN) techniques)
applied to samples from $S+Z$ would only attain much slower rates of order
$O(n^{-1/d})$, despite the smoothness of $P_{S+Z}$.
As an application, which was in fact our original motivation for the problem,
we estimate information flows in deep neural networks and discuss Tishby's
Information Bottleneck and the compression conjecture, among others."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1810.05728,https://arxiv.org/abs/1810.05728,"Abstract:  We study the flow of information and the evolution of internal
representations during deep neural network (DNN) training, aiming to demystify
the compression aspect of the information bottleneck theory. The theory
suggests that DNN training comprises a rapid fitting phase followed by a slower
compression phase, in which the mutual information $I(X;T)$ between the input
$X$ and internal representations $T$ decreases. Several papers observe
compression of estimated mutual information on different DNN models, but the
true $I(X;T)$ over these networks is provably either constant (discrete $X$) or
infinite (continuous $X$). This work explains the discrepancy between theory
and experiments, and clarifies what was actually measured by these past works.
To this end, we introduce an auxiliary (noisy) DNN framework for which $I(X;T)$
is a meaningful quantity that depends on the network's parameters. This noisy
framework is shown to be a good proxy for the original (deterministic) DNN both
in terms of performance and the learned representations. We then develop a
rigorous estimator for $I(X;T)$ in noisy DNNs and observe compression in
various models. By relating $I(X;T)$ in the noisy DNN to an
information-theoretic communication problem, we show that compression is driven
by the progressive clustering of hidden representations of inputs from the same
class. Several methods to directly monitor clustering of hidden
representations, both in noisy and deterministic DNNs, are used to show that
meaningful clusters form in the $T$ space. Finally, we return to the estimator
of $I(X;T)$ employed in past works, and demonstrate that while it fails to
capture the true (vacuous) mutual information, it does serve as a measure for
clustering. This clarifies the past observations of compression and isolates
the geometric clustering of hidden representations as the true phenomenon of
interest."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1806.04195,https://arxiv.org/abs/1806.04195,"Abstract:  In this paper we propose a method of proving impossibility results based on
applying strong data-processing inequalities to estimate mutual information
between sets of variables forming certain Markov random fields. The end result
is that mutual information between two `far away' (as measured by the graph
distance) variables is bounded by the probability of existence of open path in
a bond-percolation problem on the same graph. Furthermore, stronger bounds can
be obtained by establishing mutual comparison results with an erasure model on
the same graph, with erasure probabilities given by the contraction
coefficients.
As application, we show that our method gives sharp threshold for partially
recovering a rank-one perturbation of a random Gaussian matrix (spiked Wigner
model), recovers (and generalizes) the best known upper bound on noise-level
for group synchronization due to Abbe and Boix, and establishes new
impossibility result for a $k$-community detection (stochastic block model)."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1805.03027,https://arxiv.org/abs/1805.03027,"Abstract:  Most information systems store data by modifying the local state of matter,
in the hope that atomic (or sub-atomic) local interactions would stabilize the
state for a sufficiently long time, thereby allowing later recovery. In this
work we initiate the study of information retention in locally-interacting
systems. The evolution in time of the interacting particles is modeled via the
stochastic Ising model (SIM). The initial spin configuration $X_0$ serves as
the user-controlled input. The output configuration $X_t$ is produced by
running $t$ steps of the Glauber chain. Our main goal is to evaluate the
information capacity $I_n(t)\triangleq\max_{p_{X_0}}I(X_0;X_t)$ when the time
$t$ scales with the size of the system $n$. For the zero-temperature SIM on the
two-dimensional $\sqrt{n}\times\sqrt{n}$ grid and free boundary conditions, it
is easy to show that $I_n(t) = \Theta(n)$ for $t=O(n)$. In addition, we show
that on the order of $\sqrt{n}$ bits can be stored for infinite time in striped
configurations. The $\sqrt{n}$ achievability is optimal when $t\to\infty$ and
$n$ is fixed.
One of the main results of this work is an achievability scheme that stores
more than $\sqrt{n}$ bits (in orders of magnitude) for superlinear (in $n$)
times. The analysis of the scheme decomposes the system into $\Omega(\sqrt{n})$
independent Z-channels whose crossover probability is found via the (recently
rigorously established) Lifshitz law of phase boundary movement. We also
provide results for the positive but small temperature regime. We show that an
initial configuration drawn according to the Gibbs measure cannot retain more
than a single bit for $t\geq e^{cn^{\frac{1}{4}+\epsilon}}$. On the other hand,
when scaling time with $\beta$, the stripe-based coding scheme (that stores for
infinite time at zero temperature) is shown to retain its bits for time that is
exponential in $\beta$."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1803.07527,https://arxiv.org/abs/1803.07527,"Abstract:  We study the following generalization of the well-known model of broadcasting
on trees. Consider an infinite directed acyclic graph (DAG) with a unique
source node $X$. Let the collection of nodes at distance $k$ from $X$ be called
the $k$th layer. At time zero, the source node is given a bit. At time $k\geq
1$, each node in the $(k-1)$th layer inspects its inputs and sends a bit to its
descendants in the $k$th layer. Each bit is flipped with a probability of error
$\delta \in \left(0,\frac{1}{2}\right)$ in the process of transmission. The
goal is to be able to recover the original bit with probability of error better
than $\frac{1}{2}$ from the values of all nodes at an arbitrarily deep layer
$k$.
Besides its natural broadcast interpretation, the DAG broadcast is a natural
model of noisy computation. Some special cases of the model represent
information flow in biological networks, and other cases represent noisy finite
automata models.
We show that there exist DAGs with bounded degree and layers of size
$\omega(\log(k))$ that permit recovery provided $\delta$ is sufficiently small
and find the critical $\delta$ for the DAGs constructed. Our result
demonstrates a doubly-exponential advantage for storing a bit in bounded degree
DAGs compared to trees. On the negative side, we show that if the DAG is a
two-dimensional regular grid, then recovery is impossible for any $\delta \in
\left(0,\frac{1}{2}\right)$ provided all nodes use either AND or XOR for their
processing functions."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1801.09481,https://arxiv.org/abs/1801.09481,"Abstract:  Consider a binary linear code of length $N$, minimum distance
$d_{\text{min}}$, transmission over the binary erasure channel with parameter
$0 < \epsilon < 1$ or the binary symmetric channel with parameter $0 < \epsilon
< \frac12$, and block-MAP decoding. It was shown by Tillich and Zemor that in
this case the error probability of the block-MAP decoder transitions ""quickly""
from $\delta$ to $1-\delta$ for any $\delta>0$ if the minimum distance is
large. In particular the width of the transition is of order
$O(1/\sqrt{d_{\text{min}}})$. We strengthen this result by showing that under
suitable conditions on the weight distribution of the code, the transition
width can be as small as $\Theta(1/N^{\frac12-\kappa})$, for any $\kappa>0$,
even if the minimum distance of the code is not linear. This condition applies
e.g., to Reed-Mueller codes. Since $\Theta(1/N^{\frac12})$ is the smallest
transition possible for any code, we speak of ""almost"" optimal scaling. We
emphasize that the width of the transition says nothing about the location of
the transition. Therefore this result has no bearing on whether a code is
capacity-achieving or not. As a second contribution, we present a new estimate
on the derivative of the EXIT function, the proof of which is based on the
Blowing-Up Lemma."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1710.10663,https://arxiv.org/abs/1710.10663,"Abstract:  We consider list-decoding in the zero-rate regime for two cases: the binary
alphabet and the spherical codes in Euclidean space. Specifically, we study the
maximal $\tau \in [0,1]$ for which there exists an arrangement of $M$ balls of
relative Hamming radius $\tau$ in the binary hypercube (of arbitrary dimension)
with the property that no point of the latter is covered by $L$ or more of
them. As $M\to \infty$ the maximal $\tau$ decreases to a well-known critical
value $\tau_L$. In this work, we prove several results on the rate of this
convergence.
For the binary case, we show that the rate is $\Theta(M^{-1})$ when $L$ is
even, thus extending the classical results of Plotkin and Levenshtein for
$L=2$. For $L=3$ the rate is shown to be $\Theta(M^{-\tfrac{2}{3}})$.
For the similar question about spherical codes, we prove the rate is
$\Omega(M^{-1})$ and $O(M^{-\tfrac{2L}{L^2-L+2}})$."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1707.02531,https://arxiv.org/abs/1707.02531,"Abstract:  In this paper, we consider a problem of sampling a Wiener process, with
samples forwarded to a remote estimator over a channel that is modeled as a
queue. The estimator reconstructs an estimate of the real-time signal value
from causally received samples. We study the optimal online sampling strategy
that minimizes the mean square estimation error subject to a sampling rate
constraint.We prove that the optimal sampling strategy is a threshold policy,
and find the optimal threshold. This threshold is determined by how much the
Wiener process varies during the random service time and the maximum allowed
sampling rate. Further, if the sampling times are independent of the observed
Wiener process, the optimal sampling problem reduces to an age of information
optimization problem that has been recently solved. This reveals an interesting
connection between age of information and remote estimation. Our comparisons
show that the estimation error of the optimal sampling policy can be much
smaller than those of age-optimal sampling, zero-wait sampling, and classic
periodic sampling."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1706.05972,https://arxiv.org/abs/1706.05972,"Abstract:  It is well known that the mutual information between two random variables can
be expressed as the difference of two relative entropies that depend on an
auxiliary distribution, a relation sometimes referred to as the golden formula.
This paper is concerned with a finite-blocklength extension of this relation.
This extension consists of two elements: 1) a finite-blocklength channel-coding
converse bound by Polyanskiy and Verdú (2014), which involves the ratio of
two Neyman-Pearson $\beta$ functions (beta-beta converse bound); and 2) a novel
beta-beta channel-coding achievability bound, expressed again as the ratio of
two Neyman-Pearson $\beta$ functions.
To demonstrate the usefulness of this finite-blocklength extension of the
golden formula, the beta-beta achievability and converse bounds are used to
obtain a finite-blocklength extension of Verdú's (2002) wideband-slope
approximation. The proof parallels the derivation of the latter, with the
beta-beta bounds used in place of the golden formula.
The beta-beta (achievability) bound is also shown to be useful in cases where
the capacity-achieving output distribution is not a product distribution due
to, e.g., a cost constraint or structural constraints on the codebook, such as
orthogonality or constant composition. As an example, the bound is used to
characterize the channel dispersion of the additive exponential-noise channel
and to obtain a finite-blocklength achievability bound (the tightest to date)
for multiple-input multiple-output Rayleigh-fading channels with perfect
channel state information at the receiver."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1704.06962,https://arxiv.org/abs/1704.06962,"Abstract:  In this paper we consider a channel model that is often used to describe the
mobile wireless scenario: multiple-antenna additive white Gaussian noise
channels subject to random (fading) gain with full channel state information at
the receiver. Dynamics of the fading process are approximated by a
piecewise-constant process (frequency non-selective isotropic block fading).
This work addresses the finite blocklength fundamental limits of this channel
model. Specifically, we give a formula for the channel dispersion -- a quantity
governing the delay required to achieve capacity. Multiplicative nature of the
fading disturbance leads to a number of interesting technical difficulties that
required us to enhance traditional methods for finding channel dispersion.
Alas, one difficulty remains: the converse (impossibility) part of our result
holds under an extra constraint on the growth of the peak-power with
blocklength.
Our results demonstrate, for example, that while capacities of $n_t\times
n_r$ and $n_r \times n_t$ antenna configurations coincide (under fixed received
power), the coding delay can be quite sensitive to this switch. For example, at
the received SNR of $20$ dB the $16\times 100$ system achieves capacity with
codes of length (delay) which is only $60\%$ of the length required for the
$100\times 16$ system. Another interesting implication is that for the MISO
channel, the dispersion-optimal coding schemes require employing orthogonal
designs such as Alamouti's scheme -- a surprising observation considering the
fact that Alamouti's scheme was designed for reducing demodulation errors, not
improving coding rate. Finding these dispersion-optimal coding schemes
naturally gives a criteria for producing orthogonal design-like inputs in
dimensions where orthogonal designs do not exist."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1702.05574,https://arxiv.org/abs/1702.05574,"Abstract:  The problem of population recovery refers to estimating a distribution based
on incomplete or corrupted samples. Consider a random poll of sample size $n$
conducted on a population of individuals, where each pollee is asked to answer
$d$ binary questions. We consider one of the two polling impediments: (a) in
lossy population recovery, a pollee may skip each question with probability
$\epsilon$, (b) in noisy population recovery, a pollee may lie on each question
with probability $\epsilon$. Given $n$ lossy or noisy samples, the goal is to
estimate the probabilities of all $2^d$ binary vectors simultaneously within
accuracy $\delta$ with high probability.
This paper settles the sample complexity of population recovery. For lossy
model, the optimal sample complexity is
$\tilde\Theta(\delta^{-2\max\{\frac{\epsilon}{1-\epsilon},1\}})$, improving the
state of the art by Moitra and Saks in several ways: a lower bound is
established, the upper bound is improved and the result depends at most on the
logarithm of the dimension. Surprisingly, the sample complexity undergoes a
phase transition from parametric to nonparametric rate when $\epsilon$ exceeds
$1/2$. For noisy population recovery, the sharp sample complexity turns out to
be more sensitive to dimension and scales as $\exp(\Theta(d^{1/3}
\log^{2/3}(1/\delta)))$ except for the trivial cases of $\epsilon=0,1/2$ or
$1$.
For both models, our estimators simply compute the empirical mean of a
certain function, which is found by pre-solving a linear program (LP).
Curiously, the dual LP can be understood as Le Cam's method for lower-bounding
the minimax risk, thus establishing the statistical optimality of the proposed
estimators. The value of the LP is determined by complex-analytic methods."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1701.06734,https://arxiv.org/abs/1701.06734,"Abstract:  In this paper, we consider a problem of sampling a Wiener process, with
samples forwarded to a remote estimator via a channel that consists of a queue
with random delay. The estimator reconstructs a real-time estimate of the
signal from causally received samples. Motivated by recent research on
age-of-information, we study the optimal sampling strategy that minimizes the
mean square estimation error subject to a sampling frequency constraint. We
prove that the optimal sampling strategy is a threshold policy, and find the
optimal threshold. This threshold is determined by the sampling frequency
constraint and how much the Wiener process varies during the channel delay. An
interesting consequence is that even in the absence of the sampling frequency
constraint, the optimal strategy is not zero-wait sampling in which a new
sample is taken once the previous sample is delivered; rather, it is optimal to
wait for a non-zero amount of time after the previous sample is delivered, and
then take the next sample. Further, if the sampling times are independent of
the observed Wiener process, the optimal sampling problem reduces to an
age-of-information optimization problem that has been recently solved. Our
comparisons show that the estimation error of the optimal sampling policy is
much smaller than those of age-optimal sampling, zero-wait sampling, and
classic uniform sampling."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1610.02578,https://arxiv.org/abs/1610.02578,"Abstract:  This paper addresses the problem of adding redundancy to a collection of
physical objects so that the overall system is more robust to failures. In
contrast to its information counterpart, which can exploit parity to protect
multiple information symbols from a single erasure, physical redundancy can
only be realized through duplication and substitution of objects. We propose a
bipartite graph model for designing defect-tolerant systems in which defective
objects are replaced by judiciously connected redundant objects. The
fundamental limits of this model are characterized under various asymptotic
settings and both asymptotic and finite-size systems that approach these limits
are constructed. Among other results, we show that simple modular redundancy is
in general suboptimal. As we develop, this combinatorial problem of defect
tolerant system design has a natural interpretation as one of graph coloring,
and the analysis is significantly different from that traditionally used in
information redundancy for error-control codes."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1609.06877,https://arxiv.org/abs/1609.06877,"Abstract:  This paper studies the basic question of whether a given channel $V$ can be
dominated (in the precise sense of being more noisy) by a $q$-ary symmetric
channel. The concept of ""less noisy"" relation between channels originated in
network information theory (broadcast channels) and is defined in terms of
mutual information or Kullback-Leibler divergence. We provide an equivalent
characterization in terms of $\chi^2$-divergence. Furthermore, we develop a
simple criterion for domination by a $q$-ary symmetric channel in terms of the
minimum entry of the stochastic matrix defining the channel $V$. The criterion
is strengthened for the special case of additive noise channels over finite
Abelian groups. Finally, it is shown that domination by a symmetric channel
implies (via comparison of Dirichlet forms) a logarithmic Sobolev inequality
for the original channel."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1607.06384,https://arxiv.org/abs/1607.06384,"Abstract:  The capacity of a graph is defined as the rate of exponential growth of
independent sets in the strong powers of the graph. In the strong power an edge
connects two sequences if at each position their letters are equal or adjacent.
We consider a variation of the problem where edges in the power graphs are
removed between sequences which differ in more than a fraction $\delta$ of
coordinates. The proposed generalization can be interpreted as the problem of
determining the highest rate of zero undetected-error communication over a link
with adversarial noise, where only a fraction $\delta$ of symbols can be
perturbed and only some substitutions are allowed.
We derive lower bounds on achievable rates by combining graph homomorphisms
with a graph-theoretic generalization of the Gilbert-Varshamov bound. We then
give an upper bound, based on Delsarte's linear programming approach, which
combines Lovász' theta function with the construction used by McEliece et al.
for bounding the minimum distance of codes in Hamming spaces."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1607.05064,https://arxiv.org/abs/1607.05064,"Abstract:  We give new bounds on the reliability function of a typewriter channel with 5
inputs and crossover probability $1/2$. The lower bound is more of theoretical
than practical importance; it improves very marginally the expurgated bound,
providing a counterexample to a conjecture on its tightness by Shannon,
Gallager and Berlekamp which does not need the construction of
algebraic-geometric codes previously used by Katsman, Tsfasman and
Vlăduţ. The upper bound is derived by using an adaptation of the linear
programming bound and it is essentially useful as a low-rate anchor for the
straight line bound."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1606.07491,https://arxiv.org/abs/1606.07491,"Abstract:  Log-Sobolev inequalities (LSIs) upper-bound entropy via a multiple of the
Dirichlet form (i.e. norm of a gradient). In this paper we prove a family of
entropy-energy inequalities for the binary hypercube which provide a non-linear
comparison between the entropy and the Dirichlet form and improve on the usual
LSIs for functions with small support. These non-linear LSIs, in turn, imply a
new version of the hypercontractivity for such functions. As another
consequence, we derive a sharp form of the uncertainty principle for the
hypercube: a function whose energy is concentrated on a set of small size, and
whose Fourier energy is concentrated on a small Hamming ball must be zero. The
tradeoff between the sizes that we derive is asymptotically optimal. This new
uncertainty principle implies a new estimate on the size of Fourier
coefficients of sparse Boolean functions. We observe that an analogous
(asymptotically optimal) uncertainty principle in the Euclidean space follows
from the sharp form of Young's inequality due to Beckner. This hints that
non-linear LSIs augment Young's inequality (which is sharp for finite groups)."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1601.05880,https://arxiv.org/abs/1601.05880,"Abstract:  A channel coding achievability bound expressed in terms of the ratio between
two Neyman-Pearson $\beta$ functions is proposed. This bound is the dual of a
converse bound established earlier by Polyanskiy and Verdú (2014). The new
bound turns out to simplify considerably the analysis in situations where the
channel output distribution is not a product distribution, for example due to a
cost constraint or a structural constraint (such as orthogonality or constant
composition) on the channel inputs. Connections to existing bounds in the
literature are discussed. The bound is then used to derive 1) an achievability
bound on the channel dispersion of additive non-Gaussian noise channels with
random Gaussian codebooks, 2) the channel dispersion of the exponential-noise
channel, 3) a second-order expansion for the minimum energy per bit of an AWGN
channel, and 4) a lower bound on the maximum coding rate of a multiple-input
multiple-output Rayleigh-fading channel with perfect channel state information
at the receiver, which is the tightest known achievability result."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1512.06429,https://arxiv.org/abs/1512.06429,"Abstract:  This paper quantifies the intuitive observation that adding noise reduces
available information by means of non-linear strong data processing
inequalities. Consider the random variables $W\to X\to Y$ forming a Markov
chain, where $Y=X+Z$ with $X$ and $Z$ real-valued, independent and $X$ bounded
in $L_p$-norm. It is shown that $I(W;Y) \le F_I(I(W;X))$ with $F_I(t)<t$
whenever $t>0$, if and only if $Z$ has a density whose support is not disjoint
from any translate of itself. A related question is to characterize for what
couplings $(W,X)$ the mutual information $I(W;Y)$ is close to maximum possible.
To that end we show that in order to saturate the channel, i.e. for $I(W;Y)$ to
approach capacity, it is mandatory that $I(W;X)\to\infty$ (under suitable
conditions on the channel). A key ingredient for this result is a deconvolution
lemma which shows that post-convolution total variation distance bounds the
pre-convolution Kolmogorov-Smirnov distance. Explicit bounds are provided for
the special case of the additive Gaussian noise channel with quadratic cost
constraint. These bounds are shown to be order-optimal. For this case
simplified proofs are provided leveraging Gaussian-specific tools such as the
connection between information and estimation (I-MMSE) and Talagrand's
information-transportation inequality."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1508.06025,https://arxiv.org/abs/1508.06025,"Abstract:  The data-processing inequality, that is, $I(U;Y) \le I(U;X)$ for a Markov
chain $U \to X \to Y$, has been the method of choice for proving impossibility
(converse) results in information theory and many other disciplines. Various
channel-dependent improvements (called strong data-processing inequalities, or
SDPIs) of this inequality have been proposed both classically and more
recently. In this note we first survey known results relating various notions
of contraction for a single channel. Then we consider the basic extension:
given SDPI for each constituent channel in a Bayesian network, how to produce
an end-to-end SDPI?
Our approach is based on the (extract of the) Evans-Schulman method, which is
demonstrated for three different kinds of SDPIs, namely, the usual
Ahslwede-Gács type contraction coefficients (mutual information), Dobrushin's
contraction coefficients (total variation), and finally the $F_I$-curve (the
best possible non-linear SDPI for a given channel). Resulting bounds on the
contraction coefficients are interpreted as probability of site percolation. As
an example, we demonstrate how to obtain SDPI for an $n$-letter memoryless
channel with feedback given an SDPI for $n=1$.
Finally, we discuss a simple observation on the equivalence of a linear SDPI
and comparison to an erasure channel (in the sense of ""less noisy"" order). This
leads to a simple proof of a curious inequality of Samorodnitsky (2015), and
sheds light on how information spreads in the subsets of inputs of a memoryless
channel."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1508.03020,https://arxiv.org/abs/1508.03020,"Abstract:  The capacity of a graph is defined as the rate of exponential grow of
independent sets in the strong powers of the graph. In strong power, an edge
connects two sequences if at each position letters are equal or adjacent. We
consider a variation of the problem where edges in the power graphs are removed
among sequences which differ in more than a fraction $\delta$ of coordinates.
For odd cycles, we derive an upper bound on the corresponding rate which
combines Lovász' bound on the capacity with Delsarte's linear programming
bounds on the minimum distance of codes in Hamming spaces. For the pentagon,
this shows that for $\delta \ge {1-{1\over\sqrt{5}}}$ the Lovász rate is the
best possible, while we prove by a Gilbert-Varshamov-type bound that a higher
rate is achievable for $\delta < {2\over 5}$.
Communication interpretation of this question is the problem of sending
quinary symbols subject to $\pm 1\mod 5$ disturbance. The maximal communication
rate subject to the zero undetected-error equals capacity of a pentagon. The
question addressed here is how much this rate can be increased if only a
fraction $\delta$ of symbols is allowed to be disturbed"
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1507.03843,https://arxiv.org/abs/1507.03843,"Abstract:  This paper investigates the minimum energy required to transmit $k$
information bits with a given reliability over a multiple-antenna Rayleigh
block-fading channel, with and without channel state information (CSI) at the
receiver. No feedback is assumed. It is well known that the ratio between the
minimum energy per bit and the noise level converges to $-1.59$ dB as $k$ goes
to infinity, regardless of whether CSI is available at the receiver or not.
This paper shows that lack of CSI at the receiver causes a slowdown in the
speed of convergence to $-1.59$ dB as $k\to\infty$ compared to the case of
perfect receiver CSI. Specifically, we show that, in the no-CSI case, the gap
to $-1.59$ dB is proportional to $((\log k) /k)^{1/3}$, whereas when perfect
CSI is available at the receiver, this gap is proportional to $1/\sqrt{k}$. In
both cases, the gap to $-1.59$ dB is independent of the number of transmit
antennas and of the channel's coherence time. Numerically, we observe that,
when the receiver is equipped with a single antenna, to achieve an energy per
bit of $ - 1.5$ dB in the no-CSI case, one needs to transmit at least $7\times
10^7$ information bits, whereas $6\times 10^4$ bits suffice for the case of
perfect CSI at the receiver."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1504.04419,https://arxiv.org/abs/1504.04419,"Abstract:  It is shown that under suitable regularity conditions, differential entropy
is a Lipschitz functional on the space of distributions on $n$-dimensional
Euclidean space with respect to the quadratic Wasserstein distance. Under
similar conditions, (discrete) Shannon entropy is shown to be Lipschitz
continuous in distributions over the product space with respect to Ornstein's
$\bar d$-distance (Wasserstein distance corresponding to the Hamming distance).
These results together with Talagrand's and Marton's transportation-information
inequalities allow one to replace the unknown multi-user interference with its
i.i.d. approximations. As an application, a new outer bound for the two-user
Gaussian interference channel is proved, which, in particular, settles the
""missing corner point"" problem of Costa (1985)."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1503.02779,https://arxiv.org/abs/1503.02779,"Abstract:  A mapping of $k$-bit strings into $n$-bit strings is called an
$(\alpha,\beta)$-map if $k$-bit strings which are more than $\alpha k$ apart
are mapped to $n$-bit strings that are more than $\beta n$ apart. This is a
relaxation of the classical problem of constructing error-correcting codes,
which corresponds to $\alpha=0$. Existence of an $(\alpha,\beta)$-map is
equivalent to existence of a graph homomorphism $\bar H(k,\alpha k)\to \bar
H(n,\beta n)$, where $H(n,d)$ is a Hamming graph with vertex set $\{0,1\}^n$
and edges connecting vertices differing in $d$ or fewer entries.
This paper proves impossibility results on achievable parameters
$(\alpha,\beta)$ in the regime of $n,k\to\infty$ with a fixed ratio ${n\over
k}= \rho$. This is done by developing a general criterion for existence of
graph-homomorphism based on the semi-definite relaxation of the independence
number of a graph (known as the Schrijver's $\theta$-function). The criterion
is then evaluated using some known and some new results from coding theory
concerning the $\theta$-function of Hamming graphs. As an example, it is shown
that if $\beta>1/2$ and $n\over k$ -- integer, the ${n\over k}$-fold repetition
map achieving $\alpha=\beta$ is asymptotically optimal.
Finally, constraints on configurations of points and hyperplanes in
projective spaces over $\mathbb{F}_2$ are derived."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1501.07640,https://arxiv.org/abs/1501.07640,"Abstract:  This paper quantifies the fundamental limits of variable-length transmission
of a general (possibly analog) source over a memoryless channel with noiseless
feedback, under a distortion constraint. We consider excess distortion, average
distortion and guaranteed distortion ($d$-semifaithful codes). In contrast to
the asymptotic fundamental limit, a general conclusion is that allowing
variable-length codes and feedback leads to a sizable improvement in the
fundamental delay-distortion tradeoff. In addition, we investigate the minimum
energy required to reproduce $k$ source samples with a given fidelity after
transmission over a memoryless Gaussian channel, and we show that the required
minimum energy is reduced with feedback and an average (rather than maximal)
power constraint."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1412.7512,https://arxiv.org/abs/1412.7512,"Abstract:  Motivated by the current interest in ultra-reliable, low-latency,
machine-type communication systems, we investigate the tradeoff between
reliability, throughput, and latency in the transmission of information over
multiple-antenna Rayleigh block-fading channels. Specifically, we obtain
finite-blocklength, finite-SNR upper and lower bounds on the maximum coding
rate achievable over such channels for a given constraint on the packet error
probability. Numerical evidence suggests that our bounds delimit tightly the
maximum coding rate already for short blocklengths (packets of about 100
symbols). Furthermore, our bounds reveal the existence of a tradeoff between
the rate gain obtainable by spreading each codeword over all available
time-frequency-spatial degrees of freedom, and the rate loss caused by the need
of estimating the fading coefficients over these degrees of freedom. In
particular, our bounds allow us to determine the optimal number of transmit
antennas and the optimal number of time-frequency diversity branches that
maximize the rate. Finally, we show that infinite-blocklength performance
metrics such as the ergodic capacity and the outage capacity yield inaccurate
throughput estimates."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1409.7765,https://arxiv.org/abs/1409.7765,"Abstract:  Consider the problem of packing Hamming balls of a given relative radius
subject to the constraint that they cover any point of the ambient Hamming
space with multiplicity at most $L$. For odd $L\ge 3$ an asymptotic upper bound
on the rate of any such packing is proven. Resulting bound improves the best
known bound (due to Blinovsky'1986) for rates below a certain threshold. Method
is a superposition of the linear-programming idea of Ashikhmin, Barg and Litsyn
(that was used previously to improve the estimates of Blinovsky for $L=2$) and
a Ramsey-theoretic technique of Blinovsky. As an application it is shown that
for all odd $L$ the slope of the rate-radius tradeoff is zero at zero rate."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1406.5422,https://arxiv.org/abs/1406.5422,"Abstract:  This paper investigates the maximal channel coding rate achievable at a given
blocklength $n$ and error probability $\epsilon$, when the codewords are
subject to a long-term (i.e., averaged-over-all-codeword) power constraint. The
second-order term in the large-$n$ expansion of the maximal channel coding rate
is characterized both for additive white Gaussian noise (AWGN) channels and for
quasi-static fading channels with perfect channel state information available
at both the transmitter and the receiver. It is shown that in both cases the
second-order term is proportional to $\sqrt{n^{-1}\ln n}$. For the quasi-static
fading case, this second-order term is achieved by truncated channel inversion,
namely, by concatenating a dispersion-optimal code for an AWGN channel subject
to a short-term power constraint, with a power controller that inverts the
channel whenever the fading gain is above a certain threshold. Easy-to-evaluate
approximations of the maximal channel coding rate are developed for both the
AWGN and the quasi-static fading case."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1405.3629,https://arxiv.org/abs/1405.3629,"Abstract:  One of the basic tenets in information theory, the data processing inequality
states that output divergence does not exceed the input divergence for any
channel. For channels without input constraints, various estimates on the
amount of such contraction are known, Dobrushin's coefficient for the total
variation being perhaps the most well-known. This work investigates channels
with average input cost constraint. It is found that while the contraction
coefficient typically equals one (no contraction), the information nevertheless
dissipates. A certain non-linear function, the \emph{Dobrushin curve} of the
channel, is proposed to quantify the amount of dissipation. Tools for
evaluating the Dobrushin curve of additive-noise channels are developed based
on coupling arguments. Some basic applications in stochastic control,
uniqueness of Gibbs measures and fundamental limits of noisy circuits are
discussed.
As an application, it shown that in the chain of $n$ power-constrained relays
and Gaussian channels the end-to-end mutual information and maximal squared
correlation decay as $\Theta(\frac{\log\log n}{\log n})$, which is in stark
contrast with the exponential decay in chains of discrete channels. Similarly,
the behavior of noisy circuits (composed of gates with bounded fan-in) and
broadcasting of information on trees (of bounded degree) does not experience
threshold behavior in the signal-to-noise ratio (SNR). Namely, unlike the case
of discrete channels, the probability of bit error stays bounded away from
$1\over 2$ regardless of the SNR."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1402.0608,https://arxiv.org/abs/1402.0608,"Abstract:  This paper studies the fundamental limits of the minimum average length of
lossless and lossy variable-length compression, allowing a nonzero error
probability $\epsilon$, for lossless compression. We give non-asymptotic bounds
on the minimum average length in terms of Erokhin's rate-distortion function
and we use those bounds to obtain a Gaussian approximation on the speed of
approach to the limit which is quite accurate for all but small blocklengths:
$$(1 - \epsilon) k H(\mathsf S) - \sqrt{\frac{k V(\mathsf S)}{2 \pi} } e^{-
\frac {(Q^{-1}(\epsilon))^2} 2 }$$ where $Q^{-1}(\cdot)$ is the functional
inverse of the standard Gaussian complementary cdf, and $V(\mathsf S)$ is the
source dispersion. A nonzero error probability thus not only reduces the
asymptotically achievable rate by a factor of $1 - \epsilon$, but this
asymptotic limit is approached from below, i.e. larger source dispersions and
shorter blocklengths are beneficial. Variable-length lossy compression under an
excess distortion constraint is shown to exhibit similar properties."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1401.5551,https://arxiv.org/abs/1401.5551,"Abstract:  Directed acyclic graphical models (DAGs) are often used to describe common
structural properties in a family of probability distributions. This paper
addresses the question of classifying DAGs up to an isomorphism. By considering
Gaussian densities, the question reduces to verifying equality of certain
algebraic varieties. A question of computing equations for these varieties has
been previously raised in the literature. Here it is shown that the most
natural method adds spurious components with singular principal minors, proving
a conjecture of Sullivant. This characterization is used to establish an
algebraic criterion for isomorphism, and to provide a randomized algorithm for
checking that criterion. Results are applied to produce a list of the
isomorphism classes of tree models on 4,5, and 6 nodes. Finally, some evidence
is provided to show that projectivized DAG varieties contain useful information
in the sense that their relative embedding is closely related to efficient
inference."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1311.2012,https://arxiv.org/abs/1311.2012,"Abstract:  This paper investigates the maximal achievable rate for a given blocklength
and error probability over quasi-static multiple-input multiple-output (MIMO)
fading channels, with and without channel state information (CSI) at the
transmitter and/or the receiver. The principal finding is that outage capacity,
despite being an asymptotic quantity, is a sharp proxy for the
finite-blocklength fundamental limits of slow-fading channels. Specifically,
the channel dispersion is shown to be zero regardless of whether the fading
realizations are available at both transmitter and receiver, at only one of
them, or at neither of them. These results follow from analytically tractable
converse and achievability bounds. Numerical evaluation of these bounds
verifies that zero dispersion may indeed imply fast convergence to the outage
capacity as the blocklength increases. In the example of a particular $1 \times
2$ single-input multiple-output (SIMO) Rician fading channel, the blocklength
required to achieve $90\%$ of capacity is about an order of magnitude smaller
compared to the blocklength required for an AWGN channel with the same
capacity. For this specific scenario, the coding/decoding schemes adopted in
the LTE-Advanced standard are benchmarked against the finite-blocklength
achievability and converse bounds."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1309.3014,https://arxiv.org/abs/1309.3014,"Abstract:  Consider the linear space of functions on the binary hypercube and the linear
operator $S_\delta$ acting by averaging a function over a Hamming sphere of
radius $\delta n$ around every point. It is shown that this operator has a
dimension-independent bound on the norm $L_p \to L_2$ with $p =
1+(1-2\delta)^2$. This result evidently parallels a classical estimate of
Bonami and Gross for $L_p \to L_q$ norms for the operator of convolution with a
Bernoulli noise. The estimate for $S_\delta$ is harder to obtain since the
latter is neither a part of a semigroup, nor a tensor power. The result is
shown by a detailed study of the eigenvalues of $S_\delta$ and $L_p\to L_2$
norms of the Fourier multiplier operators $\Pi_a$ with symbol equal to a
characteristic function of the Hamming sphere of radius $a$ (in the notation
common in boolean analysis $\Pi_a f=f^{=a}$, where $f^{=a}$ is a degree-$a$
component of function $f$). A sample application of the result is given: Any
set $A\subset \FF_2^n$ with the property that $A+A$ contains a large portion of
some Hamming sphere (counted with multiplicity) must have cardinality a
constant multiple of $2^n$."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1309.0141,https://arxiv.org/abs/1309.0141,"Abstract:  This paper studies several properties of channel codes that approach the
fundamental limits of a given (discrete or Gaussian) memoryless channel with a
non-vanishing probability of error. The output distribution induced by an
$\epsilon$-capacity-achieving code is shown to be close in a strong sense to
the capacity achieving output distribution. Relying on the concentration of
measure (isoperimetry) property enjoyed by the latter, it is shown that regular
(Lipschitz) functions of channel outputs can be precisely estimated and turn
out to be essentially non-random and independent of the actual code. It is also
shown that the output distribution of a good code and the capacity achieving
one cannot be distinguished with exponential reliability. The random process
produced at the output of the channel is shown to satisfy the asymptotic
equipartition property. Using related methods it is shown that quadratic forms
and sums of $q$-th powers when evaluated at codewords of good AWGN codes
approach the values obtained from a randomly generated Gaussian codeword."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1308.5239,https://arxiv.org/abs/1308.5239,"Abstract:  Locally decodable channel codes form a special class of error-correcting
codes with the property that the decoder is able to reconstruct any bit of the
input message from querying only a few bits of a noisy codeword. It is well
known that such codes require significantly more redundancy (in particular have
vanishing rate) compared to their non-local counterparts. In this paper, we
define a dual problem, i.e. locally decodable source codes (LDSC). We consider
both almost lossless (block error) and lossy (bit error) cases. In almost
lossless case, we show that optimal compression (to entropy) is possible with
O(log n) queries to compressed string by the decompressor. We also show the
following converse bounds: 1) linear LDSC cannot achieve any rate below one,
with a bounded number of queries, 2) rate of any source coding with linear
decoder (not necessarily local) in one, 3) for 2 queries, any code construction
cannot have a rate below one. In lossy case, we show that any rate above rate
distortion is achievable with a bounded number of queries. We also show that,
rate distortion is achievable with any scaling number of queries. We provide an
achievability bound in the finite block-length regime and compare it with the
existing bounds in succinct data structures literature."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1306.6295,https://arxiv.org/abs/1306.6295,"Abstract:  The problem of estimating frequency moments of a data stream has attracted a
lot of attention since the onset of streaming algorithms [AMS99]. While the
space complexity for approximately computing the $p^{\rm th}$ moment, for
$p\in(0,2]$ has been settled [KNW10], for $p>2$ the exact complexity remains
open. For $p>2$ the current best algorithm uses $O(n^{1-2/p}\log n)$ words of
space [AKO11,BO10], whereas the lower bound is of $\Omega(n^{1-2/p})$ [BJKS04].
In this paper, we show a tight lower bound of $\Omega(n^{1-2/p}\log n)$ words
for the class of algorithms based on linear sketches, which store only a sketch
$Ax$ of input vector $x$ and some (possibly randomized) matrix $A$. We note
that all known algorithms for this problem are linear sketches."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1302.1302,https://arxiv.org/abs/1302.1302,"Abstract:  We investigate the maximal achievable rate for a given blocklength and error
probability over quasi-static single-input multiple-output (SIMO) fading
channels. Under mild conditions on the channel gains, it is shown that the
channel dispersion is zero regardless of whether the fading realizations are
available at the transmitter and/or the receiver. The result follows from
computationally and analytically tractable converse and achievability bounds.
Through numerical evaluation, we verify that, in some scenarios, zero
dispersion indeed entails fast convergence to outage capacity as the
blocklength increases. In the example of a particular 1*2 SIMO Rician channel,
the blocklength required to achieve 90% of capacity is about an order of
magnitude smaller compared to the blocklength required for an AWGN channel with
the same capacity."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1302.0084,https://arxiv.org/abs/1302.0084,"Abstract:  Consider a problem of forward error-correction for the additive white
Gaussian noise (AWGN) channel. For finite blocklength codes the backoff from
the channel capacity is inversely proportional to the square root of the
blocklength. In this paper it is shown that codes achieving this tradeoff must
necessarily have peak-to-average power ratio (PAPR) proportional to logarithm
of the blocklength. This is extended to codes approaching capacity slower, and
to PAPR measured at the output of an OFDM modulator. As a by-product the
convergence of (Smith's) amplitude-constrained AWGN capacity to Shannon's
classical formula is characterized in the regime of large amplitudes. This
converse-type result builds upon recent contributions in the study of empirical
output distributions of good channel codes."
Yury Polyanskiy,Polyanskiy_Yury,arXiv:1204.2927,https://arxiv.org/abs/1204.2927,"Abstract:  We study the maximal achievable rate R*(n, \epsilon) for a given block-length
n and block error probability \epsilon over Rayleigh block-fading channels in
the noncoherent setting and in the finite block-length regime. Our results show
that for a given block-length and error probability, R*(n, \epsilon) is not
monotonic in the channel's coherence time, but there exists a rate maximizing
coherence time that optimally trades between diversity and cost of estimating
the channel."
Rajeev Ram,Ram_Rajeev,arXiv:1808.00429,https://arxiv.org/abs/1808.00429,"Abstract:  We present a low-loss integrated photonics platform in the visible and near
ultraviolet regime. Fully-etched waveguides based on atomic layer deposition
(ALD) of aluminum oxide operate in a single transverse mode with $<$3 dB/cm
propagation loss at a wavelength of 371 nm. Ring resonators with intrinsic
quality factors exceeding 470,000 are demonstrated at 405 nm, and the thermo
optic coefficient of ALD aluminum oxide is estimated to be $2.75\times10^{-5}$
[RIU/$^\circ$C]. Absorption loss is sufficiently low to allow on-resonance
operation with intra-cavity powers up to at least 12.5 mW, limited by available
laser power. Experimental and simulated data indicates the propagation loss is
dominated by sidewall roughness, suggesting lower loss in the blue and UV is
achievable."
Rajeev Ram,Ram_Rajeev,arXiv:1805.07688,https://arxiv.org/abs/1805.07688,"Abstract:  In this work, we propose a two-stage algorithm based on Bayesian modeling and
computation aiming at quantifying analyte concentrations or quantities in
complex mixtures with Raman spectroscopy. A hierarchical Bayesian model is
built for spectral signal analysis, and reversible-jump Markov chain Monte
Carlo (RJMCMC) computation is carried out for model selection and spectral
variable estimation. Processing is done in two stages. In the first stage, the
peak representations for a target analyte spectrum are learned. In the second,
the peak variables learned from the first stage are used to estimate the
concentration or quantity of the target analyte in a mixture. Numerical
experiments validated its quantification performance over a wide range of
simulation conditions and established its advantages for analyte quantification
tasks under the small training sample size regime over conventional
multivariate regression algorithms. We also used our algorithm to analyze
experimental spontaneous Raman spectroscopy data collected for glucose
concentration estimation in biopharmaceutical process monitoring applications.
Our work shows that this algorithm can be a promising complementary tool
alongside conventional multivariate regression algorithms in Raman
spectroscopy-based mixture quantification studies, especially when collecting a
large training dataset with high quality is challenging or resource-intensive."
Rajeev Ram,Ram_Rajeev,arXiv:1607.00107,https://arxiv.org/abs/1607.00107,"Abstract:  We present the design and characterization of waveguide grating devices that
couple visible-wavelength light at $\lambda=674$ nm from single-mode, high
index-contrast dielectric waveguides to free-space beams forming micron-scale
diffraction-limited spots a designed distance and angle from the grating. With
a view to application in spatially-selective optical addressing, and in
contrast to previous work on similar devices, deviations from the main Gaussian
lobe up to $25$ microns from the focus and down to the $5\times10^{-6}$ level
in relative intensity are characterized as well; we show that along one
dimension the intensity of these weak sidelobes approaches the limit imposed by
diffraction from the finite field extent in the grating region. Additionally,
we characterize the polarization purity in the focal region, observing at the
center of the focus a low impurity $< 3 \times 10^{-4}$ in relative intensity.
Our approach allows quick, intuitive design of devices with such performance,
which may be applied in trapped-ion quantum information processing and
generally in any systems requiring optical routing to or from objects 10s--100s
of microns from a chip surface, but benefitting from the parallelism and
density of planar-fabricated dielectric integrated optics."
Rajeev Ram,Ram_Rajeev,arXiv:1606.08870,https://arxiv.org/abs/1606.08870,"Abstract:  We report on vertically-illuminated photodiodes fabricated in the
GlobalFoundries 45nm 12SOI node and on a packaging concept for
optically-interconnected chips. The photodiodes are responsive at 1180 nm, a
wavelength currently used in chip-to-chip communications. They have further a
wide field-of-view which enables chip-to-board positional feedback in
chip-board assemblies. Monolithic integration enables on-chip processing of the
positional data."
Rajeev Ram,Ram_Rajeev,arXiv:1601.05046,https://arxiv.org/abs/1601.05046,"Abstract:  A microring depletion modulator is demonstrated with T-shaped lateral p-n
junctions used to realize efficient modulation while maximizing the RC limited
bandwidth. The device having a 3 dB bandwidth of 13 GHz has been fabricated in
a standard 45 nm microelectronics CMOS process. The cavity has a linewidth of
17 GHz and an average wavelength-shift of 9 pm/V in reverse-bias conditions."
Rajeev Ram,Ram_Rajeev,arXiv:1601.00542,https://arxiv.org/abs/1601.00542,"Abstract:  A photodiode with 0.55$\pm$0.1 A/W responsivity at a wavelength of 1176.9 nm
has been fabricated in a 45 nm microelectronics silicon-on-insulator foundry
process. The resonant waveguide photodetector exploits carrier generation in
silicon-germanium (SiGe) within a microring which is compatible with
high-performance electronics. A 3 dB bandwidth of 5 GHz at -4 V bias is
obtained with a dark current of less than 20 pA."
Rajeev Ram,Ram_Rajeev,arXiv:1510.05618,https://arxiv.org/abs/1510.05618,"Abstract:  The long coherence times and strong Coulomb interactions afforded by trapped
ion qubits have enabled realizations of the necessary primitives for quantum
information processing (QIP), and indeed the highest-fidelity quantum
operations in any qubit to date. But while light delivery to each individual
ion in a system is essential for general quantum manipulations and readout,
experiments so far have employed optical systems cumbersome to scale to even a
few tens of qubits. Here we demonstrate lithographically defined nanophotonic
waveguide devices for light routing and ion addressing fully integrated within
a surface-electrode ion trap chip. Ion qubits are addressed at multiple
locations via focusing grating couplers emitting through openings in the trap
electrodes to ions trapped 50 $\mu$m above the chip; using this light we
perform quantum coherent operations on the optical qubit transition in
individual $^{88}$Sr$^+$ ions. The grating focuses the beam to a
diffraction-limited spot near the ion position with a 2 $\mu$m 1/$e^2$-radius
along the trap axis, and we measure crosstalk errors between $10^{-2}$ and
$4\times10^{-4}$ at distances 7.5-15 $\mu$m from the beam center. Owing to the
scalability of the planar fabrication employed, together with the tight
focusing and stable alignment afforded by optics integration within the trap
chip, this approach presents a path to creating the optical systems required
for large-scale trapped-ion QIP."
Rajeev Ram,Ram_Rajeev,arXiv:1504.03669,https://arxiv.org/abs/1504.03669,"Abstract:  Recently, the authors have demonstrated large-scale integrated systems with
several million transistors and hundreds of photonic elements. Yielding such
large-scale integrated systems requires a design-for-manufacture rigour that is
embodied in the 10 000 to 50 000 design rules that these designs must comply
within advanced complementary metal-oxide semiconductor manufacturing. Here,
the authors present a photonic design automation tool which allows automatic
generation of layouts without design-rule violations. This tool is written in
SKILL, the native language of the mainstream electric design automation
software, Cadence. This allows seamless integration of photonic and electronic
design in a single environment. The tool leverages intuitive photonic layer
definitions, allowing the designer to focus on the physical properties rather
than on technology-dependent details. For the first time the authors present an
algorithm for removal of design-rule violations from photonic layouts based on
Manhattan discretisation, Boolean and sizing operations. This algorithm is not
limited to the implementation in SKILL, and can in principle be implemented in
any scripting language. Connectivity is achieved with software-defined
waveguide ports and low-level procedures that enable auto-routing of waveguide
connections."
Rajeev Ram,Ram_Rajeev,arXiv:cond-mat/0108236,https://arxiv.org/abs/cond-mat/0108236,"Abstract:  A comprehensive model for the photon number fluctuations and the current
noise in quantum cascade lasers is presented. It is shown that the photon
intensity noise in quantum cascade lasers exhibits little amplitude squeezing
even when noise in the drive current is suppressed below the shot noise value.
This is in contrast to interband semiconductor diode lasers in which the laser
intensity noise can be squeezed well below the shot noise limit by high
impedance suppression of fluctuations in the drive current. The theoretical
model presented in this paper self-consistently accounts for the suppression of
current noise in electron transport in multiple quantum well structures due to
various electronic correlations. The nature of these electronic correlations is
discussed. Mechanisms responsible for the reduced photon number squeezing in
intersubband lasers are elucidated. Scaling of the laser intensity noise and
the current noise with the number of cascaded gain stages is also described.
Direct current modulation response of quantum cascade lasers is also studied,
and it is shown that contrary to the predictions in the literature of terahertz
modulation bandwidth for these lasers, bandwidth of almost all quantum cascade
lasers that have been reported in the literature is limited by the inverse
photon lifetime inside the laser cavity to tens of gigahertz."
Martin Rinard,Rinard_Martin,arXiv:1807.01624,https://arxiv.org/abs/1807.01624,"Abstract:  Modern out-of-order processors have increased capacity to exploit instruction
level parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide
superscalar pipelines and vector execution units, as well as deep buffers for
in-flight memory requests. These resources, however, often exhibit poor
utilization rates on workloads with large working sets, e.g., in-memory
databases, key-value stores, and graph analytics, as compilers and hardware
struggle to expose ILP and MLP from the instruction stream automatically.
In this paper, we introduce the IMLP (Instruction and Memory Level
Parallelism) task programming model. IMLP tasks execute as coroutines that
yield execution at annotated long-latency operations, e.g., memory accesses,
divisions, or unpredictable branches. IMLP tasks are interleaved on a single
thread, and integrate well with thread parallelism and vectorization. Our DSL
embedded in C++, Cimple, allows exploration of task scheduling and
transformations, such as buffering, vectorization, pipelining, and prefetching.
We demonstrate state-of-the-art performance on core algorithms used in
in-memory databases that operate on arrays, hash tables, trees, and skip lists.
Cimple applications reach 2.5x throughput gains over hardware multithreading on
a multi-core, and 6.4x single thread speedup."
Martin Rinard,Rinard_Martin,arXiv:1804.09241,https://arxiv.org/abs/1804.09241,"Abstract:  We present Warp, a hardware platform to support research in approximate
computing, sensor energy optimization, and energy-scavenged systems. Warp
incorporates 11 state-of-the-art sensor integrated circuits, computation, and
an energy-scavenged power supply, all within a miniature system that is just
3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a
total of 21 sensors with a range of precisions and accuracies for measuring
eight sensing modalities of acceleration, angular rate, magnetic flux density
(compass heading), humidity, atmospheric pressure (elevation), infrared
radiation, ambient temperature, and color. Warp uses a combination of analog
circuits and digital control to facilitate further tradeoffs between sensor and
communication accuracy, energy efficiency, and performance. This article
presents the design of Warp and presents an evaluation of our hardware
implementation. The results show how Warp's design enables performance and
energy efficiency versus ac- curacy tradeoffs."
Martin Rinard,Rinard_Martin,arXiv:1803.08420,https://arxiv.org/abs/1803.08420,"Abstract:  The sizes of compressed images depend on their spatial resolution (number of
pixels) and on their color resolution (number of color quantization levels). We
introduce DaltonQuant, a new color quantization technique for image compression
that cloud services can apply to images destined for a specific user with known
color vision deficiencies. DaltonQuant improves compression in a user-specific
but reversible manner thereby improving a user's network bandwidth and data
storage efficiency. DaltonQuant quantizes image data to account for
user-specific color perception anomalies, using a new method for incremental
color quantization based on a large corpus of color vision acuity data obtained
from a popular mobile game. Servers that host images can revert DaltonQuant's
image requantization and compression when those images must be transmitted to a
different user, making the technique practical to deploy on a large scale. We
evaluate DaltonQuant's compression performance on the Kodak PC reference image
set and show that it improves compression by an additional 22%-29% over the
state-of-the-art compressors TinyPNG and pngquant."
Martin Rinard,Rinard_Martin,arXiv:1803.07244,https://arxiv.org/abs/1803.07244,"Abstract:  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software."
Martin Rinard,Rinard_Martin,arXiv:1606.06368,https://arxiv.org/abs/1606.06368,"Abstract:  Can we train a system that, on any new input, either says ""don't know"" or
makes a prediction that is guaranteed to be correct? We answer the question in
the affirmative provided our model family is well-specified. Specifically, we
introduce the unanimity principle: only predict when all models consistent with
the training data predict the same output. We operationalize this principle for
semantic parsing, the task of mapping utterances to logical forms. We develop a
simple, efficient method that reasons over the infinite set of all consistent
models by only checking two of the models. We prove that our method obtains
100% precision even with a modest amount of training data from a possibly
adversarial distribution. Empirically, we demonstrate the effectiveness of our
approach on the standard GeoQuery dataset."
Martin Rinard,Rinard_Martin,arXiv:1602.05643,https://arxiv.org/abs/1602.05643,"Abstract:  We present the first systematic analysis of the characteristics of patch
search spaces for automatic patch generation systems. We analyze the search
spaces of two current state-of-the-art systems, SPR and Prophet, with 16
different search space configurations. Our results are derived from an analysis
of 1104 different search spaces and 768 patch generation executions. Together
these experiments consumed over 9000 hours of CPU time on Amazon EC2.
The analysis shows that 1) correct patches are sparse in the search spaces
(typically at most one correct patch per search space per defect), 2) incorrect
patches that nevertheless pass all of the test cases in the validation test
suite are typically orders of magnitude more abundant, and 3) leveraging
information other than the test suite is therefore critical for enabling the
system to successfully isolate correct patches.
We also characterize a key tradeoff in the structure of the search spaces.
Larger and richer search spaces that contain correct patches for more defects
can actually cause systems to find fewer, not more, correct patches. We
identify two reasons for this phenomenon: 1) increased validation times because
of the presence of more candidate patches and 2) more incorrect patches that
pass the test suite and block the discovery of correct patches. These
fundamental properties, which are all characterized for the first time in this
paper, help explain why past systems often fail to generate correct patches and
help identify challenges, opportunities, and productive future directions for
the field."
Martin Rinard,Rinard_Martin,arXiv:1306.6054,https://arxiv.org/abs/1306.6054,"Abstract:  We prove several decidability and undecidability results for the
satisfiability and validity problems for languages that can express solutions
to word equations with length constraints. The atomic formulas over this
language are equality over string terms (word equations), linear inequality
over the length function (length constraints), and membership in regular sets.
These questions are important in logic, program analysis, and formal
verification. Variants of these questions have been studied for many decades by
mathematicians. More recently, practical satisfiability procedures (aka SMT
solvers) for these formulas have become increasingly important in the context
of security analysis for string-manipulating programs such as web applications.
We prove three main theorems. First, we give a new proof of undecidability
for the validity problem for the set of sentences written as a forall-exists
quantifier alternation applied to positive word equations. A corollary of this
undecidability result is that this set is undecidable even with sentences with
at most two occurrences of a string variable. Second, we consider Boolean
combinations of quantifier-free formulas constructed out of word equations and
length constraints. We show that if word equations can be converted to a solved
form, a form relevant in practice, then the satisfiability problem for Boolean
combinations of word equations and length constraints is decidable. Third, we
show that the satisfiability problem for quantifier-free formulas over word
equations in regular solved form, length constraints, and the membership
predicate over regular expressions is also decidable."
Martin Rinard,Rinard_Martin,arXiv:1202.0359,https://arxiv.org/abs/1202.0359,"Abstract:  We propose a novel approach to improving software security called
Cryptographic Path Hardening, which is aimed at hiding security vulnerabilities
in software from attackers through the use of provably secure and obfuscated
cryptographic devices to harden paths in programs.
By ""harden"" we mean that certain error-checking if-conditionals in a given
program P are replaced by equivalent"" we mean that adversaries cannot use
semi-automatic program analysis techniques to reason about the hardened program
paths and thus cannot discover as-yet-unknown errors along those paths, except
perhaps through black-box dictionary attacks or random testing (which we can
never prevent).
Other than these unpreventable attack methods, we can make program analysis
aimed at error-finding ""provably hard"" for a resource-bounded attacker, in the
same sense that cryptographic schemes are hard to break. Unlike
security-through-obscurity, in Cryptographic Path Hardening we use
provably-secure crypto devices to hide errors and our mathematical arguments of
security are the same as the standard ones used in cryptography.
One application of Cryptographic Path Hardening is that software patches or
filters often reveal enough information to an attacker that they can be used to
construct error-revealing inputs to exploit an unpatched version of the
program. By ""hardening"" the patch we make it difficult for the attacker to
analyze the patched program to construct error-revealing inputs, and thus
prevent him from potentially constructing exploits."
Martin Rinard,Rinard_Martin,arXiv:1110.2849,https://arxiv.org/abs/1110.2849,"Abstract:  Administrative role-based access control (ARBAC) is the first comprehensive
administrative model proposed for role-based access control (RBAC). ARBAC has
several features for designing highly expressive policies, but current work has
not highlighted the utility of these expressive policies. In this report, we
present a case study of designing an ARBAC policy for a bank comprising 18
branches. Using this case study we provide an assessment about the features of
ARBAC that are likely to be used in realistic policies."
Martin Rinard,Rinard_Martin,arXiv:cs/0609104,https://arxiv.org/abs/cs/0609104,"Abstract:  One of the main challenges in the verification of software systems is the
analysis of unbounded data structures with dynamic memory allocation, such as
linked data structures and arrays. We describe Bohne, a new analysis for
verifying data structures. Bohne verifies data structure operations and shows
that 1) the operations preserve data structure invariants and 2) the operations
satisfy their specifications expressed in terms of changes to the set of
objects stored in the data structure. During the analysis, Bohne infers loop
invariants in the form of disjunctions of universally quantified Boolean
combinations of formulas. To synthesize loop invariants of this form, Bohne
uses a combination of decision procedures for Monadic Second-Order Logic over
trees, SMT-LIB decision procedures (currently CVC Lite), and an automated
reasoner within the Isabelle interactive theorem prover. This architecture
shows that synthesized loop invariants can serve as a useful communication
mechanism between different decision procedures. Using Bohne, we have verified
operations on data structures such as linked lists with iterators and back
pointers, trees with and without parent pointers, two-level skip lists, array
data structures, and sorted lists. We have deployed Bohne in the Hob and Jahob
data structure analysis systems, enabling us to combine Bohne with analyses of
data structure clients and apply it in the context of larger programs. This
report describes the Bohne algorithm as well as techniques that Bohne uses to
reduce the ammount of annotations and the running time of the analysis."
Martin Rinard,Rinard_Martin,arXiv:cs/0508123,https://arxiv.org/abs/cs/0508123,"Abstract:  Typestate systems ensure many desirable properties of imperative programs,
including initialization of object fields and correct use of stateful library
interfaces. Abstract sets with cardinality constraints naturally generalize
typestate properties: relationships between the typestates of objects can be
expressed as subset and disjointness relations on sets, and elements of sets
can be represented as sets of cardinality one. Motivated by these applications,
this paper presents new algorithms and new complexity results for constraints
on sets and their cardinalities. We study several classes of constraints and
demonstrate a trade-off between their expressive power and their complexity.
Our first result concerns a quantifier-free fragment of Boolean Algebra with
Presburger Arithmetic. We give a nondeterministic polynomial-time algorithm for
reducing the satisfiability of sets with symbolic cardinalities to constraints
on constant cardinalities, and give a polynomial-space algorithm for the
resulting problem.
In a quest for more efficient fragments, we identify several subclasses of
sets with cardinality constraints whose satisfiability is NP-hard. Finally, we
identify a class of constraints that has polynomial-time satisfiability and
entailment problems and can serve as a foundation for efficient program
analysis."
Martin Rinard,Rinard_Martin,arXiv:cs/0410073,https://arxiv.org/abs/cs/0410073,"Abstract:  Spatial conjunction is a powerful construct for reasoning about dynamically
allocated data structures, as well as concurrent, distributed and mobile
computation. While researchers have identified many uses of spatial
conjunction, its precise expressive power compared to traditional logical
constructs was not previously known. In this paper we establish the expressive
power of spatial conjunction. We construct an embedding from first-order logic
with spatial conjunction into second-order logic, and more surprisingly, an
embedding from full second order logic into first-order logic with spatial
conjunction. These embeddings show that the satisfiability of formulas in
first-order logic with spatial conjunction is equivalent to the satisfiability
of formulas in second-order logic. These results explain the great expressive
power of spatial conjunction and can be used to show that adding unrestricted
spatial conjunction to a decidable logic leads to an undecidable logic. As one
example, we show that adding unrestricted spatial conjunction to two-variable
logic leads to undecidability. On the side of decidability, the embedding into
second-order logic immediately implies the decidability of first-order logic
with a form of spatial conjunction over trees. The embedding into spatial
conjunction also has useful consequences: because a restricted form of spatial
conjunction in two-variable logic preserves decidability, we obtain that a
correspondingly restricted form of second-order quantification in two-variable
logic is decidable. The resulting language generalizes the first-order theory
of boolean algebra over sets and is useful in reasoning about the contents of
data structures in object-oriented languages."
Martin Rinard,Rinard_Martin,arXiv:cs/0408019,https://arxiv.org/abs/cs/0408019,"Abstract:  We have previously introduced role logic as a notation for describing
properties of relational structures in shape analysis, databases and knowledge
bases. A natural fragment of role logic corresponds to two-variable logic with
counting and is therefore decidable. We show how to use role logic to describe
open and closed records, as well the dual of records, inverse records. We
observe that the spatial conjunction operation of separation logic naturally
models record concatenation. Moreover, we show how to eliminate the spatial
conjunction of formulas of quantifier depth one in first-order logic with
counting. As a result, allowing spatial conjunction of formulas of quantifier
depth one preserves the decidability of two-variable logic with counting. This
result applies to two-variable role logic fragment as well. The resulting logic
smoothly integrates type system and predicate calculus notation and can be
viewed as a natural generalization of the notation for constraints arising in
role analysis and similar shape analysis approaches."
Martin Rinard,Rinard_Martin,arXiv:cs/0408018,https://arxiv.org/abs/cs/0408018,"Abstract:  We present role logic, a notation for describing properties of relational
structures in shape analysis, databases, and knowledge bases. We construct role
logic using the ideas of de Bruijn's notation for lambda calculus, an encoding
of first-order logic in lambda calculus, and a simple rule for implicit
arguments of unary and binary predicates. The unrestricted version of role
logic has the expressive power of first-order logic with transitive closure.
Using a syntactic restriction on role logic formulas, we identify a natural
fragment RL^2 of role logic. We show that the RL^2 fragment has the same
expressive power as two-variable logic with counting C^2 and is therefore
decidable. We present a translation of an imperative language into the
decidable fragment RL^2, which allows compositional verification of programs
that manipulate relational structures. In addition, we show how RL^2 encodes
boolean shape analysis constraints and an expressive description logic."
Martin Rinard,Rinard_Martin,arXiv:cs/0408015,https://arxiv.org/abs/cs/0408015,"Abstract:  We show that the first-order theory of structural subtyping of non-recursive
types is decidable. Let $\Sigma$ be a language consisting of function symbols
(representing type constructors) and $C$ a decidable structure in the
relational language $L$ containing a binary relation $\leq$. $C$ represents
primitive types; $\leq$ represents a subtype ordering. We introduce the notion
of $\Sigma$-term-power of $C$, which generalizes the structure arising in
structural subtyping. The domain of the $\Sigma$-term-power of $C$ is the set
of $\Sigma$-terms over the set of elements of $C$. We show that the
decidability of the first-order theory of $C$ implies the decidability of the
first-order theory of the $\Sigma$-term-power of $C$. Our decision procedure
makes use of quantifier elimination for term algebras and Feferman-Vaught
theorem. Our result implies the decidability of the first-order theory of
structural subtyping of non-recursive types."
Martin Rinard,Rinard_Martin,arXiv:cs/0408014,https://arxiv.org/abs/cs/0408014,"Abstract:  We introduce regular graph constraints and explore their decidability
properties. The motivation for regular graph constraints is 1) type checking of
changing types of objects in the presence of linked data structures, 2) shape
analysis techniques, and 3) generalization of similar constraints over trees
and grids. We define a subclass of graphs called heaps as an abstraction of the
data structures that a program constructs during its execution. We prove that
determining the validity of implication for regular graph constraints over the
class of heaps is undecidable. We show undecidability by exhibiting a
characterization of certain ""corresponder graphs"" in terms of presence and
absence of homomorphisms to a finite number of fixed graphs. The undecidability
of implication of regular graph constraints implies that there is no algorithm
that will verify that procedure preconditions are met or that the invariants
are maintained when these properties are expressed in any specification
language at least as expressive as regular graph constraints."
Martin Rinard,Rinard_Martin,arXiv:cs/0408013,https://arxiv.org/abs/cs/0408013,"Abstract:  We present a new role system for specifying changing referencing
relationships of heap objects. The role of an object depends, in large part, on
its aliasing relationships with other objects, with the role of each object
changing as its aliasing relationships change. Roles therefore capture
important object and data structure properties and provide useful information
about how the actions of the program interact with these properties. Our role
system enables the programmer to specify the legal aliasing relationships that
define the set of roles that objects may play, the roles of procedure
parameters and object fields, and the role changes that procedures perform
while manipulating objects. We present an interprocedural, compositional, and
context-sensitive role analysis algorithm that verifies that a program respects
the role constraints."
Martin Rinard,Rinard_Martin,arXiv:cs/0407045,https://arxiv.org/abs/cs/0407045,"Abstract:  We show that the decidability of the first-order theory of the language that
combines Boolean algebras of sets of uninterpreted elements with Presburger
arithmetic operations. We thereby disprove a recent conjecture that this theory
is undecidable. Our language allows relating the cardinalities of sets to the
values of integer variables, and can distinguish finite and infinite sets. We
use quantifier elimination to show the decidability and obtain an elementary
upper bound on the complexity.
Precise program analyses can use our decidability result to verify
representation invariants of data structures that use an integer field to
represent the number of stored elements."
Ronald Rivest,Rivest_Ronald,arXiv:1812.06361,https://arxiv.org/abs/1812.06361,"Abstract:  We present a method and software for ballot-polling risk-limiting audits
(RLAs) based on Bernoulli sampling: ballots are included in the sample with
probability $p$, independently. Bernoulli sampling has several advantages: (1)
it does not require a ballot manifest; (2) it can be conducted independently at
different locations, rather than requiring a central authority to select the
sample from the whole population of cast ballots or requiring stratified
sampling; (3) it can start in polling places on election night, before margins
are known. If the reported margins for the 2016 U.S. Presidential election are
correct, a Bernoulli ballot-polling audit with a risk limit of 5% and a
sampling rate of $p_0 = 1\%$ would have had at least a 99% probability of
confirming the outcome in 42 states. (The other states were more likely to have
needed to examine additional ballots.) Logistical and security advantages that
auditing in the polling place affords may outweigh the cost of examining more
ballots than some other methods might require."
Ronald Rivest,Rivest_Ronald,arXiv:1811.08811,https://arxiv.org/abs/1811.08811,"Abstract:  We present an approximate sampling framework and discuss how risk-limiting
audits can compensate for these approximations, while maintaining their
""risk-limiting"" properties. Our framework is general and can compensate for
counting mistakes made during audits.
Moreover, we present and analyze a simple approximate sampling
method,""$k$-cut"", for picking a ballot randomly from a stack, without counting.
Our method involves doing $k$ ""cuts"", each involving moving a random portion of
ballots from the top to the bottom of the stack, and then picking the ballot on
top. Unlike conventional methods of picking a ballot at random, $k$-cut does
not require identification numbers on the ballots or counting many ballots per
draw. We analyze how close the distribution of chosen ballots is to the uniform
distribution, and design different mitigation procedures. We show that $k=6$
cuts is enough for an risk-limiting election audit, based on empirical data,
which would provide a significant increase in efficiency."
Ronald Rivest,Rivest_Ronald,arXiv:1808.10016,https://arxiv.org/abs/1808.10016,"Abstract:  We describe a very simple method for `consistent sampling' that allows for
sampling with replacement. The method extends previous approaches to consistent
sampling, which assign a pseudorandom real number to each element, and sample
those with the smallest associated numbers. When sampling with replacement, our
extension gives the item sampled a new, larger, associated pseudorandom number,
and returns it to the pool of items being sampled."
Ronald Rivest,Rivest_Ronald,arXiv:1801.00528,https://arxiv.org/abs/1801.00528,"Abstract:  Tabulation audits for an election provide statistical evidence that a
reported contest outcome is ""correct"" (meaning that the tabulation of votes was
properly performed), or else the tabulation audit determines the correct
outcome.
Stark proposed risk-limiting tabulation audits for this purpose; such audits
are effective and are beginning to be used in practice.
We expand the study of election audits based on Bayesian methods, first
introduced by Rivest and Shen in 2012. (The risk-limiting audits proposed by
Stark are ""frequentist"" rather than Bayesian in character.)
We first provide a simplified presentation of Bayesian tabulation audits. A
Bayesian tabulation audit begins by drawing a random sample of the votes in
that contest, and tallying those votes. It then considers what effect
statistical variations of this tally have on the contest outcome. If such
variations almost always yield the previously-reported outcome, the audit
terminates, accepting the reported outcome. Otherwise the audit is repeated
with an enlarged sample.
Bayesian audits are attractive because they work with any method for
determining the winner (such as ranked-choice voting).
We then show how Bayesian audits may be extended to handle more complex
situations, such as auditing contests that \emph{span multiple jurisdictions},
or are otherwise ""stratified.""
We highlight the auditing of such multiple-jurisdiction contests where some
of the jurisdictions have an electronic cast vote record (CVR) for each cast
paper vote, while the others do not. Complex situations such as this may arise
naturally when some counties in a state have upgraded to new equipment, while
others have not. Bayesian audits are able to handle such situations in a
straightforward manner.
We also discuss the benefits and relevant considerations for using Bayesian
audits in practice."
Ronald Rivest,Rivest_Ronald,arXiv:1707.08619,https://arxiv.org/abs/1707.08619,"Abstract:  Elections seem simple---aren't they just counting? But they have a unique,
challenging combination of security and privacy requirements. The stakes are
high; the context is adversarial; the electorate needs to be convinced that the
results are correct; and the secrecy of the ballot must be ensured. And they
have practical constraints: time is of the essence, and voting systems need to
be affordable and maintainable, and usable by voters, election officials, and
pollworkers. It is thus not surprising that voting is a rich research area
spanning theory, applied cryptography, practical systems analysis, usable
security, and statistics. Election integrity involves two key concepts:
convincing evidence that outcomes are correct and privacy, which amounts to
convincing assurance that there is no evidence about how any given person
voted. These are obviously in tension. We examine how current systems walk this
tightrope."
Ronald Rivest,Rivest_Ronald,arXiv:1701.08312,https://arxiv.org/abs/1701.08312,"Abstract:  We propose a simple risk-limiting audit for elections, ClipAudit. To
determine whether candidate A (the reported winner) actually beat candidate B
in a plurality election, ClipAudit draws ballots at random, without
replacement, until either all cast ballots have been drawn, or until \[ a - b
\ge \beta \sqrt{a+b}
\] where $a$ is the number of ballots in the sample for the reported winner
A, and $b$ is the number of ballots in the sample for opponent B, and where
$\beta$ is a constant determined a priori as a function of the number $n$ of
ballots cast and the risk-limit $\alpha$. ClipAudit doesn't depend on the
unofficial margin (as does Bravo). We show how to extend ClipAudit to contests
with multiple winners or losers, or to multiple contests."
Ronald Rivest,Rivest_Ronald,arXiv:1612.01041,https://arxiv.org/abs/1612.01041,"Abstract:  In the ""correlated sampling"" problem, two players, say Alice and Bob, are
given two distributions, say $P$ and $Q$ respectively, over the same universe
and access to shared randomness. The two players are required to output two
elements, without any interaction, sampled according to their respective
distributions, while trying to minimize the probability that their outputs
disagree. A well-known protocol due to Holenstein, with close variants (for
similar problems) due to Broder, and to Kleinberg and Tardos, solves this task
with disagreement probability at most $2 \delta/(1+\delta)$, where $\delta$ is
the total variation distance between $P$ and $Q$. This protocol has been used
in several different contexts including sketching algorithms, approximation
algorithms based on rounding linear programming relaxations, the study of
parallel repetition and cryptography.
In this note, we give a surprisingly simple proof that this protocol is in
fact tight. Specifically, for every $\delta \in (0,1)$, we show that any
correlated sampling scheme should have disagreement probability at least
$2\delta/(1+\delta)$. This partially answers a recent question of Rivest.
Our proof is based on studying a new problem we call ""constrained agreement"".
Here, Alice is given a subset $A \subseteq [n]$ and is required to output an
element $i \in A$, Bob is given a subset $B \subseteq [n]$ and is required to
output an element $j \in B$, and the goal is to minimize the probability that
$i \neq j$. We prove tight bounds on this question, which turn out to imply
tight bounds for correlated sampling. Though we settle basic questions about
the two problems, our formulation also leads to several questions that remain
open."
Ronald Rivest,Rivest_Ronald,arXiv:1610.00127,https://arxiv.org/abs/1610.00127,"Abstract:  We explain why the Australian Electoral Commission should perform an audit of
the paper Senate ballots against the published preference data files. We
suggest four different post-election audit methods appropriate for Australian
Senate elections. We have developed prototype code for all of them and tested
it on preference data from the 2016 election."
Ronald Rivest,Rivest_Ronald,arXiv:1603.09526,https://arxiv.org/abs/1603.09526,"Abstract:  This paper presents a new crypto scheme whose title promises it to be so
boring that no-one will bother reading past the abstract. Because of this, the
remainder of the paper is left blank."
Ronald Rivest,Rivest_Ronald,arXiv:1602.08032,https://arxiv.org/abs/1602.08032,"Abstract:  Population protocols are a popular model of distributed computing, in which
randomly-interacting agents with little computational power cooperate to
jointly perform computational tasks. Inspired by developments in molecular
computation, and in particular DNA computing, recent algorithmic work has
focused on the complexity of solving simple yet fundamental tasks in the
population model, such as leader election (which requires stabilization to a
single agent in a special ""leader"" state), and majority (in which agents must
stabilize to a decision as to which of two possible initial states had higher
initial count). Known results point towards an inherent trade-off between the
time complexity of such algorithms, and the space complexity, i.e. size of the
memory available to each agent.
In this paper, we explore this trade-off and provide new upper and lower
bounds for majority and leader election. First, we prove a unified lower bound,
which relates the space available per node with the time complexity achievable
by a protocol: for instance, our result implies that any protocol solving
either of these tasks for $n$ agents using $O( \log \log n )$ states must take
$\Omega( n / \rm{polylog} n )$ expected time. This is the first result to
characterize time complexity for protocols which employ super-constant number
of states per node, and proves that fast, poly-logarithmic running times
require protocols to have relatively large space costs.
On the positive side, we give algorithms showing that fast, poly-logarithmic
stabilization time can be achieved using $O( \log^2 n )$ space per node, in the
case of both tasks. Overall, our results highlight a time complexity separation
between $O(\log \log n)$ and $\Theta( \log^2 n )$ state space size for both
majority and leader election in population protocols, and introduce new
techniques, which should be applicable more broadly."
Ronald Rivest,Rivest_Ronald,arXiv:1509.00127,https://arxiv.org/abs/1509.00127,"Abstract:  This paper presents DiffSum, a simple post-election risk-limiting
ballot-polling audit for two-candidate plurality elections. DiffSum
sequentially draws ballots (without replacement) until the numbers $a$, $b$, of
votes for candidates $A$, $B$ satisfies $a>b$ and $(a-b)^2 > c(a+b)$, where $A$
is the reported winner and $c$ is a suitably chosen constant. Bounds on the
error rate (chance of approving an incorrect election outcome) are obtained via
simulations. The method is compared with the Bravo method of Lindeman, Stark,
and Yates."
Ronald Rivest,Rivest_Ronald,arXiv:1504.03778,https://arxiv.org/abs/1504.03778,"Abstract:  This pamphlet describes end-to-end election verifiability (E2E-V) for a
nontechnical audience: election officials, public policymakers, and anyone else
interested in secure, transparent, evidence-based electronic elections.
This work is part of the Overseas Vote Foundation's End-to-End Verifiable
Internet Voting: Specification and Feasibility Assessment Study (E2E VIV
Project), funded by the Democracy Fund."
Ronald Rivest,Rivest_Ronald,arXiv:1409.5924,https://arxiv.org/abs/1409.5924,"Abstract:  Schools with the highest average student performance are often the smallest
schools; localities with the highest rates of some cancers are frequently small
and the effects observed in clinical trials are likely to be largest for the
smallest numbers of subjects. Informal explanations of this ""small-schools
phenomenon"" point to the fact that the sample means of smaller samples have
higher variances. But this cannot be a complete explanation: If we draw two
samples from a diffuse distribution that is symmetric about some point, then
the chance that the smaller sample has larger mean is 50\%. A particular
consequence of results proved below is that if one draws three or more samples
of different sizes from the same normal distribution, then the sample mean of
the smallest sample is most likely to be highest, the sample mean of the second
smallest sample is second most likely to be highest, and so on; this is true
even though for any pair of samples, each one of the pair is equally likely to
have the larger sample mean.
Our conclusions are relevant to certain stochastic choice models including
the following generalization of Thurstone's Law of Comparative Judgment. There
are $n$ items. Item $i$ is preferred to item $j$ if $Z_i < Z_j$, where $Z$ is a
random $n$-vector of preference scores. Suppose $\mathbb{P}\{Z_i = Z_j\} = 0$
for $i \ne j$, so there are no ties. Item $k$ is the favorite if $Z_k <
\min_{i\ne k} Z_i$. Let $p_i$ denote the chance that item $i$ is the favorite.
We characterize a large class of distributions for $Z$ for which $p_1 > p_2 >
\cdots > p_n$. Our results are most surprising when $\mathbb{P}\{Z_i < Z_j\} =
\mathbb{P}\{Z_i > Z_j\} = \frac{1}{2}$ for $i \ne j$, so neither of any two
items is likely to be preferred over the other in a pairwise comparison."
Ronald Rivest,Rivest_Ronald,arXiv:1203.3602,https://arxiv.org/abs/1203.3602,"Abstract:  We show how to hang a picture by wrapping rope around n nails, making a
polynomial number of twists, such that the picture falls whenever any k out of
the n nails get removed, and the picture remains hanging when fewer than k
nails get removed. This construction makes for some fun mathematical magic
performances. More generally, we characterize the possible Boolean functions
characterizing when the picture falls in terms of which nails get removed as
all monotone Boolean functions. This construction requires an exponential
number of twists in the worst case, but exponential complexity is almost always
necessary for general functions."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1802.08237,https://arxiv.org/abs/1802.08237,"Abstract:  We present $O(\log\log n)$-round algorithms in the Massively Parallel
Computation (MPC) model, with $\tilde{O}(n)$ memory per machine, that compute a
maximal independent set, a $1+\epsilon$ approximation of maximum matching, and
a $2+\epsilon$ approximation of minimum vertex cover, for any $n$-vertex graph
and any constant $\epsilon>0$. These improve the state of the art as follows:
-- Our MIS algorithm leads to a simple $O(\log\log \Delta)$-round MIS
algorithm in the Congested Clique model of distributed computing, which
improves on the $\tilde{O}(\sqrt{\log \Delta})$-round algorithm of Ghaffari
[PODC'17].
-- Our $O(\log\log n)$-round $(1+\epsilon)$-approximate maximum matching
algorithm simplifies or improves on the following prior work: $O(\log^2\log
n)$-round $(1+\epsilon)$-approximation algorithm of Czumaj et al. [STOC'18] and
$O(\log\log n)$-round $(1+\epsilon)$-approximation algorithm of Assadi et al.
[SODA'19].
-- Our $O(\log\log n)$-round $(2+\epsilon)$-approximate minimum vertex cover
algorithm improves on an $O(\log\log n)$-round $O(1)$-approximation of Assadi
et al. [arXiv'17]."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1711.10692,https://arxiv.org/abs/1711.10692,"Abstract:  Consider a computation on a massive random graph: Does one need to generate
the whole random graph up front, prior to performing the computation? Or, is it
possible to provide an oracle to answer queries to the random graph
""on-the-fly"" in a much more efficient manner overall? That is, to provide a
$local\ access\ generator$ which incrementally constructs the random graph
locally, at the queried portions, in a manner consistent with the random graph
model and all previous choices. Local access generators can be useful when
studying the local behavior of specific random graph models. Our goal is to
design local access generators whose required resource overhead for answering
each query is significantly more efficient than generating the whole random
graph.
Our results focus on undirected graphs with independent edge probabilities,
that is, each edge is chosen as an independent Bernoulli random variable. We
provide a general implementation for generators in this model. Then, we use
this construction to obtain the first efficient local implementations for the
Erdös-Rényi $G(n,p)$ model, and the Stochastic Block model.
As in previous local-access implementations for random graphs, we support
$vertex$-$pair$, $next$-$neighbor$ queries, and $all$-$neighbors$ queries. In
addition, we introduce a new $random$-$neighbor$ query. We also give the first
local-access generation procedure for $all$-$neighbors$ queries in the (sparse
and directed) Kleinberg's Small-World model. Note that, in the sparse case, an
$all$-$neighbors$ query can be used to simulate the other types of queries
efficiently. All of our generators require no pre-processing time, and answer
each query using $polylog(n) $ time, random bits, and additional space."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1707.05497,https://arxiv.org/abs/1707.05497,"Abstract:  We investigate the problems of identity and closeness testing over a discrete
population from random samples. Our goal is to develop efficient testers while
guaranteeing Differential Privacy to the individuals of the population. We
describe an approach that yields sample-efficient differentially private
testers for these problems. Our theoretical results show that there exist
private identity and closeness testers that are nearly as sample-efficient as
their non-private counterparts. We perform an experimental evaluation of our
algorithms on synthetic data. Our experiments illustrate that our private
testers achieve small type I and type II errors with sample size sublinear in
the domain size of the underlying distributions."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1604.07038,https://arxiv.org/abs/1604.07038,"Abstract:  Constructing a spanning tree of a graph is one of the most basic tasks in
graph theory. We consider this problem in the setting of local algorithms: one
wants to quickly determine whether a given edge $e$ is in a specific spanning
tree, without computing the whole spanning tree, but rather by inspecting the
local neighborhood of $e$. The challenge is to maintain consistency. That is,
to answer queries about different edges according to the same spanning tree.
Since it is known that this problem cannot be solved without essentially
viewing all the graph, we consider the relaxed version of finding a spanning
subgraph with $(1+\epsilon)n$ edges (where $n$ is the number of vertices and
$\epsilon$ is a given sparsity parameter). It is known that this relaxed
problem requires inspecting $\Omega(\sqrt{n})$ edges in general graphs, which
motivates the study of natural restricted families of graphs. One such family
is the family of graphs with an excluded minor. For this family there is an
algorithm that achieves constant success probability, and inspects
$(d/\epsilon)^{poly(h)\log(1/\epsilon)}$ edges (for each edge it is queried
on), where $d$ is the maximum degree in the graph and $h$ is the size of the
excluded minor. The distances between pairs of vertices in the spanning
subgraph $G'$ are at most a factor of $poly(d, 1/\epsilon, h)$ larger than in
$G$.
In this work, we show that for an input graph that is $H$-minor free for any
$H$ of size $h$, this task can be performed by inspecting only $poly(d,
1/\epsilon, h)$ edges. The distances between pairs of vertices in the spanning
subgraph $G'$ are at most a factor of $\tilde{O}(h\log(d)/\epsilon)$ larger
than in $G$. Furthermore, the error probability of the new algorithm is
significantly improved to $\Theta(1/n)$. This algorithm can also be easily
adapted to yield an efficient algorithm for the distributed setting."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1601.04233,https://arxiv.org/abs/1601.04233,"Abstract:  We study the problem of estimating the value of sums of the form $S_p
\triangleq \sum \binom{x_i}{p}$ when one has the ability to sample $x_i \geq 0$
with probability proportional to its magnitude. When $p=2$, this problem is
equivalent to estimating the selectivity of a self-join query in database
systems when one can sample rows randomly. We also study the special case when
$\{x_i\}$ is the degree sequence of a graph, which corresponds to counting the
number of $p$-stars in a graph when one has the ability to sample edges
randomly.
Our algorithm for a $(1 \pm \varepsilon)$-multiplicative approximation of
$S_p$ has query and time complexities $\O(\frac{m \log \log n}{\epsilon^2
S_p^{1/p}})$. Here, $m=\sum x_i/2$ is the number of edges in the graph, or
equivalently, half the number of records in the database table. Similarly, $n$
is the number of vertices in the graph and the number of unique values in the
database table. We also provide tight lower bounds (up to polylogarithmic
factors) in almost all cases, even when $\{x_i\}$ is a degree sequence and one
is allowed to use the structure of the graph to try to get a better estimate.
We are not aware of any prior lower bounds on the problem of join selectivity
estimation.
For the graph problem, prior work which assumed the ability to sample only
\emph{vertices} uniformly gave algorithms with matching lower bounds [Gonen,
Ron, and Shavitt. \textit{SIAM J. Comput.}, 25 (2011), pp. 1365-1411]. With the
ability to sample edges randomly, we show that one can achieve faster
algorithms for approximating the number of star subgraphs, bypassing the lower
bounds in this prior work. For example, in the regime where $S_p\leq n$, and
$p=2$, our upper bound is $\tilde{O}(n/S_p^{1/2})$, in contrast to their
$\Omega(n/S_p^{1/3})$ lower bound when no random edge queries are available."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1507.03558,https://arxiv.org/abs/1507.03558,"Abstract:  We study the question of testing structured properties (classes) of discrete
distributions. Specifically, given sample access to an arbitrary distribution
$D$ over $[n]$ and a property $\mathcal{P}$, the goal is to distinguish between
$D\in\mathcal{P}$ and $\ell_1(D,\mathcal{P})>\varepsilon$. We develop a general
algorithm for this question, which applies to a large range of
""shape-constrained"" properties, including monotone, log-concave, $t$-modal,
piecewise-polynomial, and Poisson Binomial distributions. Moreover, for all
cases considered, our algorithm has near-optimal sample complexity with regard
to the domain size and is computationally efficient. For most of these classes,
we provide the first non-trivial tester in the literature. In addition, we also
describe a generic method to prove lower bounds for this problem, and use it to
show our upper bounds are nearly tight. Finally, we extend some of our
techniques to tolerant testing, deriving nearly-tight upper and lower bounds
for the corresponding questions."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1504.06544,https://arxiv.org/abs/1504.06544,"Abstract:  In many situations, sample data is obtained from a noisy or imperfect source.
In order to address such corruptions, this paper introduces the concept of a
sampling corrector. Such algorithms use structure that the distribution is
purported to have, in order to allow one to make ""on-the-fly"" corrections to
samples drawn from probability distributions. These algorithms then act as
filters between the noisy data and the end user.
We show connections between sampling correctors, distribution learning
algorithms, and distribution property testing algorithms. We show that these
connections can be utilized to expand the applicability of known distribution
learning and property testing algorithms as well as to achieve improved
algorithms for those tasks.
As a first step, we show how to design sampling correctors using proper
learning algorithms. We then focus on the question of whether algorithms for
sampling correctors can be more efficient in terms of sample complexity than
learning algorithms for the analogous families of distributions. When
correcting monotonicity, we show that this is indeed the case when also granted
query access to the cumulative distribution function. We also obtain sampling
correctors for monotonicity without this stronger type of access, provided that
the distribution be originally very close to monotone (namely, at a distance
$O(1/\log^2 n)$). In addition to that, we consider a restricted error model
that aims at capturing ""missing data"" corruptions. In this model, we show that
distributions that are close to monotone have sampling correctors that are
significantly more efficient than achievable by the learning approach.
We also consider the question of whether an additional source of independent
random bits is required by sampling correctors to implement the correction
process."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1502.04022,https://arxiv.org/abs/1502.04022,"Abstract:  In the model of \emph{local computation algorithms} (LCAs), we aim to compute
the queried part of the output by examining only a small (sublinear) portion of
the input. Many recently developed LCAs on graph problems achieve time and
space complexities with very low dependence on $n$, the number of vertices.
Nonetheless, these complexities are generally at least exponential in $d$, the
upper bound on the degree of the input graph. Instead, we consider the case
where parameter $d$ can be moderately dependent on $n$, and aim for
complexities with subexponential dependence on $d$, while maintaining
polylogarithmic dependence on $n$. We present: a randomized LCA for computing
maximal independent sets whose time and space complexities are quasi-polynomial
in $d$ and polylogarithmic in $n$; for constant $\epsilon > 0$, a randomized
LCA that provides a $(1-\epsilon)$-approximation to maximum matching whose time
and space complexities are polynomial in $d$ and polylogarithmic in $n$."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1502.00413,https://arxiv.org/abs/1502.00413,"Abstract:  Constructing a spanning tree of a graph is one of the most basic tasks in
graph theory. Motivated by several recent studies of local graph algorithms, we
consider the following variant of this problem. Let G be a connected
bounded-degree graph. Given an edge $e$ in $G$ we would like to decide whether
$e$ belongs to a connected subgraph $G'$ consisting of $(1+\epsilon)n$ edges
(for a prespecified constant $\epsilon >0$), where the decision for different
edges should be consistent with the same subgraph $G'$. Can this task be
performed by inspecting only a {\em constant} number of edges in $G$? Our main
results are:
(1) We show that if every $t$-vertex subgraph of $G$ has expansion $1/(\log
t)^{1+o(1)}$ then one can (deterministically) construct a sparse spanning
subgraph $G'$ of $G$ using few inspections. To this end we analyze a ""local""
version of a famous minimum-weight spanning tree algorithm.
(2) We show that the above expansion requirement is sharp even when allowing
randomization. To this end we construct a family of $3$-regular graphs of high
girth, in which every $t$-vertex subgraph has expansion $1/(\log t)^{1-o(1)}$."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1412.5484,https://arxiv.org/abs/1412.5484,"Abstract:  We present simple, self-contained proofs of correctness for algorithms for
linearity testing and program checking of linear functions on finite subsets of
integers represented as n-bit numbers. In addition we explore a generalization
of self-testing to homomorphisms on a multidimensional vector space. We show
that our self-testing algorithm for the univariate case can be directly
generalized to vector space domains. The number of queries made by our
algorithms is independent of domain size."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1412.3040,https://arxiv.org/abs/1412.3040,"Abstract:  Visualizations are frequently used as a means to understand trends and gather
insights from datasets, but often take a long time to generate. In this paper,
we focus on the problem of rapidly generating approximate visualizations while
preserving crucial visual proper- ties of interest to analysts. Our primary
focus will be on sampling algorithms that preserve the visual property of
ordering; our techniques will also apply to some other visual properties. For
instance, our algorithms can be used to generate an approximate visualization
of a bar chart very rapidly, where the comparisons between any two bars are
correct. We formally show that our sampling algorithms are generally applicable
and provably optimal in theory, in that they do not take more samples than
necessary to generate the visualizations with ordering guarantees. They also
work well in practice, correctly ordering output groups while taking orders of
magnitude fewer samples and much less time than conventional sampling schemes."
Ronitt Rubinfeld,Rubinfeld_Ronitt,arXiv:1402.3835,https://arxiv.org/abs/1402.3835,"Abstract:  In this paper, we analyze and study a hybrid model for testing and learning
probability distributions. Here, in addition to samples, the testing algorithm
is provided with one of two different types of oracles to the unknown
distribution $D$ over $[n]$. More precisely, we define both the dual and
cumulative dual access models, in which the algorithm $A$ can both sample from
$D$ and respectively, for any $i\in[n]$,
- query the probability mass $D(i)$ (query access); or
- get the total mass of $\{1,\dots,i\}$, i.e. $\sum_{j=1}^i D(j)$ (cumulative
access)
These two models, by generalizing the previously studied sampling and query
oracle models, allow us to bypass the strong lower bounds established for a
number of problems in these settings, while capturing several interesting
aspects of these problems -- and providing new insight on the limitations of
the models. Finally, we show that while the testing algorithms can be in most
cases strictly more efficient, some tasks remain hard even with this additional
power."
